colour="white")+
ggtitle("A Map of NJ")+
coord_quickmap()+
theme_void()
ggplot()+
geom_polygon(data=njcounties, mapping =aes(x=long, y=lat, group=group),
colour="black", fill="white")+
ggtitle("A Map of NJ")+
coord_quickmap()+
theme_void()
njcounties <- map_data("county", region="New Jersey")
ggplot()+
geom_polygon(data=njcounties, mapping =aes(x=long, y=lat, group=group))+
ggtitle("A Map of NJ")+
coord_quickmap()+
theme_void()
unique(njcounties$subregion)
murphyvote <- data.frame(county = unique(njcounties$subregion),
murphy = c(43.92, 52.52, 53.28, 61.69, 36.69, 43.64,
73.96, 44.63, 73.56, 40.19, 65.09,
55.74, 40.31, 44.06, 31.79, 51.47,
35.01, 51.54, 31.93, 61.56, 34.56))
murphyvote
njcounties$murphyvote <- NA
njcounties$murphyvote[njcounties$region == "middlesex"] <- 55.74
njcounties$murphyvote[njcounties$region == "mercer"] <- 65.09
njcounties$murphyvote[njcounties$region == "monmouth"] <- 40.31
ggplot()+
geom_polygon(data=njcounties, mapping =aes(x=long, y=lat,
group=group,
fill=murphyvote))+
ggtitle("A Map of NJ")+
coord_quickmap()+
theme_void()
njcounties <- map_data("county", region="New Jersey")
ggplot()+
geom_polygon(data=njcounties, mapping =aes(x=long, y=lat, group=group))+
ggtitle("A Map of NJ")+
coord_quickmap()+
theme_void()
## manual entry of continous data on Murphy's Vote
njcounties$murphyvote <- NA
njcounties$murphyvote[njcounties$region == "middlesex"] <- 55.74
njcounties$murphyvote[njcounties$region == "mercer"] <- 65.09
njcounties$murphyvote[njcounties$region == "monmouth"] <- 40.31
ggplot()+
geom_polygon(data=njcounties, mapping =aes(x=long, y=lat,
group=group,
fill=murphyvote))+
ggtitle("A Map of NJ")+
coord_quickmap()+
theme_void()
njcounties$murphy <- NA
njcounties$murphy[njcounties$region == "middlesex"] <- 55.74
njcounties$murphy[njcounties$region == "mercer"] <- 65.09
njcounties$murphy[njcounties$region == "monmouth"] <- 40.31
ggplot()+
geom_polygon(data=njcounties, mapping =aes(x=long, y=lat,
group=group,
fill=murphyvote))+
## Shade the map according to the vote share
scale_fill_gradient(name="Murphy's Vote Share %", low="red", high="blue")+
ggtitle("A Map of NJ")+
coord_quickmap()+
theme_void()
njcounties <- map_data("county", region="New Jersey")
ggplot()+
geom_polygon(data=njcounties, mapping =aes(x=long, y=lat, group=group))+
ggtitle("A Map of NJ")+
coord_quickmap()+
theme_void()
## manual entry of continous data on Murphy's Vote
njcounties$murphy <- NA
njcounties$murphy[njcounties$region == "middlesex"] <- 55.74
njcounties$murphy[njcounties$region == "mercer"] <- 65.09
njcounties$murphy[njcounties$region == "monmouth"] <- 40.31
ggplot()+
geom_polygon(data=njcounties, mapping =aes(x=long, y=lat,
group=group,
fill=murphyvote))+
## Shade the map according to the vote share
scale_fill_gradient(name="Murphy's Vote Share %", low="red", high="blue")+
ggtitle("A Map of NJ")+
coord_quickmap()+
theme_void()
ggplot()+
geom_polygon(data=njcounties, mapping =aes(x=long, y=lat,
group=group,
fill=murphy))+
## Shade the map according to the vote share
scale_fill_gradient(name="Murphy's Vote Share %", low="red", high="blue")+
ggtitle("A Map of NJ")+
coord_quickmap()+
theme_void()
njcounties$murphy <- NA
njcounties$murphy[njcounties$subregion == "middlesex"] <- 55.74
njcounties$murphy[njcounties$subregion == "mercer"] <- 65.09
njcounties$murphy[njcounties$subregion == "monmouth"] <- 40.31
ggplot()+
geom_polygon(data=njcounties, mapping =aes(x=long, y=lat,
group=group,
fill=murphy))+
## Shade the map according to the vote share
scale_fill_gradient(name="Murphy's Vote Share %", low="red", high="blue")+
ggtitle("A Map of NJ")+
coord_quickmap()+
theme_void()
## Add data all at once: 2021 county election results
murphyvote <- data.frame(county = unique(njcounties$subregion),
murphy = c(43.92, 52.52, 53.28, 61.69, 36.69, 43.64,
73.96, 44.63, 73.56, 40.19, 65.09,
55.74, 40.31, 44.06, 31.79, 51.47,
35.01, 51.54, 31.93, 61.56, 34.56))
njcounties <- merge(njcounties, murphyvote,
by.x="subregion", by.y = "county",
all.x=TRUE, all.y=F)
ggplot()+
geom_polygon(data=njcounties, mapping =aes(x=long, y=lat,
group=group,
fill=murphy))+
## Shade the map according to the vote share
scale_fill_gradient(name="Murphy's Vote Share %", low="red", high="blue")+
ggtitle("A Map of NJ")+
coord_quickmap()+
theme_void()
murphyvote <- data.frame(county = unique(njcounties$subregion),
murphy = c(43.92, 52.52, 53.28, 61.69, 36.69, 43.64,
73.96, 44.63, 73.56, 40.19, 65.09,
55.74, 40.31, 44.06, 31.79, 51.47,
35.01, 51.54, 31.93, 61.56, 34.56))
njcounties <- merge(njcounties, murphyvote,
by.x="subregion", by.y = "county",
all.x=TRUE, all.y=F)
ggplot()+
geom_polygon(data=njcounties, mapping =aes(x=long, y=lat,
group=group,
fill=murphy))+
## Shade the map according to the vote share
scale_fill_gradient(name="Murphy's Vote Share %", low="red", high="blue")+
ggtitle("A Map of NJ")+
coord_quickmap()+
theme_void()
ggplot()+
geom_polygon(data=njcounties, mapping =aes(x=long, y=lat,
group=group,
fill=murphy),
colour="white")+
## Shade the map according to the vote share
scale_fill_gradient(name="Murphy's Vote Share %", low="red", high="blue")+
ggtitle("A Map of NJ")+
coord_quickmap()+
theme_void()
njcounties <- map_data("county", region="New Jersey")
ggplot()+
geom_polygon(data=njcounties, mapping =aes(x=long, y=lat, group=group),
colour="white")+
ggtitle("A Map of NJ")+
coord_quickmap()+
theme_void()
ggplot()+
geom_polygon(data=njcounties, mapping =aes(x=long, y=lat,
group=group,
fill=murphy),
colour="white")+
## Shade the map according to the vote share
scale_fill_gradient(name="Murphy's Vote Share %", low="red", high="blue")+
ggtitle("A Map of NJ")+
coord_quickmap()+
theme_void()
njcounties <- map_data("county", region="New Jersey")
ggplot()+
geom_polygon(data=njcounties, mapping =aes(x=long, y=lat, group=group),
colour="white")+
ggtitle("A Map of NJ")+
coord_quickmap()+
theme_void()
## manual entry of continous data on Murphy's Vote
njcounties$murphy <- NA
njcounties$murphy[njcounties$subregion == "middlesex"] <- 55.74
njcounties$murphy[njcounties$subregion == "mercer"] <- 65.09
njcounties$murphy[njcounties$subregion == "monmouth"] <- 40.31
ggplot()+
geom_polygon(data=njcounties, mapping =aes(x=long, y=lat,
group=group,
fill=murphy),
colour="white")+
## Shade the map according to the vote share
scale_fill_gradient(name="Murphy's Vote Share %", low="red", high="blue")+
ggtitle("A Map of NJ")+
coord_quickmap()+
theme_void()
## Add data all at once: 2021 county election results
murphyvote <- data.frame(county = unique(njcounties$subregion),
murphy = c(43.92, 52.52, 53.28, 61.69, 36.69, 43.64,
73.96, 44.63, 73.56, 40.19, 65.09,
55.74, 40.31, 44.06, 31.79, 51.47,
35.01, 51.54, 31.93, 61.56, 34.56))
njcounties <- merge(njcounties, murphyvote,
by.x="subregion", by.y = "county",
all.x=TRUE, all.y=F)
ggplot()+
geom_polygon(data=njcounties, mapping =aes(x=long, y=lat,
group=group,
fill=murphy),
colour="white")+
## Shade the map according to the vote share
scale_fill_gradient(name="Murphy's Vote Share %", low="red", high="blue")+
ggtitle("A Map of NJ")+
coord_quickmap()+
theme_void()
library(maps)
library(ggplot2)
library(maps)
library(ggplot2)
njcounties <- map_data("county", region="New Jersey")
ggplot()+
geom_polygon(data=njcounties, mapping =aes(x=long, y=lat, group=group),
colour="white")+
ggtitle("A Map of NJ")+
coord_quickmap()+
theme_void()
## add column to map data with Murphy's vote share in three counties
njcounties$murphy <- NA
njcounties$murphy[njcounties$subregion == "middlesex"] <- 55.74
njcounties$murphy[njcounties$subregion == "mercer"] <- 65.09
njcounties$murphy[njcounties$subregion == "monmouth"] <- 40.31
ggplot()+
geom_polygon(data=njcounties, mapping =aes(x=long, y=lat,
group=group,
# note fill=murphy
fill=murphy),
colour="white")+
## Shade the map according to the vote share
scale_fill_gradient(name="Murphy's Vote Share %", low="red", high="blue")+
ggtitle("Murphy's Vote Share in 2021")+
coord_quickmap()+
theme_void()
murphyvote <- data.frame(county = unique(njcounties$subregion),
murphy = c(43.92, 52.52, 53.28, 61.69, 36.69, 43.64,
73.96, 44.63, 73.56, 40.19, 65.09,
55.74, 40.31, 44.06, 31.79, 51.47,
35.01, 51.54, 31.93, 61.56, 34.56))
## open package
library(igraph)
## load data
twitter <- read.csv("https://raw.githubusercontent.com/ktmccabe/teachingdata/main/twitter-following.csv")
head(twitter)
senator <- read.csv("https://raw.githubusercontent.com/ktmccabe/teachingdata/main/twitter-senator.csv")
head(senator)
## Create adjacency matrix as a directed matrix
twitteradj <- graph_from_data_frame(d=twitter, vertices=senator, directed=T)
## view a part of it
twitteradj[1:6, 1:6]
## view a part of it
twitteradj[1:10, 1:10]
head(senator)
twitter <- read.csv("https://raw.githubusercontent.com/ktmccabe/teachingdata/main/twitter-following.csv")
head(twitter)
twitteradj
View(twitteradj)
col <- NA
col[senator$party == "R"] <- rgb(1,0,0, alpha=.5) # a new way to specificy color and transparency
col[senator$party == "D"] <- rgb(0,0,1, alpha=.5)
col[senator$party == "I"] <- "black"
plot(twitteradj,
vertex.color = col,
vertex.label = NA,
edge.arrow.size = 0.1,
edge.width = 0.5)
plot(twitteradj,
vertex.color = col,
vertex.label = NA,
edge.arrow.size = 0.1,
edge.width = 0.5)
plot(twitteradj,
vertex.color = col,
vertex.label = NA,
edge.arrow.size = 0.1,
edge.width = 0.5)
plot(twitteradj,
vertex.color = col,
vertex.label = NA,
edge.arrow.size = 0.1,
edge.width = 0.5)
plot(twitteradj,
vertex.color = "black",
vertex.label = NA,
edge.arrow.size = 0.1,
edge.width = 0.5)
plot(twitteradj,
vertex.color = "black",
vertex.label = senator$screen_name,
edge.arrow.size = 0.1,
edge.width = 0.5)
plot(twitteradj,
vertex.color = "black",
vertex.label = senator$screen_name,
edge.arrow.size = 0.1,
edge.width = 0.5)
plot(twitteradj,
vertex.color = "black",
vertex.label = senator$screen_name,
edge.arrow.size = 0.1,
edge.width = 0.5)
twitteradj
## Create adjacency matrix as a directed matrix
twitteradj <- graph_from_data_frame(d=twitter, vertices=senator, directed=F)
## view a part of it
twitteradj
colnames(twitteradj)
twitteradj
View(twitteradj)
graph_from_adjacency_matrix(
twitteradj,
mode = c("directed"),
add.colnames = senator$screen_name,
)
graph_from_adjacency_matrix(
twitteradj,
mode = c("directed"),
)
plot(twitteradj,
vertex.color = "black",
vertex.label = senator$screen_name,
edge.arrow.size = 0.1,
edge.width = 0.5)
## get france data (not available for all countries)
france <- map_data("france")
## Plot France
ggplot()+
geom_polygon(data=france, aes(x=long, y=lat, group=group),
fill="white", colour="gray")+
ggtitle("Terrorist Attacks in France 2000-2019")+
coord_quickmap()+
theme_void()
## Load outside terrorism data
load(url("https://github.com/ktmccabe/teachingdata/raw/main/gtb.RData"))
## Let's look at only recent attacks
gtbfrance <- subset(gtb, iyear > 2000 & country_txt=="France")
## Add points to plot sized according to number of fatalities
ggplot()+
geom_polygon(data=france, aes(x=long, y=lat, group=group),
fill="white", colour="gray")+
## add points with size in proportion to fatalities
## note we have a new data=gtbfrance argument
geom_point(data=gtbfrance, aes(x=longitude, y=latitude, size=nkill),
alpha=.4, colour="red")+ # alpha makes points transparent
## range specifies how big or small you want points to be
scale_size(name="Number of Fatalities", range=c(2, 10))+
ggtitle("Terrorist Attacks in France 2000-2019")+
coord_quickmap()+
theme_void()
library(gganimate)
ggplot()+
geom_polygon(data=france, aes(x=long, y=lat, group=group),
fill="white", colour="gray")+
## add points with size in proportion to fatalities
geom_point(data=gtbfrance, aes(x=longitude, y=latitude, size=nkill),
alpha=.4, colour="red")+
scale_size(name="Number of Fatalities", range=c(2, 10))+
ggtitle("Terrorist Attacks in France 2000-2019")+
coord_quickmap()+
theme_void()
setwd("~/Dropbox/GitHub2/mle23")
library(rio)
don <- import("https://github.com/ktmccabe/teachingdata/blob/main/donation.dta?raw=true")
newdf <-  data.frame(NetWorth = c(1, 3,3),
Edsum = c(2, 5, 6))
don$donation <- as.factor(don$donation)
## build a model (choose "features" you think will be good for prediction)
## you will want to remove missing data first
donsub <- don[, c("donation", "NetWorth", "Edsum")]
donsub <- na.omit(donsub)
fit <- glm(donation ~ NetWorth + Edsum,
family = binomial(link = "logit"), data = donsub)
## generate probability of donation for each observation
donsub$prob <- predict(fit, type = "response")
## set a prediction threshold
donsub$pred <- ifelse(donsub$prob > mean(donsub$donation), 1, 0)
## accuracy- proportion where prediction matches reality
mean(donsub$pred == donsub$donation)
## confusion matrix
table(truth = donsub$donation, predicted = donsub$pred)
donsub <- don[, c("donation", "NetWorth", "Edsum")]
donsub <- na.omit(donsub)
fit <- glm(donation ~ NetWorth + Edsum,
family = binomial(link = "logit"), data = donsub)
summary(fit)
## generate probability of donation for each observation
donsub$prob <- predict(fit, type = "response")
donsub$prob
as.numeric(donsub$donation)
table(donsub$donation)
prop.table(table(donsub$donation))
## set a prediction threshold
donsub$pred <- ifelse(donsub$prob > 0.035, 1, 0)
donsub$pred == donsub$donation
## accuracy- proportion where prediction matches reality
mean(donsub$pred == donsub$donation)
fit <- lm(total_donation ~ NetWorth + Edsum, data = don)
## Root mean squared error
rmse <- sqrt(sum(residuals(fit)^2)/fit$df.residual)
rmse
library(tidyverse)
library(tidymodels)
library(textrecipes)
library(themis)
library(yardstick)
library(glmnet)
library(ranger)
## Split into train vs. test
set.seed(123)
df_split <- donsub %>% initial_split(prop = 0.8)
df_train <- training(df_split)
df_test <- testing(df_split)
## recipe for pre-processing variables and text
df_rec <-   recipe(donation ~ NetWorth + Edsum, data = df_train) %>%
## Downsample based on outcome
themis::step_downsample(donation)
## Specify model: Three examples
## A penalized logistic regression model with penalty specified
logit_mod <- logistic_reg(penalty = 0.01, mixture = 1) %>%
set_mode("classification") %>%
set_engine("glmnet")
## A penalized logistic model with penalty tuning
logit_tune <- logistic_reg(penalty=tune(), mixture = 1) %>%
set_mode("classification") %>%
set_engine("glmnet")
lambda_grid <- grid_regular(penalty(), levels = 30)
## Random Forest
rf_spec <-
rand_forest() %>%
set_engine("ranger") %>%
set_mode("classification")
## Specify model: Three examples
## A penalized logistic regression model
logit_mod <- logistic_reg() %>%
set_mode("classification") %>%
set_engine("glm")
## A penalized logistic model with penalty tuning
logit_tune <- logistic_reg(penalty=tune(), mixture = 1) %>%
set_mode("classification") %>%
set_engine("glmnet")
lambda_grid <- grid_regular(penalty(), levels = 30)
## Random Forest
rf_spec <-
rand_forest() %>%
set_engine("ranger") %>%
set_mode("classification")
## workflow penalized logit
df_wf <- workflow() %>%
add_recipe(df_rec) %>%
add_model(logit_mod)
## workflow for the tuning specification
df_wft <- workflow() %>%
add_recipe(df_rec) %>%
add_model(logit_tune)
## workflow for random forest
df_wfrf <- workflow() %>%
add_recipe(df_rec) %>%
add_model(rf_spec)
## cross-validation
set.seed(123)
df_folds <- vfold_cv(df_train, v=5)
df_basic <- df_wf %>%
fit(data = df_train)
df_cv <- fit_resamples(
df_wf,
df_folds,
control = control_resamples(save_pred = TRUE),
metrics = yardstick::metric_set(accuracy, precision, recall)
)
df_rs <- tune_grid(
df_wft,
df_folds,
grid=lambda_grid, # note the use of the grid here
control = control_resamples(save_pred = TRUE),
metrics = yardstick::metric_set(accuracy, precision, recall)
)
df_rf <- fit_resamples(
df_wfrf,
df_folds,
control = control_resamples(save_pred = TRUE),
metrics = yardstick::metric_set(accuracy, precision, recall)
)
results <- df_train  %>% select(donation) %>%
bind_cols(df_basic %>%
predict(new_data = df_train))
results
results %>%
conf_mat(truth = donation, estimate = .pred_class)
eval_metrics <- metric_set(accuracy, recall, precision)
eval_metrics(data = results,
truth = donation,
estimate = .pred_class)
resultscv <- collect_metrics(df_cv)
resultscv
resultsrf <- collect_metrics(df_rf)
resultsrf
resultstuning <- collect_metrics(df_rs)
chosen_acc <- df_rs %>% select_best("accuracy")
chosen_acc
## Saving the random forest model
final_wfrf <- fit(df_wfrf, df_train)
final_wft <- finalize_workflow(df_wft, chosen_acc)  %>%
fit(data=df_train)
df_cv <- fit_resamples(
df_wf,
df_folds,
control = control_resamples(save_pred = TRUE),
metrics = yardstick::metric_set(accuracy, precision, recall)
)
