[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MLE 2023",
    "section": "",
    "text": "This document will include important links and course notes for fall 2023 Maximum Likelihood Estimation in Political Science.\n\nThis webpage will be updated throughout the semester with new content.\nSprinkled throughout the website are links to additional resources that might provide more in-depth explanations of a given topic. In particular, several of the sections have benefited from previous materials developed by Kosuke Imai and the book Quantitative Social Science, Chris Bail and SICSS, Marc Ratkovic, In Song Kim, Will Lowe, and others.\nThis is a living web resource. If you spot errors or have questions or suggestions, please email me at k.mccabe@rutgers.edu."
  },
  {
    "objectID": "02-ROverview.html#first-time-with-r-and-rstudio",
    "href": "02-ROverview.html#first-time-with-r-and-rstudio",
    "title": "2  R Overview",
    "section": "2.1 First Time with R and RStudio",
    "text": "2.1 First Time with R and RStudio\nThis next section provides a few notes on using R and RStudio now that you have installed it. This is mostly repetitive of the other resources. This includes only the bare essential information for opening an R script and digging into using R as a calculator. In this section, we cover the following materials:\n\nUsing R as a calculator and assigning objects using &lt;-\nSetting your working directory and the setwd() function.\nCreating and saving an R script\n\n\n2.1.1 Open RStudio\n\nNote: The first time you open RStudio, you likely only have the three windows above. We will want to create a fourth window by opening an R script to create the fourth window.\n\nTo do this, in RStudio, click on File -&gt; New -&gt; R script in your computer’s toolbar. This will open a blank document for text editing in the upper left of the RStudio window. We will return to this window in a moment.\n\nYou can alternatively click on the green + sign indicator in the top-left corner of the RStudio window, which should give you the option to create a new R script document.\n\n\nNow you should have something that looks like this, similar to Figure 1.1. in QSS:\n\n\nThe upper-left window has our .R script document that will contain code.\nThe lower-left window is the console. This will show the output of the code we run. We will also be able to type directly in the console.\nThe upper-right window shows the environment (and other tabs, such as the history of commands). When we load and store data in RStudio, we will see a summary of that in the environment.\nThe lower-right window will enable us to view plots and search help files, among other things.\n\n\n\n2.1.2 Using R as a Calculator\nThe bottom left window in your RStudio is the Console. You can type in this window to use R as a calculator or to try out commands. It will show the raw output of any commands you type. For example, we can try to use R as a calculator. Type the following in the Console (the bottom left window) and hit “enter” or “return” on your keyboard:\n\n5 + 3\n\n[1] 8\n\n5 - 3\n\n[1] 2\n\n5^2\n\n[1] 25\n\n5 * 3\n\n[1] 15\n\n5/3\n\n[1] 1.666667\n\n(5 + 3) * 2\n\n[1] 16\n\n\nIn the other RStudio windows, the upper right will show a history of commands that you have sent from the text editor to the R console, along with other items. The lower right will show graphs, help documents and other features. These will be useful later in the course.\n\n\n2.1.3 Working in an R Script\nEarlier, I asked you to open an R script in the upper left window by doing File, then New File, then R Script. Let’s go back to working in that window.\nSet your working directory setwd()\n(Almost) every time you work in RStudio, the first thing you will do is set your working directory. This is a designated folder in your computer where you will save your R scripts and datasets.\nThere are many ways to do this.\n\nAn easy way is to go to Session \\(\\rightarrow\\) Set Working Directory \\(\\rightarrow\\) Choose Directory. I suggest choosing a folder in your computer that you can easily find and that you will routinely use for this class. Go ahead and create/select it.\nNote: when you selected your directory, code came out in the bottom left Console window. This is the setwd() command which can also be used directly to set your working directory in the future.\nIf you aren’t sure where your directory has been set, you can also type getwd() in your Console. Try it now\n\n\n## Example of where my directory was\ngetwd()\n\nIf I want to change the working directory, I can go to the top toolbar of my computer and use Session \\(\\rightarrow\\) Set Working Directory \\(\\rightarrow\\) Choose Directory or just type my file pathway using the setwd() below:\n\n## Example of setting the working directory using setwd().\n## Your computer will have your own file path.\nsetwd(\"/Users/ktmccabe/Dropbox/Rutgers Teaching/\")\n\nSaving the R Script\nLet’s now save our R script to our working directory and give it an informative name. To do so, go to File, then Save As, make sure you are in the same folder on your computer as the folder you chose for your working directory.\nGive the file an informative name, such as: “McCabeWeek1.R”. Note: all of your R scripts will have the .R extension.\n\n\n2.1.4 Preparing your R script\nNow that we have saved our R script, let’s work inside of it. Remember, we are in the top-left RStudio window now.\n\nJust like the beginning of a paper, you will want to title your R script. In R, any line that you start with a # will not be treated as a programming command. You can use this to your advantage to write titles/comments. Below is a screenshot example of a template R script.\nYou can specify your working directory at the top, too. Add your own filepath inside setwd()\n\n\n\nThen you can start answering problems in the rest of the script.\nThink of the R script as where you write the final draft of your paper. In the Console (the bottom-left window), you can mess around and try different things, like you might when you are taking notes or outlining an essay. Then, write the final programming steps that lead you to your answer in the R script. For example, if I wanted to add 5 + 3, I might try different ways of typing it in the Console, and then when I found out 5 + 3 is the right approach, I would type that into my script.\n\n\n\n2.1.5 Running Commands in your R script\nThe last thing we will note in this initial handout is how to execute commands in your R script.\nTo run / execute a command in your R script (the upper left window), you can\n\nHighlight the code you want to run, and then hold down “command + return” on a Mac or “control + enter” on Windows\nPlace your cursor at the end of the line of code (far right), and hit “command + return” on a Mac or “control + return” on Windows, or\nDo 1 or 2, but instead of using the keyboard to execute the commands, click “Run” in the top right corner of the upper-left window.\n\nTry it: Type 5 + 3 in the R script. Then, try to execute 5 + 3. It should look something like this:\n\nAfter you executed the code, you should see it pop out in your Console:\n\n5 + 3\n\n[1] 8\n\n\n\nNote: The symbol # also allows for annotation behind commands or on a separate line. Everything that follows # will be ignored by R. You can annotate your own code so that you and others can understand what each part of the code is designed to do.\n\n## Example\nsum53 &lt;- 5 + 3 # example of assigning an addition calculation\n\n\n\n2.1.6 Objects\nSometimes we will want to store our calculations as “objects” in R. We use &lt;- to assign objects by placing it to the left of what we want to store. For example, let’s store the calculation 5 + 3 as an object named sum53:\n\nsum53 &lt;- 5 + 3\n\nAfter we execute this code, sum53 now stores the calculation. This means, that if we execute a line of code that just hassum53`, it will output 8. Try it:\n\nsum53\n\n[1] 8\n\n\nNow we no longer have to type 5 + 3, we can just type sum53. For example, let’s say we wanted to subtract 2 from this calculation. We could do:\n\nsum53 - 2\n\n[1] 6\n\n\nLet’s say we wanted to divide two stored calculations:\n\nten &lt;- 5 + 5\ntwo &lt;- 1 + 1\nten / two\n\n[1] 5\n\n\nThe information stored does not have to be numeric. For example, it can be a word, or what we would call a character string, in which case you need to use quotation marks.\n\nmccabe &lt;- \"professor for this course\"\nmccabe\n\n[1] \"professor for this course\"\n\n\nNote: Object names cannot begin with numbers and no spacing is allowed. Avoid using special characters such as % and $, which have specific meanings in R. Finally, use concise and intuitive object names.}\n\nGOOD CODE: practice.calc &lt;- 5 + 3\nBAD CODE: meaningless.and.unnecessarily.long.name &lt;- 5 + 3\n\nWhile these are simple examples, we will use objects all the time for more complicated things to store (e.g., like full datasets!) throughout the course.\nWe can also store an array or “vector” of information using c()\n\nsomenumbers &lt;- c(3, 6, 8, 9)\nsomenumbers\n\n[1] 3 6 8 9\n\n\nImportance of Clean Code\nIdeally, when you are done with your R script, you should be able to highlight the entire script and execute it without generating any error messages. This means your code is clean. Code with typos in it may generate a red error message in the Console upon execution. This can happen when there are typos or commands are misused.\nFor example, R is case sensitive. Let’s say we assigned our object like before:\n\nsum53 &lt;- 5 + 3\n\nHowever, when we went to execute sum53, we accidentally typed Sum53:\n\nSum53\n\nError in eval(expr, envir, enclos): object 'Sum53' not found\n\n\nOnly certain types of objects can be used in mathematical calculations. Let’s say we tried to divide mccabe by 2:\n\nmccabe / 2\n\nError in mccabe/2: non-numeric argument to binary operator\n\n\nA big part of learning to use R will be learning how to troubleshoot and detect typos in your code that generate error messages.\n\n\n\n2.1.7 Practice\nBelow is an exercise that will demonstrate you are able to use R as a calculator and create R scripts.\n\nCreate an R script saved as ``LastnameSetup1.R” (use your last name). Within the R script, follow the example from this handout and title the script.\nSet your working directory, and include the file pathway (within setwd()) at the top of your .R script.\nDo the calculation 4 + 3 - 2 in R. Store it as an object with an informative name.\nDo the calculation 5 \\(\\times\\) 4 in R. Store it as an object with an informative name.\nAdd these two calculations together. In R, try to do this by adding together the objects you created, not the underlying raw calculations."
  },
  {
    "objectID": "02-ROverview.html#data-wrangling",
    "href": "02-ROverview.html#data-wrangling",
    "title": "2  R Overview",
    "section": "2.2 Data Wrangling",
    "text": "2.2 Data Wrangling\nSo you have some data…. AND it’s a mess!!!\nA lot of the data we may encounter in courses has been simplified to allow students to focus on other concepts. We may have data that look like the following:\nnicedata &lt;- data.frame(gender = c(\"Male\", \"Female\", \"Female\", \"Male\"),\n           age = c(16, 20, 66, 44),\n           voterturnout = c(1, 0, 1, 0))\n\n\n  gender age voterturnout\n1   Male  16            1\n2 Female  20            0\n3 Female  66            1\n4   Male  44            0\n\n\nIn the real world, our data may hit us like a ton of bricks, like the below:\nuglydata &lt;- data.frame(VV160002 = c(2, NA, 1, 2),\n           VV1400068 = c(16, 20, 66, 44),\n           VV20000 = c(1, NA, 1, NA))\n\n\n  VV160002 VV1400068 VV20000\n1        2        16       1\n2       NA        20      NA\n3        1        66       1\n4        2        44      NA\n\n\nA lot of common datasets we use in the social sciences are messy, uninformative, sprawling, misshaped, and/or incomplete. What do I mean by this?\n\nThe data might have a lot of missing values. For example, we may have NA values in R, or perhaps a research firm has used some other notation for missing data, such as a 99.\nThe variable names may be uninformative.\n\nFor example, there may be no way to know by looking at the data, which variable represents gender. We have to look at a codebook.\n\nEven if we can tell what a variable is, its categories may not be coded in a way that aligns with how we want to use the data for our research question.\n\nFor example, perhaps you are interested in the effect of a policy on people below vs. 65 and over in age. Well, your age variables might just be a numeric variable. You will have to create a new variable that aligns with your theoretical interest.\n\nDatasets are often sprawling. Some datasets may have more than 1000 variables. It is hard to sort through all of them. Likewise, datasets may have millions of observations. We cannot practically look through all the values of a column to know what is there.\nSometimes we have data shaped into separate columns when we’d rather it be reshaped into different rows.\nMaybe you have encountered a beautiful dataset that provides many measures of your independent variables of interest, but there’s one catch– it has no variable related to your outcome! You have to merge data from multiple sources to answer your research question.\n\nBelow are a few tips and resources. Ultimately, research is a constant debugging process. Loving R means seeing red error messages. The nice thing about R is that a lot of researchers constantly post coding tips and questions online. Google ends up being your friend, but it’s entirely normal to have to devote several hours (days?) to cleaning data.\n\n\n2.2.1 Dealing with Uninformative Variable Names\nHopefully, there is an easy fix for dealing with uninformative variable names. I say “hopefully” because hopefully when you encounter a dataset with uninformative variable names, the place where you downloaded the data will also include a codebook telling you what each variable name means, and how the corresponding values are coded.\nUnfortunately, this may not always be the case. One thing you can do as a researcher is when you create a dataset for your own work, keep a record (in written form, on a word document or in a pdf or code file) of what each variable means (e.g., the survey question it corresponds to or the exact economic measure), as well as how the values of the variables are coded. This good practice will help you in the short-term, as you pause and come back to working on a project over the course of a year, as well as benefit other researchers in the long run after you publish your research.\nFor examples of large codebooks, you can view the 2016 American National Election Study Survey and click on a codebook.\nI recommend that once you locate the definition of a variable of interest, rename the variable in your dataset to be informative. You can do this by creating a new variable or overwriting the name of the existing variable. You might also comment a note for yourself of what the values mean.\n\n## Option 1: create new variable\n## gender 2=Male, 1=Female\nuglydata$gender &lt;- uglydata$VV160002\n\n## Option 2: Overwrite\nnames(uglydata)[1] &lt;- \"gender2\"\n\n\n\n2.2.2 Dealing with Missing Data\nWhen we have a column with missing data, it is best to do a few things:\n\nTry to quantify how much missing data there is and poke at the reason why data are missing.\n\nIs it minor non-response data?\nOr is it indicative of a more systematic issue? For example, maybe data from a whole group of people or countries is missing for certain variables.\n\nIf the data are missing at a very minor rate and/or there is a logical explanation for the missing data that should not affect your research question, you may choose to “ignore” the missing data when performing common analyses, such as taking the mean or running a regression.\n\nIf missing data are a bigger problem, you may consider alternative solutions, such as “imputing” missing data or similarly using some type of auxilliary information to help fill in the missing values.\n\n\nIf we want to figure out how much missing data we have in a variable, we have a couple of approaches:\n\n## Summarize this variable\nsummary(uglydata$gender)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  1.000   1.500   2.000   1.667   2.000   2.000       1 \n\n## What is the length of the subset of the variable where the data are missing\nlength(uglydata$gender[is.na(uglydata$gender) == T])\n\n[1] 1\n\n\nIf we choose to ignore missing data, this can often be easily accomplished in common operations. For example, when taking the mean we just add an argument na.rm = T:\n\nmean(uglydata$VV1400068, na.rm=T)\n\n[1] 36.5\n\n\nIf we do a regression using lm or glm, R will automatically “listwise” delete any observation that has missing data (NA) on any of the variables in our regression model.\nWe should always be careful with missing data to understand how R is treating it in a particular scenario.\nFor example if we were to run table(uglydata$gender), we would have no idea there were missing data unless we knew that the total number of observations nrow(uglydata) was greater than 3. The table() command is omitting the missing values by default.\n\ntable(gender= uglydata$gender)\n\ngender\n1 2 \n1 2 \n\n\n\n\n2.2.3 Dealing with Variable Codings that Aren’t Quite Right\nOften times the ways that variables are coded in datasets we get off-the-shelf are not coded exactly as how we were dreaming up operationalizing our concepts. Instead, we are going to have to wrangle the data to get them into shape.\nThis may involve creating new variables that recode certain values, creating new variables that collapse some values into a smaller number of categories, combining multiple variables into a single variable (e.g., representing the average), or setting some of the variable values to be missing (NA). All of these scenarios may come up when you are dealing with real data.\nChapter 2 of Kosuke Imai’s book Quantitative Social Science walks through some examples of how to summarize your data, subset the data (2.2.3), create new variables using conditional statements (Section 2.2.4, e.g., “If age is below 65, assign the new variable a value of”0”, otherwise, assign it a value of “1”), and creating new factor variables (2.2.5, e.g., coding anyone who is Protestant, Catholic, or Lutheran in the data as “Christian”).\nHere is a short video working through the example from 2.2.4 using conditional statements to construct new variables. It uses the resume dataframe, which can be loaded below.\n\nresume &lt;- read.csv(\"https://raw.githubusercontent.com/ktmccabe/teachingdata/main/resume.csv\")\n\n\nR Studio has its own set of primers on various topics, including summarizing and working with data. See the Work with Data primer, as well as the full list of other topics. These will often rely on tidyverse coding.\n\n\n2.2.4 Dealing with Incomplete Data (Merging!)\nSometimes in order to answer our research questions, we need to combine data from multiple sources. If we have a large amount of data, this may be quite daunting. Fortunately, R has several commands that allow us to merge or append datasets.\nHere is a video working through examples of merging and appending data based on the tutorial below.\n\nHere are a few resources on merging and appending data:\n\nUsing the merge command in R. See Explanation.\n\nIt will look for variable(s) held in common between datasets and join the datasets by the matching values on these variables.\n\nAppending data in R (e.g., Maybe you have one dataset from 2010 and another from 2012, and you want to stack them on top of each other). See Explanation.\n\nSome merging problems are extremely difficult. For example, some researchers need to merge large datasets–like the voter file– with other administrative records. However, how someone’s name is displayed in one dataset might not match at all with the other dataset. For these complex problems, we might need “fuzzy matching.” Here is an R package that helps with this more complex case and related paper.\n\n\n2.2.5 Dealing with Poorly Shaped Data\nData can come in a variety of shapes and sizes. It’s a beautiful disaster.\nSometimes it’s particularly useful to have data in wide formats, where every row relates to a particular unit of data– such as a country or a survey respondent. And perhaps each column represents information about that unit at a particular point in time. For example, perhaps you have a column with information on that subject for the past five years.\ncountrywide &lt;- data.frame(country = c(\"Canada\", \"USA\"),\n                          economy2016 = c(10, 12),\n                          economy2017 = c(11, 11),\n                          economy2018 = c(9, 5),\n                          economy2019 = c(13, 8),\n                          economy2020 = c(12, 6))\n\n\n  country economy2016 economy2017 economy2018 economy2019 economy2020\n1  Canada          10          11           9          13          12\n2     USA          12          11           5           8           6\n\n\nHowever, other times, it would be more useful to you, as you dig into your data analysis, to have this information arranged in “long” format, such that every row is now a unit-year combination. You have a row for Canada in 2020, a row for Canada in 2019, and so on. Countries are now represented in multiple rows of your data.\ncountrylong &lt;- data.frame(country = rep(c(\"Canada\", \"USA\"),5),\n                          year = 2016:2020,\n                          economy= c(10, 12,11, 11,9, 5,13, 8,12, 6))\n\n\n   country year economy\n1   Canada 2016      10\n2      USA 2017      12\n3   Canada 2018      11\n4      USA 2019      11\n5   Canada 2020       9\n6      USA 2016       5\n7   Canada 2017      13\n8      USA 2018       8\n9   Canada 2019      12\n10     USA 2020       6\n\n\nUltimately, different shapes of data are advantageous for different research questions. This means it is best if we have a way to (at least somewhat) easily shift between the two formats.\nHere is a resource on how to “reshape” your data between wide and long from UCLA.\nHere are a few additional resources:\n\nMore on reshape – For the tidyverse fans. Using gather() and spread() in tidyverse from R for Data Science and explained by Chris Bail here.\n\n\n\n2.2.6 Subsetting data by rows and columns\nSometimes we do not want to deal with our entire dataset for an analysis. Instead, we might want to only analyze certain rows (e.g., maybe if we are just studying Democrats, for example). Similarly, we might have a dataframe with 1000 columns, from which we are only using about 20. We might want to remove those extra columns to make it easier to work with our dataframes.\nBelow are a few examples of subsetting data and selecting columns. We will use the resume dataset from the Kosuke Imai QSS book for demonstration. This is a dataset from an experiment describing whether certain applicants, who varied in the gender (sex) and race (race) signaled by their name (firstname), received callbacks (call) for their employment applications.\nHere is a short video working through these examples.\n\nLet’s load the data.\n\nresume &lt;- read.csv(\"https://raw.githubusercontent.com/ktmccabe/teachingdata/main/resume.csv\")\n\nSubset particular rows\nTo do this, put the row numbers you want to keep on the left side of the comma. Putting nothing on the right side means you want to keep all columns.\n\n## numerically\nresume[1,] # first row\n\n  firstname    sex  race call\n1   Allison female white    0\n\nresume[1:4,] # first through 4th rows\n\n  firstname    sex  race call\n1   Allison female white    0\n2   Kristen female white    0\n3   Lakisha female black    0\n4   Latonya female black    0\n\nresume[c(1, 3, 4),] # 1, 3, 4 rows\n\n  firstname    sex  race call\n1   Allison female white    0\n3   Lakisha female black    0\n4   Latonya female black    0\n\n\nUsing the subset command with logical expressions &gt; &gt;= == &lt; &lt;= !=\n\n## by logical expressions\nwomen &lt;- subset(resume, sex == \"female\")\nwomen &lt;- resume[resume$sex == \"female\", ] ## alternative\n\ncalledback &lt;- subset(resume, call == 1)\ncalledback &lt;- subset(resume, call &gt; 0)\n\nAnd or Or statements & or |\n\nblackwomen &lt;- subset(resume, sex == \"female\" & race == \"black\")\nbradbrendan &lt;- subset(resume, firstname == \"Brad\" | \n                        firstname == \"Brendan\")\n\nThe tidyverse also has commands for subsetting. Here is an example using filter.\n\nlibrary(tidyverse)\nblackwomen &lt;- resume %&gt;%\n  filter(sex == \"female\" & race == \"black\")\n\nSelecting particular columns\nNote, now we are working on the right side of the comma.\n\n## numerically\nfirst &lt;- resume[, 1] # first column\nfirstsecond &lt;- resume[, 1:2] # first and second column\nnotfourth &lt;- resume[, -4] # all but the fourth column\n\n\n## by labels\njustthese &lt;- resume[, c(\"firstname\", \"sex\")]\n\nUsing the select command\n\n## install.packages(\"dplyr\")\nlibrary(dplyr)\nsubdata &lt;- resume %&gt;% dplyr::select(firstname, sex) ## just these\nsubdata2 &lt;- resume %&gt;% dplyr::select(-firstname, -sex) ## all but these two\n\n\n\n2.2.7 Visualizing Data\nThere are many commands for plotting your data in R. The most common functions in base R are plot(), barplot() and hist(). You will see many examples throughout the notes with each of these functions.\nTo get you started, the most simple thing to note about the plot() command is that it is based on a coordinate system. You specify the x and y coordinates for which you want to plot a series of points.\nFor example, here is a plot at points 1,40; 3,50; and 4,60.\n\nplot(x = c(1,3,4), y=c(40, 50, 60))\n\n\n\n\nInstead of putting raw numbers as the coordinates, you can provide object names. E.g.,\n\nxcoord &lt;- c(1,3,4)\nycoord &lt;- c(40, 50, 60)\nplot(x = xcoord, y=ycoord)\n\nBeyond that, you can play around with many aesthetics in R, such as the type, pch, lty, as well as labels main, ylab, xlab, font sizes cex.main, cex.axis, cex.lab, and axis limits ylim, xlim. Below is an example. Play around with changing some of the specifications, and see how the plot changes.\n\nxcoord &lt;- c(1,3,4)\nycoord &lt;- c(40, 50, 60)\nplot(x = xcoord, y=ycoord,\n     main = \"Example plot\",\n     ylab= \"Example y axis\",\n     xlab = \"Example x axis\",\n     cex.main = .8,\n     ylim = c(0, 80),\n     xlim = c(1,4),\n     pch = 15,\n     col=\"red\",\n     type = \"b\",\n     lty=2)\n\n\n\n\nThe function barplot takes a single vector of values. This can be a raw vector you have created or a table object or tapply object, for example, displaying the counts of different observations or means.\nYou can add a names.arg argument to specify particular names for each bar. Many of the other aesthetics are the same as plot. You can play around with adding aesthetics.\nExample:\n\nbarplot(ycoord,\n        names.arg = c(\"First\", \"Second\", \"Third\"),\n        col=\"blue\")\n\n\n\n\nFor more on visualizing data, you can see the RStudio primers.\nR also has a package called ggplot2 which includes the function ggplot and many elaborate ways to plot your data. The gg stands for the grammar of graphics. For a video introduction to ggplot I recommend watching Ryan Womack’s video from 27:30 on. It uses the data diamonds which can be loaded in R through the following command. See approximately minute 35 for an example with a bar plot.\n\nlibrary(ggplot2)\ndata(diamonds)\n\n\n\n\n2.2.8 Reproducing your steps\nIt is really important to keep a record of all of the changes you have made to the original data. An R script or R markdown file is a useful way to do this, so long as you add comments that explain what you are doing.\nYou want to get your code to a place when a stranger can open your R file, load your data, and reproduce each step you took to get to the final results— all while never even needing to contact you with questions. That can be difficult, but it’s good to aim for that high bar, even if sometimes, we fall short in how we are documenting each step.\nThis website provides a nice introduction to R Markdown, one tool for embedding R code inside textual documents. See here.\nIf you want to get advanced with reproducibility, you can watch Chris Bail’s on the subject describing the uses of R Markdown and GitHub among other tools for communicating and collaboration. He also links to other resources."
  },
  {
    "objectID": "02-ROverview.html#tools-for-writing-up-results",
    "href": "02-ROverview.html#tools-for-writing-up-results",
    "title": "2  R Overview",
    "section": "2.3 Tools for writing up results",
    "text": "2.3 Tools for writing up results\n\n2.3.1 R Markdown\nR Markdown is a free tool within RStudio that allows you to weave together text and code (along with images and tables) into the same document. It can compile these documents (written inside R Studio) into html, pdf, or Word doc output files. R Markdown can be incredibly helpful for doing things like generating replication files, writing up problem sets, or even writing papers.\nThis site includes an introduction to R Markdown.\n\nSee also here and here\n\nR Markdown has its own syntax, which includes functionality for writing mathematical equations. The pdf output option in R Markdown requires LaTex, described in the next section. Below we work with just the html output.\nR Markdown documents can be “compiled” into html, pdf, or docx documents by clicking the Knit button on top of the upper-left window. Below is an example of what a compiled html file looks like.\n\nNote that the image has both written text and a gray chunk, within which there is some R code, as well as the output of the R code (e.g., the number 8 and the image of the histogram plot. \n\nWe say this is a “compiled” RMarkdown document because it differs from the raw version of the file, which is a .Rmd file format. Below is an example of what the raw .Rmd version looks like, compared to the compiled html version.\n \nGetting started with RMarkdown\nJust like with a regular R script, to work in R Markdown, you will open up RStudio.\n\nFor additional support beyond the notes below, you can also follow the materials provided by RStudio for getting started with R Markdown https://rmarkdown.rstudio.com/lesson-1.html.\n\nThe first time you will be working in R Markdown, you will want to install two packages: rmarkdown and knitr. You can do this in the Console window in RStudio (remember the lower-left window!).\nType the following into the Console window and hit enter/return.\n\ninstall.packages(\"rmarkdown\")\ninstall.packages(\"knitr\")\n\nOnce you have those installed, now, each time you want to create an R Markdown document, you will open up a .Rmd R Markdown file and get to work.\n\nGo to File -&gt; New File -&gt; R Markdown in RStudio\n\nAlternatively, you can click the green + symbol at the top left of your RStudio window\n\nThis should open up a window with several options, similar to the image below\n\nCreate an informative title and change the author name to match your own\nFor now, we will keep the file type as html. In the future, you can create pdf or .doc documents. However, these require additional programs installed on your computer, which we will not cover in the course.\n\n\n\n\nAfter you hit “OK” a new .Rmd script file will open in your top-left window with some template language and code chunks, similar to the image below. Alternatively, you can start from scratch by clicking “Create Empty Document” or open a template .Rmd file of your own saved on your computer.\n\n\n\nSave as .Rmd file. Save the file by going to “File -&gt; Save as” in RStudio\n\n\nGive the file an informative name like your LastnamePractice1.Rmd\n\n\nKey Components. Now you are ready to work within the Rmd script file. We will point to four basic components of this file, and you can build your knowledge of RMarkdown from there.\n\nThe top part bracketed by --- on top and bottom is the YAML component. This tells RStudio the pertinent information about how to “compile” the Rmd file.\n\nMost of the time you can leave this alone, but you can always edit the title, author, or date as you wish.\n\nThe next component are the global options for the document. It is conveniently labeled “setup.” By default what this is saying is that the compiled version will “echo” (i.e., display all code chunks and output) unless you specifically specify otherwise. For example, note that it says include = FALSE for the setup chunk. That setting means that this code chunk will “run” but it will not appear in the nicely compiled .html file.\n\nMost of the time you will not need to edit those settings.\n\nThe third component I want to bring attention to is the body text. The # symbol in RMarkdown is used to indicate that you have a new section of the document. For example, in the compiled images at the beginning, this resulted in the text being larger and bolded when it said “Problem 2.” In addition to just using a single #, using ## or ### can indicate subsections or subsubsections. Other than that symbol, you can generally write text just as you would in any word processing program, with some exceptions, such as how to make text bold or italicized.\nThe final component I want to call attention to are the other main body code chunks. These are specific parts of the document where you want to create a mini R script. To create these, you can simply click the + C symbol toward the top of the top left window of RStudio and indicate you want an R chunk.\n\n\n\n\nWriting R Code. Within a code chunk, you can type R code just like you would in any R script, as explained in the previous section. However, in RMarkdown, you also have the option of running an entire code chunk at once by hitting the green triangle at the top-right of a given code chunk.\n\n\n\nKnitting the document. Once you have added a code chunk and/or some text, you are ready to compile or “Knit” the document. This is what generates the .html document.\n\nTo do so, click on the Knit button toward the top of the top-left window of Rstudio. After a few moments, this should open up a preview window displaying the compiled html file.\nIt will also save an actual .html file in your working directory (the same location on your computer where you have saved the .Rmd file)\nTry to locate this compiled .html file on your computer and open it. For most computers, .html files will open in your default web browser, such as Google Chrome or Safari.\nThis step is a common place where errors are detected and generated. Sometimes the compiling process fails due to errors in the R code in your code chunks or an error in the Markdown syntax. If your document fails to knit, the next step is to try to troubleshoot the error messages the compiling process generates. The best way to reduce and more easily detect errors is to “knit as you go.” Try to knit your document after each chunk of code you create.\n\n\n\n\n2.3.2 LaTex\nLaTex is a typesetting program for drafting documents, much like Microsoft Word. Some advantages of using LaTex in writing empirical research papers is, once you learn the basics of the program, it can become easier to add tables, figures, and equations into a paper with little effort. The downside is that it has its own syntax, which takes a little time to learn. LaTex also has a feature called “beamer” which uses LaTex to generate slides. You can use this for presentations. LaTex “compiles” documents into pdf files.\nHere is one introduction for getting started with LaTex that starts from the installation stage. Here also is a link to a set of slides from Overleaf discussing the basics of LaTex: Slides\nYou can download the Tex distribution for your operating system here. In addition to this, there are many programs available for running LaTex on your computer, which range from free, very basic tools (e.g., TexWorks) to tools with fancy capabilities.\nOverleaf is an online program for drafting LaTex documents. It has a nice feature where it allows you to share a document so that multiple people can work on the document simultaneously. This makes Overleaf a great program for projects where you have co-authors. The basic tools in Overleaf are available for free, but if you want to start sharing documents with a lot of co-authors, it requires a paid account.\nLaTex also has the ability to integrate citations into your documents. The part 2 tutorial from Overleaf goes over this.\nRStudio also has a program built-in called Sweave (.Rnw) documents that works with knitR, which weave together R code and LaTex syntax, allowing you to compile them into pdf documents and slide presentations. This is very similar to how R Markdown works, but with somewhat different syntax. See here for an overview. Your problem sets are generally Sweave/knitR documents.\n\n\n2.3.3 Formatting and Exporting R Results\nR has a number of tools, including the packages texreg, xtable, and stargazer, which can be used to export tables made in R to nicely formatted LaTex or html output.\nHere is a link to the texreg package documentation. Section 5 has examples of the texreg and htmlreg functions within the texreg package. These can be integrated into R Markdown and Sweave documents, and their output can be pasted into LaTex or Microsoft Word.\nYour choice of function will depend on where you ultimately want your results to be compiled. If you are generating results that will be compiled to pdf using LaTex, then texreg works well. If you are exporting results to Word, than you may wish to use the htmlreg function within the texreg package, which will generate output that can be pasted into Word.\nA simple example using R Markdown html output. (Note, if you wanted to export the table to Word, you would add an argument specifying file = \"myfit.doc\" to the function. See the above link for examples:\n\nmydata &lt;- read.csv(\"https://raw.githubusercontent.com/ktmccabe/teachingdata/main/resume.csv\")\nfit &lt;- lm(call ~ race, data=mydata)\n\n\n## First time you use texreg, install it\ninstall.packages(\"texreg\")\n\nlibrary(texreg)\nhtmlreg(list(fit),\n        stars=c(0.001, 0.01, 0.05),\n        caption = \"Regression of Call Backs on Race\")\n\n\n\nRegression of Call Backs on Race\n\n\n\n\n \n\n\nModel 1\n\n\n\n\n\n\n(Intercept)\n\n\n0.06***\n\n\n\n\n \n\n\n(0.01)\n\n\n\n\nracewhite\n\n\n0.03***\n\n\n\n\n \n\n\n(0.01)\n\n\n\n\nR2\n\n\n0.00\n\n\n\n\nAdj. R2\n\n\n0.00\n\n\n\n\nNum. obs.\n\n\n4870\n\n\n\n\n\n\n***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05\n\n\n\n\n\nYou can add more arguments to the function to customize the name of the model and the coefficients. You can also add multiple models inside the list argument, for example, if you wanted to present a table with five regression models at once. Here is an example with two:\nfit2 &lt;- lm(call ~ race + sex, data=mydata)\n\nlibrary(texreg)\nhtmlreg(list(fit, fit2),\n        stars=c(0.001, 0.01, 0.05),\n        caption = \"Regression of Call Backs on Race and Sex\")\n\n\n\nRegression of Call Backs on Race and Sex\n\n\n\n\n \n\n\nModel 1\n\n\nModel 2\n\n\n\n\n\n\n(Intercept)\n\n\n0.06***\n\n\n0.07***\n\n\n\n\n \n\n\n(0.01)\n\n\n(0.01)\n\n\n\n\nracewhite\n\n\n0.03***\n\n\n0.03***\n\n\n\n\n \n\n\n(0.01)\n\n\n(0.01)\n\n\n\n\nsexmale\n\n\n \n\n\n-0.01\n\n\n\n\n \n\n\n \n\n\n(0.01)\n\n\n\n\nR2\n\n\n0.00\n\n\n0.00\n\n\n\n\nAdj. R2\n\n\n0.00\n\n\n0.00\n\n\n\n\nNum. obs.\n\n\n4870\n\n\n4870\n\n\n\n\n\n\n***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05\n\n\n\n\n\n\n\n2.3.4 Additional formatting examples\nHere are some additional examples with different formats. You can run them on your own computer to see what the output looks like.\nThe package texreg has three primary formats\n\ntexreg() for LATEX output;\nhtmlreg() for HTML, Markdown-compatible and Microsoft Word-compatible output;\nscreenreg() for text output to the R console.\n\nIf you are working with a LaTex document, I recommend using texreg(), which will output LaTex syntax in your R console, which you can copy and paste into your article document.\nNote: this function allows you to customize model and coefficient names.\n\nlibrary(texreg)\ntexreg(list(fit, fit2),\n        stars=c(0.001, 0.01, 0.05),\n        caption = \"Regression of Call Backs on Race and Sex\",\n       custom.model.names = c(\"Bivariate\", \"Includes Sex\"),\n       custom.coef.names = c(\"Intercept\",\n                             \"Race- White\",\n                             \"Sex- Male\"))\n\nIf you are working with a Microsoft Word document, I recommend using htmlreg() and specifying a file name for your output. This will export a file to your working directory, which you can copy and paste into your Word article document. Otherwise, the syntax is the same as above.\n\nlibrary(texreg)\nhtmlreg(list(fit, fit2), file = \"models.doc\",\n        stars=c(0.001, 0.01, 0.05),\n        caption = \"Regression of Call Backs on Race and Sex\",\n       custom.model.names = c(\"Bivariate\", \"Includes Sex\"),\n       custom.coef.names = c(\"Intercept\",\n                             \"Race- White\",\n                             \"Sex- Male\"))\n\nIf you are trying to read the output in your R console, that’s when I would use screenreg(). However, for professional manuscript submissions, I would recommend the other formats.\n\nlibrary(texreg)\nscreenreg(list(fit, fit2), \n        stars=c(0.001, 0.01, 0.05),\n        caption = \"Regression of Call Backs on Race and Sex\",\n       custom.model.names = c(\"Bivariate\", \"Includes Sex\"),\n       custom.coef.names = c(\"Intercept\",\n                             \"Race- White\",\n                             \"Sex- Male\"))\n\nThe package stargazer allows similar options. I don’t think there are particular advantages to either package. Whatever comes easiest to you. The default for stargazer will output LaTex code into your R console.\n\nNote that the syntax is similar but has slightly different argument names from the texreg package.\nAlso, the intercept is at the bottom by default for stargazer. Be careful of the covariate ordering when you add labels.\n\n\nlibrary(stargazer)\nstargazer(list(fit, fit2), \n        star.cutoffs=c(0.05,0.01, 0.001),\n        title= \"Regression of Call Backs on Race and Sex\",\n        dep.var.labels.include = F,\n       column.labels = c(\"Call Back\", \"Call Back\"),\n       covariate.labels = c(\"Race- White\",\n                             \"Sex- Male\",\n                             \"Intercept\"))\n\nYou can adjust the type of output in stargazer for other formats, similar to texreg. Here is an example of Microsoft Word output.\n\nlibrary(stargazer)\nstargazer(list(fit, fit2), out = \"modelstar.doc\", type=\"html\",\n        star.cutoffs=c(0.05,0.01, 0.001),\n        dep.var.labels.include = F,\n        title= \"Regression of Call Backs on Race and Sex\",\n       column.labels = c(\"Call Back\", \"Call Back\"),\n       covariate.labels = c(\"Race- White\",\n                             \"Sex- Male\",\n                             \"Intercept\"))\n\n\n\n2.3.5 Additional Table Types\nSometimes you might want to create tables that are not from regression models, such as tables for descriptive statistics. R has other packages for tables of this type.\nFor example xtable can create simple html and latex tables. You just have to supply the function with a table object or matrix.\n\nlibrary(xtable)\ntable1 &lt;- table(race = mydata$race, sex = mydata$sex)\n\n\n## LaTeX\nxtable(table1)\n\n\n## Word\nprint(xtable(table1), type=\"html\", file = \"crosstab.doc\")\n\n## Html\nprint(xtable(table1), type=\"html\")\n\n\n\n\n\n\n\n\n\nfemale\n\n\nmale\n\n\n\n\nblack\n\n\n1886\n\n\n549\n\n\n\n\nwhite\n\n\n1860\n\n\n575"
  },
  {
    "objectID": "02-ROverview.html#exploratory-data-analysis-tools",
    "href": "02-ROverview.html#exploratory-data-analysis-tools",
    "title": "2  R Overview",
    "section": "2.4 Exploratory Data Analysis Tools",
    "text": "2.4 Exploratory Data Analysis Tools\nOne of the first things you may want to do when you have a new dataset is to explore! Get a sense of the variables you have, their class, and how they are coded. There are many functions in base R that help with this, such as summary(), table(), and descriptive statistics like mean or quantile.\nLet’s try this with the built-in mtcars data.\n\ndata(\"mtcars\")\n\nsummary(mtcars$cyl)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  4.000   4.000   6.000   6.188   8.000   8.000 \n\nquantile(mtcars$wt)\n\n     0%     25%     50%     75%    100% \n1.51300 2.58125 3.32500 3.61000 5.42400 \n\nmean(mtcars$mpg, na.rm=T)\n\n[1] 20.09062\n\nsd(mtcars$mpg, na.rm=T)\n\n[1] 6.026948\n\ntable(gear=mtcars$gear, carb=mtcars$carb)\n\n    carb\ngear 1 2 3 4 6 8\n   3 3 4 3 5 0 0\n   4 4 4 0 4 0 0\n   5 0 2 0 1 1 1\n\n\nAs discussed in the visualization, you can also quickly describe univariate data with histograms, barplots, or density plots using base R or ggplot.\n\nhist(mtcars$mpg, breaks=20, main=\"Histogram of MPG\")\n\n\n\nplot(density(mtcars$mpg, na.rm=T),  \n     main=\"Distribution of MPG\")\n\n\n\nbarplot(table(mtcars$gear), main=\"Barplot of Gears\")\n\n\n\n\n\nlibrary(ggplot2)\nggplot(mtcars, aes(mpg))+\n  geom_histogram(bins=20)+\n  ggtitle(\"Histogram of MPG\")\n\n\n\nggplot(mtcars, aes(mpg))+\n  geom_density()+\n  ggtitle(\"Distribution of MPG\")\n\n\n\nggplot(mtcars, aes(gear))+\n  geom_bar(stat=\"count\")+\n  ggtitle(\"Barplot of Gears\")"
  },
  {
    "objectID": "03-TheMATH.html#mathematical-operations",
    "href": "03-TheMATH.html#mathematical-operations",
    "title": "3  The Math",
    "section": "3.1 Mathematical Operations",
    "text": "3.1 Mathematical Operations\nIn this first section, we will review mathematical operations that you have probably encountered before. In many cases, this will be a refresher on the rules and how to read notation.\n\n3.1.1 Order of Operations\nMany of you may have learned the phrase, “Please Excuse My Dear Aunt Sally” which stands for Parentheses (and other grouping symbols), followed by Exponents, followed by Multiplication and Division from left to right, followed by Addition and Subtraction from left to right.\nThere will be many equations in our future, and we must remember these rules.\n\nExample: \\(((1+2)^3)^2 = (3^3)^2 = 27^2 = 729\\)\n\nTo get the answer, we focused on respecting the parentheses first, identifying the inner-most expression \\(1 + 2 = 3\\), we then moved out and conducted the exponents to get to \\((3^3)^2 = 27^2\\).\nNote how this is different from the answer to \\(1 + (2^3)^2 = 1 + 8^2 = 65\\), where the addition is no longer part of the parentheses.\n\n\n3.1.2 Exponents\nHere is a cheat sheet of some basic rules for exponents. These can be hard to remember if you haven’t used them in a long time. Think of \\(a\\) in this case as a number, e.g., 4, and \\(b\\), \\(k\\), and \\(l\\), as other numbers.\n\n\\(a^0 = 1\\)\n\\(a^1 = a\\)\n\\(a^k * a^l = a^{k + l}\\)\n\\((a^k)^l = a^{kl}\\)\n\\((\\frac{a}{b})^k = (\\frac{a^k}{b^k})\\)\n\nThese last two rules can be somewhat tricky. Note that a negative exponent can be re-written as a fraction. Likewise an exponent that is a fraction, the most common of which we will encounter is \\(\\frac{1}{2}\\) can be re-written as a root, in this case the square root (e.g., \\(\\sqrt{a}\\)).\n\n\\(a^{-k} = \\frac{1}{a^k}\\)\n\\(a^{1/2} = \\sqrt{a}\\)\n\n\n\n3.1.3 Summations and Products\nThe symbol \\(\\sum\\) can be read “take the sum of” whatever is to the right of the symbol. This is used to make the written computation of a sum much shorter than it might be otherwise. For example, instead of writing the addition operations separately in the example below, we can simplify it with the \\(\\sum\\) symbol. This is especially helpful if you would need to add together 100 or 1000 or more things. We will see these appear a lot in the course, for better or worse, so getting comfortable with the notation will be useful.\nUsually, there is notation just below and just above the symbol (e.g., \\(\\sum_{i=1}^3\\)). This can be read as “take the sum of the following from \\(i=1\\) to \\(i=3\\). We perform the operation in the expression, each time changing \\(i\\) to a different number, from 1 to 2 to 3. We then add each expression’s output together.\n\nExample: \\(\\sum_{i=1}^3 (i + 1)^2 = (1 + 1)^2 + (2 + 1)^2 + (3+1)^2 = 29\\)\n\nWe will also encounter the product symbol in this course: \\(\\prod\\). This is similar to the summation symbol, but this time we are multiplying instead of adding.\n\nExample: \\(\\prod_{k = 1}^3 k^2 = 1^2 \\times 2^2 \\times 3^2 = 36\\)\n\n\n\n3.1.4 Logarithms\nIn this class, we will generally assume that \\(\\log\\) takes the natural base \\(e\\), which is a mathematical constant equal to 2.718…. In other books, the base might be 10 by default.\n\nIf we have, \\(\\log_{10} x = 2\\), this is like saying 10^2 = 100.\nWith base \\(e\\), we have \\(\\log_e x =2\\), which is \\(e^2 = 7.389\\).\nWe are just going to write \\(\\log_e\\) as \\(\\log\\) but know that the \\(e\\) is there.\n\nA key part of maximum likelihood estimation is writing down the log of the likelihood equation, so this is a must-have for later in the course.\nHere is a cheat sheet of common rules for working with logarithms.\n\n\\(\\log x = 8 \\rightarrow e^8 = x\\)\n\\(e^\\pi = y \\rightarrow \\log y = \\pi\\)\n\\(\\log (a \\times b) = \\log a + \\log b\\)\n\\(\\log a^n = n \\log a\\)\n\\(\\log \\frac{a}{b} = \\log a - \\log b\\)\n\nWhy logarithms? There are many different reasons why social scientists use logs.\n\nSome social phenomena grow exponentially, and logs make it is easier to visualize exponential growth, as is the case in visualizing the growth in COVID cases. See this example.\nRelatedly, taking the log of a distribution that is skewed, will make it look more normal or symmetrical, which has some nice properties.\nSometimes the rules of logarithms are more convenient than non-logarithms. In MLE, we will take particular advantage of this rule: \\(\\log (a \\times b) = \\log a + \\log b\\), which turns a multiplication problem into an addition problem."
  },
  {
    "objectID": "03-TheMATH.html#mathematical-operations-in-r",
    "href": "03-TheMATH.html#mathematical-operations-in-r",
    "title": "3  The Math",
    "section": "3.2 Mathematical Operations in R",
    "text": "3.2 Mathematical Operations in R\nWe will use R for this course, and these operations are all available with R code, allowing R to become a calculator for you. Here are some examples applying the tools above.\n\n3.2.1 PEMDAS\n\n((1+2)^3)^2 \n\n[1] 729\n\n1 + (2^3)^2 \n\n[1] 65\n\n\n\n\n3.2.2 Exponents\nYou can compare the computation below to match with the rules above. Note how the caret ^ symbol is used for exponents, and the asterisk * is used for multiplication. We also have a function sqrt() for taking the square root.\n\n## Let's say a = 4 for our purposes\na &lt;- 4\n## And let's say k= 3 and l=5, b=2\nk &lt;- 3\nl &lt;- 5\nb &lt;- 2\n\n\n\\(a^0 = 1\\)\n\\(a^1 = a\\)\n\n\na^0\na^1\n\n[1] 1\n[1] 4\n\n\nNote how we use parentheses in R to make it clear that the exponent includes not just k but (k + l)\n\n\\(a^k * a^l = a^{k + l}\\)\n\n\na^k * a^l \na^(k + l)\n\n[1] 65536\n[1] 65536\n\n\nNote how we use the asterisk to make it clear we want to multiply k*l\n\n\\((a^k)^l = a^{kl}\\)\n\n\n(a^k)^l\na^(k*l)\n\n[1] 1073741824\n[1] 1073741824\n\n\n\n\\((\\frac{a}{b})^k = (\\frac{a^k}{b^k})\\)\n\n\n(a / b)^k\n(a ^k)/(b^k)\n\n[1] 8\n[1] 8\n\n\n\n\\(a^{-k} = \\frac{1}{a^k}\\)\n\n\na^(-k) \n1 / (a^k)\n\n[1] 0.015625\n[1] 0.015625\n\n\n\n\\(a^{1/2} = \\sqrt{a}\\)\n\n\na^(1/2) \nsqrt(a)\n\n[1] 2\n[1] 2\n\n\n\n\n3.2.3 Summations\nSummations and products are a little more nuanced in R, depending on what you want to accomplish. But here is one example.\nLet’s take a vector (think a list of numbers) that goes from 1 to 4. We will call it ourlist.\n\nourlist &lt;- c(1,2,3,4)\nourlist\n## An alternative is to write this: ourlist &lt;- 1:4\n\n[1] 1 2 3 4\n\n\nNow let’s do \\(\\sum_{i = 1}^4 (i + 1)^2 = (1 +1)^2 + (1+2)^2 + (1 + 3)^2 + (1 + 4)^2 = 54\\)\nIn R, when you add a number to a vector, it will add that number to each entry in the vector. Example\n\nourlist + 1\n\n[1] 2 3 4 5\n\n\nWe can use that now to do the inside part of the summation. Note: the exponent works the same way, squaring each element of the inside expression.\n\n(ourlist + 1)^2\n\n[1]  4  9 16 25\n\n\nNow, we will embed this expression inside a function in R called sum standing for summation. It will add together each of the inside components.\n\nsum((ourlist + 1)^2)\n\n[1] 54\n\n\nIf instead we wanted the product, multiplying each element of the inside together, we could use prod(). We won’t use that function very much in this course.\n\n\n3.2.4 Logarithms\nR also has functions related to logarithms, called log() and exp() for the for the natural base \\(e\\). By default, the natural exponential is the base in the log() R function.\n\n\\(\\log x = 8 \\rightarrow e^8 = x\\)\n\n\nexp(8)\n\n[1] 2980.958\n\nlog(2980.958)\n\n[1] 8\n\n\nNote that R also has a number of built-in constants, like pi.\n\n\\(e^\\pi = y \\rightarrow \\log y = \\pi\\)\n\n\nexp(pi)\n\n[1] 23.14069\n\nlog(23.14069)\n\n[1] 3.141593\n\n\n\n\\(\\log (a \\times b) = \\log a + \\log b\\)\n\n\nlog(a * b)\nlog(a) + log(b)\n\n[1] 2.079442\n[1] 2.079442\n\n\nLet’s treat \\(n=3\\) in this example and enter 3 directly where we see \\(n\\) below. Alternatively you could store 3 as n, as we did with the other letters above.\n\n\\(\\log a^n = n \\log a\\)\n\n\nlog(a ^ 3)\n\n3 * log(a)\n\n[1] 4.158883\n[1] 4.158883\n\n\n\n\\(\\log \\frac{a}{b} = \\log a - \\log b\\)\n\n\nlog(a/b)\nlog(a) - log(b)\n\n[1] 0.6931472\n[1] 0.6931472"
  },
  {
    "objectID": "03-TheMATH.html#derivatives",
    "href": "03-TheMATH.html#derivatives",
    "title": "3  The Math",
    "section": "3.3 Derivatives",
    "text": "3.3 Derivatives\nWe will need to take some derivatives in the course. The reason is because a derivative gets us closer to understanding how to minimize and maximize certain functions, where a function is a relationship that maps elements of a set of inputs into a set of outputs, where each input is related to one output.\nThis is useful in social science, with methods such as linear regression and maximum likelihood estimation because it helps us estimate the values that we think will best describe the relationship between our independent variables and a dependent variable.\nFor example, in ordinary least squares (linear regression), we choose coefficients, which describe the relationship between the independent and dependent variables (for every 1 unit change in x, we estimate \\(\\hat \\beta\\) amount of change in y), based on a method that tries to minimize the squared error between our estimated outcomes and the actual outcomes. In MLE, we will have a different quantity, which we will try to maximize.\nTo understand derivatives, we will briefly define limits.\nLimits\nA limit describes how a function behaves as it approaches (gets very close to) a certain value\n\n\\(\\lim_{x \\rightarrow a} f(x) = L\\)\n\nExample: \\(\\lim_{x \\rightarrow 3} x^2 = 9\\) The limit of this function as \\(x\\) approaches three, is 9. Limits will appear in the expression for calculating derivatives.\n\n3.3.1 Derivatives\nFor intuition on a derivative, watch this video from The Math Sorcerer.\nA derivative is the instantaneous rate at which the function is changing at x: the slope of a function at a particular point.\nThere are different notations for indicating something is a derivative. Below, we use \\(f'(x)\\) because we write our functions as \\(f(x) = x\\). Many times you might see a function equation like \\(y = 3x\\) There, it will be common for the derivative to be written like \\(\\frac{dy}{dx}\\).\nLet’s break down the definition of a derivative by looking at its similarity to the simple definition of a slope, as the rise over the run:\n\nSlope (on average): rise over run: change in \\(f(x)\\) over an interval (\\([c, b]\\) where \\(b-c =h\\)): \\(\\frac{f(b) - f(c)}{b-c}\\)\n\nFor slope at a specific point \\(x\\) (the derivative of f(x) at x), we just make the interval \\(h\\) very small:\n\n\\(f'(x)= \\lim_{h \\rightarrow 0}\\frac{f(a + h) - f(a)}{h}\\)\n\nExample \\(f(x) = 2x + 3\\).\n\n\\(f'(x) = \\lim_{h \\rightarrow 0}\\frac{2(x + h) + 3 - (2x + 3)}{h} = \\lim_{h \\rightarrow 0}\\frac{2x + 2h - 2x}{h} = \\lim_{h \\rightarrow 0}2 = 2\\)\n\nThis twitter thread by the brilliant teacher and statistician Allison Horst, provides a nice cartoon-based example of the derivative. (Note that sometimes the interval \\(h\\) is written as \\(\\Delta x\\), the change in \\(x\\)).\n\n\n3.3.2 Critical Points for Minima or Maxima\nIn both OLS and MLE, we reach points where take what are called “first order conditions.” This means we take the derivative with respect to a parameter of interest and then set the derivative = 0 and solve for the parameter to get an expression for our estimator. (E.g., In OLS, we take the derivative of the sum of squared residuals, set it equal to zero, and solve to get an expression for \\(\\hat \\beta\\)).\nThe reason we are interested in when the derivative is zero, is because this is when the instanaeous rate of change is zero, i.e., the slope at a particular point is zero. When does this happen? At a critical point- maximum or minimum. Think about it– at the top of a mountain, there is no more rise (and no decline). You are completing level on the mountaintop. The slope at that point is zero.\nLet’s take an example. The function \\(f(x) = x^2 + 1\\) has the derivative \\(f'(x) = 2x\\). This is zero when \\(x = 0\\).\nThe question remains: How do we know if it is a maximum or minimum?\nWe need to figure out if our function is concave or convex around this critical value. Convex is a “U” shape, meaning we are at a minimum, while concavity is an upside-down-U, which means we are at a maximum. We do so by taking the second derivative. This just means we take the derivative of the expression we already have for our first derivative. In our case, \\(f''(x) = 2\\). So what? Well the key thing we are looking for is if this result is positive or negative. Here, it is positive, which means our function is convex at this critical value, and therefore, we are at a minimum.\nJust look at the function in R if we plot it.\n\n## Let's define an arbitrary set of values for x\nx &lt;- -3:3\n## Now let's map the elements of x into y using our function\nfx &lt;- x^2 + 1\n\n## Let's plot the results\nplot(x = x, y=fx, \n     xlab = \"x\", type = \"l\",\n     main = \"f(x) = x^2 + 1\")\n\n\n\n\nNotice that when x=0, we are indeed at a minimum, just as the positive value of the second derivative would suggest.\nA different example: \\(f(x) = -2x^2 +1\\). \\(f'(x) = -4x\\) When we set this equal to 0 we find a critical value at \\(x = 0\\). \\(f''(x) = -4\\). Here, the value is negative, and we know it is concave. Sure enough, let’s plot it, and notice how we can draw a horiztonal line at the maximum, representing that zero slope at the critical point:\n\nx &lt;- -3:3\nfx &lt;- -2*x^2 + 1\nplot(x = x, y = fx,\n    ylab = \"f(x)\",\n     xlab = \"x\", type = \"l\",\n     main = \"f(x) = -2x^2 + 1\")\nabline(h=1, col = \"red\", lwd=2)\n\n\n\n\n\n\n3.3.3 Common Derivative Rules\nBelow is a cheat sheet of rules for quickly identifying the derivatives of functions.\nThe derivative of a constant is 0.\n\n\\(f(x) = a; f'(x) = 0\\)\n\nExample: The derivative of 5 is 0.\n\n\nHere is the power rule.\n\n\\(f(x) = ax^n; f'(x) = n\\times a \\times x^{n-1}\\)\n\nExample: The derivative of \\(x^3 = 3x^{(3-1)} = 3x^2\\)\n\n\nWe saw logs in the last section, and, yes, we see logs again here.\n\n\\(f(x) = e^{ax}; f'(x) = ae^{ax}\\)\n\\(f(x) = \\log(x); f'(x) = \\frac{1}{x}\\)\n\nA very convenient rule is that a derivative of a sum = sum of the derivatives.\n\n\\(f(x) = g(x) + h(x); f'(x) = g'(x) + h'(x)\\)\n\nProducts can be more of a headache. In this course, we will turn some product expressions into summation expressions to avoid the difficulties of taking derivatives with products.\n\nProduct Rule: \\(f(x) = g(x)h(x); f'(x) = g'(x)h(x) + h'(x)g(x)\\)\n\nThe chain rule below looks a bit tricky, but it can be very helpful for simplifying the way you take a derivative. See this video from NancyPi for a helpful explainer, as well as a follow-up for more complex applications here.\n\nChain Rule: \\(f(x) = g(h(x)); f'(x) = g'(h(x))h'(x)\\)\n\nExample: What is the derivative of \\(f(x) = \\log 5x\\)?\n\nFirst, we will apply the rule which tells us the derivative of a \\(\\log x\\) is \\(\\frac{1}{x}\\).\nHowever, here, we do not just have \\(x\\), we have \\(5x\\). We are in chain rule territory.\nAfter we apply the derivative to the log, which is \\(\\frac{1}{5x}\\), we then have to take the derivative of \\(5x\\) and multiply the two expressions together.\nThe derivative of \\(5x\\) is \\(5\\).\nSo, putting this together, our full derivative is f′(x) = 5 ∗ \\(\\frac{1}{5x}\\) = \\(\\frac{1}{x}\\)."
  },
  {
    "objectID": "03-TheMATH.html#vectors-and-matrices",
    "href": "03-TheMATH.html#vectors-and-matrices",
    "title": "3  The Math",
    "section": "3.4 Vectors and Matrices",
    "text": "3.4 Vectors and Matrices\nVectors\nFor our purposes, a vector is a list or “array” of numbers. For example, this might be a variable in our data– a list of the ages of all politicians in a country.\nAddition\n\nIf we have two vectors \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\), where\n\n\\(\\mathbf{u} \\ = \\ (u_1, u_2, \\dots u_n)\\) and\n\\(\\mathbf{v} \\ = \\ (v_1, v_2, \\dots v_n)\\),\n\\(\\mathbf{u} + \\mathbf{v} = (u_1 + v_1, u_2 + v_2, \\dots u_n + v_n)\\)\n\nNote: \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) must be of the same dimensionality - number of elements in each must be the same - for addition.\n\nScalar multiplication\n\nIf we have a scalar (i.e., a single number) \\(\\lambda\\) and a vector \\(\\mathbf{u}\\)\n\\(\\lambda \\mathbf{u} = (\\lambda u_1, \\lambda u_2, \\dots \\lambda u_n)\\)\n\nWe can implement vector addition and scalar multiplication in R.\nLet’s create a vector \\(\\mathbf{u}\\), a vector \\(\\mathbf{v}\\), and a number lambda.\n\nu &lt;- c(33, 44, 22, 11)\nv &lt;- c(6, 7, 8, 2)\nlambda &lt;- 3\n\nWhen you add two vectors in R, it adds each component together.\n\nu + v\n\n[1] 39 51 30 13\n\n\nWe can multiply each element of a vector, u by lambda:\n\nlambda * u\n\n[1]  99 132  66  33\n\n\nElement-wise Multiplication\nNote: When you multiply two vectors together in R, it will take each element of one vector and multiply it by each element of the other vector.\n\nu * v \\(= (u_1 * v_1, u_2 * v_2, \\dots u_n * v_n)\\)\n\n\nu * v\n\n[1] 198 308 176  22\n\n\n\n3.4.1 Matrix Basics\nA matrix represents arrays of numbers in a rectangle, with rows and columns.\n\nA matrix with \\(m\\) rows and \\(n\\) columns is defined as (\\(m\\) x \\(n\\)). What is the dimensionality of the matrix A below?\n\n\\(A = \\begin{pmatrix} a_{11} & a_{12} & a_{13}\\\\ a_{21} & a_{22} & a_{23} \\\\ a_{31} & a_{32} & a_{33} \\\\ a_{41} & a_{42} & a_{43} \\end{pmatrix}\\)\nIn R, we can think of a matrix as a set of vectors. For example, we could combine the vectors u and v we created above into a matrix defined as W.\n\n## cbind() binds together vectors as columns\nWcol &lt;- cbind(u, v)\nWcol\n\n      u v\n[1,] 33 6\n[2,] 44 7\n[3,] 22 8\n[4,] 11 2\n\n## rbind() binds together vectors as rows\nWrow &lt;- rbind(u, v)\nWrow\n\n  [,1] [,2] [,3] [,4]\nu   33   44   22   11\nv    6    7    8    2\n\n\nThere are other ways to create matrices in R, but using cbind and rbind() are common.\nWe can find the dimensions of our matrices using dim() or nrow() and ncol() together. For example:\n\ndim(Wcol)\n\n[1] 4 2\n\nnrow(Wcol)\n\n[1] 4\n\nncol(Wcol)\n\n[1] 2\n\n\nNote how the dimensions are different from the version created with rbind():\n\ndim(Wrow)\n\n[1] 2 4\n\nnrow(Wrow)\n\n[1] 2\n\nncol(Wrow)\n\n[1] 4\n\n\nExtracting specific components\nThe element \\(a_{ij}\\) signifies the element is in the \\(i\\)th row and \\(j\\)th column of matrix A. For example, \\(a_{12}\\) is in the first row and second column.\n\nSquare matrices have the same number of rows and columns\nVectors have just one row or one column (e.g., \\(x_1\\) element of \\(\\mathbf{x}\\) vector)\n\nIn R, we can use brackets to extract a specific \\(ij\\) element of a matrix or vector.\n\nWcol\n\n      u v\n[1,] 33 6\n[2,] 44 7\n[3,] 22 8\n[4,] 11 2\n\nWcol[2,1] # element in the second row, first column\n\n u \n44 \n\nWcol[2,] # all elements in the second row\n\n u  v \n44  7 \n\nWcol[, 1] # all elements in the first column\n\n[1] 33 44 22 11\n\n\nFor matrices, to extract a particular entry in R, you have a comma between entries because there are both rows and columns. For vectors, you only have one entry, so no comma is needed.\n\nu\n\n[1] 33 44 22 11\n\nu[2] # second element in the u vector\n\n[1] 44\n\n\n\n\n3.4.2 Matrix Operations\nMatrix Addition\n\nTo be able to add matrix A and matrix B, they must have the same dimensions.\nLike vector addition, to add matrices, you add each of the components together.\n\n\\(A = \\begin{pmatrix} a_{11} & a_{12} & a_{13}\\\\ a_{21} & a_{22} & a_{23} \\\\ a_{31} & a_{32} & a_{33} \\end{pmatrix}\\) and \\(B = \\begin{pmatrix} b_{11} & b_{12} & b_{13}\\\\ b_{21} & b_{22} & b_{23} \\\\ b_{31} & b_{32} & b_{33} \\end{pmatrix}\\)\n\\(A + B = \\begin{pmatrix} a_{11} + b_{11} & a_{12} + b_{12} & a_{13} + b_{13}\\\\ a_{21} + b_{21} & a_{22} + b_{22} & a_{23} + b_{23} \\\\ a_{31} + b_{31} & a_{32} + b_{32} & a_{33} + b_{33} \\end{pmatrix}\\)\n\\(Q = \\begin{pmatrix} 2 & 4 & 1\\\\ 6 & 1 & 5 \\end{pmatrix}\\) \\(+\\) \\(R = \\begin{pmatrix} 9 & 4 & 2\\\\ 11 & 8 & 7 \\end{pmatrix} = Q + R = \\begin{pmatrix} 11 & 8 & 3\\\\ 17 & 9 & 12 \\end{pmatrix}\\)\nScalar Multiplication\nTake a scalar \\(\\nu\\). Just like vectors, we multiply each component of a matrix by the scalar.\n\\(\\nu Q = \\begin{pmatrix} \\nu q_{11} & \\nu q_{12} & \\dots & \\nu q_{1n}\\\\ \\nu q_{21} & \\nu q_{22} & \\dots & \\nu q_{2n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\nu q_{m1} & \\nu q_{m2} & \\dots & \\nu q_{mn} \\end{pmatrix}\\)\nExample: Take \\(c = 2\\) and a matrix A.\n\\(cA = c *\\begin{pmatrix} 4 & 6 & 1\\\\ 3 & 2 & 8 \\end{pmatrix}\\) = \\(\\begin{pmatrix} 8 & 12 & 2\\\\ 6 & 4 & 16 \\end{pmatrix}\\)\nNote the Commutativity/Associativity: For scalar \\(c\\): \\(c(AB) = (cA)B = A(cB) = (AB)c\\).\nMatrix Multiplication\nA matrix A and B must be conformable to multiply AB.\n\nTo be comformable, for \\(m_A\\) x \\(n_A\\) matrix A and \\(m_B\\) x \\(n_B\\) matrix B, the “inside” dimensions must be equal: \\(n_A = m_B\\).\nThe resulting AB has the “outside” dimensions: \\(m_A\\) x \\(n_B\\).\n\nFor each \\(c_{ij}\\) component of \\(C = AB\\), we take the inner product of the \\(i^{th}\\) row of matrix A and the \\(j^{th}\\) column of matrix B.\n\nTheir product C = AB is the \\(m\\) x \\(n\\) matrix where:\n\\(c_{ij} =a_{i1}b_{1j} + a_{i2}b_{2j} + \\dots + a_{ik}b_{kj}\\)\n\nExample: This \\(2 \\times 3\\) matrix is multiplied by a \\(3 \\times 2\\) matrix, resulting in the \\(2 \\times 2\\) matrix.\n\\(\\begin{pmatrix} 4 & 6 & 1\\\\ 3 & 2 & 8 \\end{pmatrix}\\) \\(\\times\\) \\(\\begin{pmatrix} 8 & 12 \\\\ 6 & 4 \\\\ 7 & 10 \\end{pmatrix}\\) = \\(\\begin{pmatrix} (4*8 + 6*6 + 1*7) & (4*12 + 6*4 + 1*10) \\\\ (3*8 + 2*6 + 8*7) & (3*12 + 2*4 + 8*10) \\end{pmatrix}\\)\nFor example, the entry in the first row and second column of the new matrix \\(c_{12} = (a_{11} = 4* b_{11} = 12) + (a_{12} = 6*b_{21} = 4) + (a_{13} = 1*b_{31} = 10)\\)\nWe can also do matrix multiplication in R.\n\n## Create a 3 x 2 matrix A\nA &lt;- cbind(c(3, 4, 6), c(5, 6, 8))\nA\n\n     [,1] [,2]\n[1,]    3    5\n[2,]    4    6\n[3,]    6    8\n\n## Create a 2 x 4 matrix B\nB &lt;- cbind(c(6,8), c(7, 9), c(3, 6), c(1, 11))\nB\n\n     [,1] [,2] [,3] [,4]\n[1,]    6    7    3    1\n[2,]    8    9    6   11\n\n\nNote that the multiplication AB is conformable because the number of columns in A matches the number of rows in B:\n\nncol(A)\nnrow(B)\n\n[1] 2\n[1] 2\n\n\nTo multiply matrices together in R, we need to add symbols around the standard asterisk for multiplication:\n\nA %*% B\n\n     [,1] [,2] [,3] [,4]\n[1,]   58   66   39   58\n[2,]   72   82   48   70\n[3,]  100  114   66   94\n\n\nThat is necessary for multiplying matrices together. It is not necessary for scalar multiplication, where we take a single number (e.g., c = 3) and multiply it with a matrix:\n\nc &lt;- 3\nc*A\n\n     [,1] [,2]\n[1,]    9   15\n[2,]   12   18\n[3,]   18   24\n\n\nNote the equivalence of the below expressions, which combine scalar and matrix multiplication:\n\nc* (A %*% B)\n\n     [,1] [,2] [,3] [,4]\n[1,]  174  198  117  174\n[2,]  216  246  144  210\n[3,]  300  342  198  282\n\n(c* A) %*% B\n\n     [,1] [,2] [,3] [,4]\n[1,]  174  198  117  174\n[2,]  216  246  144  210\n[3,]  300  342  198  282\n\nA %*% (c * B)\n\n     [,1] [,2] [,3] [,4]\n[1,]  174  198  117  174\n[2,]  216  246  144  210\n[3,]  300  342  198  282\n\n\nIn social science, one matrix of interest is often a rectangular dataset that includes column vectors representing independent variables, as well as another vector that includes your dependent variable. These might have 1000 or more rows and a handful of columns you care about."
  },
  {
    "objectID": "03-TheMATH.html#additional-matrix-tidbits-that-will-come-up",
    "href": "03-TheMATH.html#additional-matrix-tidbits-that-will-come-up",
    "title": "3  The Math",
    "section": "3.5 Additional Matrix Tidbits that Will Come Up",
    "text": "3.5 Additional Matrix Tidbits that Will Come Up\nInverse\nAn \\(n\\) x \\(n\\) matrix A is invertible if there exists an \\(n\\) x \\(n\\) inverse matrix \\(A^{-1}\\) such that:\n\n\\(AA^{-1} = A^{-1}A = I_n\\)\nwhere \\(I_n\\) is the identity matrix (\\(n\\) x \\(n\\)), that takes diagonal elements of 1 and off-diagonal elements of 0. Example:\n\n\\(I_n = \\begin{pmatrix} 1_{11} & 0 & \\dots & 0\\\\ 0& 1_{22} & \\dots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & \\dots & 1_{nn} \\end{pmatrix}\\)\n\nMultiplying a matrix by the identity matrix returns in the matrix itself: \\(AI_n = A\\)\n\nIt’s like the matrix version of multiplying a number by one.\n\n\nNote: A matrix must be square \\(n\\) x \\(n\\) to be invertible. (But not all square matrices are invertible.) A matrix is invertible if and only if its columns are linearly independent. This is important for understanding why you cannot have two perfectly colinear variables in a regression model.\nWe will not do much solving for inverses in this course. However, the inverse will be useful in solving for and simplifying expressions.\n\n3.5.1 Transpose\nWhen we transpose a matrix, we flip the \\(i\\) and \\(j\\) components.\n\nExample: Take a 4 X 3 matrix A and find the 3 X 4 matrix \\(A^{T}\\).\nA transpose is usually denoted with as \\(A^{T}\\) or \\(A'\\)\n\n\\(A = \\begin{pmatrix} a_{11} & a_{12} & a_{13}\\\\ a_{21} & a_{22} & a_{23} \\\\ a_{31} & a_{32} & a_{33} \\\\ a_{41} & a_{42} & a_{43} \\end{pmatrix}\\) then \\(A^T = \\begin{pmatrix} a'_{11} & a'_{12} & a'_{13} & a'_{14}\\\\ a'_{21} & a'_{22} & a'_{23} & a'_{24} \\\\ a'_{31} & a'_{32} & a'_{33} & a'_{34} \\end{pmatrix}\\)\nIf \\(A = \\begin{pmatrix} 1 & 4 & 2 \\\\ 3 & 1 & 11 \\\\ 5 & 9 & 4 \\\\ 2 & 11& 4 \\end{pmatrix}\\) then \\(A^T = \\begin{pmatrix} 1 & 3 & 5 & 2\\\\ 4 & 1 & 9 & 11 \\\\ 2& 11 & 4 & 4 \\end{pmatrix}\\)\nCheck for yourself: What was in the first row (\\(i=1\\)), second column (\\(j=2\\)) is now in the second row (\\(i=2\\)), first column (\\(j=1\\)). That is \\(a_{12} =4 = a'_{21}\\).\nWe can transpose matrices in R using t(). For example, take our matrix A:\n\nA\n\n     [,1] [,2]\n[1,]    3    5\n[2,]    4    6\n[3,]    6    8\n\nt(A)\n\n     [,1] [,2] [,3]\n[1,]    3    4    6\n[2,]    5    6    8\n\n\nIn R, you can find the inverse of a square matrix with solve()\n\nsolve(A)\n\nError in solve.default(A): 'a' (3 x 2) must be square\n\n\nNote, while A is not square A’A is square:\n\nAtA &lt;- t(A) %*% A\n\nsolve(AtA)\n\n          [,1]      [,2]\n[1,]  2.232143 -1.553571\n[2,] -1.553571  1.089286\n\n\n\n\n3.5.2 Additional Matrix Properties and Rules\nThese are a few additional properties and rules that will be useful to us at various points in the course:\n\nSymmetric: Matrix A is symmetric if \\(A = A^T\\)\nIdempotent: Matrix A is idempotent if \\(A^2 = A\\)\nTrace: The trace of a matrix is the sum of its diagonal components \\(Tr(A) = a_{11} + a_{22} + \\dots + a_{mn}\\)\n\nExample of symmetric matrix:\n\\(D = \\begin{pmatrix} 1 & 6 & 22 \\\\ 6 & 4 & 7 \\\\ 22 & 7 & 11 \\end{pmatrix}\\)\n\n## Look at the equivalence\nD &lt;- rbind(c(1,6,22), c(6,4,7), c(22,7,11))\nD\n\n     [,1] [,2] [,3]\n[1,]    1    6   22\n[2,]    6    4    7\n[3,]   22    7   11\n\nt(D)\n\n     [,1] [,2] [,3]\n[1,]    1    6   22\n[2,]    6    4    7\n[3,]   22    7   11\n\n\nWhat is the trace of this matrix?\n\n## diag() pulls out the diagonal of a matrix\nsum(diag(D))\n\n[1] 16\n\n\n\n\n3.5.3 Matrix Rules\nDue to conformability and other considerations, matrix operations are somewhat more restrictive, particularly when it comes to commutativity.\n\nAssociative \\((A + B) + C = A + (B + C)\\) and \\((AB) C = A(BC)\\)\nCommutative \\(A + B = B + A\\)\nDistributive \\(A(B + C) = AB + AC\\) and \\((A + B) C = AC + BC\\)\nCommutative law for multiplication does not hold– the order of multiplication matters: $ AB BA$\n\nRules for Inverses and Transposes\nThese rules will be helpful for simplifying expressions. Treat \\(A\\), \\(B\\), and \\(C\\) as matrices below, and \\(s\\) as a scalar.\n\n\\((A + B)^T = A^T + B^T\\)\n\\((s A)^T\\) \\(= s A^T\\)\n\\((AB)^T = B^T A^T\\)\n\\((A^T)^T = A\\) and \\(( A^{-1})^{-1} = A\\)\n\\((A^T)^{-1} = (A^{-1})^T\\)\n\\((AB)^{-1} = B^{-1} A^{-1}\\)\n\\((ABCD)^{-1} = D^{-1} C^{-1} B^{-1} A^{-1}\\)\n\n\n\n3.5.4 Derivatives with Matrices and Vectors\nLet’s say we have a \\(p \\times 1\\) “column” vector \\(\\mathbf{x}\\) and another \\(p \\times 1\\) vector \\(\\mathbf{a}\\).\nTaking the derivative with respect to vector \\(\\mathbf{x}\\).\nLet’s say we have \\(y = \\mathbf{x}'\\mathbf{a}\\). This process is explained here. Taking the derivative of this is called the gradient.\n\n\\(\\frac{\\delta y}{\\delta x} = \\begin{pmatrix}\\frac{\\delta y}{\\delta x_1} \\\\ \\frac{\\delta y}{\\delta x_2} \\\\ \\vdots \\\\ \\frac{dy}{dx_p} \\end{pmatrix}\\)\n\\(y\\) will have dimensions \\(1 \\times 1\\). \\(y\\) is a scalar.\n\nNote: \\(y = a_1x_1 + a_2x_2 + ... + a_px_p\\). From this expression, we can take a set of “partial derivatives”:\n\\(\\frac{\\delta y}{\\delta x_1} = a_1\\)\n\\(\\frac{\\delta y}{\\delta x_2} = a_2\\), and so on\n\\(\\frac{\\delta y}{\\delta x} = \\begin{pmatrix}\\frac{\\delta y}{\\delta x_1} \\\\ \\frac{\\delta y}{\\delta x_2} \\\\ \\vdots \\\\ \\frac{\\delta y}{\\delta x_p} \\end{pmatrix} = \\begin{pmatrix} a_1 \\\\ a_2 \\\\ \\vdots \\\\ a_p \\end{pmatrix}\\)\n\nWell, this is just vector \\(\\mathbf{a}\\)\n\nAnswer: \\(\\frac{\\delta }{\\delta x} \\mathbf{x}^T\\mathbf{a} = \\mathbf{a}\\). We can apply this general rule in other situations.\nExample 2\nLet’s say we want to differentiate the following where vector \\(\\mathbf{y}\\) is \\(n \\times 1\\), \\(X\\) is \\(n \\times k\\), and \\(\\mathbf{b}\\) is \\(k \\times 1\\). Take the derivative with respect to \\(b\\).\n\n\\(\\mathbf{y}'\\mathbf{y} - 2\\mathbf{b}'X'\\mathbf{y}\\)\nNote that the dimensions of the output are \\(1 \\times 1\\), a scalar quantity.\n\nRemember the derivative of a sum is the sum of derivatives. This allows us to focus on particular terms.\n\nThe first term has no \\(\\mathbf{b}\\) in it, so this will contribute 0.\nThe second term is \\(2\\mathbf{b}'X'\\mathbf{y}\\). We can think about this like the previous example\n\n\\(\\frac{\\delta }{\\delta b} 2\\mathbf{b}'X'\\mathbf{y} = \\begin{pmatrix} \\frac{\\delta }{\\delta b_1}\\\\ \\frac{\\delta }{\\delta b_2} \\\\ \\vdots \\\\ \\frac{\\delta }{\\delta b_k} \\end{pmatrix}\\)\nThe output is needs to be \\(k \\times 1\\) like \\(\\mathbf{b}\\), which is what \\(2 * X'\\mathbf{y}\\) is.\n\nThe derivative is \\(-2X'\\mathbf{y}\\)\n\nExample 3\nAnother useful rule when a matrix \\(A\\) is symmetric: \\(\\frac{\\delta}{\\delta \\mathbf{x}} \\mathbf{x}^TA\\mathbf{x} = (A + A^T)\\mathbf{x} = 2A\\mathbf{x}\\).\nDetails on getting to this result. We are treating the vector \\(\\mathbf{x}\\) as \\(n \\times 1\\) and the matrix \\(A\\) as symmetric.\nWhen we take \\(\\frac{\\delta}{\\delta \\mathbf{x}}\\) (the derivative with respect to \\(\\mathbf{x}\\)), we will be looking for a result with the same dimensions \\(\\mathbf{x}\\).\n\\(\\frac{\\delta }{\\delta \\mathbf{x}} = \\begin{pmatrix}\\frac{\\delta }{\\delta x_1} \\\\ \\frac{\\delta }{\\delta x_2} \\\\ \\vdots \\\\ \\frac{d}{dx_n} \\end{pmatrix}\\)\nLet’s inspect the dimensions of \\(\\mathbf{x}^TA\\mathbf{x}\\). They are \\(1 \\times 1\\). If we perform this matrix multiplication, we would be multiplying:\n\\(\\begin{pmatrix} x_1 & x_2 & \\ldots & x_i \\end{pmatrix} \\times \\begin{pmatrix} a_{11} & a_{12} & \\ldots & a_{1j} \\\\ a_{21} & a_{22} & \\ldots & a_{2j} \\\\ \\vdots & \\vdots & \\vdots & \\vdots \\\\ a_{i1} & a_{i2} & \\ldots & a_{ij} \\end{pmatrix} \\times \\begin{pmatrix}x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_j \\end{pmatrix}\\)\nTo simplify things, let’s say we have the following matrices, where \\(A\\) is symmetric:\n\\(\\begin{pmatrix} x_1 & x_2 \\end{pmatrix} \\times \\begin{pmatrix} a_{11} & a_{12} \\\\ a_{21} & a_{22} \\end{pmatrix} \\times \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix}\\)\nWe can perform the matrix multiplication for the first two quantities, which will result in a \\(1 \\times 2\\) vector. Recall in matrix multiplication we take the sum of the element-wise multiplication of the \\(ith\\) row of the first object by the \\(jth\\) column of the second object. This means multiply the first row of \\(\\mathbf{x}\\) by the first column of \\(A\\) for the entry in cell \\(i=1; j=1\\), and so on.\n\\(\\begin{pmatrix} (x_1a_{11} + x_2a_{21}) & (x_1a_{12} + x_2a_{22}) \\end{pmatrix}\\)\nWe can then multiply this quantity by the last quantity\n\\(\\begin{pmatrix} (x_1a_{11} + x_2a_{21}) & (x_1a_{12} + x_2a_{22}) \\end{pmatrix} \\times \\begin{pmatrix}x_1 \\\\ x_2 \\end{pmatrix}\\)\nThis will results in the \\(1 \\times 1\\) quantity: \\((x_1a_{11} + x_2a_{21})x_1 + (x_1a_{12} + x_2a_{22})x_2 = x_1^2a_{11} + x_2a_{21}x_1 + x_1a_{12}x_2 + x_2^2a_{22}\\)\nWe can now take the derivatives with respect to \\(\\mathbf{x}\\). Because \\(\\mathbf{x}\\) is \\(2 \\times 1\\), our derivative will be a vector of the same dimensions with components:\n\\(\\frac{\\delta }{\\delta \\mathbf{x}} = \\begin{pmatrix}\\frac{\\delta }{\\delta x_1} \\\\ \\frac{\\delta }{\\delta x_2} \\end{pmatrix}\\)\nThese represent the partial derivatives of each component within \\(\\mathbf{x}\\)\nLet’s focus on the first: \\(\\frac{\\delta }{\\delta x_1}\\).\n\\[\\begin{align*}\n\\frac{\\delta }{\\delta x_1} x_1^2a_{11} + x_2a_{21}x_1 +   x_1a_{12}x_2 + x_2^2a_{22} &=\\\\\n&= 2x_1a_{11} + x_2a_{21} + a_{12}x_2\\\\\n\\end{align*}\\]\nWe can repeat this for \\(\\frac{\\delta }{\\delta x_2}\\)\n\\[\\begin{align*}\n\\frac{\\delta }{\\delta x_2} x_1^2a_{11} + x_2a_{21}x_1 +   x_1a_{12}x_2 + x_2^2a_{22} &=\\\\\n&= a_{21}x_1  + x_1a_{12} + 2x_2a_{22}\\\\\n\\end{align*}\\]\nNow we can put the result back into our vector format:\n\\(\\begin{pmatrix}\\frac{\\delta }{\\delta x_1} = 2x_1a_{11} + x_2a_{21} + a_{12}x_2\\\\ \\frac{\\delta }{\\delta x_2} = a_{21}x_1 + x_1a_{12} + 2x_2a_{22}\\end{pmatrix}\\)\nNow it’s just about simplifying to show that we have indeed come back to the rule.\nRecall that for a symmetric matrix, the elements in rows and columns \\(ij\\) = the elements in \\(ji\\). This allows us to read \\(a_{21} = a_{12}\\) and combine those terms (e.g., \\(x_2a_{21} + a_{12}x_2 =2a_{12}x_2\\)) :\n\\(\\begin{pmatrix}\\frac{\\delta }{\\delta x_1} = 2x_1a_{11} + 2a_{12}x_2\\\\ \\frac{\\delta }{\\delta x_2} = 2a_{21}x_1 + 2x_2a_{22}\\end{pmatrix}\\)\nSecond, we can now bring the 2 out front.\n\\(\\begin{pmatrix}\\frac{\\delta }{\\delta x_1} \\\\ \\frac{\\delta }{\\delta x_2} \\end{pmatrix} = 2 * \\begin{pmatrix} x_1a_{11} + a_{12}x_2\\\\ a_{21}x_1 + x_2a_{22}\\end{pmatrix}\\)\nFinally, let’s inspect this and show it is equivalent to this multiplication where we have a \\(2 \\times 2\\) \\(A\\) matrix multiplied by a \\(2 \\times 1\\) \\(\\mathbf x\\) vector. Minor note: Because any individual element of a vector is just a single quantity, we can change the order (e.g., \\(a_{11}*x_1\\) vs. \\(x_1*a_{11}\\)). We just can’t do that for full vectors or matrices\n\\(2A\\mathbf{x} = 2* \\begin{pmatrix} a_{11} & a_{12} \\\\ a_{21} & a_{22} \\end{pmatrix} \\times \\begin{pmatrix}x_1 \\\\ x_2 \\end{pmatrix} = 2 * \\begin{pmatrix} a_{11}x_1 + a_{12}x_2 \\\\ a_{21}x_1 + a_{22}x_2 \\end{pmatrix}\\)\nThe last quantity is the same as the previous step. That’s the rule!\nApplying the rules\n\nThis has a nice analogue to the derivative we’ve seen before \\(q^2 = 2*q\\).\nLet’s say we want to take the derivative of \\(\\mathbf{b}'X'X\\mathbf{b}\\) with respect to \\(\\mathbf{b}\\).\nWe can think of \\(X'X\\) as if it is \\(A\\).\n\nThis gives us \\(2X'X\\mathbf{b}\\) as the result.\n\n\nWhy on earth would we care about this? For one, it helps us understand how we get to our estimates for \\(\\hat \\beta\\) in linear regression. When we have multiple variables, we don’t just want the best estimate for one coefficient, but a vector of coefficients. See more here.\nIn MLE, we will find the gradient of the log likelihood function. We will further go into the second derivatives to arrive at what is called the Hessian. More on that later."
  },
  {
    "objectID": "03-TheMATH.html#practice-problems",
    "href": "03-TheMATH.html#practice-problems",
    "title": "3  The Math",
    "section": "3.6 Practice Problems",
    "text": "3.6 Practice Problems\n\nWhat is \\(24/3 + 5^2 - (8 -4)\\)?\nWhat is \\(\\sum_{i = 1}^5 (i*3)\\)?\nTake the derivative of \\(f(x) =v(4x^2 + 6)^2\\) with respect to \\(x\\).\nTake the derivative of \\(f(x) = e^{2x + 3}\\) with respect to \\(x\\).\nTake the derivative of \\(f(x) = log (x + 3)^2\\) with respect to \\(x\\).\n\nGiven \\(X\\) is an \\(n\\) x \\(k\\) matrix,\n\n\\((X^{T}X)^{-1}X^{T}X\\) can be simplified to?\n\\(((X^{T}X)^{-1}X^{T})^{T} =\\) ?\nIf \\(\\nu\\) is a constant, how does \\((X^{T}X)^{-1}X^{T} \\nu X(X^{T}X)^{-1}\\) simplify?\nIf a matrix \\(P\\) is idempotent, \\(PP =\\) ?\n\n\n3.6.1 Practice Problem Solutions\nBelow are the solutions for the above problems. Remember, try on your own first, then\n\n\nClick for the solution\n\n\nWhat is \\(24/3 + 5^2 - (8 -4)\\)?\n\n\n24/3 + 5^2 - (8 -4)\n\n[1] 29\n\n\n\nWhat is \\(\\sum_{i = 1}^5 (i*3)\\)?\n\nBy hand: \\(1 \\times 3 + 2 \\times 3 + 3 \\times 3 + 4 \\times 3 + 5 \\times 3\\)\n\n\n\n## sol 1\n1*3 + 2*3 + 3*3 + 4*3 + 5*3\n\n[1] 45\n\n## sol 2\ni &lt;- 1:5\nsum(i*3)\n\n[1] 45\n\n\n\nTake the derivative of \\(f(x) =v(4x^2 + 6)^2\\) with respect to \\(x\\).\n\nWe can treat \\(v\\) as a number.\n\n\n\\[\\begin{align*}\nf'(x) &= 2* v(4x^2 + 6) * 8x\\\\\n&= 16vx(4x^2 + 6)\n\\end{align*}\\]\n\nTake the derivative of \\(f(x) = e^{2x + 3}\\) with respect to \\(x\\).\n\n\\[\\begin{align*}\nf'(x) &= 2* e^{2x + 3}\\\\\n&= 2e^{2x + 3}\n\\end{align*}\\]\n\nTake the derivative of \\(f(x) = log (x + 3)^2\\) with respect to \\(x\\).\n\nNote we can re-write this as \\(2 * log (x + 3)\\).\n\n\n\\[\\begin{align*}\nf'(x) &= 2 * \\frac{1}{(x + 3)} * 1\\\\\n&= \\frac{2}{(x + 3)}\n\\end{align*}\\] If we didn’t take that simplifying step, we can still solve:\n\\[\\begin{align*}\n      f'(x) &= \\frac{1}{(x + 3)^2} * 2 * (x + 3) *1\\\\\n      &= \\frac{2}{(x + 3)}\n      \\end{align*}\\]\nGiven \\(X\\) is an \\(n\\) x \\(k\\) matrix,\n\n\\((X^{T}X)^{-1}X^{T}X\\) can be simplified to?\n\n\\(I_k\\) the identity matrix\n\n\\(((X^{T}X)^{-1}X^{T})^{T} =\\) ?\n\nRecall our rule \\((AB)^T = B^TA^T\\)\n\\(X(X^TX)^{-1}\\)\n\nIf \\(\\nu\\) is a constant, how does \\((X^{T}X)^{-1}X^{T} \\nu X(X^{T}X)^{-1}\\) simplify?\n\nWe can pull it out front.\n\n\n\\[\\begin{align*}\n    &= \\nu(X^{T}X)^{-1}X^{T}X(X^{T}X)^{-1} \\\\\n    &= \\nu (X^{T}X)^{-1}\n    \\end{align*}\\]\n\nIf a matrix \\(P\\) is idempotent, \\(PP =\\) ?\n\n\\(P\\) from section 3.5.2"
  },
  {
    "objectID": "04-ReviewofOLS.html#introducing-ols-regression",
    "href": "04-ReviewofOLS.html#introducing-ols-regression",
    "title": "4  Review of OLS",
    "section": "4.1 Introducing OLS Regression",
    "text": "4.1 Introducing OLS Regression\nThe regression method describes how one variable depends on one or more other variables. Ordinary Least Squares regression is a linear model with the matrix representation:\n\\(Y = \\alpha + X\\beta + \\epsilon\\)\nGiven values of variables in \\(X\\), the model predicts the average of an outcome variable \\(Y\\). For example, if \\(Y\\) is a measure of how wealthy a country is, \\(X\\) may contain measures related to the country’s natural resources and/or features of its institutions (things that we think might contribute to how wealthy a country is.) In this equation:\n\n\\(Y\\) is the outcome variable (\\(n \\times 1\\)).1\n\\(\\alpha\\) is a parameter representing the intercept\n\\(\\beta\\) is a parameter representing the slope/marginal effect (\\(k \\times 1\\)), and\n\\(\\epsilon\\) is the error term (\\(n \\times 1\\)).\n\nIn OLS, we estimate a line of best fit to predict \\(\\hat{Y}\\) values for different values of X:\n\n\\(\\hat{Y} = \\hat{\\alpha} + X\\hat{\\beta}\\).\nWhen you see a “\\(\\hat{hat}\\)” on top of a letter, that means it is an estimate of a parameter.\nAs we will see in the next section, in multiple regression, sometimes this equation is represented as just \\(\\hat{Y} = X\\hat{\\beta}\\), where this generally means that \\(X\\) is a matrix that includes several variables and \\(\\hat \\beta\\) is a vector that includes several coefficients, including a coefficient representing the intercept \\(\\hat \\alpha\\)\n\nWe interpret linear regression coefficients as describing how a dependent variable is expected to change when a particular independent variable changes by a certain amount. Specifically:\n\n“Associated with each one unit increase in a variable \\(x_1\\), there is a \\(\\hat{\\beta_1}\\) estimated expected average increase in \\(y\\).”\nIf we have more than one explanatory variable (i.e., a multiple regression), we add the phrase “controlling on/ holding constant other observed factors included in the model.”\n\nWe can think of the interpretation of a coefficient in multiple regression using an analogy to a set of light switches:\n\nWe ask: How much does the light in the room change when we flip one switch, while holding constant the position of all the other switches?\nThis would be a good place to review the Wheelan chapter and Gelman and Hill 3.1 and 3.2 to reinforce what a regression is and how to interpret regression results."
  },
  {
    "objectID": "04-ReviewofOLS.html#diving-deeper-into-ols-matrix-representation",
    "href": "04-ReviewofOLS.html#diving-deeper-into-ols-matrix-representation",
    "title": "4  Review of OLS",
    "section": "4.2 Diving Deeper into OLS Matrix Representation",
    "text": "4.2 Diving Deeper into OLS Matrix Representation\nIn this section, we will review the matrix representation of the OLS regression in more detail and discuss how to derive the estimators for the regression coefficients.2\nOLS in Matrix Form: Let \\(X\\) be an \\(n \\times k\\) matrix where we have observations on k independent variables for n observations. Since our model will usually contain a constant term, one of the columns in the X matrix will contain only ones. This column should be treated exactly the same as any other column in the X matrix.\n\nLet \\(Y\\) be an \\(n \\times 1\\) vector of observations on the dependent variable. Note: because \\(Y\\) is a vector (a matrix with just one column), sometimes it is written in lowercase notation as \\(\\mathbf y\\).\nLet \\(\\epsilon\\) be an \\(n \\times 1\\) vector of disturbances or errors.\nLet \\(\\beta\\) be an \\(k \\times 1\\) vector of unknown population parameters that we want to estimate.\n\n\\(\\begin{pmatrix} y_1 \\\\ y_2 \\\\ y_3 \\\\ y_4 \\\\ ... \\\\ y_n \\end{pmatrix}\\) = \\(\\begin{pmatrix} 1 & x_{11} & x_{12} & x_{13} & ... & x_{1k}\\\\ 1 & x_{21} & x_{22} & x_{23} & ... & x_{2k} \\\\ 1 & x_{31} & x_{32} & x_{33} & ... & x_{3k}\\\\ 1 & x_{41} & x_{42} & x_{43} & ... & x_{4k} \\\\ ... & ... & ... & ... & ... & ... \\\\ 1 & x_{n1} & x_{n2} & x_{n3} & ... & x_{nk}\\end{pmatrix}\\) X \\(\\begin{pmatrix} \\alpha \\\\ \\beta_1 \\\\ \\beta_2 \\\\ \\beta_3 \\\\ ... \\\\ \\beta_k \\end{pmatrix}\\) + \\(\\begin{pmatrix} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\epsilon_3 \\\\ \\epsilon_4 \\\\ ... \\\\ \\epsilon_n \\end{pmatrix}\\)\nOur estimates are then \\(\\mathbf{ \\hat y} = X\\hat \\beta\\). What are the dimensions of this quantity?\nGelman and Hill Section 3.4, pg. 38 provides a nice visual of how this representation maps onto what a typical dataset may look like, where we will try to estimate a set of coefficients that map the relationship between the columns of \\(X\\) and \\(\\mathbf y\\):\n\\\nThis is a good place to review Gelman and Hill 3.4 on different notations for representing the regression model.\n\n4.2.1 Estimating the Coefficients\nModels generally start with some goal. In OLS, our goal is to minimize the sum of squared “residuals.” Here is a video I created to explain why we can represent this as \\(\\mathbf{e'}\\mathbf{e}\\).\n\nNote: at the end of the video it should read \\(X\\hat\\beta\\), not \\(\\hat X \\beta\\)\nWhat is a residual? It’s the difference between y and our estimate of y: \\(y - \\hat y\\). It represents the error in our prediction– how far off our estimate is of the outcome.\nWe can write this in matrix notation in the following way where \\(\\mathbf e\\) is an \\(n \\times 1\\) vector of residuals– a residual for each observation in the data:\n\\[\\begin{align*}\n\\mathbf{e'}\\mathbf{e} &= (Y' - \\hat{\\beta}'X')(Y - X\\hat{\\beta})\\\\\n&=Y'Y - \\hat{\\beta}'X'Y - Y'X\\hat{\\beta} + \\hat{\\beta}'X'X\\hat{\\beta} \\\\\n&= Y'Y - 2\\hat{\\beta}'X'Y + \\hat{\\beta}'X'X\\hat{\\beta}\n\\end{align*}\\]\nRecall we want a line that minimizes this quantity. We minimize the sum of squared residuals by taking the derivative with respect to \\(\\beta\\). (We want to identify the coefficients that help us achieve the goal of minimizing the squared error.) Because we are now deriving an estimate, we will use the hat over \\(\\beta\\):\n\n\\(\\frac{\\delta }{\\delta \\hat \\beta} = -2X'Y + 2X'X\\hat{\\beta}\\)\nSo what is our estimate for \\(\\hat{\\beta}\\)? We take first order conditions\n\n\\[\\begin{align*}\n  0 &=-2X'Y + 2X'X\\hat{\\beta}\\\\\n   \\hat{\\beta} &= (X'X)^{-1}X'Y\n   \\end{align*}\\]\nYou may wonder how we got to these answers. Don’t worry, you will get your chance to solve this! The important thing to note for now, is that we have an analytic solution to our coefficient estimates."
  },
  {
    "objectID": "04-ReviewofOLS.html#ols-regression-in-r",
    "href": "04-ReviewofOLS.html#ols-regression-in-r",
    "title": "4  Review of OLS",
    "section": "4.3 OLS Regression in R",
    "text": "4.3 OLS Regression in R\nTo run a linear regression in R, we use the lm() function.\nThe syntax is lm(y ~ x1, data = mydata) for a regression with y as the name of your dependent variable and there is one explanatory variable x1 where mydata is the name of your data frame.\nlm(y ~ x1 + x2 , data = mydata) is the syntax for a regression with two explanatory variables x1 and x2, where you would add additional variables for larger multivariate regressions. By default, R will include an intercept term in the regression.\n\n4.3.1 Example: Predicting Current Election Votes from Past Election Votes\nIn the American presidential election in 2000, there was an actual controversy in how ballots were cast in the state of Florida. Social scientists used data comparing the election results from 1996 in the state with 2000 as one way to help detect irregularities in the 2000 vote count. For more information on the background of this example, you can watch this video.\nWe will use the data florida.csv available here:\n\n## Load Data\nflorida &lt;- read.csv(\"https://raw.githubusercontent.com/ktmccabe/teachingdata/main/florida.csv\")\n\nThis data set includes several variables described below, where each row represents the voting information for a particular county in Florida.\n\n\n\nName\nDescription\n\n\n\n\ncounty\ncounty name\n\n\nClinton96\nClinton’s votes in 1996\n\n\nDole96\nDole’s votes in 1996\n\n\nPerot96\nPerot’s votes in 1996\n\n\nBush00\nBush’s votes in 2000\n\n\nGore00\nGore’s votes in 2000\n\n\nBuchanan00\nBuchanan’s votes in 2000\n\n\n\nIn 2000, Buchanan was a third party candidate, similar to Perot in 1996. One might think that counties where Perot received a lot of votes in 1996 should also receive a lot in 2000. That is: with a one-vote increase in Perot’s vote, we might expect an average increase in Buchanan’s 2000 vote.\nWe can translate that language into a regression equation:\n\n\\(Buchanan2000 = \\alpha + Perot1996 * \\beta + \\epsilon\\)\n\nIn R, we run this regression the following way. We will save it as an object fit.1. You can name your regression objects anything you want.\n\nfit.1 &lt;- lm(Buchanan00 ~ Perot96, data = florida)\n\n\nsummary(model) provides the summary statistics of the model. In particular, the following statistics are important\n\nEstimate: point estimate of each coefficient\nStd. Error: standard error of each estimate\nt value: indicates the \\(t\\)-statistic of each coefficient under the null hypothesis that it equals zero\nPr(&gt;|t|): indicates the two-sided \\(p\\)-value corresponding to this \\(t\\)-statistic where asterisks indicate the level of statistical significance.\nMultiple R-squared: The coefficient of determination\nAdjusted R-squared: The coefficient of determination adjusting for the degrees of freedom\n\n\nWe will say more to define these quantities in future sections.\n\nsummary(fit.1)\n\n\nCall:\nlm(formula = Buchanan00 ~ Perot96, data = florida)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-612.74  -65.96    1.94   32.88 2301.66 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.34575   49.75931   0.027    0.979    \nPerot96      0.03592    0.00434   8.275 9.47e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 316.4 on 65 degrees of freedom\nMultiple R-squared:  0.513, Adjusted R-squared:  0.5055 \nF-statistic: 68.48 on 1 and 65 DF,  p-value: 9.474e-12\n\n\nR also allows several shortcuts for accessing particular elements of your regression results. Examples:\n\n## Vector of the coefficient estimates only\ncoef(fit.1)\n\n(Intercept)     Perot96 \n 1.34575212  0.03591504 \n\n## Compute confidence intervals for these coefficients\nconfint(fit.1)\n\n                   2.5 %       97.5 %\n(Intercept) -98.03044506 100.72194929\nPerot96       0.02724733   0.04458275\n\n## Table of coefficient results only\nsummary(fit.1)$coefficients\n\n              Estimate   Std. Error    t value     Pr(&gt;|t|)\n(Intercept) 1.34575212 49.759306434 0.02704523 9.785065e-01\nPerot96     0.03591504  0.004340068 8.27522567 9.473505e-12\n\n## Extract standard errors only\nsummary(fit.1)$coefficients[,2]\n\n (Intercept)      Perot96 \n49.759306434  0.004340068 \n\n## Variance-Covariance matrix\nvcov(fit.1)\n\n             (Intercept)       Perot96\n(Intercept) 2475.9885768 -1.360074e-01\nPerot96       -0.1360074  1.883619e-05\n\n## Note that the square root of the diagonal of this matrix provides the standard errors\nsqrt(diag(vcov(fit.1)))\n\n (Intercept)      Perot96 \n49.759306434  0.004340068 \n\n## Degrees of freedom\nfit.1$df.residual\n\n[1] 65\n\n\n\n\n4.3.2 Plotting Regression Results\nWe often don’t want to hide our data under a bushel basket or in complicated regression models. Instead, we might also want to visualize data in R. The function plot() and the function ggplot() from the package ggplot2 are two terrific and flexible functions for visualizing data. We will use the plot() function to visualize the relationship between Perot and Buchanan votes. The example below provides a few arguments you can use within each of these functions, but they are capable of much more.\nAt the core, plotting functions generally work as coordinate systems. You tell R specifically at which x and y coordinates you want your points to be located (e.g., by providing R with a vector of x values and a vector of y values). Then, each function has its own way of allowing you to add bells and whistles to your figure, such as labels (e.g., main, xlab, ylab), point styles (pch), additional lines and points and text (e.g., abline(), lines(), points(), text()), or x and y scales for the dimensions of your axes (e.g., xlim, ylim). You can create a plot without these additional features, but most of the time, you will add them to make your plots look good! and be informative! We will do a lot of plotting this semester.\nNote: feel free to use plot() or ggplot() or both. ggplot has similar capabilities as plot but relies on a different “grammar” of graphics. For example, see the subtle differences in the two plots below.\n\n## Plot\nplot(x = florida$Perot96, # x-values\n     y = florida$Buchanan00, # y-values\n     main = \"Perot and Buchanan Votes\", # label for main title\n     ylab = \"Buchanan Votes\", # y-axis label\n     xlab = \"Perot Votes\", # x-axis label\n     pch = 20) # point type\nabline(fit.1, col = \"red\") # adds a red regression line\n\n\n\n\n\n## ggplot version\nlibrary(ggplot2)\nggplot(data = florida, # which data frame\n       mapping = aes(x = Perot96, y = Buchanan00)) + # x and y coordinates\n  geom_point() +  # tells R we want a scatterplot\n  geom_smooth(method = \"lm\",\n              se = FALSE, colour = \"red\",\n              data = florida, aes(x=Perot96, y=Buchanan00)) + # adds lm regression line\n  ggtitle(\"Perot and Buchanan Votes\") + # main title\n  labs(x = \"Perot Votes\", y = \"Buchanan Votes\") + # x and y labels\n  theme_bw() # changes theme (e.g., color of background)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n## Note: data = florida, aes(x=Perot96, y=Buchanan00) in the geom_smooth line is not necessary if it is the same mapping at the first line. Required if data are different\n\nTip: you might want to save your plots as .pdf or .png after you create it. You can do this straight from your R code. How you do it varies by function. The files will save to your working directory unless you specify a different file path. The code below is the same as above except it has additional lines for saving the plots:\n\n## Plot\npdf(file = \"myfirstmleplot.pdf\", width = 7, height = 5) # play around with the dimensions\nplot(x = florida$Perot96, # x-values\n     y = florida$Buchanan00, # y-values\n     main = \"Perot and Buchanan Votes\", # label for main title\n     ylab = \"Buchanan Votes\", # y-axis label\n     xlab = \"Perot Votes\", # x-axis label\n     pch = 20) # point type\nabline(fit.1, col = \"red\") # adds a red regression line\ndev.off() # this closes your pdf file\n\n## ggplot version\nggplot(data = florida, # which data frame\n       mapping = aes(x = Perot96, y = Buchanan00)) + # x and y coordinates\n  geom_point() +  # tells R we want a scatterplot\n  geom_smooth(method = \"lm\",  \n              se = FALSE, colour = \"red\",\n              data = florida, aes(x=Perot96, y=Buchanan00)) + # adds lm regression line\n  ggtitle(\"Perot and Buchanan Votes\") + # main title\n  labs(x = \"Perot Votes\", y = \"Buchanan Votes\") + # x and y labels\n  theme(plot.title = element_text(hjust = 0.5)) +# centers the title\n  theme_bw() # changes theme (e.g., color of background)\nggsave(\"myfirstmleggplot.png\", device=\"png\", width = 7, height = 5) # saves the last ggplot\n\n\n\n4.3.3 Finding Coefficients without lm\nLet’s put our matrix algebra and R knowledge together. In the previous section, we found that \\(\\hat \\beta = (X'X)^{-1}X'Y\\). If we do that math directly in R, there is no need to use lm() to find those coefficients.\nTo do so, we need \\(X\\) and \\(Y\\).\nRecall \\(Y\\) is an \\(n \\times 1\\) vector representing the outcome of our model. In this case, \\(Y\\) is Buchanan00.\n\nY &lt;- florida$Buchanan00\n\nRecall, \\(X\\) is a \\(n \\times k\\) matrix representing our independent variables and a column of 1’s for the intercept. Let’s build this matrix using cbind which was introduced in section 2.\n\nX &lt;- cbind(1, florida$Perot96)\ndim(X)\n\n[1] 67  2\n\n\nGreat, now we have \\(X\\) and \\(Y\\), so it’s just about a little math. Because \\(Y\\) is a vector, let’s make sure R knows to treat it like an \\(n \\times 1\\) matrix.\n\nY &lt;- cbind(Y)\ndim(Y)\n\n[1] 67  1\n\n\nRecall the solve() and t() functions take the inverse and transpose of matrices.\n\nbetahat &lt;- solve(t(X) %*% X) %*% t(X) %*% Y\n\nFinally, let’s compare the results from our model using lm() with these results.\n\nbetahat\ncoef(fit.1)\n\n              Y\n[1,] 1.34575212\n[2,] 0.03591504\n(Intercept)     Perot96 \n 1.34575212  0.03591504 \n\n\nWe did it! In the problem set, you will get more experience using the analytic solutions to solve for quantities of interest instead of the built-in functions.\n\n\n4.3.4 OLS Practice Problems\nHere are a couple of (ungraded) problems to modify the code above and gain additional practice with data wrangling and visualization in R. As you might have noticed in the example, there is a big outlier in the data. We will see how this observation affects the results.\n\nUsing a linear regression examine the relationship between Perot and Buchanan votes, controlling for Bill Clinton’s 1996 votes.\n\n\nProvide a one sentence summary of the relationship between Perot and Buchanan’s votes.\nIs the relationship significant at the \\(p &lt; 0.05\\) level? What about the relationship between Clinton and Buchanan votes?\nWhat are the confidence intervals for the Perot coefficient results?\nWhat is the residual for the estimate for Palm Beach County– PalmBeach in the county variable?\n\n\nLet’s go back to the bivariate case.\n\n\nSubset the data to remove the county PalmBeach.\nCreate a scatterplot of the relationship between Perot votes and Buchanan votes within this subset. This time make the points blue.\nAdd a regression line based on this subset of data.\nAdd a second regression line in a different color based on the initial bivariate regression we ran in the example, where all data were included.\nDescribe the differences in the regression lines.\n\n\n\n4.3.5 Code for solutions\n\nfit.multiple &lt;- lm(Buchanan00 ~ Perot96 + Clinton96, data = florida)\nsummary(fit.multiple)\n\n\nCall:\nlm(formula = Buchanan00 ~ Perot96 + Clinton96, data = florida)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-705.06  -49.17   -4.71   27.34 2254.89 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept) 14.110353  51.644141   0.273  0.78556   \nPerot96      0.027394   0.010095   2.714  0.00854 **\nClinton96    0.001283   0.001372   0.935  0.35325   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 316.7 on 64 degrees of freedom\nMultiple R-squared:  0.5196,    Adjusted R-squared:  0.5046 \nF-statistic: 34.61 on 2 and 64 DF,  p-value: 6.477e-11\n\nconfint(fit.multiple)[2,]\n\n      2.5 %      97.5 % \n0.007228254 0.047560638 \n\nflorida$res &lt;- residuals(fit.multiple)\nflorida$res[florida$county == \"PalmBeach\"]\n\n[1] 2254.893\n\nflorida.pb &lt;- subset(florida, subset = (county != \"PalmBeach\"))\nfit2 &lt;- lm(Buchanan00 ~ Perot96, data = florida.pb)\n\nggplot(data = florida.pb, # which data frame\n       mapping = aes(x = Perot96, y = Buchanan00)) + # x and y coordinates\n  geom_point(color=\"blue\") +  # tells R we want a scatterplot\n  geom_smooth(method = \"lm\", \n              se = FALSE, colour = \"green\",\n              data = florida.pb, aes(x=Perot96, y=Buchanan00)) + # adds lm regression line\n    geom_smooth(method = \"lm\", \n              se = FALSE, colour = \"red\",\n              data = florida, aes(x=Perot96, y=Buchanan00)) + # adds lm regression line\n  ggtitle(\"Perot and Buchanan Votes\") + # main title\n  labs(x = \"Perot Votes\", y = \"Buchanan Votes\") + # x and y labels\n  theme(plot.title = element_text(hjust = 0.5)) +# centers the title\n  theme_bw() # changes theme (e.g., color of background)\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "04-ReviewofOLS.html#extra-practice-building-and-breaking-regression",
    "href": "04-ReviewofOLS.html#extra-practice-building-and-breaking-regression",
    "title": "4  Review of OLS",
    "section": "4.4 Extra practice building and breaking regression",
    "text": "4.4 Extra practice building and breaking regression\nBelow is additional practice with regression and R, showing how to work with different variables and diagnosing errors.\nWhat is the association between race and income?\nLet’s say we want to explore the relationship between race and income, where the people in our sample take on the values white, Black, Asian, and Hispanic for race. We can write this as:\n\\(Income_i = \\alpha + \\beta*race_i + \\epsilon\\)\nHowever, race is not a numeric variable. This complicates our regression equation because what exactly is a 1-unit change in race? Sure, we could assign numeric values to each racial category in our data (e.g., white = 1, Black = 2, Hispanic = 3, Asian = 4), but we would have no reason to assume that the change in income would be linear as you change in race by units. Why should the difference in income between white and Black individuals be estimated as the same difference between Black and Hispanic individuals?\nIn a linear regression, when you have categorical independent variables, what should you typically do?\n\n4.4.1 Categorical variables\nLet’s build some data\nBuild a matrix with dummy variables for each race\nRun the code below and see what is in X.\n\n## Dummy variable example\nresprace &lt;- c(\"white\", \"white\", \"asian\", \"black\", \"hispanic\",\n              \"hispanic\", \"hispanic\", \"asian\", \"white\", \"black\", \n              \"black\", \"black\", \"asian\", \"asian\", \"white\", \"white\")\n\n## \"Dummy variables\"\nwhite &lt;- rep(0, length(resprace))\nwhite[resprace == \"white\"] &lt;- 1\nasian &lt;- rep(0, length(resprace))\nasian[resprace == \"asian\"] &lt;- 1\nblack &lt;- rep(0, length(resprace))\nblack[resprace == \"black\"] &lt;- 1\nhispanic &lt;- rep(0, length(resprace))\nhispanic[resprace == \"hispanic\"] &lt;- 1\n\n## Matrix\nX &lt;- cbind(white, asian, black, hispanic)\nX\n\n      white asian black hispanic\n [1,]     1     0     0        0\n [2,]     1     0     0        0\n [3,]     0     1     0        0\n [4,]     0     0     1        0\n [5,]     0     0     0        1\n [6,]     0     0     0        1\n [7,]     0     0     0        1\n [8,]     0     1     0        0\n [9,]     1     0     0        0\n[10,]     0     0     1        0\n[11,]     0     0     1        0\n[12,]     0     0     1        0\n[13,]     0     1     0        0\n[14,]     0     1     0        0\n[15,]     1     0     0        0\n[16,]     1     0     0        0\n\n\nLet’s build toward a regression model\nLet’s create a Y variable representing our outcome for income. Let’s also add an intercept to our X matrix. Take a look into our new X.\n\n## Dependent variable\nY &lt;- cbind(c(10, 11, 9, 8, 9, 7, 7, 13, 12, 11, 8, 7, 4, 13, 8, 7))\n\nX &lt;- cbind(1, X)\n\nLet’s now apply the formula \\((X'X)^{-1}X'Y\\) to estimate our coefficients.\nAssign the output to an object called betahat\n\nbetahat &lt;- solve(t(X) %*% X) %*% t(X) %*% Y\nbetahat\n\n\n\n4.4.2 Formalizing Linear Dependence\nWhy were our dummy variables linear dependent?\nIf we inspect X we can see that taking each column \\(\\mathbf{x_1} - \\mathbf{x_2} - \\mathbf{x_3}-\\mathbf{x_4}=0\\). There is a linear relationship between the variables.\nFormally, a set of vectors (e.g.,\\(\\mathbf{x_1}, \\mathbf{x_2}, ...\\mathbf{x_k}\\)) is linearly independent if the equation \\(\\mathbf{x_1}*a_1 + \\mathbf{x_2}*a_2 +... + \\mathbf{x_k}*a_k= 0\\) only in the trivial case where \\(a_1\\) and \\(a_2\\) through \\(a_k\\) are 0. A set of vectors has a linearly dependent relation if there is a solution \\(\\mathbf{x_1}*a_1 + \\mathbf{x_2}*a_2 +... + \\mathbf{x_k}*a_k = 0\\) where not all \\(a_1, a_2\\) through \\(a_k\\) are 0.\nFor OLS, we must assume no perfect collinearity.\n\nNo independent variable is constant\nNo exactly linear relationships among the independent variables\nThe rank of X is \\(k\\) where the rank of a matrix is the maximum number of linearly independent columns.\n\nAs discussed in the course notes, a square matrix is only invertible if its columns are linearly independent. In OLS, in order to estimate unique solutions for \\(\\hat \\beta\\), we need to invert \\((X'X)^{-1}\\). When we have perfect collinearity, we cannot do this.\nNote the linear dependence in X\n\n## Matrix\nX[, 1] - X[, 2] - X[, 3] - X[, 4] - X[,5]\n\n [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n\n\nTry to take \\((X'X)^{-1}\\)\n\nsolve(t(X) %*% X)\n\nHow do we correct this?\nTo address this, we are going to drop one of the categorical variables when we run the regression. Consequently, our coefficients will now be interpreted as differences between this reference category (the category left out, e.g., white) and the particular group (e.g., white vs. Asian or white vs. Black or white vs. Hispanic).\nRedefine X removing the white column, and calculate \\((X'X)^{-1}X'Y\\)\n\nX &lt;- cbind(1, asian, black, hispanic)\nbetahat &lt;- solve(t(X) %*% X) %*% t(X) %*% Y\nbetahat \n\n              [,1]\n          9.600000\nasian     0.150000\nblack    -1.100000\nhispanic -1.933333\n\n\nCheck this with the lm() function\n\nsummary(lm(Y ~ asian + black + hispanic))\n\n\nCall:\nlm(formula = Y ~ asian + black + hispanic)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.7500 -0.9375 -0.5000  1.6500  3.2500 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    9.600      1.181   8.132 3.18e-06 ***\nasian          0.150      1.771   0.085    0.934    \nblack         -1.100      1.771  -0.621    0.546    \nhispanic      -1.933      1.928  -1.003    0.336    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.64 on 12 degrees of freedom\nMultiple R-squared:  0.1105,    Adjusted R-squared:  -0.1119 \nF-statistic: 0.4967 on 3 and 12 DF,  p-value: 0.6914\n\n\nIn R and many other statistical softwares, the regression function will forcibly drop one of your variables if it encounters this type of linear dependence. See below when we include all four race dummies in the model.\nCheck what happens with the lm() function\n\nsummary(lm(Y ~ white + asian + black + hispanic))\n\n\nCall:\nlm(formula = Y ~ white + asian + black + hispanic)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.7500 -0.9375 -0.5000  1.6500  3.2500 \n\nCoefficients: (1 not defined because of singularities)\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   7.6667     1.5240   5.031 0.000294 ***\nwhite         1.9333     1.9278   1.003 0.335711    \nasian         2.0833     2.0161   1.033 0.321820    \nblack         0.8333     2.0161   0.413 0.686650    \nhispanic          NA         NA      NA       NA    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.64 on 12 degrees of freedom\nMultiple R-squared:  0.1105,    Adjusted R-squared:  -0.1119 \nF-statistic: 0.4967 on 3 and 12 DF,  p-value: 0.6914\n\n\nAn alternative way to enter categorical variables in a regression is to let the function create the dummy variables for you using factor(var, levels = ) to make sure R knows it is a factor variables.\n\nresprace &lt;- factor(resprace, levels = c(\"white\", \"asian\", \"black\", \"hispanic\"))\nsummary(lm(Y ~ resprace))\n\n\nCall:\nlm(formula = Y ~ resprace)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.7500 -0.9375 -0.5000  1.6500  3.2500 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         9.600      1.181   8.132 3.18e-06 ***\nrespraceasian       0.150      1.771   0.085    0.934    \nrespraceblack      -1.100      1.771  -0.621    0.546    \nrespracehispanic   -1.933      1.928  -1.003    0.336    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.64 on 12 degrees of freedom\nMultiple R-squared:  0.1105,    Adjusted R-squared:  -0.1119 \nF-statistic: 0.4967 on 3 and 12 DF,  p-value: 0.6914\n\n\n\n\n4.4.3 Other examples of breaking the no perfect collinearity rule\nRecall, for OLS, we must assume no perfect collinearity.\n\nNo independent variable is constant\nNo exactly linear relationships among the independent variables\nThe rank of X is \\(k\\) where the rank of a matrix is the maximum number of linearly independent columns.\n\nLet’s say we wanted to control for age, but all of our sample was 18 years old. Let’s try to add this to the X matrix.\nRedefine X adding age column, and calculate \\((X'X)^{-1}X'Y\\)\n\nage &lt;- rep(18, length(resprace))\nX &lt;- cbind(1, asian, black, hispanic, age)\nbetahat &lt;- solve(t(X) %*% X) %*% t(X) %*% Y\nbetahat \n\nLet’s visit the Florida example. Let’s say we had two Perot96 variables– one the raw votes and one where votes were multiplied by 1000 to adjust the order of magnitude.\nRegress Buchanan’s votes on Perot and Perot adjusted values\n\nflorida &lt;- read.csv(\"https://raw.githubusercontent.com/ktmccabe/teachingdata/main/florida.csv\")\n\nflorida$perotadjusted &lt;- florida$Perot96 * 1000\n\nY &lt;- florida$Buchanan00\nX &lt;- cbind(1, florida$Perot96, florida$perotadjusted)\nbetahat &lt;- solve(t(X) %*% X) %*% t(X) %*% Y\nbetahat"
  },
  {
    "objectID": "04-ReviewofOLS.html#uncertainty-and-regression",
    "href": "04-ReviewofOLS.html#uncertainty-and-regression",
    "title": "4  Review of OLS",
    "section": "4.5 Uncertainty and Regression",
    "text": "4.5 Uncertainty and Regression\nWe have now gone through the process of minimizing the sum of squared errors (\\(\\mathbf{e'e}\\)) and deriving estimates for the OLS coefficients \\(\\hat \\beta = (X'X)^{-1}X'Y\\). In this section, we will discuss how to generate estimates of the uncertainty around these estimates.\nWhere we are going:\n\nIn the last section, we visited an example related to the 2000 election in Florida. We regressed county returns for Buchanan in 2000 (Y) on county returns for Perot in 1996 (X).\n\n\n## Load Data\nflorida &lt;- read.csv(\"https://raw.githubusercontent.com/ktmccabe/teachingdata/main/florida.csv\")\nfit.1 &lt;- lm(Buchanan00 ~ Perot96, data = florida)\nsummary(fit.1)\n\n\nCall:\nlm(formula = Buchanan00 ~ Perot96, data = florida)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-612.74  -65.96    1.94   32.88 2301.66 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.34575   49.75931   0.027    0.979    \nPerot96      0.03592    0.00434   8.275 9.47e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 316.4 on 65 degrees of freedom\nMultiple R-squared:  0.513, Adjusted R-squared:  0.5055 \nF-statistic: 68.48 on 1 and 65 DF,  p-value: 9.474e-12\n\n\nThe summary output of the model shows many different quantities in addition to the coefficient estimates. In particular, in the second column of the summary, we see the standard errors of the coefficients. Like many statistical software programs, the lm() function neatly places these right next to the coefficients. We will now discuss how we get to these values.\n\n4.5.1 Variance of the Coefficients\nThe standard error is the square root of the variance, representing the typical deviation we would expect to see between our estimates \\(\\hat \\beta\\) of the parameter \\(\\beta\\) across repeated samples. So to get to the standard error, we just need to get to an estimate of the variance.\nLet’s take the journey. First the math. As should start becoming familiar, we have our initial regression equation, which describes the relationship between the independent variables and dependent variables.\n\nStart with the model: \\(Y = X\\beta + \\epsilon\\)\n\nWe want to generate uncertainty for our estimate of \\(\\hat \\beta =(X'X)^{-1}X'Y\\)\n\nNote: Conditional on fixed values of \\(X\\) (I say fixed values because this is our data. We know \\(X\\) from our dataset.), the only random component is \\(\\epsilon\\).\n\nWhat does that mean? Essentially, the random error term in our regression equation is what is giving us the uncertainty. If \\(Y\\) was a deterministic result of \\(X\\), we would have no need for it, but it’s not. The relationship is not exact, varies sample to sample, subject to random perturbations, represented by \\(\\epsilon\\).\n\n\nBelow we go through how to arrive at the mathematical quantity representing the variance of \\(\\hat \\beta\\) which we will notate as \\(\\mathbf{V}(\\hat\\beta)\\). The first part of the math below is just substituting terms:\n\\[\\begin{align*}\n\\mathbf{V}(\\widehat{\\beta}) &=\n\\mathbf{V}( (X^T X) ^{-1} X^T Y))  \\\\\n&= \\underbrace{\\mathbf{V}( (X^T X) ^{-1} X^T (X\\beta + \\epsilon))}_\\text{Sub in the expression for Y from above}  \\\\\n&= \\underbrace{\\mathbf{V}((X^T X) ^{-1} X^T X \\beta + (X^T X) ^{-1} X^T \\epsilon)}_\\text{Distribute the term to the items in the parentheses}  \\\\\n&= \\underbrace{\\mathbf{V}(\\beta + (X^T X) ^{-1} X^T \\epsilon)}_\\text{Using the rules of inverses, the two terms next to $\\beta$ canceled each other out}  \n\\end{align*}\\]\nThe next part of the math requires us to use knowledge of the definition of variance and the rules associated. We draw on two in particular:\n\nThe variance of a constant is zero.\nWhen you have a constant multipled by a random variable, e.g., \\(\\mathbf{V}(4d)\\), it can come out of the variance operator, but must be squared: \\(16\\mathbf{V}(d)\\)\nPutting these together: \\(\\mathbf{V}(2 + 4d)= 16\\mathbf{V}(d)\\)\n\nKnowing these rules, we can proceed: \\[\\begin{align*}\n\\mathbf{V}(\\widehat{\\beta}) &= \\mathbf{V}(\\beta + (X^T X) ^{-1} X^T \\epsilon) \\\\\n&=\\underbrace{ \\mathbf{V}((X^T X) ^{-1} X^T \\epsilon)}_\\text{$\\beta$ drops out because in a regression it is an unkown \"parameter\"-- it's constant, which means its variance is zero.}\\\\\n&= \\underbrace{(X^T X)^{-1}X^T \\mathbf{V}( \\epsilon) ((X^T X)^{-1}X^T)^T}_\\text{We can move $(X^T X)^{-1}X^T$ out front because our data are fixed quantities, but in doing so, we have to \"square\" the matrix.}\\\\\n&= (X^T X)^{-1}X^T \\mathbf{V}( \\epsilon) X (X^T X)^{-1}\n\\end{align*}\\]\nThe resulting quantity is our expression for the \\(\\mathbf{V}(\\hat \\beta)\\). However, in OLS, we make an additional assumption that allows us to further simplify the expression. We assume homoscedasticity aka “constant” or “equal error variance” which says that the variance of the errors are the same across observations: \\(\\mathbf{V}(\\epsilon) = \\sigma^2 I_n\\).\n\nIf we assume homoscedastic errors, then Var\\((\\epsilon) = \\sigma^2 I_n\\)\n\n\\[\\begin{align*}\n\\mathbf{V}(\\widehat{\\beta})  &= (X^T X)^{-1}X^T \\mathbf{V}( \\epsilon) X (X^T X)^{-1}\\\\\n&= \\underbrace{(X^T X)^{-1}X^T \\sigma^2I_n X (X^T X)^{-1}}_\\text{Assume homoskedasticity}\\\\\n&= \\underbrace{\\sigma^2(X^T X)^{-1} X^T X (X^T X)^{-1}}_\\text{Because it is a constant, we can move it out in front of the matrix multiplication, and then simplify the terms.}  \\\\\n&= \\sigma^2(X^T X)^{-1}\n\\end{align*}\\]\nAll done! This expression: \\(\\sigma^2(X^T X)^{-1}\\) represents the variance of our coefficient estimates. Note its dimensions: \\(k \\times k\\). It has the same number of rows and columns as the number of our independent variables (plus the intercept).\nThere is one catch, though. How do we know what \\(\\sigma^2\\) is? Well, we don’t. Just like the unknown parameter \\(\\beta\\), we have to estimate it in our regression model.\nJust like with the coefficients, we notate our estimate as \\(\\widehat{\\sigma}^2\\). Our estimate is based on the observed residual errors in the model and is as follows:\n\n\\(\\widehat{\\sigma}^2 = \\frac{1}{N-K}\\sum_{i=1}^N \\widehat{\\epsilon_i^2} = \\frac{1}{N-K} \\mathbf{e'e}\\)\n\nThat means our estimate of the variance of the coefficients is found within: \\(\\hat \\sigma^2(X^T X)^{-1}\\)\nAgain, this is a \\(k \\times k\\) matrix and is often called the variance covariance matrix. We can extract this quantity from our linear models in R using vcov().\n\nvcov(fit.1)\n\n             (Intercept)       Perot96\n(Intercept) 2475.9885768 -1.360074e-01\nPerot96       -0.1360074  1.883619e-05\n\n\nThis is the same that we would get if manually we took the residuals and multiplied it by our \\(X\\) matrix according to the formula above:\n\nX &lt;- cbind(1, florida$Perot96)\ne &lt;- cbind(residuals(fit.1))\nsigmahat &lt;- ((t(e) %*% e) / (nrow(florida) -2)) \n## tell r to stop treating sigmahat as a matrix\nsigmahat &lt;-as.numeric(sigmahat)\nXtX &lt;- solve(t(X) %*%X)\nsigmahat * XtX\n\n             [,1]          [,2]\n[1,] 2475.9885768 -1.360074e-01\n[2,]   -0.1360074  1.883619e-05\n\n\nThe terms on the diagonal represent the variance of a particular coefficient in the model.The standard error of a particular coefficient \\(k\\) is: s.e.(\\(\\hat{\\beta_k}) = \\sqrt{\\widehat{\\sigma}^2 (X'X)^{-1}}_{kk}\\). The off-diagonal components represent the covariances between the coefficients.\nRecall that the standard error is just the square root of the variance. So, to get the nice standard errors we saw in the summary output, we can take the square root of the quantities on the diagonal of this matrix.\n\nsqrt(diag(vcov(fit.1)))\n\n (Intercept)      Perot96 \n49.759306434  0.004340068 \n\nsummary(fit.1)$coefficients[,2]\n\n (Intercept)      Perot96 \n49.759306434  0.004340068 \n\n\nWhy should I care?\n\nWell R actually doesn’t make it that easy to extract standard errors from the summary output. You can see above that the code for extracting the standard errors using what we know about them being the square root of the variance is about as efficient as extracting the second column of the coefficient component of the summary of the model.\nSometimes, we may think that the assumption of equal error variance is not feasible and that we have unequal error variance or “heteroscedasticity.” Researchers have developed alternative expressions to model unequal error variance. Generally, what this means is they can no longer make that simplifying assumption, have to stop at the step with the uglier expression \\((X^T X)^{-1}X^T \\mathbf{V}( \\epsilon) X (X^T X)^{-1}\\) and then assume something different about the structure of the errors in order to estimate the coefficients. These alternative variance estimators are generally what are referred to as “robust standard errors.” There are many different robust estimators, and you will likely come across them in your research.\n\nSome of you may have learned the formal, general definition for variance as defined in terms of expected value: \\(\\mathbb{E}[(\\widehat{m} - \\mathbb{E}(\\widehat{m}))^2 ]\\). We could also start the derivation there. This is not required for the course, but it is below if you find it useful. In particular, it can help show why we wend up needing to square a term when we move it outside the variance operator:\n\\[\\begin{align*}\n\\mathbf{V}(\\widehat{\\beta}) &= \\mathbb{E}[(\\widehat{\\beta} - \\mathbb{E}(\\hat \\beta))^2)] \\\\\n&= \\mathbb{E}[(\\widehat{\\beta} - \\beta)^2]\\\\\n&= \\mathbb{E}[(\\widehat{\\beta} - \\beta)(\\widehat{\\beta} - \\beta)^T ] \\\\ &=\n\\mathbb{E}[(X^T X) ^{-1} X^TY - \\beta)(X^T X) ^{-1} X^TY - \\beta)^T]  \\\\\n&= \\mathbb{E}[(X^T X) ^{-1} X^T(X\\beta + \\epsilon) - \\beta)(X^T X) ^{-1} X^T(X\\beta + \\epsilon) - \\beta)^T]\\\\\n&=  \\mathbb{E}[(X^T X) ^{-1} X^TX\\beta + (X^T X) ^{-1} X^T\\epsilon - \\beta)(X^T X) ^{-1} X^TX\\beta + (X^T X) ^{-1} X^T\\epsilon - \\beta)^T]\\\\\n&=  \\mathbb{E}[(\\beta + (X^T X) ^{-1} X^T\\epsilon - \\beta)(\\beta + (X^T X) ^{-1} X^T\\epsilon - \\beta)^T]\\\\\n&=  \\mathbb{E}[(X^T X) ^{-1} X^T\\epsilon)(X^T X) ^{-1} X^T\\epsilon)^T]\\\\\n&= (X^T X) ^{-1} X^T\\mathbb{E}(\\epsilon\\epsilon^T)X(X^T X) ^{-1}\\\\\n&= \\underbrace{(X^T X) ^{-1} X^T\\sigma^2I_nX(X^T X) ^{-1}}_\\text{Assume homoskedasticity}\\\\\n&= \\sigma^2(X^T X) ^{-1} X^TX(X^T X) ^{-1}\\\\\n&= \\sigma^2(X^T X) ^{-1}\n\\end{align*}\\]\nNote: Along the way, in writing \\(\\mathbb{E}(\\hat \\beta) = \\beta\\), we have implicitly assumed that \\(\\hat \\beta\\) is an “unbiased” estimator of \\(\\beta\\). This is not free. It depends on an assumption that the error term in the regression \\(\\epsilon\\) is independent of our independent variables. This can be violated in some situations, such as when we have omitted variable bias, which is discussed at the end of our OLS section.\n\n\n4.5.2 Hypothesis Testing\nMost of the time in social science, we run a regression because we have some hypothesis about how a change in our independent variable affects the change in our outcome variable.\nIn OLS, we can perform a hypothesis test for each independent variable in our data. The structure of the hypothesis test is:\n\nNull hypothesis: \\(\\beta_k = 0\\)\n\nThis essentially means that we don’t expect a particular \\(x_k\\) independent variable to have a relationship with our outcome variable.\n\nAlternative hypothesis: \\(\\beta_k \\neq 0\\)\n\nWe do expect a positive or negative relationship between a particular \\(x_k\\) and the dependent variable.\n\n\nWe can use our estimates for \\(\\hat \\beta\\) coefficients and their standard errors to come to a conclusion about rejecting or failing to reject the null hypothesis of no relationship by using a t-test.\nIn a t-test, we take our coefficient estimates and divide them by the standard error in order to “standardize” them on a scale that we can use to determine how likely it is we would have observed a value for \\(\\hat \\beta\\) as extreme or more extreme as the one we observed in a world where the true \\(\\beta = 0\\). This is just like a t-test you might have encountered before for a difference in means between groups, except this time our estimate is \\(\\hat \\beta\\).\n\\[\\begin{align*}\nt_{\\hat \\beta_k} &= \\frac{\\hat \\beta_k}{s.e.(\\hat \\beta_k)}\n\\end{align*}\\]\nGenerally speaking, when \\(t\\) is about +/-2 or greater in magnitude, the coefficient will be “significant” at conventional levels (i.e., \\(p &lt;0.05\\)), meaning that we are saying that it is really unlikely we would have observed a value as big as \\(\\hat \\beta_k\\) if the null hypothesis were true. Therefore, we can reject the null hypothesis.\nHowever, to get a specific quantity, we need to calculate the p-value, which depends on the t-statistic and the degrees of freedom in the model. The degrees of freedom in a regression model are \\(N-k\\), the number of observations in the model minus the number of independent variables plus the intercept.\nIn R, we can calculate p-values using the pt() function. By default, most people use two-sided hypothesis tests for regression. So to do that, we are going to find the area on each side of the t values, or alternatively, multiply the area to the right of our positive t-value by 2.\n\n## Let's say t was 2.05 and \n## And there were 32 observations and 3 variables in the regression plus an intercept\nt &lt;- 2.05\ndf.t &lt;- 32 -4\np.value &lt;- 2 * (pt(abs(t), df=df.t, lower.tail = FALSE))\np.value\n\n[1] 0.04983394\n\n\nLet’s do this for the florida example. First, we can find t by dividing our coefficients by the standard errors.\n\nt &lt;- coef(fit.1) / (sqrt(diag(vcov(fit.1))))\nt \n\n(Intercept)     Perot96 \n 0.02704523  8.27522567 \n\n## Compare with output\nsummary(fit.1)$coefficients[, 3]\n\n(Intercept)     Perot96 \n 0.02704523  8.27522567 \n\n\nWe can then find the p-values.\n\nt &lt;- coef(fit.1) / (sqrt(diag(vcov(fit.1))))\ndf.t &lt;- fit.1$df.residual\np.value &lt;- 2 * (pt(abs(t), df=df.t, lower.tail = FALSE))\np.value\n\n (Intercept)      Perot96 \n9.785065e-01 9.473505e-12 \n\nsummary(fit.1)$coefficients[, 4]\n\n (Intercept)      Perot96 \n9.785065e-01 9.473505e-12 \n\n\nWe see that the coefficient for Perot96 is significant. The p-value is tiny. In R, for small numbers, R automatically shifts to scientific notation. The 9.47e-12 means the p-value is essentially zero, with the stars in the summary output indicating the p-value is \\(p &lt; 0.001\\). R will also output a test of the significance of the intercept using the same formula as all other coefficients. This generally does not have much interpretive value, so you are usually safe to ignore it.\nConfidence Intervals\nInstead of representing the significance using p-values, sometimes it is helpful to report confidence intervals around the coefficients. This can be particularly useful when visualizing the coefficients. The 95% confidence interval represents roughly 2 standard errors above and below the coefficient. The key thing to look for is whether it overlaps with zero (not significant) or does not (in which case the coefficient is significant).\nThe precise formula is\n\\(\\widehat{\\beta}_k\\) Confidence intervals: \\(\\widehat{\\beta}_k - t_{crit.value} \\times s.e._{\\widehat{\\beta}_k}, \\widehat{\\beta}_k + t_{crit.value} \\times s.e_{\\widehat{\\beta}_k}\\)\nIn R, we can use qt() to get the specific critical value associated with a 95% confidence interval. This will be around 2, but fluctuates depending on the degrees of freedom in your model (which are function of your sample size and how many variables you have in the model.) R also has a shortcut confint() function to extract the coefficients from the model. Below we do this for the Perot96 coefficient.\n\n## Critical values from t distribution at .95 level\nqt(.975, df = fit.1$df.residual) # n- k degrees of freedom\n\n[1] 1.997138\n\n## Shortcut\nconfint(fit.1)[2,]\n\n     2.5 %     97.5 % \n0.02724733 0.04458275 \n\n## By hand\ncoef(fit.1)[2] - qt(.975, df = fit.1$df.residual)*sqrt(diag(vcov(fit.1)))[2]\n\n   Perot96 \n0.02724733 \n\ncoef(fit.1)[2] + qt(.975, df = fit.1$df.residual)*sqrt(diag(vcov(fit.1)))[2]\n\n   Perot96 \n0.04458275 \n\n\n\n\n4.5.3 Goodness of Fit\nA last noteworthy component to the standard regression output is the goodness of fit statistics. For this class, we can put less attention on these, though there will be some analogues when we get into likelihood.\nThese are measures of how much of the total variation in our outcome measure can be explained by our model, as well as how far off are our estimates from the truth.\nFor the first two measures R-squared and Adjusted R-squared, we draw on three quantities:\n\nTotal Sum of Squares–how much variance in \\(Y_i\\) is there to explain?\n\n\\(TSS: \\sum_{i=1}^N (Y_i -\\overline Y_i)^2\\)\n\nEstimated Sum of Squares–how much of this variance do we explain?\n\n\\(ESS: \\sum_{i=1}^N (\\widehat Y_i -\\overline Y_i)^2\\)\n\nResidual Sum of Squares–how much variance is unexplained?\n\n\\(RSS: \\sum_{i=1}^N ( Y_i -\\widehat Y_i)^2\\)\n\n\\(TSS = ESS + RSS\\)\nMultiple R-squared: \\(\\frac{ESS}{TSS}\\)\n\nThis is a value from 0 to 1, representing the proportion of the variance in the outcome that can be explained by the model. Higher values are generally considered better, but there are many factors that can affect R-squared values. In most social science tasks where the goal is to engage in hypothesis testing of coefficients, this measure is of less value.\n\nAdjusted R-squared: \\(1 - \\frac{\\frac{RSS}{n - k}}{\\frac{TSS}{n - 1}}\\)\n\nThis is essentially a penalized version of R-squared. When you add additional predictors to a model, the R-squared value can never decrease, even if the predictors are useless. The Adjusted R-squared adds a consideration for the degrees of freedom into the equation, creating a penalty for adding more and more predictors.\n\nResidual standard error aka root mean squared error aka square root of the mean squared residual: \\(r.s.e = \\sqrt{\\frac{RSS}{n-k}}\\)\n\nThis represents the typical deviation of an estimate of the outcome from the actual outcome. This quantity is often used to assess the quality of prediction exercises. It is used less often in social science tasks where the goal is hypothesis testing of the relationship between one or more independent variables and the outcome.\n\n\nF-Statistic\nSo far we have conducted hypothesis tests for each individual coefficient. We can also conduct a global hypothesis test, where the null hypothesis is that all coefficients are zero, with the alternative being that at least one coefficient is nonzero. This is the test represented by the F-statistic in the regression output.\nThe F-statistic helps us test the null hypothesis that all of the regression slopes are 0: \\(H_0 = \\beta_1 = \\beta_2 = \\dots = \\beta_k = 0\\)\n\n\\(F_0 = \\frac{ESS/(k - 1)}{RSS/(n - k)}\\)\nThe F-Statistic has two separate degrees of freedom.\n\nThe model sum of squares degrees of freedom (ESS) are \\(k - 1\\).\nThe residual error degrees of freedom (RSS) are \\(n - k\\).\nIn a regression output, the model degrees of freedom are generally the first presented: “F-statistic: 3.595 on \\((k - 1) = 1\\) and \\((n - k) = 48\\) DF.”\n\n\nNote: This test is different from our separate hypothesis tests that a \\(k\\) regression slope is 0. For that, we use the t-tests discussed above."
  },
  {
    "objectID": "04-ReviewofOLS.html#generating-predictions-from-regression-models",
    "href": "04-ReviewofOLS.html#generating-predictions-from-regression-models",
    "title": "4  Review of OLS",
    "section": "4.6 Generating predictions from regression models",
    "text": "4.6 Generating predictions from regression models\nThe regression coefficients tell us how much \\(Y\\) is expected to change for a one-unit change in \\(x_k\\). It does not immediately tell us the values we estimate our outcome (\\(\\hat Y\\)) to take conditional on particular values of \\(x_k\\). While often knowing our independent variables have a significant effect on the outcome and the size of the coefficient is sufficient for testing our hypotheses, it can be helpful for interpretation’s sake, to see the estimated values for the outcome. This is going to be particularly important once we get into models like logistic regression, where the coefficients won’t be immediately interpretable.\nRecall that our equation for estimating values of our outcomes is:\n\n\\(\\hat Y = X\\hat \\beta\\) This can also be written out in long form for any particular observation \\(i\\):\n\\(\\hat y_i = \\hat \\alpha + \\hat \\beta_1*x_1i + \\hat \\beta_2*x_2i + ... \\hat\\beta_k*x_ki\\)\n\nThe estimated values of our regression \\(\\hat Y\\) are often called the “fitted values.” In R, you can identify the estimated values for each observation using the fitted() command.\n\n## Y hat for the first observation in the data\nfitted(fit.1)[1]\n\n      1 \n291.252 \n\n\nAgain, this is just the multiplication of the matrix \\(X\\) and \\(\\hat \\beta\\). If we have already run a regression model in R, one shortcut for getting the \\(X\\) matrix, is to use the model.matrix command. We can get \\(\\hat \\beta\\) using the coef() command.\n\nX &lt;- model.matrix(fit.1)\nhead(X) # head() shows about the first six values of an object\n\n  (Intercept) Perot96\n1           1    8072\n2           1     667\n3           1    5922\n4           1     819\n5           1   25249\n6           1   38964\n\nbetahat &lt;- coef(fit.1)\n\nOur fitted values are then just\n\nyhat &lt;- X %*% betahat\nhead(yhat)\n\n        [,1]\n1  291.25196\n2   25.30108\n3  214.03462\n4   30.76017\n5  908.16461\n6 1400.73939\n\n\nIf I want to generate an estimate for any particular observation, I could also just extract its specific value for Perot96.\n\nflorida$Perot96[1]\n\n[1] 8072\n\n\nLet’s estimate the Buchanan 2000 votes for the first county in the data with Perot 96 votes of 8072. We can write it out as \\(\\hat Buchanan00_1 =\\hat \\alpha + \\hat \\beta*Perot96_1\\)\n\nbuch00hat &lt;- coef(fit.1)[1] + coef(fit.1)[2]*florida$Perot96[1]\nbuch00hat\n\n(Intercept) \n    291.252 \n\n\nWhat is useful about this is that now we have the coefficient estimates, we can apply them to any values of \\(X\\) we wish in order to generate estimates/predictions of the values \\(Y\\) will take given particular values of our independent variables.\nOne function that is useful for this (as a shortcut) is the predict(fit, newdata=newdataframe) function in R. It allows you to enter in “newdata”– meaning values of the \\(X\\) variables for which you want to generate estimates of \\(Y\\) based on the coefficient estimates of your regression model.\nFor example, let’s repeat the calculation from above for Perot96 = 8072.\n\npredict(fit.1, newdata = data.frame(Perot96 = 8072))\n\n      1 \n291.252 \n\n\nWe can also generate confidence intervals around these estimates by adding interval = \"confidence\" in the command.\n\npredict(fit.1, newdata = data.frame(Perot96 = 8072), interval=\"confidence\")\n\n      fit      lwr      upr\n1 291.252 213.7075 368.7964\n\n\nWe can also simultaneously generate multiple predictions by supplying a vector of values in the predict() command. For example, let’s see the estimated Buchanan votes for when the Perot 1996 votes took values of 1000 to 10,000 by intervals of 1,000.\n\npredict(fit.1, newdata = data.frame(Perot96 = c(1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000, 9000, 10000)))\n\n        1         2         3         4         5         6         7         8 \n 37.26079  73.17583 109.09087 145.00591 180.92095 216.83599 252.75104 288.66608 \n        9        10 \n324.58112 360.49616 \n\n\nThe important thing to note about the predict() command is that if you have multiple independent variables, you have to specify the values you want each of them to take when generating the estimated values of y.\n\nfit.2 &lt;- lm(Buchanan00 ~ Perot96 + Clinton96, data = florida)\n\nFor example, let’s build a second model with Clinton96 as an additional predictor. In order to generate the same prediction for different values of Perot 1996 votes, we need to tell R at what values we should “hold constant” Clinton96. I.e., we want to see how hypothetical changes in Perot96 votes influence changes in Buchanan 2000 votes while also leaving the Clinton votes identical. This is that lightswitch metaphor– flipping one switch, while keeping the rest untouched.\nThere are two common approaches to doing this. 1) We can hold constant Clinton96 votes at its mean value in the data 2) We can keep Clinton96 at its observed values in the data. In linear regression, it’s not going to matter which approach you take. In other models we will talk about later, this distinction may matter more substantially because of how our quantities of interest change across different values of \\(X\\hat \\beta\\).\nThe first approach is easily implemented in predict.\n\npredict(fit.2, newdata = data.frame(Perot96 = 8072, Clinton96 = mean(florida$Clinton96)))\n\n      1 \n283.997 \n\n\nFor the second approach, what we will do is generate an estimate for Buchanan’s votes in 2000 when Perot96 takes 8072 votes, and we keep Clinton96’s votes at whatever value it currently is in the data. That is, we will generate \\(n\\) estimates for Buchanan’s votes when Perot takes 8072. Then, we will take the mean of this as our “average estimate” of Buchanan’s votes in 2000 based on Perot’s votes at a level of 8072. We can do this in one of two ways:\n\n## Manipulating the X matrix\nX &lt;- model.matrix(fit.2)\n## Replace Perot96 column with all 8072 values\nX[, \"Perot96\"] &lt;- 8072\nhead(X) #take a peek\n\n  (Intercept) Perot96 Clinton96\n1           1    8072     40144\n2           1    8072      2273\n3           1    8072     17020\n4           1    8072      3356\n5           1    8072     80416\n6           1    8072    320736\n\n## Generate yhat\nyhats &lt;-X %*% coef(fit.2)\n\n## take the mean\nmean(yhats)\n\n[1] 283.997\n\n\n\n## Use predict\nyhats &lt;- predict(fit.2, newdata = data.frame(Perot96=8072, Clinton96=florida$Clinton96))\nmean(yhats)\n\n[1] 283.997\n\n\nNow often, after we generate these predicted values, we want to display them for the whole world to see. You will get a chance to visualize values like this using the plotting functions in the problem sets. We have already seen one example of this in the simple bivariate case, when R plotted the bivariate regression line in section 4.3.2. However, the predict function extends are capabilities to plot very specific values of \\(X\\) and \\(\\hat Y\\) for bivariate or multiple regressions.\n\nThe predict() function is also very relevant when we move to logistic, probit, etc. regressions. This is just the start of a beautiful friendship between you and predict() and associated functions."
  },
  {
    "objectID": "04-ReviewofOLS.html#wrapping-up-ols",
    "href": "04-ReviewofOLS.html#wrapping-up-ols",
    "title": "4  Review of OLS",
    "section": "4.7 Wrapping up OLS",
    "text": "4.7 Wrapping up OLS\nLinear regression is a great way to explain the relationship between one or more independent variables and an outcome variables. However, there is no free lunch. We have already mentioned a couple of assumptions along the way. Below we will summarize these and other assumptions. These are things you should be mindful of when you use linear regression in your own work. Some conditions that generate violations of these assumptions can also motivate why we will seek out alternative methods, such as those that rely on maximum likelihood estimation.\nThis is a good place to review Gelman section 3.6.\n\nExogeneity. This one we haven’t discussed yet, but is an important assumption for letting us interpret our coefficients \\(\\hat \\beta\\) as “unbiased” estimates of the true parameters \\(\\beta\\). We assume that the random error in the regression model \\(\\epsilon\\) is indeed random, and uncorrelated with and independent of our independent variables \\(X\\). Formally:\n\n\\(\\mathbb{E}(\\epsilon| X) = \\mathbb{E}(\\epsilon) = 0\\).\nThis can be violated, for example, when we suffer from Omitted Variable Bias due to having an “endogenous explanatory variable” that is correlated with some unobserved or unaccounted for factor. This bias comes from a situation where there is some variable that we have left out of the model (\\(Z\\)), and is therefore a part of the unobserved error term. Moreover this variable which is correlated with–and a pre-cursor of– our independent variables and is a cause of our dependent variable. A failure to account for omitted variables can create bias in our coefficient estimates. Concerns about omitted variable bias often prompt people to raise their hands in seminars and ask questions like, “Well have you accounted for this? Have you accounted for that? How do you know it is \\(X\\) driving your results and not \\(Z\\)?” If we omit important covariates, we may wrongly attribute an effect to \\(X\\) when it was really the result of our omitted factor \\(Z\\). Messing discusses this here.\nThis is a really tough assumption. The only real way to guarantee the independence of your error term and the independent variables is if you have randomly assigned values to the independent variables (such as what you do when you randomly assign people to different treatment conditions in an experiment). Beyond random assignment, you have to rely on theory to understand what variables you need to account for in the regression model to be able to plausibly claim your estimate of the relationship between a given independent variable and the dependent variable is unbiased. Failing to control for important factors can lead to misleading results, such as what happens in Simpson’s paradox, referenced in the Messing piece.\nDanger Note 1: The danger here, though, is that the motivation for avoiding omitted variable bias might be to keep adding control after control after control into the regression model. However, model building in this way can sometimes be atheoretical and result in arbitrary fluctuations in the size of your coefficients and their significance. At its worse, it can lead to “p-hacking” where researchers keep changing their models until they find the results they like. The Lenz and Sahn article on Canvas talks more about the dangers of arbitrarily adding controls to the model.\nDanger Note 2: We also want to avoid adding “bad controls” to the model. Messing talks about this in the medium article as it relates to collider bias. We want to avoid adding controls to our model, say \\(W\\) that are actually causes of \\(Y\\) and causes of \\(X\\) instead of the other way around.\nModel building is a delicate enterprise that depends a lot on having a solid theory that guides the choice of variables.\n\nHomoscedasticity. We saw this when defining the variance estimator for the OLS coefficients. We assume constant error variance. This can be violated when we think observations at certain values of our independent variables may have different magnitudes of error than observations at other values of our independent variables.\n\nNo correlation in the errors. The error terms are not correlated with each other. This can be violated in time series models (where we might think past, present, and future errors are correlated) or in cases where our observations are nested in some hierarchical structures (e.g., perhaps students in a school) and the errors are correlated.\n\nNo perfect collinearity. The \\(X\\) matrix must be full rank: We cannot have linear dependence between columns in our X matrix. We saw this in the tutorial when we tried to add the dummy variables for all of our racial groups into a regression at once. When there is perfect collinearity between variables, our regression will fail.\n\nWe should also avoid situations where we have severe multicollinearity. This can happen when we include two or more variables in a regression model that are highly correlated (just not perfectly correlated). While the regression will still run in this case, it can inflate the standard errors of the coefficients, making it harder to detect significant effects. This is particularly problematic in smaller samples.\n\nLinearity. The relationship between the independent and dependent variables needs to be linear in the parameters. It should be modeled as the addition of constants or parameters multiplied by the independent variables. If instead the model requires the multiplication of parameters, this is no longer linear (e.g., \\(\\beta^2\\)). Linearity also often refers to the shape of the model. Our coefficients tell us how much change we expect in the outcome for each one-unit change in an independent variable. We might think some relationships are nonlinear– meaning this rate of change varies across values of the independent variables. If that is the case, we need to shift the way our model is specified to account for this or change modeling approaches.\n\nFor example, perhaps as people get older (one-unit changes in age), they become more politically engaged, but at some age level, their political engagement starts to decline. This would mean the slope (that expected change in political engagement for each one-unit change in age) is not constant across all levels of age. There, we might be violating linearity in the curvature of the relationship between the independent and dependent variables. This is sometimes why you might see \\(age^2\\) or other nonlinear terms in regression equations to better model this curvature.\nLikewise, perhaps each additional level of education doesn’t result in the same average increase in \\(y\\). If not, you could consider including categorical dummy variables for different levels of education instead of treating education as a numeric variable.\n\nNormality. We assume that the errors are normally distributed. As Gelman 3.6 notes, this is a less important assumption and is generally not required.\n\nOLS Properties\nWhy we like OLS. When we meet our assumptions, OLS produces the best linear unbiased estimates (BLUE). A discussion of this here. We have linearity in our parameters (e.g., \\(\\beta\\) and not \\(\\beta^2\\) for example). The unbiasedness means that the expected value (aka the average over repeated samples) of our estimates \\(\\mathbb{E}(\\hat \\beta)= \\beta\\) is the true value. Our estimates are also efficient, which has to do with the variance, not only are our estimates true in expectation, but we also have lower variance than an alternative linear unbiased estimator could get us. If our assumptions fail, then we might no longer have BLUE. OLS estimates are also consistent, meaning that as the sample gets larger and larger, the estimates start converging to the truth.\nNow, a final hidden assumption in all of this is that the sample of our data is representative of the population we are trying to make inferences about. If that is not the case, then we may no longer be making unbiased observations to that population level. Further adjustments may be required (e.g., analyses of survey data sometimes use weights to adjust estimates to be more representative).\nWhen we violate these assumptions, OLS may no longer be best, and we may opt for other approaches. More soon!\n\n4.7.1 Practice Problems\n\nLet’s use the florida data. Run a regression according to the following formula:\n\n\\(Buchanan00_i = \\alpha + \\beta_1*Perot96_i + \\beta_2*Dole96 + \\beta_3*Gore00 + \\epsilon\\)\n\nReport the coefficient for Perot96. What do you conclude about the null hypothesis that there is no relationship between 1996 Perot votes and 2000 Buchanan votes?\nWhat is the confidence interval for the Perot96 coefficient estimate?\nWhen Perot 1996 vote is 5500, what is the expected 2000 Buchanan vote?\n\n\n\n4.7.2 Practice Problem Code for Solutions\n\nfit.practice &lt;- lm(Buchanan00 ~ Perot96 + Dole96 + Gore00, data = florida)\n\ncoef(fit.practice)[\"Perot96\"]\n\n   Perot96 \n0.02878927 \n\nconfint(fit.practice)[\"Perot96\", ]\n\n      2.5 %      97.5 % \n0.004316382 0.053262150 \n\nexpbuch &lt;- model.matrix(fit.practice)\nexpbuch[,\"Perot96\"] &lt;- 5500\nmean(expbuch %*% as.matrix(coef(fit.practice)))\n\n[1] 211.1386"
  },
  {
    "objectID": "04-ReviewofOLS.html#week-2-example",
    "href": "04-ReviewofOLS.html#week-2-example",
    "title": "4  Review of OLS",
    "section": "4.8 Week 2 Example",
    "text": "4.8 Week 2 Example\nThis example is based on Dancygier, Rafaela; Egami, Naoki; Jamal, Amaney; Rischke, Ramona, 2020, “Hate Crimes and Gender Imbalances: Fears over Mate Competition and Violence against Refugees”, published in the American Journal of Political Science. Replication data is available here. We will draw on the survey portion of the article and replicate Table 1 in the paper. The pre-print is available here.\nThe abstract is: As the number of refugees rises across the world, anti-refugee violence has become a pressing concern. What explains the incidence and support of such hate crime? We argue that fears among native men that refugees pose a threat in the competition for female partners is a critical but understudied factor driving hate crime. Employing a comprehensive dataset on the incidence of hate crime across Germany, we first demonstrate that hate crime rises where men face disadvantages in local mating markets. Next, we complement this ecological evidence with original survey measures and confirm that individual-level support for hate crime increases when men fear that the inflow of refugees makes it more difficult to find female partners. Mate competition concerns remain a robust predictor even when controlling for antirefugee views, perceived job competition, general frustration, and aggressiveness. We conclude that a more complete understanding of hate crime and immigrant conflict must incorporate marriage markets and mate competition.\nThe authors summarize their hypotheses as, “the notion that male refugees are engaged in romantic relationships with German women has received considerable media attention from a variety of sources, with coverage ranging from the curious to the outright hostile. We argue that the prospect of refugee-native mate competition can trigger or compound resentment against refugees, including support for hate crime” pg. 14\n\nlibrary(foreign)\ndat_use &lt;- read.dta(\"https://github.com/ktmccabe/teachingdata/blob/main/dat_use.dta?raw=true\")\n\nThe data include wave 4 of an online survey fielded in Germany through Respondi from September 2016 to December 2017). Each wave was designed to be nationally representative on age (starting at 18), gender, and state (Bundesland) with a sample of about 3,000 respondents in each wave.\nKey variables include\n\nhate_violence_means representing respondents’ agreement or disagreement to the Only Means question: “When it comes to the refugee problem, violence is sometimes the only means that citizens have to get the attention of German politicians.” from (1) disagree strongly to (4) agree strongly.\nMateComp_cont, Mate Competition. “The inflow of refugees makes it more difficult for native men to find female partners.” from (1) disagree strongly to (4) agree strongly.\nThe data include several other variables related to the demographics of the respondents and measures representing potential alternative explanations, such as JobComp_cont (agreement with “the inflow of young male refugees makes it more difficult for young native men to find apprenticeships and jobs”) and LifeSatis_cont (0-10 scale, ranging from extremely dissatisfied to extremely satisfied).\n\nLet’s pause here to ask a few questions about research design.\n\nWhat is the outcome? What is the independent variable of interest?\n\nHow would we write out the bivariate regression model?\n\nWhy OLS? (e.g., why not experiment?)\nWhat types of alternative explanations might exist?\n\nOk let’s move to replication of the first two regression models in the table:\n\n\n\nTry to code these on your own, then click for the solution\n\n\nlm1 &lt;- lm(hate_violence_means ~ MateComp_cont, data=dat_use)\n\nlm2 &lt;- lm(hate_violence_means ~ MateComp_cont + JobComp_cont + LifeSatis_cont, data=dat_use)\n\n\nNow, let’s compare the summary output of each output.\n\n\nTry on your own, then click for the solution\n\n\nsummary(lm1)\n\n\nCall:\nlm(formula = hate_violence_means ~ MateComp_cont, data = dat_use)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.6804 -0.3694 -0.3694  0.6306  2.6306 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    0.93235    0.03302   28.24   &lt;2e-16 ***\nMateComp_cont  0.43702    0.01635   26.73   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7993 on 3017 degrees of freedom\nMultiple R-squared:  0.1915,    Adjusted R-squared:  0.1912 \nF-statistic: 714.6 on 1 and 3017 DF,  p-value: &lt; 2.2e-16\n\nsummary(lm2)\n\n\nCall:\nlm(formula = hate_violence_means ~ MateComp_cont + JobComp_cont + \n    LifeSatis_cont, data = dat_use)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.8275 -0.4783 -0.1842  0.3171  2.8452 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     0.788623   0.057849   13.63   &lt;2e-16 ***\nMateComp_cont   0.263437   0.020261   13.00   &lt;2e-16 ***\nJobComp_cont    0.249956   0.018672   13.39   &lt;2e-16 ***\nLifeSatis_cont -0.014725   0.006292   -2.34   0.0193 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7751 on 3015 degrees of freedom\nMultiple R-squared:  0.2403,    Adjusted R-squared:  0.2395 \nF-statistic: 317.9 on 3 and 3015 DF,  p-value: &lt; 2.2e-16\n\n\n\nQuestions about the output\n\nHow should we interpret the coefficients?\n\nDo they support the researchers’ hypotheses?\n\nHow would we extract confidence intervals from the coefficients?\nHow should we interpret the goodness of fit statistics at the bottom of the output?\n\nAdditional Models We can also run regressions with even more covariates, as the authors do in models 3-6 in the paper.\n\n\nClick to reveal regression code below.\n\n\nlm3 &lt;- lm(hate_violence_means ~ MateComp_cont + JobComp_cont + LifeSatis_cont + \n            factor(age_group) +     # age group\n            factor(gender) +     # gender \n            factor(state) +     # state  \n            factor(citizenship) +    # german citizen\n            factor(marital) +    # marital status\n            factor(religion) +    # religious affiliation\n            eduyrs +    # education\n            factor(occupation) +    # main activity\n            factor(income) +   # income\n            factor(household_size) +   # household size\n            factor(self_econ),    # subjective social status\n          data=dat_use)\n\nlm4 &lt;- lm(hate_violence_means ~ MateComp_cont + JobComp_cont + LifeSatis_cont + \n            factor(age_group) +   # age group\n            factor(gender) +   # gender \n            factor(state) +   # state  \n            factor(citizenship) +  # german citizen\n            factor(marital) +  # marital status\n            factor(religion) +  # religious affiliation\n            eduyrs +  # education\n            factor(occupation) +  # main activity\n            factor(income) + # income\n            factor(household_size) + # household size\n            factor(self_econ) + # subjective social status\n            factor(ref_integrating) + # Refugee Index (National-level; Q73) 8 in total\n            factor(ref_citizenship) + factor(ref_reduce) + factor(ref_moredone) + factor(ref_cultgiveup) + \n            factor(ref_economy) + factor(ref_crime) + factor(ref_terror),\n          data=dat_use)\n\nlm5 &lt;- lm(hate_violence_means ~ MateComp_cont + JobComp_cont + LifeSatis_cont + \n            factor(age_group) +      # age group\n            factor(gender) +      # gender \n            factor(state) +      # state  \n            factor(citizenship) +     # german citizen\n            factor(marital) +     # marital status\n            factor(religion) +     # religious affiliation\n            eduyrs + # education\n            factor(occupation) +     # main activity\n            factor(income) +    # income\n            factor(household_size) +    # household size\n            factor(self_econ) +    # subjective social status\n            factor(ref_integrating) + # Refugee Index (National-level; Q73) 8 in total\n            factor(ref_citizenship) + factor(ref_reduce) + factor(ref_moredone) + factor(ref_cultgiveup) + \n            factor(ref_economy) + factor(ref_crime) + factor(ref_terror) + \n            factor(ref_loc_services) +    # Refugee Index (Local, Q75)\n            factor(ref_loc_economy) + factor(ref_loc_crime) + factor(ref_loc_culture) + factor(ref_loc_islam) + \n            factor(ref_loc_schools) + factor(ref_loc_housing) + factor(ref_loc_wayoflife), ## end\n          data=dat_use)\n\nformula.5 &lt;- \n  as.character(\"hate_violence_means ~ MateComp_cont + JobComp_cont + \n               LifeSatis_cont +  factor(age_group) + factor(gender) + \n               factor(state) + factor(citizenship) + factor(marital) + \n               factor(religion) + eduyrs + factor(occupation) + \n               factor(income) + factor(household_size) + factor(self_econ) + \n               factor(ref_integrating) + factor(ref_citizenship) + factor(ref_reduce) + \n               factor(ref_moredone) + factor(ref_cultgiveup) + \n               factor(ref_economy) + factor(ref_crime) + factor(ref_terror)  + \n               factor(ref_loc_services) +  factor(ref_loc_economy) + factor(ref_loc_crime) + \n               factor(ref_loc_culture) + factor(ref_loc_islam) + \n               factor(ref_loc_schools) + factor(ref_loc_housing) + factor(ref_loc_wayoflife)\")\n\nformula.6 &lt;- paste(formula.5, \"factor(distance_ref) + factor(settle_ref)\", \n                   \"lrscale + afd + muslim_ind + afd_ind + contact_ind\", \n                   sep=\"+\", collapse=\"+\") \n\nlm6 &lt;- lm(as.formula(formula.6), data=dat_use)\n\n\n\n\nTable 1: Mate Competition Predicts Support for Hate Crime.\n\n\n\n\n \n\n\nModel 1\n\n\nModel 2\n\n\nModel 3\n\n\nModel 4\n\n\nModel 5\n\n\nModel 6\n\n\n\n\n\n\n(Intercept)\n\n\n0.9323***\n\n\n0.7886***\n\n\n1.3982***\n\n\n1.4437***\n\n\n1.4372***\n\n\n1.3878***\n\n\n\n\n \n\n\n(0.0330)\n\n\n(0.0578)\n\n\n(0.2293)\n\n\n(0.2296)\n\n\n(0.2388)\n\n\n(0.2372)\n\n\n\n\nMateComp_cont\n\n\n0.4370***\n\n\n0.2634***\n\n\n0.2361***\n\n\n0.2064***\n\n\n0.1848***\n\n\n0.1550***\n\n\n\n\n \n\n\n(0.0163)\n\n\n(0.0203)\n\n\n(0.0206)\n\n\n(0.0194)\n\n\n(0.0195)\n\n\n(0.0189)\n\n\n\n\nJobComp_cont\n\n\n \n\n\n0.2500***\n\n\n0.2358***\n\n\n0.0772***\n\n\n0.0650***\n\n\n0.0559***\n\n\n\n\n \n\n\n \n\n\n(0.0187)\n\n\n(0.0189)\n\n\n(0.0195)\n\n\n(0.0196)\n\n\n(0.0189)\n\n\n\n\nLifeSatis_cont\n\n\n \n\n\n-0.0147**\n\n\n-0.0136*\n\n\n-0.0034\n\n\n-0.0020\n\n\n-0.0001\n\n\n\n\n \n\n\n \n\n\n(0.0063)\n\n\n(0.0070)\n\n\n(0.0065)\n\n\n(0.0065)\n\n\n(0.0062)\n\n\n\n\nfactor(age_group)30-39\n\n\n \n\n\n \n\n\n-0.1323**\n\n\n-0.1800***\n\n\n-0.1821***\n\n\n-0.1957***\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.0525)\n\n\n(0.0489)\n\n\n(0.0488)\n\n\n(0.0471)\n\n\n\n\nfactor(age_group)40-49\n\n\n \n\n\n \n\n\n-0.2088***\n\n\n-0.2771***\n\n\n-0.2709***\n\n\n-0.2808***\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.0525)\n\n\n(0.0490)\n\n\n(0.0490)\n\n\n(0.0474)\n\n\n\n\nfactor(age_group)50-59\n\n\n \n\n\n \n\n\n-0.2876***\n\n\n-0.3621***\n\n\n-0.3480***\n\n\n-0.3580***\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.0535)\n\n\n(0.0501)\n\n\n(0.0502)\n\n\n(0.0486)\n\n\n\n\nfactor(age_group)60 and older\n\n\n \n\n\n \n\n\n-0.3362***\n\n\n-0.3427***\n\n\n-0.3199***\n\n\n-0.3073***\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.0678)\n\n\n(0.0631)\n\n\n(0.0631)\n\n\n(0.0610)\n\n\n\n\nfactor(gender)Female\n\n\n \n\n\n \n\n\n-0.0247\n\n\n-0.0528*\n\n\n-0.0451\n\n\n-0.0233\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.0299)\n\n\n(0.0281)\n\n\n(0.0282)\n\n\n(0.0272)\n\n\n\n\nfactor(state)Bayern\n\n\n \n\n\n \n\n\n0.0097\n\n\n-0.0168\n\n\n-0.0148\n\n\n-0.0229\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.0531)\n\n\n(0.0494)\n\n\n(0.0491)\n\n\n(0.0474)\n\n\n\n\nfactor(state)Berlin\n\n\n \n\n\n \n\n\n0.0106\n\n\n-0.0023\n\n\n-0.0259\n\n\n0.0037\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.0776)\n\n\n(0.0722)\n\n\n(0.0720)\n\n\n(0.0706)\n\n\n\n\nfactor(state)Brandenburg\n\n\n \n\n\n \n\n\n-0.1572*\n\n\n-0.1023\n\n\n-0.0949\n\n\n-0.1082\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.0896)\n\n\n(0.0833)\n\n\n(0.0834)\n\n\n(0.0805)\n\n\n\n\nfactor(state)Bremen\n\n\n \n\n\n \n\n\n-0.1266\n\n\n-0.1252\n\n\n-0.1750\n\n\n-0.0508\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.1531)\n\n\n(0.1423)\n\n\n(0.1415)\n\n\n(0.1365)\n\n\n\n\nfactor(state)Hamburg\n\n\n \n\n\n \n\n\n-0.0208\n\n\n-0.0140\n\n\n-0.0255\n\n\n-0.0269\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.1016)\n\n\n(0.0946)\n\n\n(0.0941)\n\n\n(0.0914)\n\n\n\n\nfactor(state)Hessen\n\n\n \n\n\n \n\n\n-0.1207*\n\n\n-0.0931\n\n\n-0.0766\n\n\n-0.0853\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.0647)\n\n\n(0.0604)\n\n\n(0.0601)\n\n\n(0.0578)\n\n\n\n\nfactor(state)Mecklenburg-Vorpommern\n\n\n \n\n\n \n\n\n-0.0849\n\n\n-0.1008\n\n\n-0.1015\n\n\n-0.1572*\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.1035)\n\n\n(0.0961)\n\n\n(0.0959)\n\n\n(0.0928)\n\n\n\n\nfactor(state)Niedersachsen\n\n\n \n\n\n \n\n\n-0.0993\n\n\n-0.1052*\n\n\n-0.1055*\n\n\n-0.1190**\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.0607)\n\n\n(0.0564)\n\n\n(0.0561)\n\n\n(0.0543)\n\n\n\n\nfactor(state)Nordrhein-Westfalen\n\n\n \n\n\n \n\n\n-0.0299\n\n\n-0.0277\n\n\n-0.0414\n\n\n-0.0414\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.0501)\n\n\n(0.0465)\n\n\n(0.0465)\n\n\n(0.0450)\n\n\n\n\nfactor(state)Rheinland-Pfalz\n\n\n \n\n\n \n\n\n-0.1178\n\n\n-0.1137\n\n\n-0.1089\n\n\n-0.1407**\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.0750)\n\n\n(0.0700)\n\n\n(0.0697)\n\n\n(0.0675)\n\n\n\n\nfactor(state)Saarland\n\n\n \n\n\n \n\n\n-0.0264\n\n\n0.0227\n\n\n0.0353\n\n\n-0.0250\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.1293)\n\n\n(0.1203)\n\n\n(0.1199)\n\n\n(0.1162)\n\n\n\n\nfactor(state)Sachsen\n\n\n \n\n\n \n\n\n-0.0357\n\n\n-0.0813\n\n\n-0.1118\n\n\n-0.1470**\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.0734)\n\n\n(0.0683)\n\n\n(0.0684)\n\n\n(0.0662)\n\n\n\n\nfactor(state)Sachsen-Anhalt\n\n\n \n\n\n \n\n\n-0.0193\n\n\n-0.0811\n\n\n-0.0765\n\n\n-0.1024\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.0927)\n\n\n(0.0862)\n\n\n(0.0863)\n\n\n(0.0836)\n\n\n\n\nfactor(state)Schleswig-Holstein\n\n\n \n\n\n \n\n\n-0.2402***\n\n\n-0.1693**\n\n\n-0.1725**\n\n\n-0.1839**\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.0862)\n\n\n(0.0806)\n\n\n(0.0802)\n\n\n(0.0773)\n\n\n\n\nfactor(state)Thringen\n\n\n \n\n\n \n\n\n0.0090\n\n\n-0.0076\n\n\n-0.0081\n\n\n-0.0654\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.0957)\n\n\n(0.0889)\n\n\n(0.0887)\n\n\n(0.0858)\n\n\n\n\nfactor(citizenship)1\n\n\n \n\n\n \n\n\n-0.0621\n\n\n-0.0831\n\n\n-0.0739\n\n\n-0.0314\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.1064)\n\n\n(0.0990)\n\n\n(0.0983)\n\n\n(0.0946)\n\n\n\n\nfactor(marital)With partner, not living together\n\n\n \n\n\n \n\n\n0.0825\n\n\n0.0323\n\n\n0.0145\n\n\n0.0099\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.0572)\n\n\n(0.0532)\n\n\n(0.0529)\n\n\n(0.0509)\n\n\n\n\nfactor(marital)With partner, living together\n\n\n \n\n\n \n\n\n0.0968*\n\n\n0.0570\n\n\n0.0582\n\n\n0.0342\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.0562)\n\n\n(0.0524)\n\n\n(0.0521)\n\n\n(0.0501)\n\n\n\n\nfactor(marital)Married\n\n\n \n\n\n \n\n\n0.0884\n\n\n0.0487\n\n\n0.0509\n\n\n0.0165\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.0538)\n\n\n(0.0500)\n\n\n(0.0497)\n\n\n(0.0479)\n\n\n\n\nfactor(marital)Registered partnership\n\n\n \n\n\n \n\n\n0.0982\n\n\n0.1345\n\n\n0.1601\n\n\n0.1976\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.1880)\n\n\n(0.1753)\n\n\n(0.1744)\n\n\n(0.1679)\n\n\n\n\nfactor(marital)Divorced / separated\n\n\n \n\n\n \n\n\n0.1150*\n\n\n0.0938\n\n\n0.0853\n\n\n0.0877\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.0616)\n\n\n(0.0573)\n\n\n(0.0569)\n\n\n(0.0549)\n\n\n\n\nfactor(marital)Widowed\n\n\n \n\n\n \n\n\n0.1612*\n\n\n0.1556*\n\n\n0.1309\n\n\n0.1243\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.0920)\n\n\n(0.0855)\n\n\n(0.0855)\n\n\n(0.0824)\n\n\n\n\nfactor(religion)Roman Catholic\n\n\n \n\n\n \n\n\n-0.0034\n\n\n-0.0300\n\n\n-0.0333\n\n\n-0.0713*\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.0407)\n\n\n(0.0378)\n\n\n(0.0377)\n\n\n(0.0364)\n\n\n\n\nfactor(religion)Protestant\n\n\n \n\n\n \n\n\n-0.0640*\n\n\n-0.0396\n\n\n-0.0249\n\n\n-0.0556*\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.0375)\n\n\n(0.0348)\n\n\n(0.0349)\n\n\n(0.0337)\n\n\n\n\nfactor(religion)Protestant Free Church\n\n\n \n\n\n \n\n\n-0.0225\n\n\n-0.0240\n\n\n-0.0170\n\n\n-0.0780\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.1022)\n\n\n(0.0951)\n\n\n(0.0945)\n\n\n(0.0911)\n\n\n\n\nfactor(religion)Other Protestant\n\n\n \n\n\n \n\n\n0.7822**\n\n\n0.9286***\n\n\n0.9234***\n\n\n0.8768**\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.3855)\n\n\n(0.3587)\n\n\n(0.3565)\n\n\n(0.3441)\n\n\n\n\nfactor(religion)Eastern Orthodox\n\n\n \n\n\n \n\n\n0.2751\n\n\n0.1666\n\n\n0.1350\n\n\n0.1455\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.1869)\n\n\n(0.1744)\n\n\n(0.1735)\n\n\n(0.1672)\n\n\n\n\nfactor(religion)Other Christian\n\n\n \n\n\n \n\n\n-0.0119\n\n\n0.0406\n\n\n0.0645\n\n\n0.0954\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.1633)\n\n\n(0.1518)\n\n\n(0.1514)\n\n\n(0.1458)\n\n\n\n\nfactor(religion)Jewish\n\n\n \n\n\n \n\n\n0.0827\n\n\n-0.1329\n\n\n-0.0855\n\n\n-0.2074\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.3460)\n\n\n(0.3217)\n\n\n(0.3202)\n\n\n(0.3081)\n\n\n\n\nfactor(religion)Muslim\n\n\n \n\n\n \n\n\n-0.0578\n\n\n0.0586\n\n\n0.0046\n\n\n0.0906\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.1667)\n\n\n(0.1554)\n\n\n(0.1549)\n\n\n(0.1509)\n\n\n\n\nfactor(religion)Eastern religion (Buddhism, Hinduism, Sikhism, Shinto, Tao, etc.)\n\n\n \n\n\n \n\n\n-0.0026\n\n\n0.0043\n\n\n0.0289\n\n\n0.0138\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.1215)\n\n\n(0.1130)\n\n\n(0.1129)\n\n\n(0.1086)\n\n\n\n\nfactor(religion)Other non-Christian religion\n\n\n \n\n\n \n\n\n0.3904*\n\n\n0.4759**\n\n\n0.4412**\n\n\n0.2675\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.2333)\n\n\n(0.2175)\n\n\n(0.2168)\n\n\n(0.2089)\n\n\n\n\nfactor(religion)Christian, but not close to a particular religious community\n\n\n \n\n\n \n\n\n-0.0270\n\n\n0.0102\n\n\n0.0083\n\n\n-0.0177\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.0611)\n\n\n(0.0567)\n\n\n(0.0566)\n\n\n(0.0544)\n\n\n\n\nfactor(religion)No answer\n\n\n \n\n\n \n\n\n0.1273\n\n\n0.2257**\n\n\n0.2106**\n\n\n0.1926*\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.1116)\n\n\n(0.1039)\n\n\n(0.1036)\n\n\n(0.0999)\n\n\n\n\neduyrs\n\n\n \n\n\n \n\n\n-0.0179***\n\n\n-0.0139***\n\n\n-0.0121***\n\n\n-0.0088**\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.0042)\n\n\n(0.0039)\n\n\n(0.0039)\n\n\n(0.0038)\n\n\n\n\nfactor(occupation)Parental leave\n\n\n \n\n\n \n\n\n0.1940\n\n\n0.1368\n\n\n0.1319\n\n\n0.1606\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.1589)\n\n\n(0.1478)\n\n\n(0.1473)\n\n\n(0.1417)\n\n\n\n\nfactor(occupation)In schooling / vocational training, student\n\n\n \n\n\n \n\n\n-0.0690\n\n\n-0.1196\n\n\n-0.1279\n\n\n-0.1244\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.0926)\n\n\n(0.0862)\n\n\n(0.0859)\n\n\n(0.0828)\n\n\n\n\nfactor(occupation)Unemployed / seeking work\n\n\n \n\n\n \n\n\n0.0088\n\n\n-0.0339\n\n\n-0.0420\n\n\n-0.0451\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.1080)\n\n\n(0.1004)\n\n\n(0.0999)\n\n\n(0.0961)\n\n\n\n\nfactor(occupation)Retired\n\n\n \n\n\n \n\n\n0.1055\n\n\n0.0493\n\n\n0.0470\n\n\n0.0298\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.0816)\n\n\n(0.0758)\n\n\n(0.0755)\n\n\n(0.0726)\n\n\n\n\nfactor(occupation)Permanently sick or disabled\n\n\n \n\n\n \n\n\n-0.1574\n\n\n-0.1490\n\n\n-0.1458\n\n\n-0.1507\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.1287)\n\n\n(0.1198)\n\n\n(0.1194)\n\n\n(0.1149)\n\n\n\n\nfactor(occupation)Unskilled worker\n\n\n \n\n\n \n\n\n0.1800*\n\n\n0.0893\n\n\n0.0642\n\n\n0.0221\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.0934)\n\n\n(0.0870)\n\n\n(0.0865)\n\n\n(0.0833)\n\n\n\n\nfactor(occupation)Skilled worker\n\n\n \n\n\n \n\n\n0.2026**\n\n\n0.1379*\n\n\n0.1393*\n\n\n0.1073\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.0871)\n\n\n(0.0812)\n\n\n(0.0808)\n\n\n(0.0778)\n\n\n\n\nfactor(occupation)Employee in low / medium position\n\n\n \n\n\n \n\n\n0.1065\n\n\n0.0560\n\n\n0.0524\n\n\n0.0653\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.0740)\n\n\n(0.0688)\n\n\n(0.0685)\n\n\n(0.0659)\n\n\n\n\nfactor(occupation)Employee in high position\n\n\n \n\n\n \n\n\n0.0536\n\n\n-0.0116\n\n\n-0.0165\n\n\n-0.0292\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.0824)\n\n\n(0.0766)\n\n\n(0.0763)\n\n\n(0.0735)\n\n\n\n\nfactor(occupation)Civil servant\n\n\n \n\n\n \n\n\n-0.0009\n\n\n-0.1061\n\n\n-0.1342\n\n\n-0.1687\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.1307)\n\n\n(0.1214)\n\n\n(0.1209)\n\n\n(0.1166)\n\n\n\n\nfactor(occupation)Senior civil servant\n\n\n \n\n\n \n\n\n0.0173\n\n\n0.0085\n\n\n0.0262\n\n\n-0.0256\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.1481)\n\n\n(0.1380)\n\n\n(0.1372)\n\n\n(0.1325)\n\n\n\n\nfactor(occupation)Senior civil servant &lt;96&gt; highest level\n\n\n \n\n\n \n\n\n-0.0083\n\n\n-0.1016\n\n\n-0.0793\n\n\n-0.0611\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.1262)\n\n\n(0.1174)\n\n\n(0.1169)\n\n\n(0.1127)\n\n\n\n\nfactor(occupation)Self-employed / freelancer\n\n\n \n\n\n \n\n\n0.1171\n\n\n0.0323\n\n\n0.0396\n\n\n0.0693\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.0889)\n\n\n(0.0828)\n\n\n(0.0824)\n\n\n(0.0794)\n\n\n\n\nfactor(occupation)Other\n\n\n \n\n\n \n\n\n0.2269\n\n\n0.0467\n\n\n0.0232\n\n\n0.0080\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.1724)\n\n\n(0.1607)\n\n\n(0.1601)\n\n\n(0.1540)\n\n\n\n\nfactor(income)500 to below 1,000 &lt;80&gt;\n\n\n \n\n\n \n\n\n0.0260\n\n\n0.0768\n\n\n0.0750\n\n\n-0.0028\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.1024)\n\n\n(0.0953)\n\n\n(0.0948)\n\n\n(0.0914)\n\n\n\n\nfactor(income)1,000 to below 1,500 &lt;80&gt;\n\n\n \n\n\n \n\n\n0.0677\n\n\n0.0714\n\n\n0.0642\n\n\n-0.0274\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.1011)\n\n\n(0.0943)\n\n\n(0.0937)\n\n\n(0.0904)\n\n\n\n\nfactor(income)1,500 to below 2,000 &lt;80&gt;\n\n\n \n\n\n \n\n\n0.1360\n\n\n0.1289\n\n\n0.1319\n\n\n0.0564\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.1024)\n\n\n(0.0955)\n\n\n(0.0949)\n\n\n(0.0914)\n\n\n\n\nfactor(income)2,000 to below 2,500 &lt;80&gt;\n\n\n \n\n\n \n\n\n0.1320\n\n\n0.1155\n\n\n0.1146\n\n\n0.0028\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.1045)\n\n\n(0.0974)\n\n\n(0.0969)\n\n\n(0.0935)\n\n\n\n\nfactor(income)2,500 to below 3,000 &lt;80&gt;\n\n\n \n\n\n \n\n\n0.0479\n\n\n0.0606\n\n\n0.0615\n\n\n-0.0466\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.1071)\n\n\n(0.0998)\n\n\n(0.0992)\n\n\n(0.0956)\n\n\n\n\nfactor(income)3,000 to below 3,500 &lt;80&gt;\n\n\n \n\n\n \n\n\n0.1659\n\n\n0.1531\n\n\n0.1557\n\n\n0.0384\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.1108)\n\n\n(0.1031)\n\n\n(0.1025)\n\n\n(0.0989)\n\n\n\n\nfactor(income)3,500 to below 4,000 &lt;80&gt;\n\n\n \n\n\n \n\n\n0.2256**\n\n\n0.2133**\n\n\n0.2101**\n\n\n0.0785\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.1138)\n\n\n(0.1059)\n\n\n(0.1054)\n\n\n(0.1017)\n\n\n\n\nfactor(income)4,000 to below 4,500 &lt;80&gt;\n\n\n \n\n\n \n\n\n0.0770\n\n\n0.0396\n\n\n0.0271\n\n\n-0.0996\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.1211)\n\n\n(0.1127)\n\n\n(0.1121)\n\n\n(0.1081)\n\n\n\n\nfactor(income)4,500 to below 5,000 &lt;80&gt;\n\n\n \n\n\n \n\n\n0.2446*\n\n\n0.1782\n\n\n0.1755\n\n\n0.0431\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.1274)\n\n\n(0.1188)\n\n\n(0.1182)\n\n\n(0.1140)\n\n\n\n\nfactor(income)5,000 or more &lt;80&gt;\n\n\n \n\n\n \n\n\n0.2017\n\n\n0.1350\n\n\n0.1128\n\n\n0.0250\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.1227)\n\n\n(0.1143)\n\n\n(0.1136)\n\n\n(0.1095)\n\n\n\n\nfactor(income)No answer\n\n\n \n\n\n \n\n\n0.0325\n\n\n0.0498\n\n\n0.0501\n\n\n-0.0453\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.1062)\n\n\n(0.0990)\n\n\n(0.0984)\n\n\n(0.0949)\n\n\n\n\nfactor(household_size)2\n\n\n \n\n\n \n\n\n0.0362\n\n\n0.0390\n\n\n0.0316\n\n\n0.0617\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.0511)\n\n\n(0.0475)\n\n\n(0.0473)\n\n\n(0.0457)\n\n\n\n\nfactor(household_size)3\n\n\n \n\n\n \n\n\n0.0404\n\n\n0.0374\n\n\n0.0403\n\n\n0.0675\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.0574)\n\n\n(0.0535)\n\n\n(0.0533)\n\n\n(0.0515)\n\n\n\n\nfactor(household_size)4\n\n\n \n\n\n \n\n\n0.0289\n\n\n0.0129\n\n\n0.0114\n\n\n0.0516\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.0647)\n\n\n(0.0602)\n\n\n(0.0599)\n\n\n(0.0578)\n\n\n\n\nfactor(household_size)5\n\n\n \n\n\n \n\n\n0.0161\n\n\n0.0255\n\n\n0.0347\n\n\n0.0345\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.1046)\n\n\n(0.0972)\n\n\n(0.0968)\n\n\n(0.0934)\n\n\n\n\nfactor(household_size)6\n\n\n \n\n\n \n\n\n0.3162*\n\n\n0.3629**\n\n\n0.4030**\n\n\n0.3646**\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.1793)\n\n\n(0.1666)\n\n\n(0.1658)\n\n\n(0.1599)\n\n\n\n\nfactor(household_size)7\n\n\n \n\n\n \n\n\n0.0387\n\n\n0.0311\n\n\n0.0495\n\n\n-0.0145\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.3181)\n\n\n(0.2957)\n\n\n(0.2939)\n\n\n(0.2838)\n\n\n\n\nfactor(household_size)8\n\n\n \n\n\n \n\n\n0.7654**\n\n\n0.9534***\n\n\n0.8352**\n\n\n0.7004**\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.3876)\n\n\n(0.3615)\n\n\n(0.3615)\n\n\n(0.3479)\n\n\n\n\nfactor(household_size)12\n\n\n \n\n\n \n\n\n-0.0289\n\n\n0.0946\n\n\n-0.0435\n\n\n-0.1011\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.7761)\n\n\n(0.7217)\n\n\n(0.7188)\n\n\n(0.6915)\n\n\n\n\nfactor(self_econ)2\n\n\n \n\n\n \n\n\n-0.0271\n\n\n-0.1166\n\n\n-0.1141\n\n\n-0.0368\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.1629)\n\n\n(0.1517)\n\n\n(0.1511)\n\n\n(0.1457)\n\n\n\n\nfactor(self_econ)3\n\n\n \n\n\n \n\n\n-0.2019\n\n\n-0.2069\n\n\n-0.2058\n\n\n-0.1859\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.1513)\n\n\n(0.1408)\n\n\n(0.1405)\n\n\n(0.1353)\n\n\n\n\nfactor(self_econ)4\n\n\n \n\n\n \n\n\n-0.1501\n\n\n-0.1394\n\n\n-0.1358\n\n\n-0.1343\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.1510)\n\n\n(0.1405)\n\n\n(0.1403)\n\n\n(0.1351)\n\n\n\n\nfactor(self_econ)5\n\n\n \n\n\n \n\n\n-0.2279\n\n\n-0.1700\n\n\n-0.1705\n\n\n-0.1569\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.1491)\n\n\n(0.1389)\n\n\n(0.1386)\n\n\n(0.1335)\n\n\n\n\nfactor(self_econ)6\n\n\n \n\n\n \n\n\n-0.2812*\n\n\n-0.2191\n\n\n-0.2186\n\n\n-0.2051\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.1503)\n\n\n(0.1400)\n\n\n(0.1397)\n\n\n(0.1345)\n\n\n\n\nfactor(self_econ)7\n\n\n \n\n\n \n\n\n-0.3444**\n\n\n-0.2527*\n\n\n-0.2484*\n\n\n-0.2383*\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.1518)\n\n\n(0.1415)\n\n\n(0.1414)\n\n\n(0.1360)\n\n\n\n\nfactor(self_econ)8\n\n\n \n\n\n \n\n\n-0.2107\n\n\n-0.1598\n\n\n-0.1765\n\n\n-0.1973\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.1573)\n\n\n(0.1466)\n\n\n(0.1462)\n\n\n(0.1407)\n\n\n\n\nfactor(self_econ)9\n\n\n \n\n\n \n\n\n-0.1747\n\n\n-0.0476\n\n\n-0.0684\n\n\n-0.0658\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.1933)\n\n\n(0.1804)\n\n\n(0.1801)\n\n\n(0.1731)\n\n\n\n\nfactor(self_econ)10 ( TOP )\n\n\n \n\n\n \n\n\n0.3679\n\n\n0.2960\n\n\n0.2701\n\n\n0.2253\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.2349)\n\n\n(0.2192)\n\n\n(0.2183)\n\n\n(0.2100)\n\n\n\n\nfactor(self_econ)0 ( BOTTOM )\n\n\n \n\n\n \n\n\n-0.0023\n\n\n-0.0450\n\n\n-0.0278\n\n\n0.0017\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.2077)\n\n\n(0.1933)\n\n\n(0.1925)\n\n\n(0.1853)\n\n\n\n\nfactor(ref_integrating)2\n\n\n \n\n\n \n\n\n \n\n\n-0.0585\n\n\n-0.0390\n\n\n-0.0304\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0913)\n\n\n(0.0924)\n\n\n(0.0890)\n\n\n\n\nfactor(ref_integrating)3\n\n\n \n\n\n \n\n\n \n\n\n-0.0921\n\n\n-0.0692\n\n\n-0.0772\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0935)\n\n\n(0.0951)\n\n\n(0.0918)\n\n\n\n\nfactor(ref_integrating)4\n\n\n \n\n\n \n\n\n \n\n\n0.0787\n\n\n0.0928\n\n\n0.0585\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0998)\n\n\n(0.1015)\n\n\n(0.0980)\n\n\n\n\nfactor(ref_citizenship)2\n\n\n \n\n\n \n\n\n \n\n\n0.0020\n\n\n-0.0202\n\n\n-0.0245\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0444)\n\n\n(0.0447)\n\n\n(0.0429)\n\n\n\n\nfactor(ref_citizenship)3\n\n\n \n\n\n \n\n\n \n\n\n0.0893*\n\n\n0.0720\n\n\n0.0349\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0493)\n\n\n(0.0497)\n\n\n(0.0480)\n\n\n\n\nfactor(ref_citizenship)4\n\n\n \n\n\n \n\n\n \n\n\n0.1626***\n\n\n0.1425**\n\n\n0.1000*\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0571)\n\n\n(0.0581)\n\n\n(0.0561)\n\n\n\n\nfactor(ref_reduce)2\n\n\n \n\n\n \n\n\n \n\n\n-0.0253\n\n\n-0.0103\n\n\n-0.0041\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0595)\n\n\n(0.0599)\n\n\n(0.0576)\n\n\n\n\nfactor(ref_reduce)3\n\n\n \n\n\n \n\n\n \n\n\n0.0162\n\n\n0.0326\n\n\n-0.0106\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0636)\n\n\n(0.0644)\n\n\n(0.0623)\n\n\n\n\nfactor(ref_reduce)4\n\n\n \n\n\n \n\n\n \n\n\n0.0354\n\n\n0.0465\n\n\n-0.1045\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0744)\n\n\n(0.0754)\n\n\n(0.0736)\n\n\n\n\nfactor(ref_moredone)2\n\n\n \n\n\n \n\n\n \n\n\n0.0930*\n\n\n0.0802*\n\n\n0.0559\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0482)\n\n\n(0.0484)\n\n\n(0.0467)\n\n\n\n\nfactor(ref_moredone)3\n\n\n \n\n\n \n\n\n \n\n\n0.1947***\n\n\n0.1834***\n\n\n0.0920*\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0528)\n\n\n(0.0532)\n\n\n(0.0520)\n\n\n\n\nfactor(ref_moredone)4\n\n\n \n\n\n \n\n\n \n\n\n0.3050***\n\n\n0.2874***\n\n\n0.1561**\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0618)\n\n\n(0.0623)\n\n\n(0.0609)\n\n\n\n\nfactor(ref_cultgiveup)2\n\n\n \n\n\n \n\n\n \n\n\n-0.0125\n\n\n-0.0212\n\n\n-0.0309\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0586)\n\n\n(0.0590)\n\n\n(0.0569)\n\n\n\n\nfactor(ref_cultgiveup)3\n\n\n \n\n\n \n\n\n \n\n\n0.0145\n\n\n-0.0054\n\n\n-0.0505\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0582)\n\n\n(0.0587)\n\n\n(0.0568)\n\n\n\n\nfactor(ref_cultgiveup)4\n\n\n \n\n\n \n\n\n \n\n\n0.1507**\n\n\n0.1281*\n\n\n0.0733\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0652)\n\n\n(0.0656)\n\n\n(0.0636)\n\n\n\n\nfactor(ref_economy)2\n\n\n \n\n\n \n\n\n \n\n\n0.0237\n\n\n0.0456\n\n\n0.0510\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0549)\n\n\n(0.0582)\n\n\n(0.0561)\n\n\n\n\nfactor(ref_economy)3\n\n\n \n\n\n \n\n\n \n\n\n-0.0004\n\n\n0.0343\n\n\n0.0145\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0606)\n\n\n(0.0663)\n\n\n(0.0638)\n\n\n\n\nfactor(ref_economy)4\n\n\n \n\n\n \n\n\n \n\n\n0.1657**\n\n\n0.2379***\n\n\n0.1524**\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0702)\n\n\n(0.0778)\n\n\n(0.0750)\n\n\n\n\nfactor(ref_crime)2\n\n\n \n\n\n \n\n\n \n\n\n0.0183\n\n\n-0.0033\n\n\n-0.0037\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0606)\n\n\n(0.0645)\n\n\n(0.0620)\n\n\n\n\nfactor(ref_crime)3\n\n\n \n\n\n \n\n\n \n\n\n0.0794\n\n\n-0.0061\n\n\n-0.0290\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0661)\n\n\n(0.0715)\n\n\n(0.0688)\n\n\n\n\nfactor(ref_crime)4\n\n\n \n\n\n \n\n\n \n\n\n0.2506***\n\n\n0.1343\n\n\n0.0431\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0774)\n\n\n(0.0835)\n\n\n(0.0806)\n\n\n\n\nfactor(ref_terror)2\n\n\n \n\n\n \n\n\n \n\n\n-0.0689\n\n\n-0.0975*\n\n\n-0.1060*\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0568)\n\n\n(0.0578)\n\n\n(0.0556)\n\n\n\n\nfactor(ref_terror)3\n\n\n \n\n\n \n\n\n \n\n\n-0.0330\n\n\n-0.0818\n\n\n-0.1054*\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0608)\n\n\n(0.0617)\n\n\n(0.0595)\n\n\n\n\nfactor(ref_terror)4\n\n\n \n\n\n \n\n\n \n\n\n-0.0338\n\n\n-0.0865\n\n\n-0.1144*\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0707)\n\n\n(0.0715)\n\n\n(0.0689)\n\n\n\n\nfactor(ref_loc_services)2\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n0.0852\n\n\n0.0888\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0680)\n\n\n(0.0653)\n\n\n\n\nfactor(ref_loc_services)3\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n0.0765\n\n\n0.0788\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0681)\n\n\n(0.0655)\n\n\n\n\nfactor(ref_loc_services)4\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n0.0577\n\n\n0.0699\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0761)\n\n\n(0.0732)\n\n\n\n\nfactor(ref_loc_economy)2\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n-0.1186*\n\n\n-0.1209*\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0718)\n\n\n(0.0690)\n\n\n\n\nfactor(ref_loc_economy)3\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n-0.1420*\n\n\n-0.1562**\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0767)\n\n\n(0.0738)\n\n\n\n\nfactor(ref_loc_economy)4\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n-0.2892***\n\n\n-0.2972***\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0864)\n\n\n(0.0832)\n\n\n\n\nfactor(ref_loc_crime)2\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n0.0813\n\n\n0.0727\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0582)\n\n\n(0.0560)\n\n\n\n\nfactor(ref_loc_crime)3\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n0.2474***\n\n\n0.2050***\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0662)\n\n\n(0.0640)\n\n\n\n\nfactor(ref_loc_crime)4\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n0.3110***\n\n\n0.2766***\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0800)\n\n\n(0.0774)\n\n\n\n\nfactor(ref_loc_culture)2\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n0.0051\n\n\n0.0068\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0524)\n\n\n(0.0507)\n\n\n\n\nfactor(ref_loc_culture)3\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n0.0297\n\n\n0.0078\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0612)\n\n\n(0.0594)\n\n\n\n\nfactor(ref_loc_culture)4\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n0.1232*\n\n\n0.0013\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0737)\n\n\n(0.0718)\n\n\n\n\nfactor(ref_loc_islam)2\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n-0.0128\n\n\n-0.0085\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0531)\n\n\n(0.0511)\n\n\n\n\nfactor(ref_loc_islam)3\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n-0.0085\n\n\n-0.0453\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0550)\n\n\n(0.0531)\n\n\n\n\nfactor(ref_loc_islam)4\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n-0.0281\n\n\n-0.1068*\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0660)\n\n\n(0.0638)\n\n\n\n\nfactor(ref_loc_schools)2\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n0.1254\n\n\n0.1084\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0887)\n\n\n(0.0853)\n\n\n\n\nfactor(ref_loc_schools)3\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n0.0552\n\n\n0.0573\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0832)\n\n\n(0.0801)\n\n\n\n\nfactor(ref_loc_schools)4\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n-0.0806\n\n\n-0.0809\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0845)\n\n\n(0.0814)\n\n\n\n\nfactor(ref_loc_housing)2\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n0.0134\n\n\n0.0095\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0586)\n\n\n(0.0566)\n\n\n\n\nfactor(ref_loc_housing)3\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n0.0068\n\n\n0.0008\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0564)\n\n\n(0.0547)\n\n\n\n\nfactor(ref_loc_housing)4\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n0.0432\n\n\n0.0433\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0608)\n\n\n(0.0590)\n\n\n\n\nfactor(ref_loc_wayoflife)2\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n-0.0586\n\n\n-0.0653\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0621)\n\n\n(0.0597)\n\n\n\n\nfactor(ref_loc_wayoflife)3\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n-0.0341\n\n\n-0.0515\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0609)\n\n\n(0.0586)\n\n\n\n\nfactor(ref_loc_wayoflife)4\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n0.0694\n\n\n0.0550\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0707)\n\n\n(0.0682)\n\n\n\n\nfactor(distance_ref)3-5 kilometers\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n-0.0375\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0362)\n\n\n\n\nfactor(distance_ref)6-10 kilometers\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n0.0256\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0405)\n\n\n\n\nfactor(distance_ref)11-20 kilometers\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n0.0165\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0494)\n\n\n\n\nfactor(distance_ref)21-50 kilometers\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n0.0644\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0534)\n\n\n\n\nfactor(distance_ref)More than 50 kilometer\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n0.0568\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0794)\n\n\n\n\nfactor(distance_ref)Don’t know\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n-0.0389\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0418)\n\n\n\n\nfactor(settle_ref)1 &lt;96&gt; 49\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n-0.0133\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0668)\n\n\n\n\nfactor(settle_ref)50 &lt;96&gt; 249\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n-0.0178\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0661)\n\n\n\n\nfactor(settle_ref)250 &lt;96&gt; 499\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n-0.0455\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0694)\n\n\n\n\nfactor(settle_ref)500 &lt;96&gt; 999\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n-0.0213\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0732)\n\n\n\n\nfactor(settle_ref)1000 and more\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n-0.0536\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0687)\n\n\n\n\nlrscale\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n0.0235***\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0078)\n\n\n\n\nafd\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n0.0044***\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0006)\n\n\n\n\nmuslim_ind\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n0.3152***\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0701)\n\n\n\n\nafd_ind\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n0.3390***\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0489)\n\n\n\n\ncontact_ind\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n0.0741\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0522)\n\n\n\n\nR2\n\n\n0.1915\n\n\n0.2403\n\n\n0.2883\n\n\n0.3942\n\n\n0.4097\n\n\n0.4592\n\n\n\n\nAdj. R2\n\n\n0.1912\n\n\n0.2395\n\n\n0.2673\n\n\n0.3712\n\n\n0.3821\n\n\n0.4308\n\n\n\n\nNum. obs.\n\n\n3019\n\n\n3019\n\n\n3008\n\n\n3008\n\n\n3008\n\n\n3008\n\n\n\n\n\n\n***p &lt; 0.01; **p &lt; 0.05; *p &lt; 0.1\n\n\n\n\n\n\nFinal Questions\nEven with all these covariates accounted for, the authors still engage in a discussion about possible violations of the OLS assumptions that could bias their results, as well as potential alternative modelling strategies.\n\nIs their survey representative? They replicate using another polling firm.\nAre there even more alternative explanations?\nIs OLS the right choice?\nValidity (discussed in Gelman and Hill). Does the outcome accurately measure the concept? They consider alternative outcomes and visualize the coefficient results in Figure 4.\n\nMessage: Attacks against refugee homes are sometimes necessary to make it clear to politicians that we have a refugee problem.\nJustified : Hostility against refugees is sometimes justified, even when it ends up in violence.\nPrevent : Xenophobic acts of violence are defensible if they result in fewer refugees settling in town.\nCondemn: Politicians should condemn attacks against refugees more forcefully.\n\n\n\nAdditional Practice Questions.\n\nFind the average expected level of “Only Means” agreement at each level of mate competition. Plot the results. Base these results on lm2.\nFit lm2 using the generalized linear model glm approach (with a normal distribution) instead of the lm\nWhat are some of the conceptual differences between ordinary least squares and maximum likelihood estimation?"
  },
  {
    "objectID": "04-ReviewofOLS.html#footnotes",
    "href": "04-ReviewofOLS.html#footnotes",
    "title": "4  Review of OLS",
    "section": "",
    "text": "Recall this notation means rows by columns, \\(Y\\) is a vector of length \\(n\\) (the number of observations), and since there is only 1 outcome measure, it is 1 column.↩︎\nThis video from Ben Lambert provides additional intuition for understanding OLS in a matrix form and how it can be useful.↩︎"
  },
  {
    "objectID": "05-IntrotoMaximumLikelihood.html#what-is-likelihood",
    "href": "05-IntrotoMaximumLikelihood.html#what-is-likelihood",
    "title": "5  Introduction to MLE",
    "section": "5.1 What is likelihood?",
    "text": "5.1 What is likelihood?\nJust like the derivation of the OLS coefficient estimators \\(\\hat \\beta\\) for the \\(\\beta\\) parameters started with a goal (minimizing least squared errors) in describing the relationship between variables, with likelihood we also start with a goal.\nThe “likelihood” is going to ask the question: What values of the unknown parameters make the data we see least surprising?\nWhen we get into likelihood, we will be drawing more directly on concepts within probability. We start by making a choice about what type of data generating process best describes our outcome data. Eventually, our likelihood function represents the “probability density of the data given the parameters and predictors.” (Definition taken from Gelman et al. 2020, pg. 105).\n\nIn MLE, we are going to choose parameter estimates \\(\\widehat{\\theta}\\) for \\(\\theta\\) that maximize the likelihood that our data came from the particular distribution.\n\nAlready, we are placing a lot of emphasis on the nature of our outcome data. The nature of our likelihood will change depending on if the data are dichotomous (e.g, similar to a set of coin flips that could be head or tails) or a count (e.g., similar to the number of events expected to occur over a certain interval) or more of a continuous numeric distribution with (e.g., where the probabilities of certain levels can be visualized similar to a bell curve). Each of these sets of data are generated through a different process, which is described by a particular probability function.\nExample\nThis introduction is based on Ben Lambert’s video. I highly recommend watching this 8-9 minute video. Below we highlight a few of the key concepts and definitions.\nTake the UK population of 70 million. We have a sample of this, and in our sample some observations are male and some female. How can we use what we have, a sample, to estimate the probability that an individual is male?1\n\nFirst, we can make a judgment about the data generating process. We can suppose there is some probability distribution function that determines the probability is a male or female. A probability density function (PDF for continuous data or PMF for discrete data) tells you the relative probability or likelihood of observing a particular value given the parameters of the distribution.\n\nLet’s call this \\(f(y_i | p)\\) where \\(y_i = 1\\) if male, 0 if female.\nWe will say \\(p\\) is the probability an individual is male. \\(p^{y_i}(1 - p)^{1-y_i}\\)\n\nWe are going to treat this like a toss of a coin, which has a Bernouilli distribution (every probability distribution we are dealing with is going to have an associated formula. You don’t need to memorize these. You can look them up on the internet when needed.) So, \\(f()\\) tells us the probability we would have gotten the value of the observation if we think of our observation as this Bernouilli toss of coin. This is the likelihood for a single observation.\nFor example, we can now plug this into our Bernoulli function for the two possible cases of values for \\(y_i\\).\n\n\\(f(1|p) = p^1(1-p)^{1-1} = p\\) probability an individual is male\n\\(f(0 |p)= p ^0(1-p)^{1-0} = 1-p\\) probability individual is female\n\nNow we want an estimate using our entire sample of observations, not just a single \\(i\\). What if we have \\(n\\) observations? \\(f(y_1, y_2, ... y_n | p)\\). We can write the joint probability as all individual probabilities multiplied together if we assume our observations are independent. This will now represent the likelihood for all observations.\n\n\\(L = P(Y_1 = y_1, Y_2=y_2, ..., Y_n = y_n)= \\prod_{i=1}^n p^y_i(1 - p)^{1-y_i}\\)\nThis answers what is the probability that \\(Y_1\\) took on the particular value \\(y_1\\)\nOk, now we have the statement \\(L = P(Y_1 = y_1, Y_2=y_2, ..., Y_n = y_n)\\). This joint probability is the likelihood (Technically it is proportionate to the likelihood, but this detail will not matter for us. What this means is there is a hidden constant \\(k(y)\\) multiplied by the joint probability. Because likelihood is a relative concept, this constant can fall out.)\n\nGenerally, we don’t know \\(p\\). We are trying to estimate it. What we want to do is choose the \\(\\hat p\\) to maximize the likelihood that we would have gotten this set of observations given that \\(Y_i\\) has a probability distribution as specified.\n\nWe have used a buzz word: “maximize.” Just as in OLS, that should be our signal that a derivative should be taken so that we can find the quantities that represent the maximum.\nWe differentiate L with respect to p, set it to 0, to give us \\(\\hat{p}\\).\n\nOur issue (or at least one of our issues) is that products are tough to differentiate. A chain rule disaster. Instead, we use a trick of taking the log of the likelihood: log \\(\\prod ()\\).2 Benefit: it turns it into a sum, much easier. \\(\\log ab = \\log a + \\log b\\). So we will actually differentiate the \\(\\log\\) of the likelihood. Yes, this is why we had logs as part of Section 3.\n\n\n5.1.1 Summarizing Steps for Maximum Likelihood\nInitial Setup\n\nWhat is the data generating process? This means think about the structure of the dependent variable. Is it continuous, is it a count, is it binary, is it ordered? Based on this, describe the probability distribution for \\(Y_i\\).\nDefine the likelihood for a single observation\nDefine the likelihood for all observations\nFind the log-likelihood\n\nThen, the derivation begins! Yes, we’ve only just begun, but that first step of deciding the data generating process is huge.\nExample: Count Data\nLet’s say we are trying to understand the relationship between an independent variable and the number of news articles reported on a topic. This is a count of data. It goes from 0 to some positive number, never extending below 0. A distribution that is a good fit for this is the Poisson. We start by specifying this as our data generating process and looking up the Poisson probability density function, which has the parameter \\(\\lambda\\).\n\nData Generating Process and probability density function.\n\n\\[\\begin{align*}\n&Y_i \\stackrel{\\rm i.i.d.}{\\sim} Pois(\\lambda)\\Rightarrow\\\\\n&\\Pr(Y=Y_i|\\lambda)=\\lambda \\frac{exp(-\\lambda) \\lambda^{Y_i}}{Y_i!}\n\\end{align*}\\]\nNote: we assume our observations are iid (independently and identically distributed (For definition.) This assumption can be relaxed.\n\nWhat is the likelihood for a single observation?\n\n\\[\\begin{align*}\n\\mathcal L(\\lambda|Y_i)=\\Pr(Y=Y_i|\\lambda)\n\\end{align*}\\]\n\nWhat is the likelihood for all observations?\n\n\\[\\begin{align*}\n\\mathcal L(\\lambda|Y)&=\\mathcal L(\\lambda|Y_1)\\times\\mathcal  L(\\lambda|Y_2)\\times \\ldots \\times \\mathcal L(\\lambda|Y_{N})\\\\\n\\mathcal L(\\lambda|Y)&=\\prod_{i=1}^N\\mathcal L(\\lambda|Y_i)\\\\\n\\end{align*}\\]\n\nEasier to work with log-likelihood\n\n\\[\\begin{align*}\n\\ell(\\lambda|Y)&=\\sum_{i=1}^N\\mathcal \\log(\\mathcal L(\\lambda|Y_i))\\\\\n\\end{align*}\\]\nGiven observed data \\(Y\\), what is the likelihood it was generated from \\(\\lambda\\)? We will be choosing estimates of the parameters that maximize the likelihood we would have seen these data. Generally, we will also consider parameters like \\(\\lambda\\) to be functions of our covariates– the things we think help explain our otucome.\nFor additonal practice, try to write down the likelihood of a single observation, the likelihood for all observations, and the log likelihood for an outcome we believe is normally distributed. We have \\(Y_i \\sim N(\\mu, \\sigma^2)\\). Our PDF is:\n\\[\\begin{align*}\nf(Y_i | \\theta) &=  \\frac{1}{\\sigma\\sqrt{2\\pi}}e^{\\frac{-(Y_i-\\mu)^2}{2\\sigma^2}}\n\\end{align*}\\]\nYou can take it from here.\n\n\nTry on your own, then expand for the solution.\n\nWe have the PDF.\n\nLet’s write the likelihood for a single observation.\n\n\\[\\begin{align*}\nL(\\theta | Y_i) = L(\\mu, \\sigma^2 | Y_i) &= \\frac{1}{\\sigma\\sqrt{2\\pi}}e^{\\frac{-(Y_i-\\mu)^2}{2\\sigma^2}}\n\\end{align*}\\]\n\nLet’s write the likelihood for all observations.\n\n\\[\\begin{align*}\nL(\\mu, \\sigma^2 | Y) &=  \\prod_{i=1}^{N} \\frac{1}{\\sigma\\sqrt{2\\pi}}e^{\\frac{-(Y_i-\\mu)^2}{2\\sigma^2}}\n\\end{align*}\\]\n\nLet’s write the log likelihood.\n\n\\[\\begin{align*}\n\\ell(\\mu, \\sigma^2 | Y) &= \\sum_{i = 1}^N \\log \\Bigg( \\frac{1}{\\sigma\\sqrt{2\\pi}}e^{\\frac{-(Y_i-\\mu)^2}{2\\sigma^2}}\\Bigg)\\\\\n&= \\sum_{i = 1}^N \\underbrace{\\log \\Bigg( \\frac{1}{\\sigma\\sqrt{2\\pi}}\\Bigg) + \\log e^{\\frac{-(Y_i-\\mu)^2}{2\\sigma^2}}}_\\text{Using the rule $\\log ab = \\log a + \\log b$}\\\\\n&= \\underbrace{\\sum_{i = 1}^N \\log  \\frac{1}{\\sigma\\sqrt{2\\pi}} - \\frac{(Y_i-\\mu)^2}{2\\sigma^2}}_\\text{The second term was of the form $\\log e ^ a$, we can re-write as $a * \\log e$. $\\log e$ cancels to 1, leaving us with just $a$.}\\\\\n&= \\sum_{i = 1}^N \\log  \\frac{1}{\\sigma\\sqrt{2\\pi}} - \\frac{(Y_i-\\mathbf{x}_i'\\beta)^2}{2\\sigma^2}\n\\end{align*}\\]\nNote how we usually will sub in \\(X\\beta\\) or \\(\\mathbf{x_i'\\beta}\\) for the parameter because we think these will vary according to covariates in our data. The \\(e\\) in the Normal PDF is the base of the natural log. It is a mathematical constant. Sometimes you might see this written as \\(exp\\) instead of \\(e\\). In R, you can use exp() to get this constant. For example, \\(\\log e^2\\) in R would be log(exp(2))."
  },
  {
    "objectID": "05-IntrotoMaximumLikelihood.html#generalized-linear-models",
    "href": "05-IntrotoMaximumLikelihood.html#generalized-linear-models",
    "title": "5  Introduction to MLE",
    "section": "5.2 Generalized Linear Models",
    "text": "5.2 Generalized Linear Models\nBefore we get into the details of deriving the estimators, we are going to discuss another connection between linear models and the types of models we will work with when we are using common maximum likelihood estimators.\nRecall our linear model: \\(y_i = \\beta_o + \\beta_1x_{i1} + ... \\beta_kx_{ik} + \\epsilon\\)\n\n\\(Y\\) is modelled by a linear function of explanatory variables \\(X\\)\n\\(\\hat \\beta\\) is our estimate of how much \\(X\\) influences \\(Y\\) (the slope of the line)\nOn average, a one-unit change in \\(X_{ik}\\) is associated with a \\(\\hat \\beta_{k}\\) change in \\(Y_i\\)\nSlope/rate of change is linear, does not depend on where you are in \\(X\\). Every one-unit change has the same expected increase or decrease\n\nSometimes we are dealing with outcome data that are restricted or “limited” in some way such that this standard linear predictor will no longer make sense. If we keep changing \\(X\\) we may eventually generate estimates of \\(\\hat y\\) that extend above or below the plausible range of values for our actual observed outcomes.\nThe generalized linear model framework helps address this problem by adding two components: a nonlinear transformation and a probability model. This allows us to make predictions of our outcomes that retain the desired bounded qualities of our observed data. Generalized linear models include linear regression as a special case (a case where no nonlinear transformation is required), but as its name suggests, is much more general and can be applied to many different outcome structures.\n\n5.2.1 GLM Model.\nIn a GLM, we still have a “linear predictor”: \\(\\eta_i = \\beta_o + \\beta_1x_{i1} + ... + \\beta_kx_{ik}\\)\n\nBut our \\(Y_i\\) might be restricted in some way (e.g., might be binary).\nSo, now we require a “link” function which tells us how \\(Y\\) depends on the linear predictor. This is the key to making sure our linear predictor, when transformed, will map into sensible units of Y.\n\nOur \\(Y_i\\) will also now be expressed in terms of a probability model, and it is this probability distribution that generates the randomness (the stochastic component of the model). For example, when we have binary outcome data, such as \\(y_i =\\) 1 or 0 for someone turning out to vote or not, we may try to estimate the probability that someone turns out to vote given certain explanatory variables. We can write this as \\(Pr(Y_i = 1 | x_i\\)).\nIn a GLM, we need a way to transform our linear predictor such that as we shift in values of \\(X\\hat \\beta\\), we stay within plausible probability ranges.\n\nTo do so we use a “link” function that is used to model the data.\n\nFor example, in logistic regression, our link function will be the “logit”:\n\n\\[\\begin{align*}\nPr(Y_i = 1 | x_i) &= \\pi_i\\\\\n\\eta_i &= \\text{logit}(\\pi_i) = \\log \\frac{\\pi_i}{1-\\pi_i} &= \\beta_o + \\beta_1x_{i1} + ... + \\beta_kx_{ik}\n\\end{align*}\\]\nOne practical implication of this is that when we generate our coefficient estimates \\(\\hat \\beta\\), these will no longer be in units if \\(y_i\\) or even in units of probability. Instead, they will be in units as specified by the link function. In logistic regression, this means they will be in “logits.”\n\nFor every one-unit change in \\(x_k\\), we get a \\(\\hat \\beta_k\\) change in logits of \\(y\\)\n\nHowever, the nice thing is that because we know the link function, with a little bit of work, we can use the “response” function to transform our estimates back into the units of \\(y_i\\) that we care about.\n\n\\[\\begin{align*}\nPr(Y_i = 1 | x_i) &= \\pi_i = g^{-1}(\\eta_i) \\\\\n&= \\text{logit}^{-1}(\\pi_i) \\\\\n&= \\frac{exp^{x_i'\\beta}}{1 + exp^{x_i'\\beta}}\n\\end{align*}\\]\n\n\n5.2.2 Linking likelihood and the GLM\nLet’s use \\(\\theta\\) to represent the parameters of the pdf/pmf that we have deemed appropriate for our outcome data. As discussed before, we can write the likelihood for an observation as a probability statement.\n\n\\(\\mathcal L (\\theta | Y_i) = \\Pr(Y=Y_i | \\theta)\\)\n\nIn social science, instead of thinking of these parameters as just constants (e.g., \\(p\\) or \\(\\mu\\)), we generally believe that they vary according to our explanatory variables in \\(X\\). We think \\(Y_i\\) is distributed according to a particular probability function and that the parameters that shape that distribution are a function of the covariates.\n\n\\(Y_i \\sim f(y_i | \\theta_i)\\) and \\(\\theta_i = g(X_i, \\beta)\\)\n\nEach type of model we come across–guided by the structure of the dependent variable– is just going to have different formulas for each of these components.\nExamples\n\n\n\n\n\n\n\n\n\n\nModel\nPDF\n\\(\\theta_i\\) ; Link\\(^{-1}\\)\n\\(\\eta_i\\)\n\n\n\n\n\nLinear\n\\(Y_i \\sim \\mathcal{N}(\\mu_i,\\sigma^2)\\)\n\\(\\mu_i = X_i^\\prime\\beta\\)\n\\(\\mu_i\\)\n\n\n\nLogit\n\\(Y_i \\sim \\rm{Bernoulli}(\\pi_i)\\)\n\\(\\pi_i=\\frac{\\exp(X_i^\\prime\\beta)}{(1+\\exp(X_i^\\prime\\beta))}\\)\nlogit\\((\\pi_i)\\)\n\n\n\nProbit\n\\(Y_i \\sim \\rm{Bernoulli}(\\pi_i)\\)\n\\(\\pi_i = \\Phi(X_i^\\prime\\beta)\\)\n\\(\\Phi^{-1}(\\pi_i)\\)\n\n\n\n\nThese generalized linear models are then fit through maximum likelihood estimation, through an approach discussed in the next section where we use algorithms to choose the most likely values of the \\(\\beta\\) parameters given the observed data.\nNote: not all ML estimators can be written as generalized linear models, though many we use in political science are indeed GLMs. To be a GLM, the distribution we specify for the data generating process has to be a part of the exponential family of probability distributions (fortunately the gaussian normal, poisson, bernouilli, binomial, gamma, and negative binomial are), and after that, we need the linear predictor and link function.\n\n\n5.2.3 GLM in R\nThe way generalized linear models work in R is very similar to lm.\nBelow is a simple example where we will specify a linear model in lm() and glm() to compare.\n\n## Load Data\nflorida &lt;- read.csv(\"https://raw.githubusercontent.com/ktmccabe/teachingdata/main/florida.csv\")\n\nfit.lm &lt;- lm(Buchanan00 ~ Perot96, data=florida)\nfit.glm &lt;- glm(Buchanan00 ~ Perot96, data=florida, \n               family=gaussian(link = \"identity\"))\n\nFor the glm, we just need to tell R the family of distributions we are using and the appropriate link function. In this example, we are going to use the normal gaussian distribution to describe the data generating process for Buchanan00. This is appropriate for nice numeric continuous data, even if it isn’t perfectly normal. The normal model has a link function, but it is the special case where the link function is just the identity. There is no nonlinear transformation that takes place. Therefore, we can still interpret the \\(\\hat \\beta\\) results in units of \\(Y\\) (votes in this case).\nIn this special case, the \\(\\hat \\beta\\) estimates from lm() and glm() will be the same.\n\ncoef(fit.lm)\n\n(Intercept)     Perot96 \n 1.34575212  0.03591504 \n\ncoef(fit.glm)\n\n(Intercept)     Perot96 \n 1.34575212  0.03591504 \n\n\nThere are some differences in the mechanics of how we get to the results in each case, but we will explore those more in the next section. I.e., these coefficients do not come out of thin air. Just like in OLS, we have to work for them."
  },
  {
    "objectID": "05-IntrotoMaximumLikelihood.html#mle-estimation",
    "href": "05-IntrotoMaximumLikelihood.html#mle-estimation",
    "title": "5  Introduction to MLE",
    "section": "5.3 MLE Estimation",
    "text": "5.3 MLE Estimation\nThis section will discuss the general process for deriving maximum likelihood estimators. It’s all very exciting. It builds on the resources from the previous sections. In the next section, we will go through this process for a binary dependent variable. Here, we lay out the overview.\n\n5.3.1 Deriving Estimators\nRecall, we’ve already gone through a few steps of maximum likelihood estimation.\nInitial Setup\n\nWhat is the data generating process? This means think about the structure of the dependent variable. Is it continuous, is it a count, is it binary, is it ordered? Based on this, describe the probability distribution for \\(Y_i\\).\nDefine the likelihood for a single observation\nDefine the likelihood for all observations\nFind the log-likelihood\n\nNow we add steps building on the log-likelihood.\n\nMaximize the function with respect to (wrt) \\(\\theta\\)\n\nTake the derivative wrt \\(\\theta\\). We call this the “score”\nSet \\(S(\\theta) = 0\\) and solve for \\(\\hat \\theta\\) (if possible)\nIf not possible (often the case), we use an optimization algorithm to maximize the log likelihood.\n\nTake the second derivative of the log likelihood to get the “hessian” and help estimate the uncertainty of the estimates.\n\n\n\n5.3.2 Score function\nThe first derivative of the log-likelihood is called the score function: \\(\\frac{\\delta \\ell}{\\delta \\theta} = S(\\theta)\\). This will tell us how steep the slope of the log likelihood is given certain values of the parameters. What we are looking for as we sift through possible values of the parameters, is the set of values that will make the slope zero, signalling that the function has reached a peak (maximizing the likelihood.)\nWe set the \\(S(\\theta) = 0\\) and solve for \\(\\hat \\theta\\) (if possible).\n\n\\(\\hat \\theta\\) are the slopes/gradient, which we use as estimates (e.g., \\(\\hat \\beta\\)).\nWe can interpret the sign and significance just as we do in OLS.\nBut, unlike OLS, most of the time, these are not linear changes in units of \\(Y\\)\nWe have to transform them into interpretable quantities\n\nExample: Normally distributed outcome\nStart with the log-likelihood\n\\[\\begin{align*}\n\\ell(\\theta | Y) &= \\sum_{i = 1}^N \\log \\Bigg( \\frac{1}{\\sigma\\sqrt{2\\pi}}e^{\\frac{-(Y_i-\\mu)^2}{2\\sigma^2}}\\Bigg)\\\\\n&= \\sum_{i = 1}^N \\underbrace{\\log \\Bigg( \\frac{1}{\\sigma\\sqrt{2\\pi}}\\Bigg) + \\log e^{\\frac{-(Y_i-\\mu)^2}{2\\sigma^2}}}_\\text{Using the rule $\\log ab = \\log a + \\log b$}\\\\\n&= \\underbrace{\\sum_{i = 1}^N \\log  \\frac{1}{\\sigma\\sqrt{2\\pi}} - \\frac{(Y_i-\\mu)^2}{2\\sigma^2}}_\\text{The second term was of the form $\\log e ^ a$, we can re-write as $a * \\log e$. $\\log e$ cancels to 1, leaving us with just $a$.}\\\\\n&= \\sum_{i = 1}^N \\log  \\frac{1}{\\sigma\\sqrt{2\\pi}} - \\frac{(Y_i-\\mathbf{x}_i'\\beta)^2}{2\\sigma^2}\n\\end{align*}\\]\n\nNote: when you see \\(\\mathbf{x}_i'\\beta\\), usually that is the representation of the multiplication of \\(k\\) covariates (a \\(1 \\times k\\) vector) for a particular observation \\(i\\) by \\(k \\times 1\\) coefficient values \\(\\beta\\). You can contrast this with \\(X\\beta\\), which represents \\(n \\times k\\) rows of observations with \\(k\\) covariates multiplied by the \\(k \\times 1\\) coefficients. You will see both notations depending on if notation is indexed by \\(i\\) or represented fully in matrix form. The \\(\\mathbf{x_i'}\\) representation tends to come up more when we are dealing with likelihood equations. Here is a short video relating these notations.\n\n\nTake the derivative wrt \\(\\theta\\). Note: we have to take two derivatives- one for \\(\\mu\\) (\\(\\beta\\)) and one for \\(\\sigma^2\\). For this example we will focus only on the derivative wrt to \\(\\beta\\), as that it what gets us the coefficient estimates.3\nNote: Below, we can simplify the expression of the log likelihood for taking the derivative with respect to \\(\\beta\\) because any term (i.e., the first term in the log likelihood in this case) that does not have a \\(\\beta\\) will fall out of the derivative expression. This is because when we take the derivative with respect to \\(\\beta\\) we treat all other terms as constants, and the slope of a constant (the rate of change of a constant) is zero. The curly \\(\\delta\\) in the expression below means “the derivative of …” with respect to \\(\\beta\\).\n\\[\\begin{align*}\n\\delta_\\beta \\ell(\\theta | Y) &= -\\frac{1}{2\\sigma^2}\\sum_{i = 1}^N \\delta_\\beta (Y_i-\\mathbf{x}_i'\\hat \\beta)^2\n\\end{align*}\\] The right term should look familiar! It is the same derivative we take when we are minimizing the least squares. Therefore, we will end up with \\(S(\\hat \\theta)_\\beta = \\frac{1}{\\sigma^2}X'(Y - X\\hat \\beta)\\). We set this equal to 0. \\[\\begin{align*}\n\\frac{1}{\\sigma^2}X'(Y - X\\hat \\beta) &= 0\\\\\n\\frac{1}{\\sigma^2}X'Y &= \\frac{1}{\\sigma^2}X'X\\hat \\beta \\\\\n(X'X)^{-1}X'Y = \\hat \\beta\n\\end{align*}\\]\n\n\n5.3.3 Hessian and Information Matrix\nThe second derivative of the log-likelihood is the Hessian \\((H(\\theta))\\).\n\nThe second derivative is a measure of the curvature of the likelihood function. This will help us confirm that we are at a maximum, and it will also help us calculate the uncertainty.\nThe more curved (i.e., the steeper the curve), the more certainty we have.\nThe \\(I\\) stands for the information matrix. The \\(H\\) stands for Hessian. \\(I(\\theta) = - \\mathbb{E}(H)\\)\n\n\\(var(\\theta) = [I(\\theta)]^{-1} = ( - \\mathbb{E}(H))^{-1}\\)\nStandard errors are the square roots of the diagonals of this \\(k \\times k\\) matrix (like vcov() in OLS)\n\n\nExample: Normal\nStart with the log-likelihood\n\\[\\begin{align*}\n\\ell(\\theta | Y) &= \\sum_{i = 1}^N \\log  \\frac{1}{\\sigma\\sqrt{2\\pi}} - \\frac{(Y_i-x_i'\\beta)^2}{2\\sigma^2}\n\\end{align*}\\]\nBecause our \\(\\theta\\) has two parameters, the Hessian actually has four components. For this example, we will focus on one: the first and second derivatives wrt \\(\\beta\\).\n\nRecall the first derivative = \\(\\frac{1}{\\sigma^2}X'(Y - X\\hat \\beta)\\).\nWe now take the second derivative with respect to \\(\\hat \\beta\\)\n\\[\\begin{align*}\n\\frac{\\delta^2}{\\delta \\hat \\beta} \\frac{1}{\\sigma^2}X'(Y - X\\hat \\beta)&= -\\frac{1}{\\sigma^2}X'X\n\\end{align*}\\]\nTo get our variance, we take the inverse of the negative (-) of this:\n\n\\(\\sigma^2(X'X)^{-1}\\) Should look familiar!\n\n\nWith this example, we can start to see why lm and glm for a normally distributed outcome generate the same estimates. The maximum likelihood estimator is the same as the least squares estimator.\n\n\n5.3.4 MLE Estimation Algorithm\nSuppose we are interested in finding the true probability \\(p\\) that a comment made on twitter is toxic, and we have a small sample of hand-coded data. Let’s say we have \\(n=8\\) observations where we could observe a \\(y_i = 1\\) or \\(0\\). For example, let’s say we read an online sample of tweets and we classified tweets as “toxic=1” or “nontoxic=0.” In our sample of \\(n=8\\), we coded 6 of them as toxic and 2 as nontoxic.\n\nWe can write down the likelihood for a single observation using the Bernouilli pmf:\n\\(L(p | y_i) = p^{y_i}*(1-p)^{(1-y_i)}\\)\nWe could then write out the likelihood for all 8 observations as follows:\n\nWhere the equation simplifies to \\(p\\) for observations where \\(y_i\\) = 1 and (1-p) for observations where \\(y_i\\) = 0. For simplicity, let’s say \\(i=1\\) to \\(6\\) were toxic, and \\(i=7\\) to \\(8\\) were nontoxic.\n\\(L(p | \\mathbf{y}) = p * p * p * p * p * p * (1-p) * (1-p)\\)\n\nNow a naive way to maximize the likelihood would be to just try out different quantities for \\(p\\) and see which give us the maximum.\n\n## Let's try this for different p's\np &lt;- seq(.1, .9, .05)\nL &lt;- p * p * p * p * p * p * (1-p) * (1-p)\n\nWe can then visualize the likelihood results and figure out about at which value for \\(\\hat p\\) we have maximized the likelihood.\n\nplot(x=p, y=L, type=\"b\",\n     xaxt=\"n\")\naxis(1, p, p)\n\n\n\n\nWhen we have more complicated models, we are taking a similar approach–trying out different values and comparing the likelihood (or log likelihood), but we will rely on a specific algorithm(s) that will help us get to the maximum a bit faster than a naive search would allow.\nDon’t worry the built-in functions in R will do this for you (e.g., what happens under the hood of glm()), but if you were to need to develop your own custom likelihood function for some reason, you could directly solve it through an optimization algorithm if no such built-in function is appropriate.\nYou can skip the details below if you wish and jump to the MLE Properties section. This content will only be involved in problem sets as extra credit, as you may not have to use optim in your own research.\nThe optim function in R provides one such approach. For this optimization approach, we will need to.\n\nDerive the likelihood and/or log likelihood function and score\nCreate an R function for the quantity to want to optimize (often the log likelihood) where given we provide the function certain values, the function returns the resulting quantity. (Kind of like when we supply the function mean() with a set of values, it returns the average of the values by computing the average under the hood of the function.)\nUse optim() to maximize\n\noptim(par, fn, ..., gr, method, control, hessian,...), where\npar: initial values of the parameters\nfn: function to be maximized (minimized)\ngr: optional argument, can include the gradient to help with optimization\n...: (specify other variables in fn)\nmethod: optimization algorithm\ncontrol: parameters to fine-tune optimization\nhessian: returns the Hessian matrix if TRUE\n\n\nBy default, optim performs minimization. Make sure to set control = list(fnscale=-1) for maximization\n\nFor starting values par, least squares estimates are often used. More sensible starting values help your optimize more quickly. You may need to adjust the maxit control parameter to make sure the optimization converges.\nA commonly used method is BFGS (a variant of Newton-Raphson), similar to what glm() uses, but there are other methods available.\n\nExample 1: estimating p\nLet’s take our relatively simple example about toxic tweets above and optimize the likelihood. First, we create a function for the likelihood that will calculate the likelihood for the values supplied. In the future, our models will be complicated enough, we will stick with the log likelihood, which allows us to take a sum instead of a product.\nOne benefit of R is that you can write your own functions, just like mean() is a built-in function in R. For more information on writing functions, you can review Imai QSS Chapter 1 pg. 19..\n\nlik.p &lt;- function(p){\n  lh &lt;- p * p * p * p * p * p * (1-p) * (1-p)\n  return(lh)\n}\n\nOk, now that we have our likelihood function, we can optimize. We just have to tell R a starting parameter for \\(\\hat p\\). Let’s give it a (relatively) bad one just to show how it works (i.e., can optim find the sensible .75 value. If you give the function too bad of a value, it might not converge before it maxes out and instead return a local min/max instead of a global one.\n\nstartphat &lt;- .25\nopt.fit &lt;- optim(par = startphat, fn=lik.p, method=\"BFGS\",\n                 control=list(fnscale=-1))\n\n## This should match our plot\nopt.fit$par\n\n[1] 0.7500035\n\n## you should check convergence. Want this to be 0 to make sure it converged\nopt.fit$convergence\n\n[1] 0\n\n\nExample 2: Linear Model\nWe can use optim to find a solution for a linear model by supplying R with our log likelihood function.\nFor the MLE of the normal linear model, our log likelihood equation is:\n\\[\\begin{align*}\n\\ell(\\theta | Y) &= \\sum_{i = 1}^N \\log  \\frac{1}{\\sigma\\sqrt{2\\pi}} - \\frac{(Y_i-\\mathbf{x}_i'\\beta)^2}{2\\sigma^2}\n\\end{align*}\\]\nNow that we have our log likelihood, we can write a function that for a given set of \\(\\hat \\beta\\) and \\(\\hat \\sigma^2\\) parameter values, \\(X\\), and \\(Y\\), it will return the log likelihood.\n\nBelow we indicate we will supply an argument par (an arbitrary name) that will inclue our estimates for the parameters: \\(k\\) values for the set of \\(\\hat \\beta\\) estimates and a \\(k + 1\\) value for the \\(\\hat \\sigma^2\\) estimate. Many models with only have one set of parameters. This is actually a slightly more tricky example.\nThe lt line is the translation of the equation above into R code\n\n\n## Log Likelihood function for the normal model\nl_lm &lt;- function(par, Y, X){\n  k &lt;- ncol(X)\n  beta &lt;- par[1:k]\n  sigma2 &lt;- par[(k+1)]\n  lt &lt;- sum(log(1/(sqrt(sigma2)*sqrt(2*pi))) - ((Y - X %*% beta)^2/(2*sigma2)))\n  return(lt)\n}\n\nNow that we have our function, we can apply it to a problem.\nLet’s use an example with a sample of Democrats from the 2016 American National Election Study dataset. This example is based on the article “Hostile Sexism, Racial Resentment, and Political Mobilization” by Kevin K. Banda and Erin C. Cassese published in Political Behavior in 2020. We are not replicating their article precisely, but we use similar data and study similar relationships.\nThe researchers were interested in how cross-pressures influence the political participation of different partisan groups. In particular, they hypothesized that Democrats in the U.S. who held more sexist views would be demobilized from political participation in 2016, a year in which Hillary Clinton ran for the presidency.\nThe data we are using are available anesdems.csv and represent a subset of the data for Democrats (including people who lean toward the Democratic party). We have a few variables of interest\n\nparticipation: a 0 to 8 variable indicating the extent of a respondent’s political participation\nfemale: a 0 or 1 variable indicating if the respondent is female\nedu: a numeric variable indicating a respondent’s education level\nage: a numeric variable indicating a respondent’s age.\nsexism: a numeric variable indicating a respondent’s score on a battery of questions designed to assess hostile sexism, where higher values indicate more hostile sexism.\n\nLet’s regress participation on these variables and estimate it using OLS, GLM, and optim. Note, OLS and GLM fit through their functions in R will automatically drop any observations that have missing data on these variables. To make it comparable with optim, we will manually eliminate missing data.\n\nanes &lt;- read.csv(\"https://raw.githubusercontent.com/ktmccabe/teachingdata/main/anesdems.csv\")\n## choose variables we will use\nanes &lt;- subset(anes, select=c(\"participation\", \"age\", \"edu\", \"sexism\", \"female\"))\n## omit observations with missing data on these variables\nanes &lt;- na.omit(anes)\n\n## OLS and GLM regression\nfit &lt;- lm(participation ~ female + edu + age + sexism, data=anes)\nfit.glm &lt;- glm(participation ~ female + edu + age + sexism, data=anes,\n               family=gaussian(link=\"identity\"))\n\nNow we will build our data for optim. We need \\(X\\), \\(Y\\), and a set of starting \\(\\hat \\beta\\) and \\(\\hat \\sigma^2\\) values.\n\n## X and Y data\nX.anes &lt;- model.matrix(fit)\nY.anes &lt;- as.matrix(anes$participation)\n## make sure dimensions are the same\nnrow(X.anes)\n\n[1] 1585\n\nnrow(Y.anes)\n\n[1] 1585\n\n## Pick starting values for parameters\nstartbetas &lt;- coef(fit)\n## Recall our estimate for sigma-squared based on the residuals\nk &lt;- ncol(X.anes)\nstartsigma &lt;- sum(fit$residuals^2) / (nrow(X.anes) - k )\nstartpar &lt;- c(startbetas, startsigma)\n\n## Fit model\n## But let's make it harder on the optimization by providing arbitrary starting values\n## (normally you wouldn't do this)\nstartpar &lt;- c(1,1,1,1,1,1)\nopt.fit &lt;- optim(par = startpar, fn=l_lm, X = X.anes,\n                 Y=Y.anes, method=\"BFGS\",\n                 control=list(fnscale=-1),\n                   hessian=TRUE)\n\nWe can compare this optimization approach to the output in glm().\nWe can first compare the log likelihoods\n\nlogLik(fit.glm)\n\n'log Lik.' -2661.428 (df=6)\n\nopt.fit$value\n\n[1] -2661.428\n\n\nWe can compare the coefficients.\n\n## Coefficients\nround(coef(fit), digits=4)\nround(coef(fit.glm), digits=4)\nround(opt.fit$par, digits=4)[1:k]\n\n(Intercept)      female         edu         age      sexism \n     0.9293     -0.2175      0.1668      0.0088     -0.9818 \n(Intercept)      female         edu         age      sexism \n     0.9293     -0.2175      0.1668      0.0088     -0.9818 \n[1]  0.9294 -0.2175  0.1668  0.0088 -0.9819\n\n\nWe can add the gradient of the log likelihood to help improve optimization. This requires specifying the first derivative (the score) of the parameters. Unfortunately this means taking the derivative of that ugly normal log likelihood above. Again, with the normal model, we have two scores because of \\(\\hat \\beta\\) and \\(\\hat \\sigma^2\\). For others, we may just have one.\n\n## first derivative function\nscore_lm &lt;- function(par, Y, X){\n  k &lt;- ncol(X)\n  beta &lt;- as.matrix(par[1:k])\n  scorebeta &lt;- (1/par[k+1]) * (t(X) %*% (Y - X %*% beta))\n  scoresigma &lt;- -nrow(X)/(par[k+1]*2) + sum((Y - X %*% beta)^2)/(2 * par[k+1]^2)\n  return(c(scorebeta, scoresigma))\n}\n\n## Fit model\nopt.fit &lt;- optim(par = startpar, fn=l_lm, gr=score_lm, X = X.anes,\n                 Y=Y.anes, method=\"BFGS\",\n                 control=list(fnscale=-1),\n                   hessian=TRUE)\n\nIn addition to using optim, we can program our own Newton-Raphson algorithm, which is a method that continually updates the coefficient estimates \\(\\hat \\beta\\) until it converges on a set of estimates. We will see this in a future section. The general algorithm involves the components we’ve seen before: values for \\(\\hat \\beta\\), the score, and the Hessian.\n\nNewton-Raphson: \\(\\hat \\beta_{new} = \\hat \\beta_{old} - H(\\beta_{old})^{-1}S(\\hat \\beta_{old})\\)"
  },
  {
    "objectID": "05-IntrotoMaximumLikelihood.html#mle-properties",
    "href": "05-IntrotoMaximumLikelihood.html#mle-properties",
    "title": "5  Introduction to MLE",
    "section": "5.4 MLE Properties",
    "text": "5.4 MLE Properties\nJust like OLS had certain properties (BLUE) that made it worthwhile, models using MLE also have desirable features under certain assumptions and regularity conditions.\nLarge sample properties\n\nMLE is consistent: \\(p\\lim \\hat \\theta^{ML} = \\theta\\)\nIt is also asymptotically normal: \\(\\hat \\theta^{ML} \\sim N(\\theta, [I(\\theta)]^{-1})\\)\n\nThis will allow us to use the normal approximation to calculate z-scores and p-values\n\nAnd it is asymptotically efficient. “In other words, compared to any other consistent and uniformly asymptotically Normal estimator, the ML estimator has a smaller asymptotic variance” (King 1998, 80).\n\nNote on consistency\nWhat does it mean to say an estimator is consistent? As samples get larger and larger, we converge to the truth.\nConsistency: \\(p\\lim \\hat{\\theta} =\\beta\\) As \\(n \\rightarrow \\infty P(\\hat{\\theta} - \\theta&gt; e) \\rightarrow 0\\).\n\nConvergence in probability: the probability that the absolute difference between the estimate and parameter being larger than \\(e\\) goes to zero as \\(n\\) gets bigger.\n\nNote that bias and consistency are different: Consistency means that as the sample size (\\(n\\)) gets large the estimate gets closer to the true value. Unbiasedness is not affected by sample size. An estimate is unbiased if over repeated samples, its expected value (average) is the true parameter.\nIt is possible for an estimator to be unbiased and consistent, biased and not consistent, or consistent yet biased.\n; \n\nImages taken from here\nWhat is a practical takeaway from this? The desirable properties of MLE kick in with larger samples. When you have a very small sample, you might use caution with your estimates.\nNo Free Lunch\nWe have hinted at some of the assumptions required, but below we can state them more formally.\n\nFirst, we assume a particular data generating process: the probability model.\nWe are generally assuming that observations are independent and identically distributed (allowing us to write the likelihood as a product)– unless we explicitly write the likelihood in a way that takes this into account.\n\nWhen we have complicated structures to our data, this assumption may be violated, such as data that is clustered in particular hierarchical entities.\n\nWe assume the model (i.e., the choice of covariates and how they are modeled) is correctly specified. (e.g., no omitted variables.)\nWe have to meet certain technical regularity conditions–meaning that our problem is a “regular” one. These, in the words of Gary King are “obviously quite technical” (1998, 75). We will not get into the details, but you can see pg. 75 of Unifying Political Methodology for the formal mathematical statements. In short, our paramaters have to be identifiable and within the parameter space of possible values (this identifiability can be violated, for example, when we have too many parameters relative to the number of observations in the sample), we have to be able to differentiate the log-likelihood (in fact, it needs to be twice continuously differentiable) along the support (the range of values) in the data. The information matrix, which we get through the second derivative, must be positive definite and finitely bounded. This helps us know we are at a maximum, and the maximum exists and is finite. You can visualize this as a smooth function, that is not too sharp (which would make it non differentiable), but has a peak (a maximum) that we can identify.\n\n\n5.4.1 Hypothesis Tests\nWe can apply the same hypothesis testing framework to our estimates here as we did in linear regression. First, we can standardize our coefficient estimates by dividing them by the standard error. This will generate a “z score.” Just like when we had the t value in OLS, we can use the z score to calculate the p-value and make assessments about the null hypothesis that a given \\(\\hat \\beta_k\\) = 0.\n\\[\\begin{align*}\nz &= \\frac{\\hat \\theta_k}{\\sqrt{Var(\\hat \\theta)_k}} \\sim N(0,1)\n\\end{align*}\\]\nNote: to get p-values, we typically now use, 2 * pnorm(abs(z), lower.tail=F) instead of pt() and our critical values are based on qnorm() instead of qt(). R will follow the same in most circumstances. In large samples, these converge to the same quantities.\n\n\n5.4.2 Model Output in R\nAs discussed, we can fit a GLM in R using the glm function:\n\nglm(formula, data, family = XXX(link = \"XXX\", ...), ...)\n\nformula: The model written in the form similar to lm()\ndata: Data frame\nfamily: Name of PDF for \\(Y_i\\) (e.g. binomial, gaussian)\nlink: Name of the link function (e.g. logit, `probit, identity, log)\n\n\n\n## Load Data\nfit.glm &lt;- glm(participation ~ female + edu + age + sexism, data=anes,\n               family=gaussian(link=\"identity\"))\n\nWe’ve already discussed the coefficient output. Like lm(), GLM wiil also display the standard errors, z-scores / t-statistics, and p-values of the model in the model summary.\n\nsummary(fit.glm)$coefficients\n\n                Estimate  Std. Error   t value     Pr(&gt;|t|)\n(Intercept)  0.929302597 0.154033186  6.033132 1.999280e-09\nfemale      -0.217453631 0.067415200 -3.225588 1.282859e-03\nedu          0.166837062 0.022402469  7.447262 1.559790e-13\nage          0.008795138 0.001874559  4.691843 2.941204e-06\nsexism      -0.981808431 0.154664950 -6.347970 2.844067e-10\n\n\nFor this example, R reverts to the t-value instead of the z-score given that we are using the linear model. In other examples, you may see z in place of t. There are only small differences in these approximations because as your sample size gets larger, the degrees of freedom (used in the calculation of p-calues for the t distribution) are big enough that the t distribution converges to the normal distribution.\nGoodness of fit\nThe glm() model has a lot of summary output.\n\nsummary(fit.glm)\n\n\nCall:\nglm(formula = participation ~ female + edu + age + sexism, family = gaussian(link = \"identity\"), \n    data = anes)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.3001  -0.8343  -0.3013   0.3651   7.2887  \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.929303   0.154033   6.033 2.00e-09 ***\nfemale      -0.217454   0.067415  -3.226  0.00128 ** \nedu          0.166837   0.022402   7.447 1.56e-13 ***\nage          0.008795   0.001875   4.692 2.94e-06 ***\nsexism      -0.981808   0.154665  -6.348 2.84e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 1.688012)\n\n    Null deviance: 2969.3  on 1584  degrees of freedom\nResidual deviance: 2667.1  on 1580  degrees of freedom\nAIC: 5334.9\n\nNumber of Fisher Scoring iterations: 2\n\n\nSome of the output represents measures of the goodness of fit of the model. However, their values are not directly interpretable from a single model.\n\nLarger (less negative) likelihood, the better the model fits the data. (logLik(mod)). The becomes relevant when comparing two or more models.\nDeviance is calculated from the likelihood. This is a measure of discrepancy between observed and fitted values. (Smaller values, better fit.)\n\nNull deviance: how well the outcome is predicted by a model that includes only the intercept. (\\(df = n - 1\\))\nResidual deviance: how well the outcome is predicted by a model with our parameters. (\\(df = n-k\\))\n\nAIC- used for model comparison. Smaller values indicate a more parsimonious model. Accounts for the number of parameters (\\(K\\)) in the model (like Adjusted R-squared, but without the ease of interpretation). Sometimes used as a criteria in prediction exercises (using a model on training data to predict test data). For more information on how AIC can be used in prediction exercises, see here.\n\nLikelihood Ratio Test\nThe likelihood ratio test compares the fit of two models, with the null hypothesis being that the full model does not add more explanatory power to the reduced model. Note: You should only compare models if they have the same number of observations.\nThis type of test is used in a similar way that people compare R-squared values across models.\n\nfit.glm2 &lt;- glm(participation ~ female + edu + age + sexism, data=anes,\n               family=gaussian(link=\"identity\"))\n\nfit.glm1 &lt;- glm(participation ~ female + edu + age, data=anes, \n               family=gaussian(link = \"identity\"))\n               \nanova(fit.glm1, fit.glm2, test = \"Chisq\") #  reject the null\n\nAnalysis of Deviance Table\n\nModel 1: participation ~ female + edu + age\nModel 2: participation ~ female + edu + age + sexism\n  Resid. Df Resid. Dev Df Deviance  Pr(&gt;Chi)    \n1      1581     2735.1                          \n2      1580     2667.1  1   68.021 2.182e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nPseudo-R-squared\nWe don’t have an exact equivalent to the R-squared in OLS, but people have developed “pseudo” measures.\nExample: McFadden’s R-squared\n\n\\(PR^2 = 1 - \\frac{\\ell(M)}{\\ell(N)}\\)\n\nwhere \\(\\ell(M)\\) is the log-likelihood for your fitted model and \\(\\ell(N)\\) is the log-likelihood for a model with only the intercept\n\nRecall, greater (less negative) values of the log-likelihood indicate better fit\nMcFadden’s values range from 0 to close to 1\n\n\n# install.packages(\"pscl\")\nlibrary(pscl)\nfit.glm.null &lt;- glm(participation ~ 1, anes, family = gaussian(link = \"identity\"))\npr &lt;- pR2(fit.glm1)\n\nfitting null model for pseudo-r2\n\npr[\"McFadden\"]\n\n  McFadden \n0.02370539 \n\n## Or, by hand:\n1 - (logLik(fit.glm1)/logLik(fit.glm.null))\n\n'log Lik.' 0.02370539 (df=5)"
  },
  {
    "objectID": "05-IntrotoMaximumLikelihood.html#footnotes",
    "href": "05-IntrotoMaximumLikelihood.html#footnotes",
    "title": "5  Introduction to MLE",
    "section": "",
    "text": "In the future, we may want to know the probability of turning out to vote, or of going to war, or voting “yes” on a particular policy question, etc.↩︎\nWhy does this work? It has to do with the shape of the log (always increasing). Details are beyond the scope.↩︎\nEssentially, you need to take derivatives with respect to each of the parameters. Some models we use will have only one parameter, which is easier.↩︎"
  },
  {
    "objectID": "06-BinaryOutcomes.html#data-generating-process",
    "href": "06-BinaryOutcomes.html#data-generating-process",
    "title": "6  Binary Dependent Variables",
    "section": "6.1 Data Generating Process",
    "text": "6.1 Data Generating Process\nLet’s say \\(Y_i\\) is a set of 0’s and 1’s for whether two states have experienced a dispute, an outcome common in IR studies.\n\\[\\begin{gather*}\nY_i = \\begin{cases}1, \\;\\text{a dispute happened}\\\\ 0,\\;\n\\text{a dispute did not happen}\\end{cases}\n\\end{gather*}\\]\n\nOutside of political science, for example, in the field of higher education, we might instead think about an outcome related to whether a student has (= 1) or has not (= 0) had a negative advising experience.\n\n\n# Example of first 20 observations (Y1, Y2, ..., Y20)\n1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n\nWe need to align these data with a data generating process and distribution.\n\nFor each \\(Y_i\\), it is like a single trial, where you have a dispute with some probability \\((\\pi)\\)\n\nThis sounds like the Bernoulli distribution! \\(Y_i \\sim Bernouli(\\pi)\\)\n\n\n\n6.1.1 MLE Estimation\nLet’s do the steps we saw in the previous section.\n\nWhat is the data generating process? Based on this, describe the probability distribution for \\(Y_i\\).\n\nNote: if you are using a function like glm() you can proceed directly there after this step. However, let’s work under the hood for a bit.\n\n\\[\\begin{align*}\nY_i \\sim f(Y_i | \\pi) &= Pr(Y_i = y_i |\\pi_i) = \\underbrace{\\pi^{y_i}(1 -\\pi)^{(1-y_i)}}_\\text{pmf for Bernoulli}\n\\end{align*}\\] for \\(y = 0,1\\)\nDefine the likelihood for a single observation\nDefine the likelihood for all observations\nFind the log-likelihood\n\n\\[\\begin{align*}\n\\mathcal L( \\pi | Y_i) &= \\underbrace{\\pi^{y_i}(1 -\\pi)^{(1-y_i)}}_\\text{Likelihood for single observation}\\\\\n\\mathcal L( \\pi | Y) &= \\underbrace{\\prod_{i=1}^n\\pi^{y_i}(1 -\\pi)^{(1-y_i)}}_\\text{Likelihood for all observations}\\\\\n\\ell( \\pi | Y) &= \\underbrace{\\sum_{i=1}^n\\log \\pi^{y_i}(1 -\\pi)^{(1-y_i)}}_\\text{Log likelihood}\\\\\n\\hat \\pi &= \\text{Next step: arg max } \\ell( \\pi | Y) \\text{ wrt $\\pi$}\n\\end{align*}\\]\nAdd step: nonlinear transformation to \\(X\\beta\\)\n\nNote that because we are likely using covariates, we need to express our parameter as a function of \\(X\\beta\\). Why? Because we don’t think there is a constant probability for a dispute. Instead, we think the probability of a dispute varies according to different independent variables, which are included in the \\(X\\) matrix and everntually will each have their own \\(\\beta_k\\) relationship with the probability of dispute.\n\nNow that we are outside of linear territory, we cannot just simply replace \\(\\pi\\) with \\(X\\beta\\) in the equation. Instead, \\(\\pi\\) is a function of \\(X\\beta\\). \\(\\pi = g(X_i, \\beta) \\neq \\mathbf{x}_i'\\beta\\)\n\nThis is because we need a transformation, such as the logit or probit to map our linear predictor into the outcome to make sure the linear predictor can be transformed back into sensible units of the outcome. The logit is one variety, as is the probit. Like two roads diverged in a yellow wood, this is the point in the process where we choose the transformation. For this first example, let’s apply a logit transformation, which restricts our estimates to between 0 and 1 (a good thing for probability!) where:\n\\(\\pi_i = \\text{logit}^{-1}(\\eta_i) = \\frac{exp^{\\eta_i}}{1 + exp^{\\eta_i}} = \\frac{exp^{\\mathbf{x}_i'\\beta}}{1 + exp^{\\mathbf{x}_i'\\beta}}\\)\n\\(\\eta_i = \\text{logit}(\\pi_i) = \\log\\frac{\\pi_i}{1-\\pi_i} = \\mathbf{x}_i'\\beta\\)\n\n\nMaximize the function with respect to (wrt) \\(\\theta\\)\n\nWhere \\(\\pi_i = \\frac{exp^{\\mathbf{x}_i'\\beta}}{1 + exp^{\\mathbf{x}_i'\\beta}}\\)\n\\[\\begin{align*}\n\\hat \\pi &= \\text{arg max } \\ell( \\pi | Y) \\text{ wrt $\\pi$} \\\\\n&= \\text{arg max} \\sum_{i=1}^n\\log \\pi_i^{y_i}(1 -\\pi_i)^{(1-y_i)}\\\\\n&= \\text{arg max} \\sum_{i=1}^n \\underbrace{y_i \\log \\Big( \\frac{exp^{\\mathbf{x}_i'\\beta}}{1 + exp^{\\mathbf{x}_i'\\beta}}\\Big) + (1-y_i)\\log \\Big(1-\\frac{exp^{\\mathbf{x}_i'\\beta}}{1 + exp^{\\mathbf{x}_i'\\beta}}\\Big)}_\\text{We replaced $\\pi_i$ and used the rule $\\log a^b = b \\log a$ to bring down the $y_i$ terms.}\n\\end{align*}\\]\nAt this point, we take the derivative with respect to \\(\\beta\\). You can try this on your own, and, if you’re lucky, it may show up on a problem set near you. With a bit of work, we should get something that looks like the below, which we can represent in terms of a sum or in matrix notation.\n\\[\\begin{align*}\nS(\\theta) &=  \\sum_{i=1}^n (Y_i - \\pi_i)\\mathbf{x}^T_i\\\\\n&= X^T(Y - \\mathbf{\\pi})\n\\end{align*}\\]\nYou can note that the matrix notation retains the dimensions \\(k \\times 1\\), which we would expect because we want to choose a set of \\(k\\) coefficients in our \\(k \\times 1\\) vector \\(\\beta\\). The score, as written in summation notation, also has length \\(k\\) but here, we use a convention as writing \\(\\mathbf{x}_i'\\) in row vector representation instead of a column vector. You could instead represent this as multiplied by \\(\\mathbf{x}_i\\), which would give us the \\(k \\times 1\\) dimensions. Either way we have \\(k\\) coefficients. These are just different notations.\n\nTake the second derivative of the log likelihood to get the “hessian” and help estimate the uncertainty of the estimates. Again, we can represent this as a sum or in matrix notation, which might be easier when translating this into R code where our data is more naturally inside a matrix.\n\n\\[\\begin{align*}\nH(\\theta) &=  - \\sum_{i=1}^n \\mathbf{x}_i\\mathbf{x}^T_i(\\pi_i)(1 - \\pi_i)\\\\\n&= -X^TVX\n\\end{align*}\\] where \\(V\\) is \\(n \\times n\\) diagonal matrix with weights that are the ith element of \\((\\pi)(1 - \\pi)\\)\nOnce we have these quantities, we can optimize the function with an algorithm or go to glm in R, which will do that for us."
  },
  {
    "objectID": "06-BinaryOutcomes.html#r-code-for-fitting-logistic-regression",
    "href": "06-BinaryOutcomes.html#r-code-for-fitting-logistic-regression",
    "title": "6  Binary Dependent Variables",
    "section": "6.2 R code for fitting logistic regression",
    "text": "6.2 R code for fitting logistic regression\nWe can fit logistic regressions in R through glm(). Let’s build on the ANES example from section 5.3 and analyze a dichotomized measure of participation where 1=participated in at least some form and 0=did not participate.\n\nanes &lt;- read.csv(\"https://raw.githubusercontent.com/ktmccabe/teachingdata/main/anesdems.csv\")\nanes$partbinary &lt;- ifelse(anes$participation &gt; 0, 1, 0)\n\nWe can then fit using glm where family = binomial(link=\"logit\")\n\nout.logit &lt;- glm(partbinary ~ female + edu + age + sexism, data=anes,\n                 family = binomial(link=\"logit\"))\n\nThe summary output includes the logit coefficients, standard errors, z-scores, and p-values.\n\nsummary(out.logit)\n\n\nCall:\nglm(formula = partbinary ~ female + edu + age + sexism, family = binomial(link = \"logit\"), \n    data = anes)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.5668   0.3328   0.4475   0.6287   1.2936  \n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  1.016734   0.334656   3.038  0.00238 ** \nfemale      -0.382087   0.151516  -2.522  0.01168 *  \nedu          0.321190   0.050945   6.305 2.89e-10 ***\nage          0.008682   0.004046   2.146  0.03188 *  \nsexism      -1.593694   0.336373  -4.738 2.16e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1361.5  on 1584  degrees of freedom\nResidual deviance: 1252.9  on 1580  degrees of freedom\n  (355 observations deleted due to missingness)\nAIC: 1262.9\n\nNumber of Fisher Scoring iterations: 5\n\n\n\n6.2.1 Writing down the regression model\nIn the articles you write, you will describe the methods you use in detail, including the variables in the model and the type of regression (e.g., logistic regression). Sometimes you may want to go a step further and be very explicit about the model that you ran. We’ve already seen the regression equations for linear models. For the GLMs, they will look very similar, but we need to make the link/response function an explicit part of the equation.\nFor example, for logistic regression we have a few ways of writing it, including:\n\n\\(\\log \\frac{\\pi_i}{1-\\pi_i} = \\mathbf{x_i'}\\beta\\), or alternatively\n\\(Pr(Y_i = 1 | \\mathbf{x}_i) = logit^{-1}(\\mathbf{x}_i'\\beta) = \\frac{exp(\\mathbf{x_i'}\\beta)}{(1 + exp(\\mathbf{x_i'}\\beta)}\\)\n\n(You can also write out the individual variable names.) There is a new R package equatiomatic that can also be used to help write the equations from regression models. It’s not perfect, but should get you there for most basic models.\n## First time, you need to install one of these\n#remotes::install_github(\"datalorax/equatiomatic\")\n#install.packages(\"equatiomatic\")\n\n## Each time after, run library\nlibrary(equatiomatic)\n\n## Will output in latex code, though see package for details on options\nextract_eq(out.logit, wrap = TRUE, terms_per_line = 3)\n\n\\[\n\\begin{aligned}\n\\log\\left[ \\frac { P( \\operatorname{partbinary} = \\operatorname{1} ) }{ 1 - P( \\operatorname{partbinary} = \\operatorname{1} ) } \\right] &= \\alpha + \\beta_{1}(\\operatorname{female}) + \\beta_{2}(\\operatorname{edu})\\ + \\\\\n&\\quad \\beta_{3}(\\operatorname{age}) + \\beta_{4}(\\operatorname{sexism})\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "06-BinaryOutcomes.html#probit-regression",
    "href": "06-BinaryOutcomes.html#probit-regression",
    "title": "6  Binary Dependent Variables",
    "section": "6.3 Probit Regression",
    "text": "6.3 Probit Regression\nProbit regression is very similar to logit except we use a different link function to map the linear predictor into the outcome. Both the logit and probit links are suitable for binary outcomes with a Bernoulli distribution. If we apply a probit transformation, this also restricts our estimates to between 0 and 1.\n\n\\(\\pi_i = Pr(Y_i = 1| X_i) = \\Phi(\\mathbf{x}_i'\\beta)\\)\n\\(\\eta_i = \\Phi^{-1}(\\pi_i) = \\mathbf{x}_i'\\beta\\)\n\nHere, our coefficients \\(\\hat \\beta\\) represent changes in “probits” or changes “z-score” units. We use the Normal CDF (\\(\\Phi()\\)) aka pnorm() in R to transform them back into probabilities, specifically, the probability that \\(Y_i\\) is 1.\nLet’s fit our binary model with probit. We just need to change the link function.\nWe can then fit using glm where family = binomial(link=\"probit\")\n\nout.probit &lt;- glm(partbinary ~ female + edu + age + sexism, data=anes,\n                 family = binomial(link=\"probit\"))\n\nLet’s apply the equation tool to this:\n## Each time after, run library\nlibrary(equatiomatic)\n\n## Will output in latex code, though see package for details on options\nextract_eq(out.probit, wrap = TRUE, terms_per_line = 3)\n\n\\[\n\\begin{aligned}\nP( \\operatorname{partbinary} = \\operatorname{1} ) &= \\Phi[\\alpha + \\beta_{1}(\\operatorname{female}) + \\beta_{2}(\\operatorname{edu})\\ + \\\\\n&\\qquad\\ \\beta_{3}(\\operatorname{age}) + \\beta_{4}(\\operatorname{sexism})]\n\\end{aligned}\n\\]\n\nThe summary output includes the probit coefficients, standard errors, z-scores, and p-values.\n\nsummary(out.probit)\n\n\nCall:\nglm(formula = partbinary ~ female + edu + age + sexism, family = binomial(link = \"probit\"), \n    data = anes)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.6343   0.3188   0.4470   0.6361   1.2477  \n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  0.603661   0.184864   3.265  0.00109 ** \nfemale      -0.202300   0.083407  -2.425  0.01529 *  \nedu          0.179264   0.027611   6.493 8.44e-11 ***\nage          0.005145   0.002257   2.280  0.02261 *  \nsexism      -0.898871   0.186443  -4.821 1.43e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1361.5  on 1584  degrees of freedom\nResidual deviance: 1250.6  on 1580  degrees of freedom\n  (355 observations deleted due to missingness)\nAIC: 1260.6\n\nNumber of Fisher Scoring iterations: 5\n\n\nWe can interepret the sign and significance of the coefficients similarly to OLS. They just aren’t in units of \\(Y\\). In the section next week, we will discuss in detail how to generate quantities of interest from this output."
  },
  {
    "objectID": "06-BinaryOutcomes.html#to-logit-or-to-probit",
    "href": "06-BinaryOutcomes.html#to-logit-or-to-probit",
    "title": "6  Binary Dependent Variables",
    "section": "6.4 To logit or to probit?",
    "text": "6.4 To logit or to probit?\nBoth approaches produce a monotonically increasing S-curve in probability between 0 and 1, which vary according to the linear predictor (\\(\\mathbf{x_i}^T\\beta\\)). In this way, either approach satisfies the need to keep our estimates, when transformed, within the plausible range of \\(Y\\).\n Image from Kosuke Imai.\n\nBoth also start with \\(Y_i\\) as bernoulli\nBoth produce the same function of the log-likelihood BUT define \\(\\pi_i\\) and link function differently\nResults–in terms of sign and significance of coefficients– are very similar\n\nLogit coefficients are roughly 1.6*probit coefficients\n\nResults–in terms of predicted probabilities– are very similar\n\nException– at extreme probabilities– Logit has “thicker tails”, gets to 0 and 1 more slowly\n\nSometimes useful–Logit can also be transformed into “odds ratios”\nBy convention, logit slightly more typically used in political science but easy enough to find examples of either\n\nNote on Odds Ratios in Logistic Regression\nCoefficients are in “logits” or changes in “log-odds” (\\(\\log \\frac{\\pi_i}{1 - \\pi}\\)). Some disciplines like to report “odds ratios”\n\nOdds ratio: \\(\\frac{\\pi_i(x1)/(1 - \\pi(x1))}{\\pi_i(x0)/(1 - \\pi(x0))}\\) (at a value of x1 vs. x0)\n\nIf \\(\\log \\frac{\\pi_i}{1 - \\pi} = logodds\\); \\(\\exp(logodds) = \\frac{\\pi_i}{1 - \\pi}\\)\nTherefore, if we exponentiate our coefficients, this represents an odds ratio: the odds of \\(Y_i = 1\\) increase by a factor of (\\(\\exp(\\hat \\beta_k)\\)) due to 1-unit change in X\n\n\n\n## odds ratio for the 4th coefficient\nexp(coef(out.logit)[4])\n\n    age \n1.00872 \n\n## CI for odds ratios\nexp(confint(out.logit)[4, ])\n\nWaiting for profiling to be done...\n\n\n   2.5 %   97.5 % \n1.000790 1.016801 \n\n\nIn political science, we usually opt to present predicted probabilities instead of odds ratios, but ultimately you should do whatever you think is best."
  },
  {
    "objectID": "06-BinaryOutcomes.html#latent-propensity-representation",
    "href": "06-BinaryOutcomes.html#latent-propensity-representation",
    "title": "6  Binary Dependent Variables",
    "section": "6.5 Latent propensity representation",
    "text": "6.5 Latent propensity representation\nSometimes you will see the binary outcome problem represented as a latent propensity where \\(Y^*_i\\) is a continuous variable that represents an unobserved propensity (e.g., to have a dispute, to be a toxic tweet, to participate), where\n\\[\\begin{gather*}\nY_i = \\begin{cases}1, \\; y^*_i &gt; \\tau \\\\ 0,\\;\ny^*_i \\leq \\tau \\end{cases}\n\\end{gather*}\\]\nand \\(\\tau\\) is some threshold after which a the event (e.g., dispute) occurs.\nThis becomes particularly relevant when the goal is to classify outcome estimates given certain \\(X\\) features. This type of threshold will also be relevant when we move into ordinal outcome variables where we want to estimate the probability an outcome belongs to a specific category."
  },
  {
    "objectID": "06-BinaryOutcomes.html#linear-probability-models",
    "href": "06-BinaryOutcomes.html#linear-probability-models",
    "title": "6  Binary Dependent Variables",
    "section": "6.6 Linear Probability Models",
    "text": "6.6 Linear Probability Models\nPeople (who me? yes, I admit, me) will sometimes still use a linear OLS model when we have dichotomous outcomes. In that case, we interpret the results as a “linear probability model” where a one-unit change in \\(x\\) is associated with a \\(\\hat \\beta\\) change in the probability that \\(Y_i = 1\\).\nThis may sound like a disaster because linear models are generally meant for nice continuous outcomes, and there is no way to prevent extreme values of \\(X\\beta\\) from extending above 1 or below 0. This is not to mention the heteroskedasticity issues that come from binary outcome because the error terms depend on the values of \\(X\\). This website has a good overview of the potential problems with linear regression with binary outcomes.\n\\\nImage from Chelsea Parlett-Pelleriti @ChelseaParlett on Twitter\nHowever, we can address some of these potential issues: 1) we can use robust standard errors to account for non-constant error variance , 2) if you look at the S-curve in the previous section, you will note that a large part of the curve is pretty linear over a wide range of \\(X\\beta\\) values. For many applications, the estimates transformed from a logit or probit into probability will look similar to the estimates from a linear probability model (i.e., OLS). 3) Linear probability models are easier to interpret, and there is no need to transform coefficients.\nLPM vs. logit/probit has spurred a lot of debate throughout the years. Reviewers disagree, twitter users disagree, some people just like to stir the pot, etc. This is just something to be aware of as you choose modeling approaches. Particularly when it comes to experiments and other causal inference approaches, there is a non-trivial push among active scholars to stick with linear probability models when your key independent variable is a discrete treatment indicator variable. See this new article from Robin Gomilla who lays out the considerations for using LPM, particularly in experimental settings, as well as follow up discussion from Andrew Gelman. That said, even if you run with an LPM and cite the Gomilla article, a reviewer may still ask you to do a logit/probit. And there are certainly circumstances where LPM will fall short. So what’s the upshot? Probably try both, and then choose your own adventure."
  },
  {
    "objectID": "06-BinaryOutcomes.html#binary-models-in-r-tutorial",
    "href": "06-BinaryOutcomes.html#binary-models-in-r-tutorial",
    "title": "6  Binary Dependent Variables",
    "section": "6.7 Binary Models in R Tutorial",
    "text": "6.7 Binary Models in R Tutorial\nThis week’s example, we will replicate a portion of “The Effectiveness of a Racialized Counterstrategy” by Antoine Banks and Heather Hicks, published in the American Journal of Political Science in 2018. The replication data are here.\nAbstract: Our article examines whether a politician charging a political candidate’s implicit racial campaign appeal as racist is an effective political strategy. According to the racial priming theory, this racialized counterstrategy should deactivate racism, thereby decreasing racially conservative whites’ support for the candidate engaged in race baiting. We propose an alternative theory in which racial liberals, and not racially conservative whites, are persuaded by this strategy. To test our theory, we focused on the 2016 presidential election. We ran an experiment varying the politician (by party and race) calling an implicit racial appeal by Donald Trump racist. We find that charging Trump’s campaign appeal as racist does not persuade racially conservative whites to decrease support for Trump. Rather, it causes racially liberal whites to evaluate Trump more unfavorably. Our results hold up when attentiveness, old-fashioned racism, and partisanship are taken into account. We also reproduce our findings in two replication studies.\nWe will replicate the analysis in Table 1 of the paper, based on an experiment the authors conducted through SSI. They exposed white survey respondents to either a news story about a Trump ad that includes an “implicit racial cue” or conditions that add to this with “explicitly racial” responses from different partisan actors calling out the ad as racist. Drawing on racial priming theory, racially prejudiced whites should be less supportive of Trump after the racial cues are made explicit. The authors test this hypothesis against their own hypothesis that this effect should be more pronounced among “racially liberal” whites.\n \nWe are going to focus on a secondary outcome related to whether respondents believed the ad to be about race: “We also suspect that whites should provide a justification for either maintaining or decreasing their support for the candidate alleged to be playing the race card. Our results support these expectations. For example, racial liberals who read about a politician calling Trump’s implicit ad racist are more likely than those in the implicit condition to believe Trump’s ad is about race. On the other hand, pointing out the racial nature of the ad does not cause resentful whites to be any more likely to believe the ad is about race. Racially resentful whites deny that Trump’s subtle racial appeal on crime is racially motivated, which provides them with the evidence they need to maintain their support for his presidency” (320).\n\n6.7.1 Loading data and fitting glm\nLet’s load the data.\n\nlibrary(rio)\nstudy &lt;- import(\"https://github.com/ktmccabe/teachingdata/blob/main/ssistudyrecode.dta?raw=true\")\n\nThe data include several key variables\n\nabtrace1: 1= if the respondent thought the ad was about race. 0= otherwise\ncondition2: 1= respondent in the implicit condition. 2= respondent in one of four explicit racism conditions.\nracresent: a 0 to 1 numeric variable measuring racial resentment\noldfash: a 0 to 1 numeric variable measuring “old-fashioned racism”\ntrumvote: 1= respondent has vote preference for Trump 0=otherwise\n\n\nLet’s try to replicate column 5 in Table 1 using probit regression, as the authors do.\n\nWrite down the equation for the regression.\nUse glm to run the regression.\nCompare the output to the table, column 5.\n\n\n\nTry on your own, then expand for the solution.\n\nWe are fitting a probit regression.\n\n## Column 5\nfit.probit5 &lt;- glm(abtrace1 ~ factor(condition2)*racresent + factor(condition2)*oldfash,\n                  data=study, family=binomial(link = \"probit\"))\nsummary(fit.probit5)\n\n\nCall:\nglm(formula = abtrace1 ~ factor(condition2) * racresent + factor(condition2) * \n    oldfash, family = binomial(link = \"probit\"), data = study)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.2659  -0.9371  -0.5194   1.0015   2.0563  \n\nCoefficients:\n                              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                     0.6320     0.2431   2.600 0.009323 ** \nfactor(condition2)2             0.9685     0.2797   3.462 0.000535 ***\nracresent                      -1.9206     0.4174  -4.601  4.2e-06 ***\noldfash                         0.5265     0.3907   1.348 0.177772    \nfactor(condition2)2:racresent  -0.8513     0.4728  -1.801 0.071777 .  \nfactor(condition2)2:oldfash    -0.4197     0.4376  -0.959 0.337476    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1376.9  on 994  degrees of freedom\nResidual deviance: 1148.7  on 989  degrees of freedom\n  (25 observations deleted due to missingness)\nAIC: 1160.7\n\nNumber of Fisher Scoring iterations: 4\n\n\nWe can write the regression as: \\[\\begin{align*}\nPr(Y_i = 1 | X) &= \\\\ \\Phi(\\alpha + \\text{Explicit Politician Condition}_i*\\beta_1 +\\\\ \\text{Racial Resentment}_i *\\beta_2 +\\\\\n\\text{Old Fashioned Racism}_i*\\beta_3 +\\\\ \\text{Explicit Politician Condition}_i*\\text{Racial Resentment}_i*\\beta_4 +\\\\\n\\text{Explicit Politician Condition}_i* \\text{Old Fashioned Racism}_i*\\beta_5)\n\\end{align*}\\]\n\nlibrary(equatiomatic)\nextract_eq(fit.probit5, wrap = TRUE, terms_per_line = 3)\n\n\\[\n\\begin{aligned}\nP( \\operatorname{abtrace1} = \\operatorname{1} ) &= \\Phi[\\alpha + \\beta_{1}(\\operatorname{factor(condition2)}_{\\operatorname{2}}) + \\beta_{2}(\\operatorname{racresent})\\ + \\\\\n&\\qquad\\ \\beta_{3}(\\operatorname{oldfash}) + \\beta_{4}(\\operatorname{factor(condition2)}_{\\operatorname{2}} \\times \\operatorname{racresent}) + \\beta_{5}(\\operatorname{factor(condition2)}_{\\operatorname{2}} \\times \\operatorname{oldfash})]\n\\end{aligned}\n\\]\n\n\n\nA few questions:\n\nHow should we interpret the coefficients?\nHow do the interactions affect this interpretation?\n\n\n\n6.7.2 Numeric Optimization\nLet’s repeat our replication of column 5, but this time, let’s use numeric optimization. We first need to make an X and Y matrix from our data. Because we have already run the models, let’s use a trick below:\n\nX &lt;- model.matrix(fit.probit5)\nY &lt;- as.matrix(fit.probit5$y)\n\nThe next thing we need to do is make a function for the log likelihood. Let’s recall the log likelihood for a Bernoulli random variable from a previous section:\n\\[\\begin{align*}\n\\mathcal L( \\pi | Y_i) &= \\underbrace{\\pi^{y_i}(1 -\\pi)^{(1-y_i)}}_\\text{Likelihood for single observation}\\\\\n\\mathcal L( \\pi | Y) &= \\underbrace{\\prod_{i=1}^n\\pi^{y_i}(1 -\\pi)^{(1-y_i)}}_\\text{Likelihood for all observations}\\\\\n\\ell( \\pi | Y) &= \\underbrace{\\sum_{i=1}^n\\log \\pi^{y_i}(1 -\\pi)^{(1-y_i)}}_\\text{Log likelihood}\n\\end{align*}\\]\nNow we just change the definition of \\(\\pi\\) to be the transformation for a probit, which is \\(\\Phi(X\\beta)\\). We code this up as a function below. Try to make the connection between the log likelihood equation above and the last line of the function. Note: in R, we can use pnorm() to express \\(\\Phi(X\\beta)\\), this is the CDF of the normal distribution.\n\nllik.probit &lt;- function(par, Y, X){\n  beta &lt;- as.matrix(par)\n  link &lt;- pnorm(X %*% beta)\n  like &lt;- sum(Y*log(link) + (1 - Y)*log(1 - link))\n  return(like)\n}\n\nLet’s generate some starting values for the optimization.\n\n## Starting values\nls.lm &lt;- lm(Y ~ X[, -1]) \nstart.par &lt;- ls.lm$coef\n\nFinally, let’s use optim to find our parameter estimates.\n\n## optim()\nout.opt &lt;- optim(start.par, llik.probit, Y=Y,\n                 X=X, control=list(fnscale=-1), method=\"BFGS\",\n                 hessian = TRUE)\n\nWe can compare the estimates recovered from glm and optim.\n\n## Log likelihood\nlogLik(fit.probit5)\nout.opt$value\n\n'log Lik.' -574.3267 (df=6)\n[1] -574.3267\n\n\n\n## Coefficients\ncbind(round(coef(fit.probit5), digits=4),\nround(out.opt$par, digits = 4))\n\n                                 [,1]    [,2]\n(Intercept)                    0.6320  0.6321\nfactor(condition2)2            0.9685  0.9684\nracresent                     -1.9206 -1.9207\noldfash                        0.5265  0.5265\nfactor(condition2)2:racresent -0.8513 -0.8512\nfactor(condition2)2:oldfash   -0.4197 -0.4196\n\n\nHow could we get the standard errors?\n\n\n6.7.3 Predicted Probabilities\nOne thing we could do for our interpretations is to push this further to generate “quantities of interest.”\n\nWe will do one example of this but will talk in more detail about this next week. Let’s generate predicted probabilities for thinking the ad is about race across levels of racial resentment in the sample, for people in the implicit and explicit conditions. For this example, we are going to hold “old-fashioned racism” at its mean value. This is slightly different from what the authors do but should generate similar results. The authors hold old-fashioned racism at its “observed values.”\nWe can rely on the predict function like we did with OLS, but here, we need to set type = response to put the results on the response scale instead of the scale of the linear predictor. What this does is apply our function \\(\\Phi(\\mathbf{x_i}' \\hat \\beta)\\) for our designated values of \\(X\\) and estimates for \\(\\hat \\beta\\).\n\npredvals.imp &lt;- predict(fit.probit5, newdata = data.frame(condition2=1, \n                                                         racresent = seq(0, 1, .0625),\n                                         oldfash = mean(study$oldfash, na.rm=T)),\n                    type=\"response\")\npredvals.exp &lt;- predict(fit.probit5, newdata = data.frame(condition2=2,\n                                                         racresent = seq(0, 1,.0625),\n                                         oldfash = mean(study$oldfash, na.rm=T)),\n                    type=\"response\")\n\n## Plot results\nplot(x=seq(0, 1, .0625), y=predvals.imp, type=\"l\",\n     ylim = c(0, 1), lty=2,\n     ylab = \"Predicted Probability\",\n     xlab = \"Racial Resentment\",\n     main = \"Predicted Probability of Viewing the Ad as about Race\",\n     cex.main = .7)\nlegend(\"bottomleft\", lty= c(2,1), c(\"Implicit\", \"Explicit\"))\npoints(x=seq(0, 1, .0625), y=predvals.exp, type=\"l\")\n\n\n\n\nAdditional Questions\n\nFor extra practice, you can try replicating column 4 in the model.\nYou can also try replicating the results in the figure with a logit model. Are the predicted probabilities similar?"
  },
  {
    "objectID": "07-QuantitiesofInterest.html#using-the-response-functions-to-generate-quantities-of-interest",
    "href": "07-QuantitiesofInterest.html#using-the-response-functions-to-generate-quantities-of-interest",
    "title": "7  Quantities of Interest",
    "section": "7.1 Using the response functions to generate quantities of interest",
    "text": "7.1 Using the response functions to generate quantities of interest\nRecall, in linear regression, to get our estimated values \\(\\hat Y\\) we said \\(\\hat Y = X\\hat\\beta\\).\n\nIn glm’s, we can do the same to get our estimated values on the scale of the linear predictor \\(\\hat \\eta = X\\hat\\beta\\).\nWe then use our \\(Link^{-1}\\) response function to transform these values into the quantity of interest.\n\nE.g., in logistic regression we want \\(\\hat{\\pi} = \\frac{\\exp(X\\hat\\beta)}{1 + \\exp(X\\hat\\beta)}\\).\nE.g., in probit regression we want \\(\\hat{\\pi} = \\Phi(X \\hat \\beta)\\).\nThese represent the predicted probability of \\(Y_i = 1\\) given our coefficient estimates and designated values of the covariates\n\n\nLet’s use a subset of the MIDs mids.txt data available here.\nThis dataset has variables related to whether a dyad of states is engaged in a militarized interstate dispute between the two countries in a given year. The variable that will be our outcome of interest is Conflict which takes the values 0 or 1. We will also look at the relationship between a few independent variables and the propensity for conflict. Data are at Dyad Level.\n\nwhether the pair of countries\n\ninclude a major power (MajorPower, 1=yes, 0=otherwise),\nare contiguous ( Contiguity, 1=yes, 0=otherwise),\nare allies (Allies, 1=yes, 0=otherwise),\nand/or have similar foreign policy portfolios (ForeignPolicy, 1=yes, 0=otherwise)\n\nBalanceofPower: balance of military power\nYearsSince: the number of years since the last dispute\n\n\n## Load data\nmids &lt;- read.table(\"https://raw.githubusercontent.com/ktmccabe/teachingdata/main/midsshort.txt\")\n\ntable(mids$Conflict)\n\n\n    0     1 \n99657   343 \n\n\nAs can be seen from the table above, conflicts (fortunately) are relatively rare in our data. This means are predicted probabilities are likely going to be pretty small in this example.\nWe will run a logistic regression with a few covariates.\n\nout.logit &lt;-glm(Conflict ~  MajorPower + Contiguity + Allies + ForeignPolicy +\n    BalanceOfPower + YearsSince, \n                family = binomial(link = \"logit\"), data = mids)\n\nOur logistic regression equation is:\n\n\\(\\log \\frac{\\pi_i}{1-\\pi_i} = \\mathbf{x_i'}\\beta\\), or alternatively\n\\(Pr(Y_i = 1 | \\mathbf{x}_i) = logit^{-1}(\\mathbf{x}_i'\\beta) = \\frac{exp(\\mathbf{x_i'}\\beta)}{1 + exp(\\mathbf{x_i'}\\beta)}\\)\n\nOur coefficients are in the “logit” aka log-odds scale of the linear predictor, so we use the response function to put them into probability estimates.\nFor logistic regression, we can generate predicted probabilities for each \\(Y_i\\) using the\n\npredict(model, type=\"response\") function, using the\nplogis function with \\(X\\hat \\beta\\), or\n\nQuestion: What would be the equivalent function for probit?\n\nmanually writing down the response function \\(\\frac{exp(\\mathbf{x_i'}\\beta)}{1 + exp(\\mathbf{x_i'}\\beta)}\\).\n\nWe use the predict function exactly the same way as before, but the key argument we need to specify is type. If you have type = link this generates answers that are still on the log-odds linear predictor scale. It is switching this to response that goes into probability in the logit case or probit case.\nExample\nFirst, let’s just generate predicted probabilities for each observation in our data, without specifiying any designated values for \\(X\\)– just keeping the values as they are in the data aka “as observed.”\n\n## Method with predict()\n## When you don't specify newdata, R assumes you want the X data from the model\npp &lt;- predict(out.logit, type = \"response\")\n\n## Manual way #1\nX &lt;- model.matrix(out.logit)\nbh &lt;- coef(out.logit)\npplogis &lt;- plogis(X %*% bh)\n\n## Manual way # 2\nppexp &lt;- exp(X %*% bh)/(1 + exp(X %*% bh))\n\n## Compare the first five rows of each to see they are the same\ncbind(pplogis, ppexp, pp)[1:5,]\n\n                                           pp\n78627  0.0004204501 0.0004204501 0.0004204501\n295818 0.0001035526 0.0001035526 0.0001035526\n251841 0.0006211178 0.0006211178 0.0006211178\n98068  0.0004270812 0.0004270812 0.0004270812\n209797 0.0001005396 0.0001005396 0.0001005396\n\n\nThe code above generates a predicted probability associated with each observation in the data. This is similar to generating a fitted value \\(\\hat y\\) for each observation in OLS."
  },
  {
    "objectID": "07-QuantitiesofInterest.html#qoi-at-designated-values",
    "href": "07-QuantitiesofInterest.html#qoi-at-designated-values",
    "title": "7  Quantities of Interest",
    "section": "7.2 QOI at Designated Values",
    "text": "7.2 QOI at Designated Values\nUsually in social science we have hypotheses about how the predicted probabilities change as one or more of our independent variables change. We will now turn to calculating predicted responses according to specific values of the independent variables.\nRecall, sometimes in linear regression, we wanted to calculate a specific estimated value of \\(\\hat Y_i\\) for when we set \\(X\\) at particular values. (e.g., What value do we estimate for \\(Y\\) when \\(X1 = 2\\) and \\(X2=4\\)?)\n\nIn OLS, this would be \\(\\hat Y_i = \\hat \\alpha + 2*\\hat \\beta_1 + 4*\\hat \\beta_2\\)\n\nHere, we can do the same for GLMs by setting specific values for \\(X\\) when we apply the \\(Link^{-1}\\) response function.\n\nE.g., What is the predicted probability of \\(Y_i = 1\\) when \\(X1 = 2\\) and \\(X2=4\\)?\n\nIn logistic regression, \\(\\hat{\\pi_i} = \\frac{\\exp(\\hat \\alpha + 2*\\hat \\beta_1 + 4*\\hat \\beta_2)}{1 + \\exp(\\hat \\alpha + 2*\\hat \\beta_1 + 4*\\hat \\beta_2)}\\)\n\n\nExample 1\nExample using the Conflict data using different approaches in R.\n\n## Predicted probability when Allies = 1, and all other covariates = 0\nallies1 &lt;- predict(out.logit, newdata = \n                     data.frame( MajorPower = 0,\n                                 Contiguity = 0, \n                                 Allies = 1, \n                                 ForeignPolicy = 0,BalanceOfPower = 0, \n                                 YearsSince = 0),\n                   type = \"response\")\nallies1\n\n          1 \n0.002632504 \n\n## for allies = 1, careful of the order to make same as coefficients\nX &lt;- cbind(1, 0, 0, 1, 0, 0 , 0) \nBh &lt;- coef(out.logit)\n\n## Approach 1\nplogis(X %*% bh)\n\n            [,1]\n[1,] 0.002632504\n\n## Approach 2\nexp(X%*% bh)/(1 + exp(X%*% Bh))\n\n            [,1]\n[1,] 0.002632504\n\n\nExample 2\nSecond example keeping X at observed values. Here the manual approach is easier given the limitations of predict (at least until we learn a new package). Now we are estimating \\(N\\) predicted probabilities, so we take the mean to get the average estimate.\n\n## for allies = 1careful of the order to make same as coefficients\nX &lt;- model.matrix(out.logit)\nX[, \"Allies\"] &lt;- 1 # change Allies to 1, leave everything else as is\nBh &lt;- coef(out.logit)\n\n## Approach 1\nmean(plogis(X %*% bh))\n\n[1] 0.002759902\n\n## Approach 2\nmean(exp(X%*% bh)/(1 + exp(X%*% Bh)))\n\n[1] 0.002759902\n\n\nThis is the average predicted probability of having a dispute when the dyad states are Allies, holding other covariates at their observed values.\nHere is a brief video with a second example of the process above, leading into the discussion of marginal effects below. It uses the anes data from Banda and Cassese in section 6.\n\n\n7.2.1 Marginal Effects\nRecall, in linear regression a one-unit change in \\(X_k\\) is associated with a \\(\\hat \\beta_k\\) change in \\(Y\\) no matter where we are in the domain of \\(X_k\\). (The slope of a line is constant!)\nThe catch for glm’s, again, is that our linear predictor (\\(\\eta\\)) is often not in the units of \\(Y\\) that we want. E.g., In logistic regression, a one-unit change in \\(X_k\\) is associated with a \\(\\hat \\beta_k\\) logits change\n\nRecall, for logit and probit, this takes us into an S-curve for \\(Pr(Y_i = 1)\\) instead of a line\nWell, the slope of an S-curve is not constant. Depending on where we are in \\(X\\), it will influence how much change we have in the predicted probability of \\(Y_i = 1\\).\nTherefore, to understand the marginal effect in glm’s we have to set \\(X\\) to particular values and be careful about the values we select.\n\nBy “careful,” this means choosing sensible, theoretically informed values of interest.\n\n\nYou can generate predictions based on any values. Here are three common approaches for understanding the marginal effect of a particular variable \\(X_k\\).\n\nMarginal effects at the mean\nAverage marginal effects\nMarginal effects at representative values\n\nWait, what do we mean by marginal effects?\n\nFor a discrete (categorical/factor) variable (\\(X_k\\)) this will be the change in predicted probability associated with a one-unit change in (\\(X_k\\)).\nFor continuous variables (\\(X_k\\)), this technically is the instantaneous rate of change (change in probability associated with a very small change in \\(X\\)).\n\nUsually instead of estimating this (what is a very small change anyway?) we will do this by hand instead, and set the specific amount of change). Often, this is called “discrete change” or “first difference” effect.\n\n\n\n\n7.2.2 Marginal effects at the mean\nIn this approach, when we calculate the difference in predicted probability resulting from a one-unit change in \\(X_k\\), we set all other covariates \\(X_j\\) for \\(j \\neq k\\) at their mean values.\n\nThis gives us 1 estimate for the difference in predicted probability\nWhen can this be problematic? (think categorical variables)\n\n\n\n7.2.3 Marginal effects at representative values\nIn this approach, when we calculate the difference in predicted probability resulting from a one-unit change in \\(X_k\\), we set all other covariates \\(X_j\\) for \\(j \\neq k\\) at values that are of theoretical interest. This could be the mean value, modal vale, or some other value that makes sense for our research question.\n\nThis gives us 1 estimate for the difference in predicted probability\nDepending on your research question, there may/may not be a particularly interesting set of representative values on all of your covariates.\n\nThe example above where we held all other covariates at zero would be an example of calculating marginal effects at representative values.\n\n\n7.2.4 Average marginal effects\nIn this approach, when we calculate the difference in predicted probability resulting from a one-unit change in \\(X_{ik}\\), we hold all covariates \\(X_{ij}\\) for \\(j \\neq k\\) at their observed values.\n\nThis gives us \\(N\\) estimates for the difference in predicted probability\nWe report the average of these estimates\n\nHere is an example for average marginal effects. Let’s sat we were interested in the difference in probability of a dispute for Allies vs. non-Allies, when all other covariates are zero. We can do this manually or in predict.\n\n## Extract beta coefficients\nBh &lt;- coef(out.logit)\n\n## Set Allies to 1, hold all other covariates as observed\nX1 &lt;- model.matrix(out.logit)\nX1[, \"Allies\"] &lt;- 1\n\n## Set Allies to 0, hold all other covariates as observed\nX0 &lt;- model.matrix(out.logit)\nX0[, \"Allies\"] &lt;- 0\n\npp1 &lt;- mean(plogis(X1 %*% Bh))\npp0 &lt;- mean(plogis(X0 %*% Bh))\npp1 - pp0\n\n[1] -0.0009506303\n\n\nThis represents the average difference in predicted probability of having a dispute for Dyads that are Allies vs. not Allies.\n\n\n7.2.5 prediction and margins packages.\nThere are functions that can make this easier so long as you understand what they are doing. One package developed by Dr. Thomas Leeper is prediction. A second is margins. Documentation available here and here. It is always important to understand what’s going on in a package because, for one, it’s possible that the package will stop being updated, and you will have to find an alternative solution.\nWe will focus on the prediction package first. The prediction function generates specific quantities of interest. An advantage it has over the built-in predict function is that it makes it easier to “hold all other variables at observed values.” In the prediction function, you specify the designated values for particular variables, and then by default, it assumes you want to hold all other variables at observed values. Here is an example of generating predicted probabilities for Allies = 1 and Allies = 0. It will generate the summary means of these two predictions.\n\n## install.packages(\"prediction\")\nlibrary(prediction)\n\n## By default, allows covariates to stay at observed values unless specified\nprediction(out.logit, at = list(Allies = c(0, 1)), \n           type = \"response\")\n\nData frame with 200000 predictions from\n glm(formula = Conflict ~ MajorPower + Contiguity + Allies + ForeignPolicy + \n    BalanceOfPower + YearsSince, family = binomial(link = \"logit\"), \n    data = mids)\nwith average predictions:\n\n\n Allies        x\n      0 0.003711\n      1 0.002760\n\n## compare with the manual calculated values above\npp0\n\n[1] 0.003710532\n\npp1\n\n[1] 0.002759902\n\n\n\n\n7.2.6 QOI Practice Problems\n\nConduct the following regression using glm\n\n\\(Pr(Conflict_i = 1 | X) = logit^{-1}(\\alpha + \\beta_1 * Allies_i + \\beta_2 * MajorPower_i + \\beta_3 * ForeignPolicy_i)\\)\n\nWhat is the predicted probability of entering a dispute when the dyad includes a major power, holding all covariates at observed values?\nRepeat the previous exercise, but now use probit. How similar/different are the predicted probability estimates?\n\n\n\nTry on your own, then expand for the solution.\n\n\n## Problem 1\nout.logit2 &lt;- glm(Conflict ~ Allies + MajorPower + ForeignPolicy, data=mids,\n                  family = binomial(link = \"logit\"))\n\n## Problem 2\nlibrary(prediction)\nprediction(out.logit, at = list(MajorPower = 1), \n           type = \"response\")\n\nData frame with 100000 predictions from\n glm(formula = Conflict ~ MajorPower + Contiguity + Allies + ForeignPolicy + \n    BalanceOfPower + YearsSince, family = binomial(link = \"logit\"), \n    data = mids)\nwith average prediction:\n\n\n MajorPower        x\n          1 0.007745\n\n## Problem 3\nout.probit &lt;- glm(Conflict ~ Allies + MajorPower + ForeignPolicy, data=mids,\n                  family = binomial(link = \"probit\"))\nprediction(out.probit, at = list(MajorPower = 1), \n           type = \"response\")\n\nData frame with 100000 predictions from\n glm(formula = Conflict ~ Allies + MajorPower + ForeignPolicy, \n    family = binomial(link = \"probit\"), data = mids)\nwith average prediction:\n\n\n MajorPower       x\n          1 0.01451\n\n## Manual approach\nX &lt;- model.matrix(out.probit)\nX[, \"MajorPower\"] &lt;- 1\nBhat &lt;- coef(out.probit)\n\nmean(pnorm(X %*% Bhat))\n\n[1] 0.01450613"
  },
  {
    "objectID": "07-QuantitiesofInterest.html#uncertainty",
    "href": "07-QuantitiesofInterest.html#uncertainty",
    "title": "7  Quantities of Interest",
    "section": "7.3 Uncertainty",
    "text": "7.3 Uncertainty\nUsually, we want to report a confidence interval around our predicted probabilities, average predicted probabilities, or around the difference in our predicted probabilities or difference in our average predicted probabilities.\nThis is different from the uncertainty of a coefficient, which we already have from our glm output. Here, if we say there is a 0.01 probability of a dispute, that is just an estimate, it is going to vary over repeated samples. We want to generate a confidence interval that represents this variability in \\(\\hat \\pi\\).\nWe have already discussed using the predict function in lm to generate confidence intervals for OLS estimates. In a limited set of cases, we can also use this shortcut for glm by taking advantage of the distribution being approximately normal on the scale of the linear predictor. When we are estimating confidence intervals around 1) one or multiple single quantities of interest (a predicted probability, as opposed to a difference in predicted probability) 2) where the \\(X\\) values are set at specific values (and not at their observed values) then, we can plug this into the predict function in the following way:\n\nGenerate the prediction and standard errors of the prediction on the link linear predictor scale.\n\nOn the scale of the linear predictor, the standard errors of the prediction are calculated as \\(\\sqrt{\\mathbb{x'}_c \\text{vcov(fit)} \\mathbb{x}_c}\\) using the delta method.\n\nCalculate the CI on the linear predictor scale: \\(CI(\\hat \\theta) = \\hat \\theta - z_{crit}*se_{\\hat \\theta}\\) ; \\(\\hat \\theta + z_{crit}*se_{\\hat \\theta}\\)\n\n\\(z_{crit}\\) for the 95% confidence interval is 1.96 (so this is saying +/- about 2 standard errors). We get this by using qnorm().\n\nConvert the prediction and confidence intervals to the response scale.\n\nHere is an example:\n\n## Predicted probability when Allies = 1 and all other covariates = 0\n## Note type = \"link\"\nallies1.link &lt;- predict(out.logit, newdata = \n                     data.frame( MajorPower = 0,\n                                 Contiguity = 0, \n                                 Allies = 1, \n                                 ForeignPolicy = 0,BalanceOfPower = 0, \n                                 YearsSince = 0),\n                   type = \"link\", se = T)\n\nallies1 &lt;-  plogis(allies1.link$fit)\nallies1.lb &lt;- plogis(allies1.link$fit - qnorm(.975)*allies1.link$se.fit)\nallies1.ub &lt;- plogis(allies1.link$fit + qnorm(.975)*allies1.link$se.fit)\n\n## Confidence interval\nc(allies1, allies1.lb, allies1.ub)\n\n          1           1           1 \n0.002632504 0.001302711 0.005312514 \n\n## By hand (using x as a k x 1 vector)\nx.c &lt;- rbind(1, 0, 0, 1, 0, 0, 0)\nse.hand &lt;- sqrt(t(x.c) %*% vcov(out.logit) %*% x.c)\np.hand &lt;- t(x.c) %*% coef(out.logit)\nallies1.hand &lt;- plogis(p.hand)\nallies1.hand.lb &lt;- plogis(p.hand- qnorm(.975)*se.hand)\nallies1.hand.ub &lt;- plogis(p.hand + qnorm(.975)*se.hand)\nc(allies1.hand, allies1.hand.lb, allies1.hand.ub)\n\n[1] 0.002632504 0.001302711 0.005312514\n\n\nBeyond this simple case, there are three general approaches to calculating the uncertainty of the quantities of interest. Here is a video with an overview of these three processes. The course notes contain additional detail below. It continues with the anes data example from Banda and Cassese in section 6, as did the other video in this section.\n\n\nDelta Method (based on calculus, first order Taylor Expansion approximation)\nBootstrapping (very flexible, common, computationally demanding)\nQuasi-bayesian (flexible, less computationally demanding), also described as simulation/Monte carlo simulation.\n\nFor now, we will focus on the second two methods, but some statistical software programs will report uncertainty estimates based on the Delta method. Here is more information on this method and the deltamethod function in R.\n\n7.3.1 Bootstrapping\nBootstrapping simulates the idea of conducting repeated samples to generate a distribution of estimates of your quantity of interests. We “resample” from our existing data to generate thousands of new datasets, and use each dataset to generate a slightly different quantity of interest. This distribution is then used to construct the confidence interval.\nProcess:\n\nSample from the data to generate new data frame\nRun the model: this gives new coefficient estimates and new covariate matrices\nUse new coefficient and covariate estimates to compute quantity of interest\nReplicate the previous process for about 1000 iterations to get 1000 estimates of quantity of interest\nUse the distribution of these estimates to calculate confidence intervals\n\nWhy? How does this work?\n\nIt is simulating the exercise of hypothetical repeated samples\nSimilar to Law of Large Numbers- with sufficient iterations, the empirical “bootstrap” distribution is a good approximation of the true distribution (will get closer and closer to the truth)\n\nIt won’t help us correct a bad estimate– have to work from the data we have.\n\nThe logic is we think the distribution of \\(\\bar x\\) sample estimate is centered on \\(\\mu\\) (the truth), and then we assume the distribution of \\(\\bar x*\\) (the bootstrapped estimate) is centered on \\(\\bar x\\)\n\nThis would be a good place to review the Bootstrap resources at the front of the section:\n\nPezullo, John. The Bootstrap Method for Standard Errors and Confidence Intervals.\nBanks, David. Lecture from Duke University.\n\nHow do we implement this procedure?\nExample\nFind the point estimate and 95% CI for the average predicted probability of conflict when the dyad are allies and all other covariates are held at observed values\n\n## Original regression\nout.logit &lt;-glm(Conflict ~  MajorPower + Contiguity + Allies + ForeignPolicy +\n    BalanceOfPower + YearsSince, \n                family = binomial(link = \"logit\"), data = mids)\n\n## We need to build our bootstrap procedure\n## Let's assume we just want 1 iteration\n\n## Step 1: sample to generate new data\n## this selects N row numbers from mids, with replacement\nwrows &lt;- sample(x =1:nrow(mids), size = nrow(mids), replace = T)\n\n## Create subset of data based on these rows\nsubdata &lt;- mids[wrows, ]\n\n## Step 2: run your regression model with the new data\nboot.logit &lt;-glm(Conflict ~  MajorPower + Contiguity + Allies + ForeignPolicy +\n    BalanceOfPower + YearsSince, \n                family = binomial(link = \"logit\"), data = subdata)\n\n## Step 3: generate average predicted probability\nXboot &lt;- model.matrix(boot.logit)\nXboot[, \"Allies\"] &lt;- 1\nBh &lt;- coef(boot.logit)\np.boot &lt;- mean(plogis(Xboot %*% Bh))\n\n\n\nExpand below for more details on what the sample function does.\n\nLet’s say we have a dataframe of different colors and shapes.\n\nsomedata &lt;- data.frame(colors = c(\"red\", \"blue\", \"yellow\", \"green\", \n                                  \"purple\", \"orange\", \"black\"),\n                       shapes = c(\"circle\", \"square\", \"triangle\", \n                                  \"rectangle\", \"diamond\", \"line\", \"sphere\"))\nsomedata                                  \n\n  colors    shapes\n1    red    circle\n2   blue    square\n3 yellow  triangle\n4  green rectangle\n5 purple   diamond\n6 orange      line\n7  black    sphere\n\n\nI could generate a new “resampled” dataset with the sample function. We tell the function three things: 1) choose from the row numbers in my dataframe (1:nrow(somedata)), 2) pick \\(N\\) row numbers in total (nrow(somedata)), 3) Each time you pick a given row number \\(i\\), put it back in the data, allowing the possibility that you may randomly sample it again (replace = TRUE).\n\nsample(1:nrow(somedata), nrow(somedata), replace = TRUE)\n\n[1] 5 3 4 4 4 2 3\n\nsample(1:nrow(somedata), nrow(somedata), replace = TRUE)\n\n[1] 4 2 2 5 1 2 3\n\nsample(1:nrow(somedata), nrow(somedata), replace = TRUE)\n\n[1] 7 4 3 2 6 6 2\n\n\nWhat happened is the function generated a set of row numbers. Note how it is possible for the same row number to be picked multiple times. Each time we run the sample function, we get slightly different row numbers.\nWe can subset our data based on these row indices.\n\n## store row indices\nwrows &lt;- sample(1:nrow(somedata), nrow(somedata), replace = TRUE)\nwrows\n\n[1] 1 2 4 6 1 1 1\n\n## subset data to include rows sampled\n## note if row indices are in wrows more than once, they will also be in the subset more than once\nsubdata &lt;- somedata[wrows,]\nsubdata\n\n    colors    shapes\n1      red    circle\n2     blue    square\n4    green rectangle\n6   orange      line\n1.1    red    circle\n1.2    red    circle\n1.3    red    circle\n\n\nGiven that each time the sample function runs, we get slightly different random samples of the data, that’s how we end up with a distribution of slightly different estimates of our quantities of interest. Each time the regression is run with a slightly different dataset.\n\nThis gives us one estimate of the average predicted probability stored in p.boot. However, the idea of a bootstrap is that we repeat this procedure at least 1000 times to generate a distribution of estimates of the quantity of interest, the average predicted probability in this case.\nWe could literally repeat that code chunk 1000 times…. but, we have better things to do than that much copy/paste. Instead, we will create a function that will do this automatically.\nTo do so, we are going to wrap our procedure above inside the syntax for creating functions in R. In R, to create a function,\n\nWe first name the function. (Let’s call this myboot. You could call yours anything.)\nThe next syntax is always myboot &lt;- function(){}.\nInside the function() part, you tell R what you are going to supply the function each time you want it to run. Sometimes functions only have one input, others like lm have multiple inputs.\n\nFor example, in the function mean(x), we always supply that function with a vector of values.\nFor this bootstrap example, we are going to write the function as one where we will supply the function with a dataframe. Let’s call this df.\n\nThe inside part of the function, between the {} is the procedure from above. All we do is\n\nInstead of writing mids, we keep it generic by writing df.\nWe add a final line that tells R what we want it to return() as the output of the function. Here, we want it to return the average predicted probability.\n\n\n\n## We need to build our bootstrap function\n## Step 4: Let's wrap our current steps into a function that we can replicate\n\n## Note: all we need as an input is our data.frame mids\n## I will label it something generic to show how a function can work\nmyboot &lt;- function(df){\n  wrows &lt;- sample(x =1:nrow(df), size = nrow(df), replace = T)\n  ## Create subset of data based on these rows\n  subdata &lt;- df[wrows, ]\n  ## Step 2: run your regression model with the new data\n  boot.logit &lt;-glm(Conflict ~  MajorPower + Contiguity + Allies + ForeignPolicy +\n    BalanceOfPower + YearsSince, \n                family = binomial(link = \"logit\"), data = subdata)\n  ## Step 3: generate average predicted probability\n  Xboot &lt;- model.matrix(boot.logit)\n  Xboot[, \"Allies\"] &lt;- 1\n  Bh &lt;- coef(boot.logit)\n  p.boot &lt;- mean(plogis(Xboot %*% Bh))\n  return(p.boot)\n}\n\nNote: here, our quantity of interest is the predicted probability of a dispute when the dyad are Allies. Let’s say, instead, we wanted the difference in predicted probability of a dispute between Allies and Non-Allies. Well, we would just adjust our function to calculate the mean probabilities for Allies and Non-Allies and return the difference in these means as the quantity of interest. We would then get 10000 estimates of this difference in probabilties.\nNow that we have the function from above, instead of copying/pasting this 1000 times, we will use the function called replicate which will do this for us. We indicate the number of estimates we want and then indicate which function (and in our case, which dataframe inside the function) we want to replicate.\n\n## This may take a minute to run. \n## We will do just 50, Normally you will want this to be more like 1000\nset.seed(1234) # this helps us get the same results each time, good for reproducibility\nmyestimates &lt;- replicate(50, myboot(mids))\n\nThe bootstrapping approach is very computationally demanding given it has to repeat an operation several (thousand) times. After you hit “run,” just sit back, relax and wait for the water to run dry.\n\n\nFor a troubleshooting tip, expand.\n\nIf you get an error message at the replicate(1000, myboot(mids)) stage, it is best to see if your function runs at all. Try just the below to see if it generates output:\n\nmyboot(mids)\n\n[1] 0.002250429\n\n\nIf you get the error here, then it means there is a bug within the function code, not the replicate code.\n\nEach time we replicate the function, it will generate slightly different results because the sample functions is randoming sampling rows of data each time. We can plot the distribution of estimates to show this.\n\nlibrary(ggplot2)\nggplot(data.frame(x = myestimates), aes(x = myestimates)) + \n  geom_histogram(aes(y=..density..)) + geom_density(color=\"red\")\n\nWarning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead.\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nThe final step after generating the bootstrap distribution of estimates is to use it to construct a confidence interval for the quantity of interest. There are a few ways to do this.\n\nNormal approximation\nPercentile\nBias correction\n\nIn each approach, we take our original “point estimate” from the computation of the quantity of interest from our original data and use the bootstrap estimates for the lower and upper bounds of the confidence interval. Here we will assume we want a 95% confidence interval.\n\n## Find the original point estimate\nBh &lt;- coef(out.logit)\nX1 &lt;- model.matrix(out.logit)\nX1[, \"Allies\"] &lt;- 1\npe1 &lt;- mean(plogis(X1 %*% Bh))\n\n## Normal\nc((pe1 - qnorm(.975)*sqrt(var(myestimates))),(pe1 + qnorm(.975)*sqrt(var(myestimates))))\n\n[1] 0.002174802 0.003345002\n\n## Percentile\nquantile(myestimates, c(0.025, .975))\n\n       2.5%       97.5% \n0.002275356 0.003431285 \n\n## Bias correction\nbc &lt;- 2*pe1 - myestimates\nquantile(bc, c(0.025, .975))\n\n       2.5%       97.5% \n0.002088520 0.003244449 \n\n\nEach of these is pretty commonly used, but they may generate slightly different results.\n\n\n7.3.2 Simulated Confidence Intervals\nQuasi-Bayesian or simulated confidence intervals take advantage of the large sample properties of our estimates \\(\\hat \\beta\\) having a Normal sampling distribution due to the Central Limit Theorem.\nLike the bootstrap, the simulation procedure also generates hypothetical new samples. However, here, we are sampling new \\(\\hat \\beta\\) estimates each time instead of sampling a new underlying dataset each time. This allows use to skip the step of generating a new dataset and running the regression 1000 times. Here, we just run the regression model once. The simulation process takes place after this step.\nProcess\n\nEstimate your model (e.g., with optim or glm)\nSample \\(\\sim\\) 1000 new estimates of the vector of \\(\\hat \\beta\\) by using the vcov of \\(\\hat \\beta\\) to generate the uncertainty\nFor each of these new \\(\\hat \\beta_c\\), calculate \\(\\hat \\theta_c\\), in this case, the predicted probabilities.\nEstimate “fundamental uncertainty” by drawing new y’s based on these parameters\n\nOnly necessary in some cases. Depends on Jensen’s Inequality discussed in the Gary King resources and in the details on the use of rbinom function below.\nWe are going to average over this, which means we are calculating “expected values.”\n\nUse this distribution to compute the CI’s\n\nThis would be a good place to review the resources from Gary King:\n\nOverview of simulation approach for calculating uncertainty from King, Tomz, and Wittenberg 2000.\nLecture video from Gary King on simulating quantities of interest\n\nExample\nThe code for this approach will more simple in a case where we are computing quantities of interest when covariates are held at means or representative values (cases where we get just one predicted probability associated with each set of \\(X\\) values). It will look a little more complex in cases where we want to hold covariates at observed values and calculate the average predicted probability.\nFirst, let’s find the point estimate and 95% CI for the predicted probability of conflict when the dyad are Allies, and all other covariates are held at zero.\n\n## install.packages(\"mvtnorm\")\nlibrary(mvtnorm)\n\n## Step 2: Sample 1000 new Bhs (we will use 50 for this example)\n## This uses the multivariate normal distribution for resampling\nset.seed(1234)\nnumsims &lt;- 50\nqb.beta &lt;- rmvnorm(numsims, coef(out.logit), vcov(out.logit)) \n## This generates numsims X k coefficients matrix\n\n## Step 3: Iterate through the estimates\n## Create an empty vector to store 1000 quantities of interest\nqestimatessimple &lt;- rep(NA, numsims)\n\n## Here, our covariate matrix stays the same each time\n## We have a 1 for intercept and 1 for Allies, everything else at zero\nX1 &lt;- cbind(1, 0, 0, 1, 0, 0 , 0) \n## X1 is a 1 X k matrix\n\n## Use a loop to iterate through each set of betas\nfor(i in 1:numsims){\n  \n  ## for each set of betas, calculate p probs\n  ## for a given set of betas, this gives us nrow(X1) predicted probabilities\n  pestimate &lt;-plogis(X1 %*% qb.beta[i,])\n  \n  ## Fundamental uncertainty\n  ## not required for logit/probit, but we will show how\n  ## rbinom generates 1000 0's or 1's based on the predicted probability\n  ## We use rbinom bc of the bernoulli, other glm's will have other distributions\n  ## then we take the mean to get our estimate of the predicted probability\n  moutcome &lt;- mean(rbinom(numsims, 1, pestimate))\n  \n  qestimatessimple[i] &lt;-moutcome\n  ## repeat for set of simulated betas\n}\n\n## Step 4: Similar to bootstrap distribution, find CI using the percentiles\nquantile(qestimatessimple, c(0.025, 0.975))\n\n 2.5% 97.5% \n 0.00  0.02 \n\n\nFor more information on loops in R, you can follow this tutorial we created for 2020 SICSS-Rutgers.\n\n\nExpand for details on rbinom.\n\nrbinom is the random generation function for the binomial distribution. If we supply it with number of trials (in the Bernoulli, this is 1), and a probability of success, it will generate our desired number of outcomes according to this distribution.\nFor example, let’s say we wanted to generate a random set of 100 coin flips for a coin that is fair– where the probability of success is .5. We will get a sample of 0’s and 1’s. If we take the mean, it will be close to .5, and with enough coin flips, will converge on .5.\n\nrb.res &lt;- rbinom(100, 1, .5)\nrb.res\n\n  [1] 0 1 0 0 0 0 1 0 0 0 1 0 1 1 0 1 0 1 0 0 1 0 1 0 1 1 0 1 0 1 0 1 1 1 1 0 0\n [38] 0 0 0 0 1 1 1 1 0 1 0 1 0 1 1 0 1 1 0 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 1 1\n [75] 1 1 1 1 0 0 1 0 1 0 0 1 0 1 1 0 1 1 1 1 1 1 0 1 0 0\n\nmean(rb.res)\n\n[1] 0.49\n\n\nIn logit/probit, this step is unneccessary because the \\(\\mathbb{E}(y_i) = \\pi_i\\). When we take the mean of our rbinom estimates, we are just going to recover the probability we supplied to it.\nHowever, in other cases, Jensen’s inequality may apply, which states that \\(\\mathbb{E}[g(X)] \\neq g(\\mathbb{E}[X])\\). For example, if we have an outcome that is distributed according to the exponential distribution: Here, the \\(\\theta\\) is \\(\\lambda\\) where \\(\\lambda = \\frac{1}{e^{X\\beta}}\\) but \\(\\mathbb{E}(y)= \\frac{1}{\\lambda}\\). Unfortunately, \\(\\mathbb{E}(\\frac{1}{\\hat \\lambda}) \\neq \\frac{1}{\\mathbb{E}(\\hat \\lambda)} = \\frac{1}{\\mathbb{E}(e^{X\\beta})}\\). For that example, the rexp() step in this case would be essential.\nIf we’re not sure when Jensen’s inequality will apply, we can just keep the fundamental uncertainty step as part of the process.\n\nFind the point estimate and 95% CI for the average predicted probability of conflict when the dyad are allies and all other covariates are held at observed values. Here, the code is more complicated, because every time we generate a predicted probability (for any observed value), we need to go through the fundamental uncertainty step (when applicable).\n\n## install.packages(\"mvtnorm\")\nlibrary(mvtnorm)\n\n## Step 2: Sample 1000 new Bhs (we will use 50 for this example)\n## This uses the multivariate normal distribution for resampling\nset.seed(1234)\nnumsims &lt;- 50\nqb.beta &lt;- rmvnorm(numsims, coef(out.logit), vcov(out.logit)) \n## This generates numsims X k coefficients matrix\n\n## Step 3: Iterate through the estimates\n## Create an empty vector to store 1000 quantities of interest\nqestimates &lt;- rep(NA, numsims)\n\n## Here, our covariate matrix stays the same\nX1 &lt;- model.matrix(out.logit)\nX1[, \"Allies\"] &lt;- 1\n\n## Use a loop to \nfor(i in 1:numsims){\n  \n  ## for each set of betas, calculate p probs\n  ## for a given set of betas, this gives us nrow(X1) predicted probabilities\n  pestimates &lt;-plogis(X1 %*% qb.beta[i,])\n  \n  ## Fundamental uncertainty\n  ## not required for logit/probit, but we will show how\n  \n  ## generate empty vector for outcomes\n  moutcomes &lt;- rep(NA, numsims)\n  \n  ## for each probability estimate, calculate the mean of simulated y's\n  for(j in 1:length(pestimates)){\n    ## rbinom generates 1000 0's or 1's based on the predicted probability\n    ## We use rbinom bc of the bernoulli, other glm's will have other distributions\n    ## then we take the mean to get our estimate of the predicted probability for a given observation\n    moutcomes[j] &lt;- mean(rbinom(numsims, 1, pestimates[j]))\n  }\n  \n  ## take the mean of the predicted probability estimates across all observations\n  qestimates[i] &lt;- mean(moutcomes)\n  ## repeat for set of simulated betas\n}\n\n## Step 4: Similar to bootstrap distribution, find CI using the percentiles\nquantile(qestimates, c(0.025, 0.975))\n\n      2.5%      97.5% \n0.00232977 0.00344796 \n\n\nBecause the shortcut applies where we do not need to calculate fundamental uncertainty in the logit / probit case, we can simplify this to:\n\n## install.packages(\"mvtnorm\")\nlibrary(mvtnorm)\n\n## Step 2: Sample 1000 new Bhs (we will use 50 for this example)\n## This uses the multivariate normal distribution for resampling\nset.seed(1234)\nnumsims &lt;- 50\nqb.beta &lt;- rmvnorm(numsims, coef(out.logit), vcov(out.logit)) \n## This generates numsims X k coefficients matrix\n\n## Step 3: Iterate through the estimates\n## Create an empty vector to store 1000 quantities of interest\nqestimates &lt;- rep(NA, numsims)\n\n## Here, our covariate matrix stays the same\nX1 &lt;- model.matrix(out.logit)\nX1[, \"Allies\"] &lt;- 1\n\nfor(i in 1:numsims){\n  pestimates &lt;-plogis(X1 %*% qb.beta[i,])\n  qestimates[i] &lt;- mean(pestimates)\n}\n\n## Step 4: Similar to bootstrap distribution, find CI using the percentiles\nquantile(qestimates, c(0.025, 0.975))\n\n       2.5%       97.5% \n0.002314344 0.003419644 \n\n\nWe can also plot the distribution of estimates\n\nlibrary(ggplot2)\nggplot(data.frame(x = qestimates), aes(x = qestimates)) + \n  geom_histogram(aes(y=..density..)) + geom_density(color=\"red\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`."
  },
  {
    "objectID": "07-QuantitiesofInterest.html#visualizing-results",
    "href": "07-QuantitiesofInterest.html#visualizing-results",
    "title": "7  Quantities of Interest",
    "section": "7.4 Visualizing Results",
    "text": "7.4 Visualizing Results\nNow that we have our point estimates of our quantities of interest and our confidence intervals, we can kick it up a notch by visualizing these results.\nLet’s plot our predicted probability when Allies = 1 with percentile Quasi-Bayesian CI’s and Bias-Corrected Bootstrap CI’s\n\nplot(x = 1:2, y= c(pe1, pe1),\n     ylim = c(0, .004),\n     xlim = c(0.5, 2.5),\n     pch = 20, cex = 1.6,\n     main = \"Average Predicted Probability of Dispute when Allies\",\n     cex.main = .7, cex.lab = .7, cex.axis = .7,\n     ylab = \"Predicted Probability\",\n     xlab = \"\", \n     xaxt = \"n\") # removes x-axis\n# add lines from c(x1, x2) on the x-axis and from c(y1, y2) on the y-axis\n# note, we don't want any horizontal movement, so we keep x1 and x2 the same c(1,1)\nlines(c(1,1), c(quantile(qestimates, c(0.025, 0.975))), lwd = 1.5)\nlines(c(2,2), c(quantile(bc, c(0.025, .975))), lwd = 1.5)\n# add text a\ntext(c(1, 2), c(0.001, 0.001), c(\"Quasi-Bayesian\", \"BC Bootstrap\"), cex = .7 )\n\n\n\n\nHere is the same but in ggplot form.\n\n## ggplot works best if you create a dataframe of the data you want to plot\nmyg &lt;- data.frame(rbind(c(pe1, quantile(qestimates, c(0.025, 0.975))),\n                       c(pe1, quantile(bc, c(0.025, .975)))))\ncolnames(myg) &lt;- c(\"pp\", \"qb\", \"bc\") \nmyg\n\n           pp          qb          bc\n1 0.002759902 0.002314344 0.003419644\n2 0.002759902 0.002088520 0.003244449\n\n## now provide this dataframe to ggplot\nggplot(myg, aes(x = 1:nrow(myg), y = pp))+ \n  geom_point(stat = \"identity\", size = 3) +\n  geom_errorbar(data=myg, aes(x =1:nrow(myg),  \n                         ymin = qb, ymax = bc), \n                width = 0, size = 1.1) +\n  xlim(.5, 2.5) +\n  ylim(0, .005) +\n  theme(axis.title.x=element_blank(),\n        axis.text.x=element_blank(),\n        axis.ticks.x=element_blank(),\n        plot.title = element_text(hjust = 0.5)) +\n  ylab(\"Predicted Probability\") +\n  ggtitle(\"Average Predicted Probability of Dispute when Allies\") +\n  annotate(\"text\", x = c(1,2), y = .004, label = c(\"Quasi-Bayesian\", \"BC Bootstrap\"))\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead."
  },
  {
    "objectID": "07-QuantitiesofInterest.html#additional-r-shortcuts",
    "href": "07-QuantitiesofInterest.html#additional-r-shortcuts",
    "title": "7  Quantities of Interest",
    "section": "7.5 Additional R shortcuts",
    "text": "7.5 Additional R shortcuts\nThere are a few R packages that help generate these quantities of interest AND estimate uncertainty. If you understand what is going on underneath the packages, then you should feel free to use them to avoid manually coding up each process.\n\n7.5.1 Prediction\nWe’ve already used the prediction package, but now let’s add uncertainty calculation to this. This package is useful for generating confidence intervals around single quantities of interest. The package below is better for constructing confidence intervals around the difference between quantities of interest.\n\nlibrary(prediction)\npreout &lt;- prediction(out.logit, at = list(Allies = c(0, 1)), \n           type = \"response\", calculate_se = T)\nsummary(preout)\n\n at(Allies) Prediction        SE      z         p    lower    upper\n          0   0.003711 0.0002270 16.343 4.878e-60 0.003266 0.004156\n          1   0.002760 0.0003135  8.804 1.322e-18 0.002145 0.003374\n\n\nI believe prediction relies on the delta method for uncertainty. The others below will have options for simulations or bootstrapped standard errors.\n\n\n7.5.2 Margins\nThomas Leeper developed a package called margins which is modeled after the Stata margins command. Here is some documentation for this package.\nWe first fit a regression model like normal.\n\nout &lt;- glm(Conflict ~  MajorPower + Contiguity + Allies + ForeignPolicy +\n    BalanceOfPower + YearsSince, data=mids, family=binomial(link = \"logit\"))\n\nOpen the package and use the margins command. It is similar to prediction but we will specify the uncertainty with vce and tell it which variable we want the marginal effect, and what type of change in that variable we want to calculate the effect.\n\n## install.packages(\"margins\")\nlibrary(margins)\n\n## Difference in Allies 0 vs. 1 holding other covariates at 0\n## Using Delta Method for uncertainty\nmarg1 &lt;- margins(out, vce = \"delta\",\n                 at = list(MajorPower=0, Contiguity=0,\n              ForeignPolicy=0, BalanceOfPower=0, YearsSince=0),\n                 variables = \"Allies\",\n                 change = c(0, 1),\n              type=\"response\") \n\nWarning in check_values(data, at): A 'at' value for 'BalanceOfPower' is outside\nobserved data range (0.000173599997651763,1)!\n\nsummary(marg1)\n\n factor MajorPower Contiguity ForeignPolicy BalanceOfPower YearsSince     AME\n Allies     0.0000     0.0000        0.0000         0.0000     0.0000 -0.0010\n     SE       z      p   lower   upper\n 0.0004 -2.3910 0.0168 -0.0018 -0.0002\n\n\nLet’s try a second example, shifting the uncertainty estimate and the X covariates.\n\n## Difference in Allies 0 vs. 1 holding MajorPower at 1, other covariates at observed values\n## Using simulations for uncertainty\nmarg2 &lt;- margins(out, vce = \"simulation\",\n                 at = list(MajorPower =1),\n                 variables = \"Allies\",\n                 change = c(0, 1),\n              type=\"response\") \nsummary(marg2)\n\n factor MajorPower     AME     SE       z      p   lower  upper\n Allies     1.0000 -0.0020 0.0010 -1.9576 0.0503 -0.0041 0.0000\n\n## Manual equivalent of point estimate\nX.marg1 &lt;- model.matrix(out)\nX.marg1[, \"Allies\"] &lt;-1\nX.marg1[, \"MajorPower\"] &lt;- 1\n\nX.marg0 &lt;- model.matrix(out)\nX.marg0[, \"Allies\"] &lt;-0\nX.marg0[, \"MajorPower\"] &lt;- 1\n\nBH &lt;- coef(out)\name &lt;- mean(plogis(X.marg1 %*% BH) - plogis(X.marg0 %*% BH))\n\n## Compare\name\n\n[1] -0.002039019\n\nsummary(marg2)$AME\n\n      Allies \n-0.002039019 \n\n\n\n\n7.5.3 Using expand.grid\nWhen we use the predict function in R, we specify a “`newdata” dataframe to indicate for which values of \\(X\\) we want to estimate values of the outcome. When we do this, we are actually building a dataframe, like the below:\n\ndf &lt;- data.frame(MajorPower = 0, Allies=1, Contiguity = 0)\ndf\n\n  MajorPower Allies Contiguity\n1          0      1          0\n\n\nIn cases where we want to make predictions for multiple values of a given variable, we can feed a vector into the data.frame:\n\ndf &lt;- data.frame(MajorPower = 0, Allies=c(0,1), Contiguity = 0)\ndf\n\n  MajorPower Allies Contiguity\n1          0      0          0\n2          0      1          0\n\n\nHowever, this becomes more tedious when we want to estimate for combinations of different variables (e.g., MajorPower and Allies as 0 or 1). You have to tell R precisely which rows should include which values. This means indicating four separate values for each variables to get all possible combinations.\n\ndf &lt;- data.frame(MajorPower = c(0, 0, 1,1), Allies=c(0,1, 0, 1), Contiguity = 0)\ndf\n\n  MajorPower Allies Contiguity\n1          0      0          0\n2          0      1          0\n3          1      0          0\n4          1      1          0\n\n\nWhat expand.grid does is let you more quickly indicate that you want to estimate for all possibles combinations of the values of the variables you supply. E.g.:\n\ndf &lt;- expand.grid(MajorPower = c(0, 1), Allies=c(0,1), Contiguity = 0)\ndf\n\n  MajorPower Allies Contiguity\n1          0      0          0\n2          1      0          0\n3          0      1          0\n4          1      1          0\n\n\nThis can be a useful shortcut when you need to look at combinations of variables that have many different values."
  },
  {
    "objectID": "07-QuantitiesofInterest.html#quantities-of-interest-tutorial",
    "href": "07-QuantitiesofInterest.html#quantities-of-interest-tutorial",
    "title": "7  Quantities of Interest",
    "section": "7.6 Quantities of Interest Tutorial",
    "text": "7.6 Quantities of Interest Tutorial\nLet’s return to the example at the end of section 6, and calculate predicted probabilities, now with estimates of uncertainty. Recall the Banks and Hicks data include the following variables below. We add partyid as a variable for this analysis.\n\nabtrace1: 1= if the respondent thought the ad was about race. 0= otherwise\ncondition2: 1= respondent in the implicit condition. 2= respondent in one of four explicit racism conditions.\nracresent: a 0 to 1 numeric variable measuring racial resentment\noldfash: a 0 to 1 numeric variable measuring “old-fashioned racism”\ntrumvote: 1= respondent has vote preference for Trump 0=otherwise\npartyid: A 1 to 7 numeric variables indicating partisanship from strong Democrat to strong Republican. Below 4 is a Democrat/Democratic leaner, above 4 is a Republican/Republican leaner.\n\nLet’s load the data again.\n\n## install.packages(\"rio\")\nlibrary(rio)\nstudy &lt;- import(\"https://github.com/ktmccabe/teachingdata/blob/main/ssistudyrecode.dta?raw=true\")\n\nLet’s set the experiment aside for now and focus on vote choice as the outcome: trumvote. Let’s suppose we were interested in understanding whether partisanship influences vote choice.\n\nWhat would be a simple regression model we could run? (e.g., what would be on the left, what would be on the right?)\nWhat type of model should it be? (e.g., OLS, glm, logit, probit, etc.?)\nRun this model below\nWhat do you conclude about the influence of party on vote choice?\n\n\n\nTry on your own, then expand for one solution.\n\nfit1 &lt;- glm(trumvote ~ partyid, data=study, family=binomial(link=\"probit\"))\n\nlibrary(texreg)\nVersion:  1.38.6\nDate:     2022-04-06\nAuthor:   Philip Leifeld (University of Essex)\n\nConsider submitting praise using the praise or praise_interactive functions.\nPlease cite the JSS article in your publications -- see citation(\"texreg\").\ntexreg::knitreg(fit1)\n\n\n\nStatistical models\n\n\n\n\n \n\n\nModel 1\n\n\n\n\n\n\n(Intercept)\n\n\n-2.06***\n\n\n\n\n \n\n\n(0.13)\n\n\n\n\npartyid\n\n\n0.43***\n\n\n\n\n \n\n\n(0.03)\n\n\n\n\nAIC\n\n\n1056.38\n\n\n\n\nBIC\n\n\n1066.23\n\n\n\n\nLog Likelihood\n\n\n-526.19\n\n\n\n\nDeviance\n\n\n1052.38\n\n\n\n\nNum. obs.\n\n\n1019\n\n\n\n\n\n\n***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05\n\n\n\n\n\nWe took a probit approach. What other approaches could we have taken?\n\nIn a glm model, we have to make a leap from a probit/logit coefficient to vaguely answer our research question. What we want to do instead, is become more focused in our research questions, hypotheses, and estimations to generate precise quantities of interest that speak directly to our theoretical questions.\nLet’s try this again. Let’s be more focused in our question/hypothesis.\n\nAs people shift from strong Democrats to strong Republicans, how does their probability of voting for Trump change?\nHow can we transform our model from before into quantities that speak directly to this question?\n\n\n\nComputing probabilities.\n\nWe only have one covariate, so this becomes an easier problem.\n\nnewdata.party &lt;- data.frame(partyid = 1:7)\n\n## Set type to be response\nresults1 &lt;- predict(fit1, newdata = newdata.party, type = \"response\")\n\n\n\n\n\n\nx\n\n\n\n\n0.0512844\n\n\n0.1147444\n\n\n0.2204047\n\n\n0.3669367\n\n\n0.5362029\n\n\n0.6990679\n\n\n0.8295963\n\n\n\n\n\n\nHow can we best communicate this to our readers?\n\n\nComputing probabilities.\n\n\nplot(x=1:7, y=results1,\n     main = \"Predicted probability by partisanship\",\n     type=\"b\",\n     xlab = \"Partisanship: Strong Dem to Strong Rep\",\n     ylab=\"Predicted Probability\")\n\n\n\nlibrary(ggplot2)\nggres &lt;- data.frame(probs=results1, partisanship=1:7)\nggplot(ggres, aes(x=partisanship, y=probs))+\n  geom_line()+\n  geom_point()+\n  xlab(\"Partisanship: Strong Dem to Strong Rep\")+\n  ylab(\"Predicted Probability\")+\n  ggtitle(\"Predicted probability by partisanship\")\n\n\n\n\n\nHow can we improve this even further?\n\nCalculate uncertainty!\n\nAgain, because we only have one covariate, the process is a little more simple. Let’s use the predict function to calculate the standard errors for us on the link scale with se.fit = T and then construct our own confidence intervals.\n\n\nTry on your own, then expand for the solution.\n\n\nresults1.link &lt;- predict(fit1, newdata=newdata.party, type=\"link\", se.fit=T)\nresults1.link\n\n$fit\n         1          2          3          4          5          6          7 \n-1.6325260 -1.2016765 -0.7708270 -0.3399775  0.0908720  0.5217215  0.9525710 \n\n$se.fit\n         1          2          3          4          5          6          7 \n0.10002223 0.07628414 0.05617531 0.04487352 0.04892006 0.06553079 0.08784628 \n\n$residual.scale\n[1] 1\n\n\nNow for each value, we get the standard error estimate. We now have to convert these to the response scale.\n\nm.results &lt;-  pnorm(results1.link$fit)\nresults1.lb &lt;- pnorm(results1.link$fit - qnorm(.975)*results1.link$se.fit)\nresults1.ub &lt;-pnorm(results1.link$fit + qnorm(.975)*results1.link$se.fit)\n\n## Let's look at the results from the original point estimates and this approach\ncbind(results1, m.results, results1.lb, results1.ub)\n\n    results1  m.results results1.lb results1.ub\n1 0.05128436 0.05128436  0.03373233  0.07543205\n2 0.11474445 0.11474445  0.08831718  0.14636254\n3 0.22040474 0.22040474  0.18917824  0.25439421\n4 0.36693674 0.36693674  0.33435178  0.40051009\n5 0.53620285 0.53620285  0.49800149  0.57407307\n6 0.69906787 0.69906787  0.65294495  0.74220540\n7 0.82959626 0.82959626  0.78242093  0.86965177\n\n\nFinally, let’s add it to our plot.\n\nplot(x=1:7, y=results1,\n     main = \"Predicted probability by partisanship\",\n     type=\"b\",\n     xlab = \"Partisanship: Strong Dem to Strong Rep\",\n     ylab=\"Predicted Probability\")\npoints(x=1:7, y= results1.ub, type=\"l\")\npoints(x=1:7, y= results1.lb, type=\"l\")\n\n\n\nlibrary(ggplot2)\nggres &lt;- data.frame(cbind(m.results, results1.lb, results1.ub), partisanship=1:7)\nggplot(ggres, aes(x=partisanship, y=m.results))+\n  geom_line()+\n  geom_point()+\n  #geom_ribbon(aes(ymin=results1.lb, ymax=results1.ub), alpha=.5)+\n  geom_errorbar(aes(ymin=results1.lb, ymax=results1.ub), width=.02)+\n  xlab(\"Partisanship: Strong Dem to Strong Rep\")+\n  ylab(\"Predicted Probability\")+\n  ggtitle(\"Predicted probability by partisanship\")\n\n\n\n\n\nThe prediction package in R will give us a shortcut by calculating the confidence intervals for us. We can repeat the previous process inside this function.\n\n# install.packages(\"prediction\")\nlibrary(prediction)\n## prediction can take a new dataframe or specific values of covariates\n\npred.results &lt;- prediction(fit1, at=list(partyid = 1:7), type=\"response\", calculate_se = T)\n\n\nsummary(pred.results)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nat(partyid)\nPrediction\nSE\nz\np\nlower\nupper\n\n\n\n\n1\n0.0512844\n0.0105264\n4.871990\n1.1e-06\n0.0306531\n0.0719157\n\n\n2\n0.1147444\n0.0147835\n7.761644\n0.0e+00\n0.0857693\n0.1437196\n\n\n3\n0.2204047\n0.0166507\n13.236974\n0.0e+00\n0.1877700\n0.2530395\n\n\n4\n0.3669367\n0.0168967\n21.716505\n0.0e+00\n0.3338199\n0.4000536\n\n\n5\n0.5362029\n0.0194359\n27.588317\n0.0e+00\n0.4981093\n0.5742965\n\n\n6\n0.6990679\n0.0228165\n30.638670\n0.0e+00\n0.6543483\n0.7437874\n\n\n7\n0.8295963\n0.0222636\n37.262407\n0.0e+00\n0.7859604\n0.8732322\n\n\n\n\nWhen we have covariates, we have a few more decisions to make about how to calculate quantities of interest and how to compute uncertainty. Let’s amend our model to include additional covariates.\n\nfit2 &lt;- glm(trumvote ~ partyid + racresent + oldfash, data=study, family=binomial(link=\"probit\"))\n\nNow, we have the same research question, but we have covariates. We have to decide how we want to calculate the predicted probabilities of voting for Trump at different levels of partisanship.\n\nWhere should we set racresent and oldfash when computing these values?\n\nLet’s suppose we want to hold them at observed values. This means we will calculate the average predicted probability of voting for Trump at each value of partisanship, holding the other covariates at observed values.\nWe can do this in a few ways.\nHere, let’s do this manually:\n\nbh &lt;- coef(fit2)\n\n## party id 1\nX.1 &lt;- model.matrix(fit2)\nX.1[, \"partyid\"] &lt;- 1\np.1 &lt;- pnorm(X.1 %*% bh)\np.1.mean &lt;- mean(p.1)\n\n## party id 2\nX.2 &lt;- model.matrix(fit2)\nX.2[, \"partyid\"] &lt;- 2\np.2 &lt;- pnorm(X.2 %*% bh)\np.2.mean &lt;- mean(p.2)\n\n## More efficient approach 1:\np.means &lt;- rep(NA, 7)\nfor(i in 1:7){\n  X &lt;- model.matrix(fit2)\n  X[, \"partyid\"] &lt;- i\n  p &lt;- pnorm(X %*% bh)\n  p.means[i] &lt;- mean(p)\n}\n\n## More efficient approach 2:\nmyest &lt;- function(value){\n  X &lt;- model.matrix(fit2)\n  X[, \"partyid\"] &lt;- value\n  p &lt;- pnorm(X %*% bh)\n  p.mean &lt;- mean(p)\n  return(p.mean)\n}\np.means &lt;- sapply(1:7, myest)\n\nOr, we can use prediction again.\n\npred.results2 &lt;- prediction(fit2, at=list(partyid = 1:7), type=\"response\")\n\nLet’s compare the output:\n\ncbind(p.means, summary(pred.results2)$Prediction)\n\n        p.means           \n[1,] 0.08296357 0.08296357\n[2,] 0.14824083 0.14824083\n[3,] 0.24098759 0.24098759\n[4,] 0.35835987 0.35835987\n[5,] 0.49072437 0.49072437\n[6,] 0.62383575 0.62383575\n[7,] 0.74331307 0.74331307\n\n\nWhat if we want uncertainty?\n\nWe can bootstrap or simulate confidence intervals around each quantity of interest, or use a function to do this for us.\n\n\n\nBootstrap Details\n\n\n## Step 1: sample new rows of the data and subset\nwrows &lt;- sample(x =1:nrow(study), size = nrow(study), replace = T)\nsubdata &lt;- study[wrows, ]\n\n## Step 2: run your regression model with the new data\nboot.probit &lt;-glm(trumvote ~ partyid + racresent + oldfash, \n                  data=subdata, family=binomial(link=\"probit\"))\n\n## Step 3: generate average predicted probability\nXboot &lt;- model.matrix(boot.probit)\nXboot[, \"partyid\"] &lt;- 1\nBh &lt;- coef(boot.probit)\np.boot &lt;- mean(pnorm(Xboot %*% Bh))\n\n## Step 4: wrap it in a function, make data generic\nmyboot &lt;- function(df){\n  wrows &lt;- sample(x =1:nrow(df), size = nrow(df), replace = T)\n  subdata &lt;- df[wrows, ]\n  boot.probit &lt;-glm(trumvote ~ partyid + racresent + oldfash, \n                  data=subdata, family=binomial(link=\"probit\"))\n  Xboot &lt;- model.matrix(boot.probit)\n  Xboot[, \"partyid\"] &lt;- 1\n  Bh &lt;- coef(boot.probit)\n  p.boot &lt;- mean(pnorm(Xboot %*% Bh))\n  return(p.boot)\n}\n\n## Step 5: Uncomment and replicate 1000 times\n#bestimates.party1 &lt;- replicate(1000, myboot(study))\n\n## Extract confidence interval\n#quantile(bestimates.party1, c(0.025, .975))\n\nWe would then repeat this for each partyid value. Alternatively, we could use prediction!"
  },
  {
    "objectID": "07-QuantitiesofInterest.html#putting-everything-together",
    "href": "07-QuantitiesofInterest.html#putting-everything-together",
    "title": "7  Quantities of Interest",
    "section": "7.7 Putting everything together",
    "text": "7.7 Putting everything together\nRecall, we can take something that looks like column 5 in the table from Antoine Banks and Heather Hicks example from the previous section and move it into a figure, as the authors did.\n \nLet’s run the model from column 5.\n\nfit.probit5 &lt;- glm(abtrace1 ~ factor(condition2)*racresent \n                   + factor(condition2)*oldfash,\n                  data=study, family=binomial(link = \"probit\"))\n\nLet’s generate predicted probabilities for thinking the ad is about race across levels of racial resentment in the sample, for people in the implicit and explicit conditions, holding all covariates at observed values.\n\nlibrary(prediction)\npr.imp &lt;- prediction(fit.probit5, at= list(racresent = seq(0, 1,.0625),\n                                               condition2=1),\n                         calculate_se = TRUE)\n## Let's store the summary output this time\n## And to make it easier to plot, we'll store as dataframe\npr.imp.df &lt;- summary(pr.imp)\n\npr.exp &lt;- prediction(fit.probit5, at= list(racresent = seq(0, 1,.0625),\n                                               condition2=2),\n                         calculate_se = TRUE)\npr.exp.df &lt;- summary(pr.exp)\n\nYou can peek inside pr.imp.df to see the format of the output.\nLet’s now visualize! We will try to stay true to the authors’ visual choices here.\n\n## Plot results\nplot(x=pr.imp.df$`at(racresent)`, y=pr.imp.df$Prediction, \n     type=\"l\",\n     ylim = c(0, 1), lty=2,\n     ylab = \"Predicted Probability\",\n     xlab = \"Racial Resentment\",\n     main = \"Predicted Probability of Viewing the Ad as about Race\",\n     cex.main = .7)\n## add explicit point values\npoints(x=pr.exp.df$`at(racresent)`, y=pr.exp.df$Prediction, type=\"l\")\n\n## add additional lines for the upper and lower confidence intervals\npoints(x=pr.exp.df$`at(racresent)`, y=pr.exp.df$lower, type=\"l\", col=\"gray\")\npoints(x=pr.exp.df$`at(racresent)`, y=pr.exp.df$upper, type=\"l\", col=\"gray\")\n\npoints(x=pr.imp.df$`at(racresent)`, y=pr.imp.df$lower, type=\"l\", lty=3)\npoints(x=pr.imp.df$`at(racresent)`, y=pr.imp.df$upper, type=\"l\", lty=3)\n\n## Legend\nlegend(\"bottomleft\", lty= c(2,1, 3,1), \n       c(\"Implicit\", \"Explicit\", \n         \"Implicit 95% CI\", \"Explicit 95% CI\"), cex=.7)\n\n\n\n\nLet’s combine the two dataframes.\n\npr.comb &lt;- rbind(pr.imp.df, pr.exp.df)\n\n\nlibrary(ggplot2)\nggplot(pr.comb, aes(x=`at(racresent)`, \n                    y= Prediction, \n                    color=as.factor(`at(condition2)`)))+\n  geom_line()+\n  geom_ribbon(aes(ymin=lower, ymax=upper, fill=as.factor(`at(condition2)`)), alpha=.5)+\n  xlab(\"Racial Resentment\")+\n  theme_bw()+\n  theme(legend.position = \"bottom\") +\n  scale_color_discrete(name=\"Condition\",\n                         breaks=c(1,2),\n                         labels=c(\"Implicit\", \"Explicit\"))+\n  scale_fill_discrete(name=\"Condition\",\n                         breaks=c(1,2),\n                         labels=c(\"Implicit\", \"Explicit\"))\n\n\n\n\nWe could instead show the difference between the conditions across levels of racial resentment.\n\nlibrary(margins)\nmarest &lt;- margins(fit.probit5, at= list(racresent = seq(0, 1,.0625)),\n                  variables=\"condition2\",\n                  change = c(1,2),\n                         vce=\"delta\",\n                  type=\"response\")\n\n## Store summary as dataframe\nmarest.df &lt;- summary(marest)\n\n## plot res\nggplot(marest.df, aes(x=racresent, y=AME))+\n  geom_line()+\n  geom_errorbar(aes(ymin=lower, ymax=upper), alpha=.5, width=0)+\n  theme_bw()+\n  xlab(\"Racial Resentment\")+\n  ggtitle(\"AME: Explicit - Implicit Condition on Pr(Ad About Race)\")+\n  geom_hline(yintercept = 0, color=\"red\")"
  }
]