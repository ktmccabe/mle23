[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MLE 2023",
    "section": "",
    "text": "This document will include important links and course notes for fall 2023 Maximum Likelihood Estimation in Political Science.\n\nThis webpage will be updated throughout the semester with new content.\nSprinkled throughout the website are links to additional resources that might provide more in-depth explanations of a given topic. In particular, several of the sections have benefited from previous materials developed by Kosuke Imai and the book Quantitative Social Science, Chris Bail and SICSS, Marc Ratkovic, In Song Kim, Will Lowe, and others.\nThis is a living web resource. If you spot errors or have questions or suggestions, please email me at k.mccabe@rutgers.edu."
  },
  {
    "objectID": "02-ROverview.html#first-time-with-r-and-rstudio",
    "href": "02-ROverview.html#first-time-with-r-and-rstudio",
    "title": "2  R Overview",
    "section": "2.1 First Time with R and RStudio",
    "text": "2.1 First Time with R and RStudio\nThis next section provides a few notes on using R and RStudio now that you have installed it. This is mostly repetitive of the other resources. This includes only the bare essential information for opening an R script and digging into using R as a calculator. In this section, we cover the following materials:\n\nUsing R as a calculator and assigning objects using &lt;-\nSetting your working directory and the setwd() function.\nCreating and saving an R script\n\n\n2.1.1 Open RStudio\n\nNote: The first time you open RStudio, you likely only have the three windows above. We will want to create a fourth window by opening an R script to create the fourth window.\n\nTo do this, in RStudio, click on File -&gt; New -&gt; R script in your computer’s toolbar. This will open a blank document for text editing in the upper left of the RStudio window. We will return to this window in a moment.\n\nYou can alternatively click on the green + sign indicator in the top-left corner of the RStudio window, which should give you the option to create a new R script document.\n\n\nNow you should have something that looks like this, similar to Figure 1.1. in QSS:\n\n\nThe upper-left window has our .R script document that will contain code.\nThe lower-left window is the console. This will show the output of the code we run. We will also be able to type directly in the console.\nThe upper-right window shows the environment (and other tabs, such as the history of commands). When we load and store data in RStudio, we will see a summary of that in the environment.\nThe lower-right window will enable us to view plots and search help files, among other things.\n\n\n\n2.1.2 Using R as a Calculator\nThe bottom left window in your RStudio is the Console. You can type in this window to use R as a calculator or to try out commands. It will show the raw output of any commands you type. For example, we can try to use R as a calculator. Type the following in the Console (the bottom left window) and hit “enter” or “return” on your keyboard:\n\n5 + 3\n\n[1] 8\n\n5 - 3\n\n[1] 2\n\n5^2\n\n[1] 25\n\n5 * 3\n\n[1] 15\n\n5/3\n\n[1] 1.666667\n\n(5 + 3) * 2\n\n[1] 16\n\n\nIn the other RStudio windows, the upper right will show a history of commands that you have sent from the text editor to the R console, along with other items. The lower right will show graphs, help documents and other features. These will be useful later in the course.\n\n\n2.1.3 Working in an R Script\nEarlier, I asked you to open an R script in the upper left window by doing File, then New File, then R Script. Let’s go back to working in that window.\nSet your working directory setwd()\n(Almost) every time you work in RStudio, the first thing you will do is set your working directory. This is a designated folder in your computer where you will save your R scripts and datasets.\nThere are many ways to do this.\n\nAn easy way is to go to Session \\(\\rightarrow\\) Set Working Directory \\(\\rightarrow\\) Choose Directory. I suggest choosing a folder in your computer that you can easily find and that you will routinely use for this class. Go ahead and create/select it.\nNote: when you selected your directory, code came out in the bottom left Console window. This is the setwd() command which can also be used directly to set your working directory in the future.\nIf you aren’t sure where your directory has been set, you can also type getwd() in your Console. Try it now\n\n\n## Example of where my directory was\ngetwd()\n\nIf I want to change the working directory, I can go to the top toolbar of my computer and use Session \\(\\rightarrow\\) Set Working Directory \\(\\rightarrow\\) Choose Directory or just type my file pathway using the setwd() below:\n\n## Example of setting the working directory using setwd().\n## Your computer will have your own file path.\nsetwd(\"/Users/ktmccabe/Dropbox/Rutgers Teaching/\")\n\nSaving the R Script\nLet’s now save our R script to our working directory and give it an informative name. To do so, go to File, then Save As, make sure you are in the same folder on your computer as the folder you chose for your working directory.\nGive the file an informative name, such as: “McCabeWeek1.R”. Note: all of your R scripts will have the .R extension.\n\n\n2.1.4 Preparing your R script\nNow that we have saved our R script, let’s work inside of it. Remember, we are in the top-left RStudio window now.\n\nJust like the beginning of a paper, you will want to title your R script. In R, any line that you start with a # will not be treated as a programming command. You can use this to your advantage to write titles/comments. Below is a screenshot example of a template R script.\nYou can specify your working directory at the top, too. Add your own filepath inside setwd()\n\n\n\nThen you can start answering problems in the rest of the script.\nThink of the R script as where you write the final draft of your paper. In the Console (the bottom-left window), you can mess around and try different things, like you might when you are taking notes or outlining an essay. Then, write the final programming steps that lead you to your answer in the R script. For example, if I wanted to add 5 + 3, I might try different ways of typing it in the Console, and then when I found out 5 + 3 is the right approach, I would type that into my script.\n\n\n\n2.1.5 Running Commands in your R script\nThe last thing we will note in this initial handout is how to execute commands in your R script.\nTo run / execute a command in your R script (the upper left window), you can\n\nHighlight the code you want to run, and then hold down “command + return” on a Mac or “control + enter” on Windows\nPlace your cursor at the end of the line of code (far right), and hit “command + return” on a Mac or “control + return” on Windows, or\nDo 1 or 2, but instead of using the keyboard to execute the commands, click “Run” in the top right corner of the upper-left window.\n\nTry it: Type 5 + 3 in the R script. Then, try to execute 5 + 3. It should look something like this:\n\nAfter you executed the code, you should see it pop out in your Console:\n\n5 + 3\n\n[1] 8\n\n\n\nNote: The symbol # also allows for annotation behind commands or on a separate line. Everything that follows # will be ignored by R. You can annotate your own code so that you and others can understand what each part of the code is designed to do.\n\n## Example\nsum53 &lt;- 5 + 3 # example of assigning an addition calculation\n\n\n\n2.1.6 Objects\nSometimes we will want to store our calculations as “objects” in R. We use &lt;- to assign objects by placing it to the left of what we want to store. For example, let’s store the calculation 5 + 3 as an object named sum53:\n\nsum53 &lt;- 5 + 3\n\nAfter we execute this code, sum53 now stores the calculation. This means, that if we execute a line of code that just hassum53`, it will output 8. Try it:\n\nsum53\n\n[1] 8\n\n\nNow we no longer have to type 5 + 3, we can just type sum53. For example, let’s say we wanted to subtract 2 from this calculation. We could do:\n\nsum53 - 2\n\n[1] 6\n\n\nLet’s say we wanted to divide two stored calculations:\n\nten &lt;- 5 + 5\ntwo &lt;- 1 + 1\nten / two\n\n[1] 5\n\n\nThe information stored does not have to be numeric. For example, it can be a word, or what we would call a character string, in which case you need to use quotation marks.\n\nmccabe &lt;- \"professor for this course\"\nmccabe\n\n[1] \"professor for this course\"\n\n\nNote: Object names cannot begin with numbers and no spacing is allowed. Avoid using special characters such as % and $, which have specific meanings in R. Finally, use concise and intuitive object names.}\n\nGOOD CODE: practice.calc &lt;- 5 + 3\nBAD CODE: meaningless.and.unnecessarily.long.name &lt;- 5 + 3\n\nWhile these are simple examples, we will use objects all the time for more complicated things to store (e.g., like full datasets!) throughout the course.\nWe can also store an array or “vector” of information using c()\n\nsomenumbers &lt;- c(3, 6, 8, 9)\nsomenumbers\n\n[1] 3 6 8 9\n\n\nImportance of Clean Code\nIdeally, when you are done with your R script, you should be able to highlight the entire script and execute it without generating any error messages. This means your code is clean. Code with typos in it may generate a red error message in the Console upon execution. This can happen when there are typos or commands are misused.\nFor example, R is case sensitive. Let’s say we assigned our object like before:\n\nsum53 &lt;- 5 + 3\n\nHowever, when we went to execute sum53, we accidentally typed Sum53:\n\nSum53\n\nError in eval(expr, envir, enclos): object 'Sum53' not found\n\n\nOnly certain types of objects can be used in mathematical calculations. Let’s say we tried to divide mccabe by 2:\n\nmccabe / 2\n\nError in mccabe/2: non-numeric argument to binary operator\n\n\nA big part of learning to use R will be learning how to troubleshoot and detect typos in your code that generate error messages.\n\n\n\n2.1.7 Practice\nBelow is an exercise that will demonstrate you are able to use R as a calculator and create R scripts.\n\nCreate an R script saved as ``LastnameSetup1.R” (use your last name). Within the R script, follow the example from this handout and title the script.\nSet your working directory, and include the file pathway (within setwd()) at the top of your .R script.\nDo the calculation 4 + 3 - 2 in R. Store it as an object with an informative name.\nDo the calculation 5 \\(\\times\\) 4 in R. Store it as an object with an informative name.\nAdd these two calculations together. In R, try to do this by adding together the objects you created, not the underlying raw calculations."
  },
  {
    "objectID": "02-ROverview.html#data-wrangling",
    "href": "02-ROverview.html#data-wrangling",
    "title": "2  R Overview",
    "section": "2.2 Data Wrangling",
    "text": "2.2 Data Wrangling\nSo you have some data…. AND it’s a mess!!!\nA lot of the data we may encounter in courses has been simplified to allow students to focus on other concepts. We may have data that look like the following:\nnicedata &lt;- data.frame(gender = c(\"Male\", \"Female\", \"Female\", \"Male\"),\n           age = c(16, 20, 66, 44),\n           voterturnout = c(1, 0, 1, 0))\n\n\n  gender age voterturnout\n1   Male  16            1\n2 Female  20            0\n3 Female  66            1\n4   Male  44            0\n\n\nIn the real world, our data may hit us like a ton of bricks, like the below:\nuglydata &lt;- data.frame(VV160002 = c(2, NA, 1, 2),\n           VV1400068 = c(16, 20, 66, 44),\n           VV20000 = c(1, NA, 1, NA))\n\n\n  VV160002 VV1400068 VV20000\n1        2        16       1\n2       NA        20      NA\n3        1        66       1\n4        2        44      NA\n\n\nA lot of common datasets we use in the social sciences are messy, uninformative, sprawling, misshaped, and/or incomplete. What do I mean by this?\n\nThe data might have a lot of missing values. For example, we may have NA values in R, or perhaps a research firm has used some other notation for missing data, such as a 99.\nThe variable names may be uninformative.\n\nFor example, there may be no way to know by looking at the data, which variable represents gender. We have to look at a codebook.\n\nEven if we can tell what a variable is, its categories may not be coded in a way that aligns with how we want to use the data for our research question.\n\nFor example, perhaps you are interested in the effect of a policy on people below vs. 65 and over in age. Well, your age variables might just be a numeric variable. You will have to create a new variable that aligns with your theoretical interest.\n\nDatasets are often sprawling. Some datasets may have more than 1000 variables. It is hard to sort through all of them. Likewise, datasets may have millions of observations. We cannot practically look through all the values of a column to know what is there.\nSometimes we have data shaped into separate columns when we’d rather it be reshaped into different rows.\nMaybe you have encountered a beautiful dataset that provides many measures of your independent variables of interest, but there’s one catch– it has no variable related to your outcome! You have to merge data from multiple sources to answer your research question.\n\nBelow are a few tips and resources. Ultimately, research is a constant debugging process. Loving R means seeing red error messages. The nice thing about R is that a lot of researchers constantly post coding tips and questions online. Google ends up being your friend, but it’s entirely normal to have to devote several hours (days?) to cleaning data.\n\n\n2.2.1 Dealing with Uninformative Variable Names\nHopefully, there is an easy fix for dealing with uninformative variable names. I say “hopefully” because hopefully when you encounter a dataset with uninformative variable names, the place where you downloaded the data will also include a codebook telling you what each variable name means, and how the corresponding values are coded.\nUnfortunately, this may not always be the case. One thing you can do as a researcher is when you create a dataset for your own work, keep a record (in written form, on a word document or in a pdf or code file) of what each variable means (e.g., the survey question it corresponds to or the exact economic measure), as well as how the values of the variables are coded. This good practice will help you in the short-term, as you pause and come back to working on a project over the course of a year, as well as benefit other researchers in the long run after you publish your research.\nFor examples of large codebooks, you can view the 2016 American National Election Study Survey and click on a codebook.\nI recommend that once you locate the definition of a variable of interest, rename the variable in your dataset to be informative. You can do this by creating a new variable or overwriting the name of the existing variable. You might also comment a note for yourself of what the values mean.\n\n## Option 1: create new variable\n## gender 2=Male, 1=Female\nuglydata$gender &lt;- uglydata$VV160002\n\n## Option 2: Overwrite\nnames(uglydata)[1] &lt;- \"gender2\"\n\n\n\n2.2.2 Dealing with Missing Data\nWhen we have a column with missing data, it is best to do a few things:\n\nTry to quantify how much missing data there is and poke at the reason why data are missing.\n\nIs it minor non-response data?\nOr is it indicative of a more systematic issue? For example, maybe data from a whole group of people or countries is missing for certain variables.\n\nIf the data are missing at a very minor rate and/or there is a logical explanation for the missing data that should not affect your research question, you may choose to “ignore” the missing data when performing common analyses, such as taking the mean or running a regression.\n\nIf missing data are a bigger problem, you may consider alternative solutions, such as “imputing” missing data or similarly using some type of auxilliary information to help fill in the missing values.\n\n\nIf we want to figure out how much missing data we have in a variable, we have a couple of approaches:\n\n## Summarize this variable\nsummary(uglydata$gender)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  1.000   1.500   2.000   1.667   2.000   2.000       1 \n\n## What is the length of the subset of the variable where the data are missing\nlength(uglydata$gender[is.na(uglydata$gender) == T])\n\n[1] 1\n\n\nIf we choose to ignore missing data, this can often be easily accomplished in common operations. For example, when taking the mean we just add an argument na.rm = T:\n\nmean(uglydata$VV1400068, na.rm=T)\n\n[1] 36.5\n\n\nIf we do a regression using lm or glm, R will automatically “listwise” delete any observation that has missing data (NA) on any of the variables in our regression model.\nWe should always be careful with missing data to understand how R is treating it in a particular scenario.\nFor example if we were to run table(uglydata$gender), we would have no idea there were missing data unless we knew that the total number of observations nrow(uglydata) was greater than 3. The table() command is omitting the missing values by default.\n\ntable(gender= uglydata$gender)\n\ngender\n1 2 \n1 2 \n\n\n\n\n2.2.3 Dealing with Variable Codings that Aren’t Quite Right\nOften times the ways that variables are coded in datasets we get off-the-shelf are not coded exactly as how we were dreaming up operationalizing our concepts. Instead, we are going to have to wrangle the data to get them into shape.\nThis may involve creating new variables that recode certain values, creating new variables that collapse some values into a smaller number of categories, combining multiple variables into a single variable (e.g., representing the average), or setting some of the variable values to be missing (NA). All of these scenarios may come up when you are dealing with real data.\nChapter 2 of Kosuke Imai’s book Quantitative Social Science walks through some examples of how to summarize your data, subset the data (2.2.3), create new variables using conditional statements (Section 2.2.4, e.g., “If age is below 65, assign the new variable a value of”0”, otherwise, assign it a value of “1”), and creating new factor variables (2.2.5, e.g., coding anyone who is Protestant, Catholic, or Lutheran in the data as “Christian”).\nHere is a short video working through the example from 2.2.4 using conditional statements to construct new variables. It uses the resume dataframe, which can be loaded below.\n\nresume &lt;- read.csv(\"https://raw.githubusercontent.com/ktmccabe/teachingdata/main/resume.csv\")\n\n\nR Studio has its own set of primers on various topics, including summarizing and working with data. See the Work with Data primer, as well as the full list of other topics. These will often rely on tidyverse coding.\n\n\n2.2.4 Dealing with Incomplete Data (Merging!)\nSometimes in order to answer our research questions, we need to combine data from multiple sources. If we have a large amount of data, this may be quite daunting. Fortunately, R has several commands that allow us to merge or append datasets.\nHere is a video working through examples of merging and appending data based on the tutorial below.\n\nHere are a few resources on merging and appending data:\n\nUsing the merge command in R. See Explanation.\n\nIt will look for variable(s) held in common between datasets and join the datasets by the matching values on these variables.\n\nAppending data in R (e.g., Maybe you have one dataset from 2010 and another from 2012, and you want to stack them on top of each other). See Explanation.\n\nSome merging problems are extremely difficult. For example, some researchers need to merge large datasets–like the voter file– with other administrative records. However, how someone’s name is displayed in one dataset might not match at all with the other dataset. For these complex problems, we might need “fuzzy matching.” Here is an R package that helps with this more complex case and related paper.\n\n\n2.2.5 Dealing with Poorly Shaped Data\nData can come in a variety of shapes and sizes. It’s a beautiful disaster.\nSometimes it’s particularly useful to have data in wide formats, where every row relates to a particular unit of data– such as a country or a survey respondent. And perhaps each column represents information about that unit at a particular point in time. For example, perhaps you have a column with information on that subject for the past five years.\ncountrywide &lt;- data.frame(country = c(\"Canada\", \"USA\"),\n                          economy2016 = c(10, 12),\n                          economy2017 = c(11, 11),\n                          economy2018 = c(9, 5),\n                          economy2019 = c(13, 8),\n                          economy2020 = c(12, 6))\n\n\n  country economy2016 economy2017 economy2018 economy2019 economy2020\n1  Canada          10          11           9          13          12\n2     USA          12          11           5           8           6\n\n\nHowever, other times, it would be more useful to you, as you dig into your data analysis, to have this information arranged in “long” format, such that every row is now a unit-year combination. You have a row for Canada in 2020, a row for Canada in 2019, and so on. Countries are now represented in multiple rows of your data.\ncountrylong &lt;- data.frame(country = rep(c(\"Canada\", \"USA\"),5),\n                          year = 2016:2020,\n                          economy= c(10, 12,11, 11,9, 5,13, 8,12, 6))\n\n\n   country year economy\n1   Canada 2016      10\n2      USA 2017      12\n3   Canada 2018      11\n4      USA 2019      11\n5   Canada 2020       9\n6      USA 2016       5\n7   Canada 2017      13\n8      USA 2018       8\n9   Canada 2019      12\n10     USA 2020       6\n\n\nUltimately, different shapes of data are advantageous for different research questions. This means it is best if we have a way to (at least somewhat) easily shift between the two formats.\nHere is a resource on how to “reshape” your data between wide and long from UCLA.\nHere are a few additional resources:\n\nMore on reshape – For the tidyverse fans. Using gather() and spread() in tidyverse from R for Data Science and explained by Chris Bail here.\n\n\n\n2.2.6 Subsetting data by rows and columns\nSometimes we do not want to deal with our entire dataset for an analysis. Instead, we might want to only analyze certain rows (e.g., maybe if we are just studying Democrats, for example). Similarly, we might have a dataframe with 1000 columns, from which we are only using about 20. We might want to remove those extra columns to make it easier to work with our dataframes.\nBelow are a few examples of subsetting data and selecting columns. We will use the resume dataset from the Kosuke Imai QSS book for demonstration. This is a dataset from an experiment describing whether certain applicants, who varied in the gender (sex) and race (race) signaled by their name (firstname), received callbacks (call) for their employment applications.\nHere is a short video working through these examples.\n\nLet’s load the data.\n\nresume &lt;- read.csv(\"https://raw.githubusercontent.com/ktmccabe/teachingdata/main/resume.csv\")\n\nSubset particular rows\nTo do this, put the row numbers you want to keep on the left side of the comma. Putting nothing on the right side means you want to keep all columns.\n\n## numerically\nresume[1,] # first row\n\n  firstname    sex  race call\n1   Allison female white    0\n\nresume[1:4,] # first through 4th rows\n\n  firstname    sex  race call\n1   Allison female white    0\n2   Kristen female white    0\n3   Lakisha female black    0\n4   Latonya female black    0\n\nresume[c(1, 3, 4),] # 1, 3, 4 rows\n\n  firstname    sex  race call\n1   Allison female white    0\n3   Lakisha female black    0\n4   Latonya female black    0\n\n\nUsing the subset command with logical expressions &gt; &gt;= == &lt; &lt;= !=\n\n## by logical expressions\nwomen &lt;- subset(resume, sex == \"female\")\nwomen &lt;- resume[resume$sex == \"female\", ] ## alternative\n\ncalledback &lt;- subset(resume, call == 1)\ncalledback &lt;- subset(resume, call &gt; 0)\n\nAnd or Or statements & or |\n\nblackwomen &lt;- subset(resume, sex == \"female\" & race == \"black\")\nbradbrendan &lt;- subset(resume, firstname == \"Brad\" | \n                        firstname == \"Brendan\")\n\nThe tidyverse also has commands for subsetting. Here is an example using filter.\n\nlibrary(tidyverse)\nblackwomen &lt;- resume %&gt;%\n  filter(sex == \"female\" & race == \"black\")\n\nSelecting particular columns\nNote, now we are working on the right side of the comma.\n\n## numerically\nfirst &lt;- resume[, 1] # first column\nfirstsecond &lt;- resume[, 1:2] # first and second column\nnotfourth &lt;- resume[, -4] # all but the fourth column\n\n\n## by labels\njustthese &lt;- resume[, c(\"firstname\", \"sex\")]\n\nUsing the select command\n\n## install.packages(\"dplyr\")\nlibrary(dplyr)\nsubdata &lt;- resume %&gt;% dplyr::select(firstname, sex) ## just these\nsubdata2 &lt;- resume %&gt;% dplyr::select(-firstname, -sex) ## all but these two\n\n\n\n2.2.7 Visualizing Data\nThere are many commands for plotting your data in R. The most common functions in base R are plot(), barplot() and hist(). You will see many examples throughout the notes with each of these functions.\nTo get you started, the most simple thing to note about the plot() command is that it is based on a coordinate system. You specify the x and y coordinates for which you want to plot a series of points.\nFor example, here is a plot at points 1,40; 3,50; and 4,60.\n\nplot(x = c(1,3,4), y=c(40, 50, 60))\n\n\n\n\nInstead of putting raw numbers as the coordinates, you can provide object names. E.g.,\n\nxcoord &lt;- c(1,3,4)\nycoord &lt;- c(40, 50, 60)\nplot(x = xcoord, y=ycoord)\n\nBeyond that, you can play around with many aesthetics in R, such as the type, pch, lty, as well as labels main, ylab, xlab, font sizes cex.main, cex.axis, cex.lab, and axis limits ylim, xlim. Below is an example. Play around with changing some of the specifications, and see how the plot changes.\n\nxcoord &lt;- c(1,3,4)\nycoord &lt;- c(40, 50, 60)\nplot(x = xcoord, y=ycoord,\n     main = \"Example plot\",\n     ylab= \"Example y axis\",\n     xlab = \"Example x axis\",\n     cex.main = .8,\n     ylim = c(0, 80),\n     xlim = c(1,4),\n     pch = 15,\n     col=\"red\",\n     type = \"b\",\n     lty=2)\n\n\n\n\nThe function barplot takes a single vector of values. This can be a raw vector you have created or a table object or tapply object, for example, displaying the counts of different observations or means.\nYou can add a names.arg argument to specify particular names for each bar. Many of the other aesthetics are the same as plot. You can play around with adding aesthetics.\nExample:\n\nbarplot(ycoord,\n        names.arg = c(\"First\", \"Second\", \"Third\"),\n        col=\"blue\")\n\n\n\n\nFor more on visualizing data, you can see the RStudio primers.\nR also has a package called ggplot2 which includes the function ggplot and many elaborate ways to plot your data. The gg stands for the grammar of graphics. For a video introduction to ggplot I recommend watching Ryan Womack’s video from 27:30 on. It uses the data diamonds which can be loaded in R through the following command. See approximately minute 35 for an example with a bar plot.\n\nlibrary(ggplot2)\ndata(diamonds)\n\n\n\n\n2.2.8 Reproducing your steps\nIt is really important to keep a record of all of the changes you have made to the original data. An R script or R markdown file is a useful way to do this, so long as you add comments that explain what you are doing.\nYou want to get your code to a place when a stranger can open your R file, load your data, and reproduce each step you took to get to the final results— all while never even needing to contact you with questions. That can be difficult, but it’s good to aim for that high bar, even if sometimes, we fall short in how we are documenting each step.\nThis website provides a nice introduction to R Markdown, one tool for embedding R code inside textual documents. See here.\nIf you want to get advanced with reproducibility, you can watch Chris Bail’s on the subject describing the uses of R Markdown and GitHub among other tools for communicating and collaboration. He also links to other resources."
  },
  {
    "objectID": "02-ROverview.html#tools-for-writing-up-results",
    "href": "02-ROverview.html#tools-for-writing-up-results",
    "title": "2  R Overview",
    "section": "2.3 Tools for writing up results",
    "text": "2.3 Tools for writing up results\n\n2.3.1 R Markdown\nR Markdown is a free tool within RStudio that allows you to weave together text and code (along with images and tables) into the same document. It can compile these documents (written inside R Studio) into html, pdf, or Word doc output files. R Markdown can be incredibly helpful for doing things like generating replication files, writing up problem sets, or even writing papers.\nThis site includes an introduction to R Markdown.\n\nSee also here and here\n\nR Markdown has its own syntax, which includes functionality for writing mathematical equations. The pdf output option in R Markdown requires LaTex, described in the next section. Below we work with just the html output.\nR Markdown documents can be “compiled” into html, pdf, or docx documents by clicking the Knit button on top of the upper-left window. Below is an example of what a compiled html file looks like.\n\nNote that the image has both written text and a gray chunk, within which there is some R code, as well as the output of the R code (e.g., the number 8 and the image of the histogram plot. \n\nWe say this is a “compiled” RMarkdown document because it differs from the raw version of the file, which is a .Rmd file format. Below is an example of what the raw .Rmd version looks like, compared to the compiled html version.\n \nGetting started with RMarkdown\nJust like with a regular R script, to work in R Markdown, you will open up RStudio.\n\nFor additional support beyond the notes below, you can also follow the materials provided by RStudio for getting started with R Markdown https://rmarkdown.rstudio.com/lesson-1.html.\n\nThe first time you will be working in R Markdown, you will want to install two packages: rmarkdown and knitr. You can do this in the Console window in RStudio (remember the lower-left window!).\nType the following into the Console window and hit enter/return.\n\ninstall.packages(\"rmarkdown\")\ninstall.packages(\"knitr\")\n\nOnce you have those installed, now, each time you want to create an R Markdown document, you will open up a .Rmd R Markdown file and get to work.\n\nGo to File -&gt; New File -&gt; R Markdown in RStudio\n\nAlternatively, you can click the green + symbol at the top left of your RStudio window\n\nThis should open up a window with several options, similar to the image below\n\nCreate an informative title and change the author name to match your own\nFor now, we will keep the file type as html. In the future, you can create pdf or .doc documents. However, these require additional programs installed on your computer, which we will not cover in the course.\n\n\n\n\nAfter you hit “OK” a new .Rmd script file will open in your top-left window with some template language and code chunks, similar to the image below. Alternatively, you can start from scratch by clicking “Create Empty Document” or open a template .Rmd file of your own saved on your computer.\n\n\n\nSave as .Rmd file. Save the file by going to “File -&gt; Save as” in RStudio\n\n\nGive the file an informative name like your LastnamePractice1.Rmd\n\n\nKey Components. Now you are ready to work within the Rmd script file. We will point to four basic components of this file, and you can build your knowledge of RMarkdown from there.\n\nThe top part bracketed by --- on top and bottom is the YAML component. This tells RStudio the pertinent information about how to “compile” the Rmd file.\n\nMost of the time you can leave this alone, but you can always edit the title, author, or date as you wish.\n\nThe next component are the global options for the document. It is conveniently labeled “setup.” By default what this is saying is that the compiled version will “echo” (i.e., display all code chunks and output) unless you specifically specify otherwise. For example, note that it says include = FALSE for the setup chunk. That setting means that this code chunk will “run” but it will not appear in the nicely compiled .html file.\n\nMost of the time you will not need to edit those settings.\n\nThe third component I want to bring attention to is the body text. The # symbol in RMarkdown is used to indicate that you have a new section of the document. For example, in the compiled images at the beginning, this resulted in the text being larger and bolded when it said “Problem 2.” In addition to just using a single #, using ## or ### can indicate subsections or subsubsections. Other than that symbol, you can generally write text just as you would in any word processing program, with some exceptions, such as how to make text bold or italicized.\nThe final component I want to call attention to are the other main body code chunks. These are specific parts of the document where you want to create a mini R script. To create these, you can simply click the + C symbol toward the top of the top left window of RStudio and indicate you want an R chunk.\n\n\n\n\nWriting R Code. Within a code chunk, you can type R code just like you would in any R script, as explained in the previous section. However, in RMarkdown, you also have the option of running an entire code chunk at once by hitting the green triangle at the top-right of a given code chunk.\n\n\n\nKnitting the document. Once you have added a code chunk and/or some text, you are ready to compile or “Knit” the document. This is what generates the .html document.\n\nTo do so, click on the Knit button toward the top of the top-left window of Rstudio. After a few moments, this should open up a preview window displaying the compiled html file.\nIt will also save an actual .html file in your working directory (the same location on your computer where you have saved the .Rmd file)\nTry to locate this compiled .html file on your computer and open it. For most computers, .html files will open in your default web browser, such as Google Chrome or Safari.\nThis step is a common place where errors are detected and generated. Sometimes the compiling process fails due to errors in the R code in your code chunks or an error in the Markdown syntax. If your document fails to knit, the next step is to try to troubleshoot the error messages the compiling process generates. The best way to reduce and more easily detect errors is to “knit as you go.” Try to knit your document after each chunk of code you create.\n\n\n\n\n2.3.2 LaTex\nLaTex is a typesetting program for drafting documents, much like Microsoft Word. Some advantages of using LaTex in writing empirical research papers is, once you learn the basics of the program, it can become easier to add tables, figures, and equations into a paper with little effort. The downside is that it has its own syntax, which takes a little time to learn. LaTex also has a feature called “beamer” which uses LaTex to generate slides. You can use this for presentations. LaTex “compiles” documents into pdf files.\nHere is one introduction for getting started with LaTex that starts from the installation stage. Here also is a link to a set of slides from Overleaf discussing the basics of LaTex: Slides\nYou can download the Tex distribution for your operating system here. In addition to this, there are many programs available for running LaTex on your computer, which range from free, very basic tools (e.g., TexWorks) to tools with fancy capabilities.\nOverleaf is an online program for drafting LaTex documents. It has a nice feature where it allows you to share a document so that multiple people can work on the document simultaneously. This makes Overleaf a great program for projects where you have co-authors. The basic tools in Overleaf are available for free, but if you want to start sharing documents with a lot of co-authors, it requires a paid account.\nLaTex also has the ability to integrate citations into your documents. The part 2 tutorial from Overleaf goes over this.\nRStudio also has a program built-in called Sweave (.Rnw) documents that works with knitR, which weave together R code and LaTex syntax, allowing you to compile them into pdf documents and slide presentations. This is very similar to how R Markdown works, but with somewhat different syntax. See here for an overview. Your problem sets are generally Sweave/knitR documents.\n\n\n2.3.3 Formatting and Exporting R Results\nR has a number of tools, including the packages texreg, xtable, and stargazer, which can be used to export tables made in R to nicely formatted LaTex or html output.\nHere is a link to the texreg package documentation. Section 5 has examples of the texreg and htmlreg functions within the texreg package. These can be integrated into R Markdown and Sweave documents, and their output can be pasted into LaTex or Microsoft Word.\nYour choice of function will depend on where you ultimately want your results to be compiled. If you are generating results that will be compiled to pdf using LaTex, then texreg works well. If you are exporting results to Word, than you may wish to use the htmlreg function within the texreg package, which will generate output that can be pasted into Word.\nA simple example using R Markdown html output. (Note, if you wanted to export the table to Word, you would add an argument specifying file = \"myfit.doc\" to the function. See the above link for examples:\n\nmydata &lt;- read.csv(\"https://raw.githubusercontent.com/ktmccabe/teachingdata/main/resume.csv\")\nfit &lt;- lm(call ~ race, data=mydata)\n\n\n## First time you use texreg, install it\ninstall.packages(\"texreg\")\n\nlibrary(texreg)\nhtmlreg(list(fit),\n        stars=c(0.001, 0.01, 0.05),\n        caption = \"Regression of Call Backs on Race\")\n\n\n\nRegression of Call Backs on Race\n\n\n\n\n \n\n\nModel 1\n\n\n\n\n\n\n(Intercept)\n\n\n0.06***\n\n\n\n\n \n\n\n(0.01)\n\n\n\n\nracewhite\n\n\n0.03***\n\n\n\n\n \n\n\n(0.01)\n\n\n\n\nR2\n\n\n0.00\n\n\n\n\nAdj. R2\n\n\n0.00\n\n\n\n\nNum. obs.\n\n\n4870\n\n\n\n\n\n\n***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05\n\n\n\n\n\nYou can add more arguments to the function to customize the name of the model and the coefficients. You can also add multiple models inside the list argument, for example, if you wanted to present a table with five regression models at once. Here is an example with two:\nfit2 &lt;- lm(call ~ race + sex, data=mydata)\n\nlibrary(texreg)\nhtmlreg(list(fit, fit2),\n        stars=c(0.001, 0.01, 0.05),\n        caption = \"Regression of Call Backs on Race and Sex\")\n\n\n\nRegression of Call Backs on Race and Sex\n\n\n\n\n \n\n\nModel 1\n\n\nModel 2\n\n\n\n\n\n\n(Intercept)\n\n\n0.06***\n\n\n0.07***\n\n\n\n\n \n\n\n(0.01)\n\n\n(0.01)\n\n\n\n\nracewhite\n\n\n0.03***\n\n\n0.03***\n\n\n\n\n \n\n\n(0.01)\n\n\n(0.01)\n\n\n\n\nsexmale\n\n\n \n\n\n-0.01\n\n\n\n\n \n\n\n \n\n\n(0.01)\n\n\n\n\nR2\n\n\n0.00\n\n\n0.00\n\n\n\n\nAdj. R2\n\n\n0.00\n\n\n0.00\n\n\n\n\nNum. obs.\n\n\n4870\n\n\n4870\n\n\n\n\n\n\n***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05\n\n\n\n\n\n\n\n2.3.4 Additional formatting examples\nHere are some additional examples with different formats. You can run them on your own computer to see what the output looks like.\nThe package texreg has three primary formats\n\ntexreg() for LATEX output;\nhtmlreg() for HTML, Markdown-compatible and Microsoft Word-compatible output;\nscreenreg() for text output to the R console.\n\nIf you are working with a LaTex document, I recommend using texreg(), which will output LaTex syntax in your R console, which you can copy and paste into your article document.\nNote: this function allows you to customize model and coefficient names.\n\nlibrary(texreg)\ntexreg(list(fit, fit2),\n        stars=c(0.001, 0.01, 0.05),\n        caption = \"Regression of Call Backs on Race and Sex\",\n       custom.model.names = c(\"Bivariate\", \"Includes Sex\"),\n       custom.coef.names = c(\"Intercept\",\n                             \"Race- White\",\n                             \"Sex- Male\"))\n\nIf you are working with a Microsoft Word document, I recommend using htmlreg() and specifying a file name for your output. This will export a file to your working directory, which you can copy and paste into your Word article document. Otherwise, the syntax is the same as above.\n\nlibrary(texreg)\nhtmlreg(list(fit, fit2), file = \"models.doc\",\n        stars=c(0.001, 0.01, 0.05),\n        caption = \"Regression of Call Backs on Race and Sex\",\n       custom.model.names = c(\"Bivariate\", \"Includes Sex\"),\n       custom.coef.names = c(\"Intercept\",\n                             \"Race- White\",\n                             \"Sex- Male\"))\n\nIf you are trying to read the output in your R console, that’s when I would use screenreg(). However, for professional manuscript submissions, I would recommend the other formats.\n\nlibrary(texreg)\nscreenreg(list(fit, fit2), \n        stars=c(0.001, 0.01, 0.05),\n        caption = \"Regression of Call Backs on Race and Sex\",\n       custom.model.names = c(\"Bivariate\", \"Includes Sex\"),\n       custom.coef.names = c(\"Intercept\",\n                             \"Race- White\",\n                             \"Sex- Male\"))\n\nThe package stargazer allows similar options. I don’t think there are particular advantages to either package. Whatever comes easiest to you. The default for stargazer will output LaTex code into your R console.\n\nNote that the syntax is similar but has slightly different argument names from the texreg package.\nAlso, the intercept is at the bottom by default for stargazer. Be careful of the covariate ordering when you add labels.\n\n\nlibrary(stargazer)\nstargazer(list(fit, fit2), \n        star.cutoffs=c(0.05,0.01, 0.001),\n        title= \"Regression of Call Backs on Race and Sex\",\n        dep.var.labels.include = F,\n       column.labels = c(\"Call Back\", \"Call Back\"),\n       covariate.labels = c(\"Race- White\",\n                             \"Sex- Male\",\n                             \"Intercept\"))\n\nYou can adjust the type of output in stargazer for other formats, similar to texreg. Here is an example of Microsoft Word output.\n\nlibrary(stargazer)\nstargazer(list(fit, fit2), out = \"modelstar.doc\", type=\"html\",\n        star.cutoffs=c(0.05,0.01, 0.001),\n        dep.var.labels.include = F,\n        title= \"Regression of Call Backs on Race and Sex\",\n       column.labels = c(\"Call Back\", \"Call Back\"),\n       covariate.labels = c(\"Race- White\",\n                             \"Sex- Male\",\n                             \"Intercept\"))\n\n\n\n2.3.5 Additional Table Types\nSometimes you might want to create tables that are not from regression models, such as tables for descriptive statistics. R has other packages for tables of this type.\nFor example xtable can create simple html and latex tables. You just have to supply the function with a table object or matrix.\n\nlibrary(xtable)\ntable1 &lt;- table(race = mydata$race, sex = mydata$sex)\n\n\n## LaTeX\nxtable(table1)\n\n\n## Word\nprint(xtable(table1), type=\"html\", file = \"crosstab.doc\")\n\n## Html\nprint(xtable(table1), type=\"html\")\n\n\n\n\n\n\n\n\n\nfemale\n\n\nmale\n\n\n\n\nblack\n\n\n1886\n\n\n549\n\n\n\n\nwhite\n\n\n1860\n\n\n575"
  },
  {
    "objectID": "02-ROverview.html#exploratory-data-analysis-tools",
    "href": "02-ROverview.html#exploratory-data-analysis-tools",
    "title": "2  R Overview",
    "section": "2.4 Exploratory Data Analysis Tools",
    "text": "2.4 Exploratory Data Analysis Tools\nOne of the first things you may want to do when you have a new dataset is to explore! Get a sense of the variables you have, their class, and how they are coded. There are many functions in base R that help with this, such as summary(), table(), and descriptive statistics like mean or quantile.\nLet’s try this with the built-in mtcars data.\n\ndata(\"mtcars\")\n\nsummary(mtcars$cyl)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  4.000   4.000   6.000   6.188   8.000   8.000 \n\nquantile(mtcars$wt)\n\n     0%     25%     50%     75%    100% \n1.51300 2.58125 3.32500 3.61000 5.42400 \n\nmean(mtcars$mpg, na.rm=T)\n\n[1] 20.09062\n\nsd(mtcars$mpg, na.rm=T)\n\n[1] 6.026948\n\ntable(gear=mtcars$gear, carb=mtcars$carb)\n\n    carb\ngear 1 2 3 4 6 8\n   3 3 4 3 5 0 0\n   4 4 4 0 4 0 0\n   5 0 2 0 1 1 1\n\n\nAs discussed in the visualization, you can also quickly describe univariate data with histograms, barplots, or density plots using base R or ggplot.\n\nhist(mtcars$mpg, breaks=20, main=\"Histogram of MPG\")\n\n\n\nplot(density(mtcars$mpg, na.rm=T),  \n     main=\"Distribution of MPG\")\n\n\n\nbarplot(table(mtcars$gear), main=\"Barplot of Gears\")\n\n\n\n\n\nlibrary(ggplot2)\nggplot(mtcars, aes(mpg))+\n  geom_histogram(bins=20)+\n  ggtitle(\"Histogram of MPG\")\n\n\n\nggplot(mtcars, aes(mpg))+\n  geom_density()+\n  ggtitle(\"Distribution of MPG\")\n\n\n\nggplot(mtcars, aes(gear))+\n  geom_bar(stat=\"count\")+\n  ggtitle(\"Barplot of Gears\")"
  },
  {
    "objectID": "03-TheMATH.html#mathematical-operations",
    "href": "03-TheMATH.html#mathematical-operations",
    "title": "3  The Math",
    "section": "3.1 Mathematical Operations",
    "text": "3.1 Mathematical Operations\nIn this first section, we will review mathematical operations that you have probably encountered before. In many cases, this will be a refresher on the rules and how to read notation.\n\n3.1.1 Order of Operations\nMany of you may have learned the phrase, “Please Excuse My Dear Aunt Sally” which stands for Parentheses (and other grouping symbols), followed by Exponents, followed by Multiplication and Division from left to right, followed by Addition and Subtraction from left to right.\nThere will be many equations in our future, and we must remember these rules.\n\nExample: \\(((1+2)^3)^2 = (3^3)^2 = 27^2 = 729\\)\n\nTo get the answer, we focused on respecting the parentheses first, identifying the inner-most expression \\(1 + 2 = 3\\), we then moved out and conducted the exponents to get to \\((3^3)^2 = 27^2\\).\nNote how this is different from the answer to \\(1 + (2^3)^2 = 1 + 8^2 = 65\\), where the addition is no longer part of the parentheses.\n\n\n3.1.2 Exponents\nHere is a cheat sheet of some basic rules for exponents. These can be hard to remember if you haven’t used them in a long time. Think of \\(a\\) in this case as a number, e.g., 4, and \\(b\\), \\(k\\), and \\(l\\), as other numbers.\n\n\\(a^0 = 1\\)\n\\(a^1 = a\\)\n\\(a^k * a^l = a^{k + l}\\)\n\\((a^k)^l = a^{kl}\\)\n\\((\\frac{a}{b})^k = (\\frac{a^k}{b^k})\\)\n\nThese last two rules can be somewhat tricky. Note that a negative exponent can be re-written as a fraction. Likewise an exponent that is a fraction, the most common of which we will encounter is \\(\\frac{1}{2}\\) can be re-written as a root, in this case the square root (e.g., \\(\\sqrt{a}\\)).\n\n\\(a^{-k} = \\frac{1}{a^k}\\)\n\\(a^{1/2} = \\sqrt{a}\\)\n\n\n\n3.1.3 Summations and Products\nThe symbol \\(\\sum\\) can be read “take the sum of” whatever is to the right of the symbol. This is used to make the written computation of a sum much shorter than it might be otherwise. For example, instead of writing the addition operations separately in the example below, we can simplify it with the \\(\\sum\\) symbol. This is especially helpful if you would need to add together 100 or 1000 or more things. We will see these appear a lot in the course, for better or worse, so getting comfortable with the notation will be useful.\nUsually, there is notation just below and just above the symbol (e.g., \\(\\sum_{i=1}^3\\)). This can be read as “take the sum of the following from \\(i=1\\) to \\(i=3\\). We perform the operation in the expression, each time changing \\(i\\) to a different number, from 1 to 2 to 3. We then add each expression’s output together.\n\nExample: \\(\\sum_{i=1}^3 (i + 1)^2 = (1 + 1)^2 + (2 + 1)^2 + (3+1)^2 = 29\\)\n\nWe will also encounter the product symbol in this course: \\(\\prod\\). This is similar to the summation symbol, but this time we are multiplying instead of adding.\n\nExample: \\(\\prod_{k = 1}^3 k^2 = 1^2 \\times 2^2 \\times 3^2 = 36\\)\n\n\n\n3.1.4 Logarithms\nIn this class, we will generally assume that \\(\\log\\) takes the natural base \\(e\\), which is a mathematical constant equal to 2.718…. In other books, the base might be 10 by default.\n\nIf we have, \\(\\log_{10} x = 2\\), this is like saying 10^2 = 100.\nWith base \\(e\\), we have \\(\\log_e x =2\\), which is \\(e^2 = 7.389\\).\nWe are just going to write \\(\\log_e\\) as \\(\\log\\) but know that the \\(e\\) is there.\n\nA key part of maximum likelihood estimation is writing down the log of the likelihood equation, so this is a must-have for later in the course.\nHere is a cheat sheet of common rules for working with logarithms.\n\n\\(\\log x = 8 \\rightarrow e^8 = x\\)\n\\(e^\\pi = y \\rightarrow \\log y = \\pi\\)\n\\(\\log (a \\times b) = \\log a + \\log b\\)\n\\(\\log a^n = n \\log a\\)\n\\(\\log \\frac{a}{b} = \\log a - \\log b\\)\n\nWhy logarithms? There are many different reasons why social scientists use logs.\n\nSome social phenomena grow exponentially, and logs make it is easier to visualize exponential growth, as is the case in visualizing the growth in COVID cases. See this example.\nRelatedly, taking the log of a distribution that is skewed, will make it look more normal or symmetrical, which has some nice properties.\nSometimes the rules of logarithms are more convenient than non-logarithms. In MLE, we will take particular advantage of this rule: \\(\\log (a \\times b) = \\log a + \\log b\\), which turns a multiplication problem into an addition problem."
  },
  {
    "objectID": "03-TheMATH.html#mathematical-operations-in-r",
    "href": "03-TheMATH.html#mathematical-operations-in-r",
    "title": "3  The Math",
    "section": "3.2 Mathematical Operations in R",
    "text": "3.2 Mathematical Operations in R\nWe will use R for this course, and these operations are all available with R code, allowing R to become a calculator for you. Here are some examples applying the tools above.\n\n3.2.1 PEMDAS\n\n((1+2)^3)^2 \n\n[1] 729\n\n1 + (2^3)^2 \n\n[1] 65\n\n\n\n\n3.2.2 Exponents\nYou can compare the computation below to match with the rules above. Note how the caret ^ symbol is used for exponents, and the asterisk * is used for multiplication. We also have a function sqrt() for taking the square root.\n\n## Let's say a = 4 for our purposes\na &lt;- 4\n## And let's say k= 3 and l=5, b=2\nk &lt;- 3\nl &lt;- 5\nb &lt;- 2\n\n\n\\(a^0 = 1\\)\n\\(a^1 = a\\)\n\n\na^0\na^1\n\n[1] 1\n[1] 4\n\n\nNote how we use parentheses in R to make it clear that the exponent includes not just k but (k + l)\n\n\\(a^k * a^l = a^{k + l}\\)\n\n\na^k * a^l \na^(k + l)\n\n[1] 65536\n[1] 65536\n\n\nNote how we use the asterisk to make it clear we want to multiply k*l\n\n\\((a^k)^l = a^{kl}\\)\n\n\n(a^k)^l\na^(k*l)\n\n[1] 1073741824\n[1] 1073741824\n\n\n\n\\((\\frac{a}{b})^k = (\\frac{a^k}{b^k})\\)\n\n\n(a / b)^k\n(a ^k)/(b^k)\n\n[1] 8\n[1] 8\n\n\n\n\\(a^{-k} = \\frac{1}{a^k}\\)\n\n\na^(-k) \n1 / (a^k)\n\n[1] 0.015625\n[1] 0.015625\n\n\n\n\\(a^{1/2} = \\sqrt{a}\\)\n\n\na^(1/2) \nsqrt(a)\n\n[1] 2\n[1] 2\n\n\n\n\n3.2.3 Summations\nSummations and products are a little more nuanced in R, depending on what you want to accomplish. But here is one example.\nLet’s take a vector (think a list of numbers) that goes from 1 to 4. We will call it ourlist.\n\nourlist &lt;- c(1,2,3,4)\nourlist\n## An alternative is to write this: ourlist &lt;- 1:4\n\n[1] 1 2 3 4\n\n\nNow let’s do \\(\\sum_{i = 1}^4 (i + 1)^2 = (1 +1)^2 + (1+2)^2 + (1 + 3)^2 + (1 + 4)^2 = 54\\)\nIn R, when you add a number to a vector, it will add that number to each entry in the vector. Example\n\nourlist + 1\n\n[1] 2 3 4 5\n\n\nWe can use that now to do the inside part of the summation. Note: the exponent works the same way, squaring each element of the inside expression.\n\n(ourlist + 1)^2\n\n[1]  4  9 16 25\n\n\nNow, we will embed this expression inside a function in R called sum standing for summation. It will add together each of the inside components.\n\nsum((ourlist + 1)^2)\n\n[1] 54\n\n\nIf instead we wanted the product, multiplying each element of the inside together, we could use prod(). We won’t use that function very much in this course.\n\n\n3.2.4 Logarithms\nR also has functions related to logarithms, called log() and exp() for the for the natural base \\(e\\). By default, the natural exponential is the base in the log() R function.\n\n\\(\\log x = 8 \\rightarrow e^8 = x\\)\n\n\nexp(8)\n\n[1] 2980.958\n\nlog(2980.958)\n\n[1] 8\n\n\nNote that R also has a number of built-in constants, like pi.\n\n\\(e^\\pi = y \\rightarrow \\log y = \\pi\\)\n\n\nexp(pi)\n\n[1] 23.14069\n\nlog(23.14069)\n\n[1] 3.141593\n\n\n\n\\(\\log (a \\times b) = \\log a + \\log b\\)\n\n\nlog(a * b)\nlog(a) + log(b)\n\n[1] 2.079442\n[1] 2.079442\n\n\nLet’s treat \\(n=3\\) in this example and enter 3 directly where we see \\(n\\) below. Alternatively you could store 3 as n, as we did with the other letters above.\n\n\\(\\log a^n = n \\log a\\)\n\n\nlog(a ^ 3)\n\n3 * log(a)\n\n[1] 4.158883\n[1] 4.158883\n\n\n\n\\(\\log \\frac{a}{b} = \\log a - \\log b\\)\n\n\nlog(a/b)\nlog(a) - log(b)\n\n[1] 0.6931472\n[1] 0.6931472"
  },
  {
    "objectID": "03-TheMATH.html#derivatives",
    "href": "03-TheMATH.html#derivatives",
    "title": "3  The Math",
    "section": "3.3 Derivatives",
    "text": "3.3 Derivatives\nWe will need to take some derivatives in the course. The reason is because a derivative gets us closer to understanding how to minimize and maximize certain functions, where a function is a relationship that maps elements of a set of inputs into a set of outputs, where each input is related to one output.\nThis is useful in social science, with methods such as linear regression and maximum likelihood estimation because it helps us estimate the values that we think will best describe the relationship between our independent variables and a dependent variable.\nFor example, in ordinary least squares (linear regression), we choose coefficients, which describe the relationship between the independent and dependent variables (for every 1 unit change in x, we estimate \\(\\hat \\beta\\) amount of change in y), based on a method that tries to minimize the squared error between our estimated outcomes and the actual outcomes. In MLE, we will have a different quantity, which we will try to maximize.\nTo understand derivatives, we will briefly define limits.\nLimits\nA limit describes how a function behaves as it approaches (gets very close to) a certain value\n\n\\(\\lim_{x \\rightarrow a} f(x) = L\\)\n\nExample: \\(\\lim_{x \\rightarrow 3} x^2 = 9\\) The limit of this function as \\(x\\) approaches three, is 9. Limits will appear in the expression for calculating derivatives.\n\n3.3.1 Derivatives\nFor intuition on a derivative, watch this video from The Math Sorcerer.\nA derivative is the instantaneous rate at which the function is changing at x: the slope of a function at a particular point.\nThere are different notations for indicating something is a derivative. Below, we use \\(f'(x)\\) because we write our functions as \\(f(x) = x\\). Many times you might see a function equation like \\(y = 3x\\) There, it will be common for the derivative to be written like \\(\\frac{dy}{dx}\\).\nLet’s break down the definition of a derivative by looking at its similarity to the simple definition of a slope, as the rise over the run:\n\nSlope (on average): rise over run: change in \\(f(x)\\) over an interval (\\([c, b]\\) where \\(b-c =h\\)): \\(\\frac{f(b) - f(c)}{b-c}\\)\n\nFor slope at a specific point \\(x\\) (the derivative of f(x) at x), we just make the interval \\(h\\) very small:\n\n\\(f'(x)= \\lim_{h \\rightarrow 0}\\frac{f(a + h) - f(a)}{h}\\)\n\nExample \\(f(x) = 2x + 3\\).\n\n\\(f'(x) = \\lim_{h \\rightarrow 0}\\frac{2(x + h) + 3 - (2x + 3)}{h} = \\lim_{h \\rightarrow 0}\\frac{2x + 2h - 2x}{h} = \\lim_{h \\rightarrow 0}2 = 2\\)\n\nThis twitter thread by the brilliant teacher and statistician Allison Horst, provides a nice cartoon-based example of the derivative. (Note that sometimes the interval \\(h\\) is written as \\(\\Delta x\\), the change in \\(x\\)).\n\n\n3.3.2 Critical Points for Minima or Maxima\nIn both OLS and MLE, we reach points where take what are called “first order conditions.” This means we take the derivative with respect to a parameter of interest and then set the derivative = 0 and solve for the parameter to get an expression for our estimator. (E.g., In OLS, we take the derivative of the sum of squared residuals, set it equal to zero, and solve to get an expression for \\(\\hat \\beta\\)).\nThe reason we are interested in when the derivative is zero, is because this is when the instanaeous rate of change is zero, i.e., the slope at a particular point is zero. When does this happen? At a critical point- maximum or minimum. Think about it– at the top of a mountain, there is no more rise (and no decline). You are completing level on the mountaintop. The slope at that point is zero.\nLet’s take an example. The function \\(f(x) = x^2 + 1\\) has the derivative \\(f'(x) = 2x\\). This is zero when \\(x = 0\\).\nThe question remains: How do we know if it is a maximum or minimum?\nWe need to figure out if our function is concave or convex around this critical value. Convex is a “U” shape, meaning we are at a minimum, while concavity is an upside-down-U, which means we are at a maximum. We do so by taking the second derivative. This just means we take the derivative of the expression we already have for our first derivative. In our case, \\(f''(x) = 2\\). So what? Well the key thing we are looking for is if this result is positive or negative. Here, it is positive, which means our function is convex at this critical value, and therefore, we are at a minimum.\nJust look at the function in R if we plot it.\n\n## Let's define an arbitrary set of values for x\nx &lt;- -3:3\n## Now let's map the elements of x into y using our function\nfx &lt;- x^2 + 1\n\n## Let's plot the results\nplot(x = x, y=fx, \n     xlab = \"x\", type = \"l\",\n     main = \"f(x) = x^2 + 1\")\n\n\n\n\nNotice that when x=0, we are indeed at a minimum, just as the positive value of the second derivative would suggest.\nA different example: \\(f(x) = -2x^2 +1\\). \\(f'(x) = -4x\\) When we set this equal to 0 we find a critical value at \\(x = 0\\). \\(f''(x) = -4\\). Here, the value is negative, and we know it is concave. Sure enough, let’s plot it, and notice how we can draw a horiztonal line at the maximum, representing that zero slope at the critical point:\n\nx &lt;- -3:3\nfx &lt;- -2*x^2 + 1\nplot(x = x, y = fx,\n    ylab = \"f(x)\",\n     xlab = \"x\", type = \"l\",\n     main = \"f(x) = -2x^2 + 1\")\nabline(h=1, col = \"red\", lwd=2)\n\n\n\n\n\n\n3.3.3 Common Derivative Rules\nBelow is a cheat sheet of rules for quickly identifying the derivatives of functions.\nThe derivative of a constant is 0.\n\n\\(f(x) = a; f'(x) = 0\\)\n\nExample: The derivative of 5 is 0.\n\n\nHere is the power rule.\n\n\\(f(x) = ax^n; f'(x) = n\\times a \\times x^{n-1}\\)\n\nExample: The derivative of \\(x^3 = 3x^{(3-1)} = 3x^2\\)\n\n\nWe saw logs in the last section, and, yes, we see logs again here.\n\n\\(f(x) = e^{ax}; f'(x) = ae^{ax}\\)\n\\(f(x) = \\log(x); f'(x) = \\frac{1}{x}\\)\n\nA very convenient rule is that a derivative of a sum = sum of the derivatives.\n\n\\(f(x) = g(x) + h(x); f'(x) = g'(x) + h'(x)\\)\n\nProducts can be more of a headache. In this course, we will turn some product expressions into summation expressions to avoid the difficulties of taking derivatives with products.\n\nProduct Rule: \\(f(x) = g(x)h(x); f'(x) = g'(x)h(x) + h'(x)g(x)\\)\n\nThe chain rule below looks a bit tricky, but it can be very helpful for simplifying the way you take a derivative. See this video from NancyPi for a helpful explainer, as well as a follow-up for more complex applications here.\n\nChain Rule: \\(f(x) = g(h(x)); f'(x) = g'(h(x))h'(x)\\)\n\nExample: What is the derivative of \\(f(x) = \\log 5x\\)?\n\nFirst, we will apply the rule which tells us the derivative of a \\(\\log x\\) is \\(\\frac{1}{x}\\).\nHowever, here, we do not just have \\(x\\), we have \\(5x\\). We are in chain rule territory.\nAfter we apply the derivative to the log, which is \\(\\frac{1}{5x}\\), we then have to take the derivative of \\(5x\\) and multiply the two expressions together.\nThe derivative of \\(5x\\) is \\(5\\).\nSo, putting this together, our full derivative is f′(x) = 5 ∗ \\(\\frac{1}{5x}\\) = \\(\\frac{1}{x}\\)."
  },
  {
    "objectID": "03-TheMATH.html#vectors-and-matrices",
    "href": "03-TheMATH.html#vectors-and-matrices",
    "title": "3  The Math",
    "section": "3.4 Vectors and Matrices",
    "text": "3.4 Vectors and Matrices\nVectors\nFor our purposes, a vector is a list or “array” of numbers. For example, this might be a variable in our data– a list of the ages of all politicians in a country.\nAddition\n\nIf we have two vectors \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\), where\n\n\\(\\mathbf{u} \\ = \\ (u_1, u_2, \\dots u_n)\\) and\n\\(\\mathbf{v} \\ = \\ (v_1, v_2, \\dots v_n)\\),\n\\(\\mathbf{u} + \\mathbf{v} = (u_1 + v_1, u_2 + v_2, \\dots u_n + v_n)\\)\n\nNote: \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) must be of the same dimensionality - number of elements in each must be the same - for addition.\n\nScalar multiplication\n\nIf we have a scalar (i.e., a single number) \\(\\lambda\\) and a vector \\(\\mathbf{u}\\)\n\\(\\lambda \\mathbf{u} = (\\lambda u_1, \\lambda u_2, \\dots \\lambda u_n)\\)\n\nWe can implement vector addition and scalar multiplication in R.\nLet’s create a vector \\(\\mathbf{u}\\), a vector \\(\\mathbf{v}\\), and a number lambda.\n\nu &lt;- c(33, 44, 22, 11)\nv &lt;- c(6, 7, 8, 2)\nlambda &lt;- 3\n\nWhen you add two vectors in R, it adds each component together.\n\nu + v\n\n[1] 39 51 30 13\n\n\nWe can multiply each element of a vector, u by lambda:\n\nlambda * u\n\n[1]  99 132  66  33\n\n\nElement-wise Multiplication\nNote: When you multiply two vectors together in R, it will take each element of one vector and multiply it by each element of the other vector.\n\nu * v \\(= (u_1 * v_1, u_2 * v_2, \\dots u_n * v_n)\\)\n\n\nu * v\n\n[1] 198 308 176  22\n\n\n\n3.4.1 Matrix Basics\nA matrix represents arrays of numbers in a rectangle, with rows and columns.\n\nA matrix with \\(m\\) rows and \\(n\\) columns is defined as (\\(m\\) x \\(n\\)). What is the dimensionality of the matrix A below?\n\n\\(A = \\begin{pmatrix} a_{11} & a_{12} & a_{13}\\\\ a_{21} & a_{22} & a_{23} \\\\ a_{31} & a_{32} & a_{33} \\\\ a_{41} & a_{42} & a_{43} \\end{pmatrix}\\)\nIn R, we can think of a matrix as a set of vectors. For example, we could combine the vectors u and v we created above into a matrix defined as W.\n\n## cbind() binds together vectors as columns\nWcol &lt;- cbind(u, v)\nWcol\n\n      u v\n[1,] 33 6\n[2,] 44 7\n[3,] 22 8\n[4,] 11 2\n\n## rbind() binds together vectors as rows\nWrow &lt;- rbind(u, v)\nWrow\n\n  [,1] [,2] [,3] [,4]\nu   33   44   22   11\nv    6    7    8    2\n\n\nThere are other ways to create matrices in R, but using cbind and rbind() are common.\nWe can find the dimensions of our matrices using dim() or nrow() and ncol() together. For example:\n\ndim(Wcol)\n\n[1] 4 2\n\nnrow(Wcol)\n\n[1] 4\n\nncol(Wcol)\n\n[1] 2\n\n\nNote how the dimensions are different from the version created with rbind():\n\ndim(Wrow)\n\n[1] 2 4\n\nnrow(Wrow)\n\n[1] 2\n\nncol(Wrow)\n\n[1] 4\n\n\nExtracting specific components\nThe element \\(a_{ij}\\) signifies the element is in the \\(i\\)th row and \\(j\\)th column of matrix A. For example, \\(a_{12}\\) is in the first row and second column.\n\nSquare matrices have the same number of rows and columns\nVectors have just one row or one column (e.g., \\(x_1\\) element of \\(\\mathbf{x}\\) vector)\n\nIn R, we can use brackets to extract a specific \\(ij\\) element of a matrix or vector.\n\nWcol\n\n      u v\n[1,] 33 6\n[2,] 44 7\n[3,] 22 8\n[4,] 11 2\n\nWcol[2,1] # element in the second row, first column\n\n u \n44 \n\nWcol[2,] # all elements in the second row\n\n u  v \n44  7 \n\nWcol[, 1] # all elements in the first column\n\n[1] 33 44 22 11\n\n\nFor matrices, to extract a particular entry in R, you have a comma between entries because there are both rows and columns. For vectors, you only have one entry, so no comma is needed.\n\nu\n\n[1] 33 44 22 11\n\nu[2] # second element in the u vector\n\n[1] 44\n\n\n\n\n3.4.2 Matrix Operations\nMatrix Addition\n\nTo be able to add matrix A and matrix B, they must have the same dimensions.\nLike vector addition, to add matrices, you add each of the components together.\n\n\\(A = \\begin{pmatrix} a_{11} & a_{12} & a_{13}\\\\ a_{21} & a_{22} & a_{23} \\\\ a_{31} & a_{32} & a_{33} \\end{pmatrix}\\) and \\(B = \\begin{pmatrix} b_{11} & b_{12} & b_{13}\\\\ b_{21} & b_{22} & b_{23} \\\\ b_{31} & b_{32} & b_{33} \\end{pmatrix}\\)\n\\(A + B = \\begin{pmatrix} a_{11} + b_{11} & a_{12} + b_{12} & a_{13} + b_{13}\\\\ a_{21} + b_{21} & a_{22} + b_{22} & a_{23} + b_{23} \\\\ a_{31} + b_{31} & a_{32} + b_{32} & a_{33} + b_{33} \\end{pmatrix}\\)\n\\(Q = \\begin{pmatrix} 2 & 4 & 1\\\\ 6 & 1 & 5 \\end{pmatrix}\\) \\(+\\) \\(R = \\begin{pmatrix} 9 & 4 & 2\\\\ 11 & 8 & 7 \\end{pmatrix} = Q + R = \\begin{pmatrix} 11 & 8 & 3\\\\ 17 & 9 & 12 \\end{pmatrix}\\)\nScalar Multiplication\nTake a scalar \\(\\nu\\). Just like vectors, we multiply each component of a matrix by the scalar.\n\\(\\nu Q = \\begin{pmatrix} \\nu q_{11} & \\nu q_{12} & \\dots & \\nu q_{1n}\\\\ \\nu q_{21} & \\nu q_{22} & \\dots & \\nu q_{2n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\nu q_{m1} & \\nu q_{m2} & \\dots & \\nu q_{mn} \\end{pmatrix}\\)\nExample: Take \\(c = 2\\) and a matrix A.\n\\(cA = c *\\begin{pmatrix} 4 & 6 & 1\\\\ 3 & 2 & 8 \\end{pmatrix}\\) = \\(\\begin{pmatrix} 8 & 12 & 2\\\\ 6 & 4 & 16 \\end{pmatrix}\\)\nNote the Commutativity/Associativity: For scalar \\(c\\): \\(c(AB) = (cA)B = A(cB) = (AB)c\\).\nMatrix Multiplication\nA matrix A and B must be conformable to multiply AB.\n\nTo be comformable, for \\(m_A\\) x \\(n_A\\) matrix A and \\(m_B\\) x \\(n_B\\) matrix B, the “inside” dimensions must be equal: \\(n_A = m_B\\).\nThe resulting AB has the “outside” dimensions: \\(m_A\\) x \\(n_B\\).\n\nFor each \\(c_{ij}\\) component of \\(C = AB\\), we take the inner product of the \\(i^{th}\\) row of matrix A and the \\(j^{th}\\) column of matrix B.\n\nTheir product C = AB is the \\(m\\) x \\(n\\) matrix where:\n\\(c_{ij} =a_{i1}b_{1j} + a_{i2}b_{2j} + \\dots + a_{ik}b_{kj}\\)\n\nExample: This \\(2 \\times 3\\) matrix is multiplied by a \\(3 \\times 2\\) matrix, resulting in the \\(2 \\times 2\\) matrix.\n\\(\\begin{pmatrix} 4 & 6 & 1\\\\ 3 & 2 & 8 \\end{pmatrix}\\) \\(\\times\\) \\(\\begin{pmatrix} 8 & 12 \\\\ 6 & 4 \\\\ 7 & 10 \\end{pmatrix}\\) = \\(\\begin{pmatrix} (4*8 + 6*6 + 1*7) & (4*12 + 6*4 + 1*10) \\\\ (3*8 + 2*6 + 8*7) & (3*12 + 2*4 + 8*10) \\end{pmatrix}\\)\nFor example, the entry in the first row and second column of the new matrix \\(c_{12} = (a_{11} = 4* b_{11} = 12) + (a_{12} = 6*b_{21} = 4) + (a_{13} = 1*b_{31} = 10)\\)\nWe can also do matrix multiplication in R.\n\n## Create a 3 x 2 matrix A\nA &lt;- cbind(c(3, 4, 6), c(5, 6, 8))\nA\n\n     [,1] [,2]\n[1,]    3    5\n[2,]    4    6\n[3,]    6    8\n\n## Create a 2 x 4 matrix B\nB &lt;- cbind(c(6,8), c(7, 9), c(3, 6), c(1, 11))\nB\n\n     [,1] [,2] [,3] [,4]\n[1,]    6    7    3    1\n[2,]    8    9    6   11\n\n\nNote that the multiplication AB is conformable because the number of columns in A matches the number of rows in B:\n\nncol(A)\nnrow(B)\n\n[1] 2\n[1] 2\n\n\nTo multiply matrices together in R, we need to add symbols around the standard asterisk for multiplication:\n\nA %*% B\n\n     [,1] [,2] [,3] [,4]\n[1,]   58   66   39   58\n[2,]   72   82   48   70\n[3,]  100  114   66   94\n\n\nThat is necessary for multiplying matrices together. It is not necessary for scalar multiplication, where we take a single number (e.g., c = 3) and multiply it with a matrix:\n\nc &lt;- 3\nc*A\n\n     [,1] [,2]\n[1,]    9   15\n[2,]   12   18\n[3,]   18   24\n\n\nNote the equivalence of the below expressions, which combine scalar and matrix multiplication:\n\nc* (A %*% B)\n\n     [,1] [,2] [,3] [,4]\n[1,]  174  198  117  174\n[2,]  216  246  144  210\n[3,]  300  342  198  282\n\n(c* A) %*% B\n\n     [,1] [,2] [,3] [,4]\n[1,]  174  198  117  174\n[2,]  216  246  144  210\n[3,]  300  342  198  282\n\nA %*% (c * B)\n\n     [,1] [,2] [,3] [,4]\n[1,]  174  198  117  174\n[2,]  216  246  144  210\n[3,]  300  342  198  282\n\n\nIn social science, one matrix of interest is often a rectangular dataset that includes column vectors representing independent variables, as well as another vector that includes your dependent variable. These might have 1000 or more rows and a handful of columns you care about."
  },
  {
    "objectID": "03-TheMATH.html#additional-matrix-tidbits-that-will-come-up",
    "href": "03-TheMATH.html#additional-matrix-tidbits-that-will-come-up",
    "title": "3  The Math",
    "section": "3.5 Additional Matrix Tidbits that Will Come Up",
    "text": "3.5 Additional Matrix Tidbits that Will Come Up\nInverse\nAn \\(n\\) x \\(n\\) matrix A is invertible if there exists an \\(n\\) x \\(n\\) inverse matrix \\(A^{-1}\\) such that:\n\n\\(AA^{-1} = A^{-1}A = I_n\\)\nwhere \\(I_n\\) is the identity matrix (\\(n\\) x \\(n\\)), that takes diagonal elements of 1 and off-diagonal elements of 0. Example:\n\n\\(I_n = \\begin{pmatrix} 1_{11} & 0 & \\dots & 0\\\\ 0& 1_{22} & \\dots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & \\dots & 1_{nn} \\end{pmatrix}\\)\n\nMultiplying a matrix by the identity matrix returns in the matrix itself: \\(AI_n = A\\)\n\nIt’s like the matrix version of multiplying a number by one.\n\n\nNote: A matrix must be square \\(n\\) x \\(n\\) to be invertible. (But not all square matrices are invertible.) A matrix is invertible if and only if its columns are linearly independent. This is important for understanding why you cannot have two perfectly colinear variables in a regression model.\nWe will not do much solving for inverses in this course. However, the inverse will be useful in solving for and simplifying expressions.\n\n3.5.1 Transpose\nWhen we transpose a matrix, we flip the \\(i\\) and \\(j\\) components.\n\nExample: Take a 4 X 3 matrix A and find the 3 X 4 matrix \\(A^{T}\\).\nA transpose is usually denoted with as \\(A^{T}\\) or \\(A'\\)\n\n\\(A = \\begin{pmatrix} a_{11} & a_{12} & a_{13}\\\\ a_{21} & a_{22} & a_{23} \\\\ a_{31} & a_{32} & a_{33} \\\\ a_{41} & a_{42} & a_{43} \\end{pmatrix}\\) then \\(A^T = \\begin{pmatrix} a'_{11} & a'_{12} & a'_{13} & a'_{14}\\\\ a'_{21} & a'_{22} & a'_{23} & a'_{24} \\\\ a'_{31} & a'_{32} & a'_{33} & a'_{34} \\end{pmatrix}\\)\nIf \\(A = \\begin{pmatrix} 1 & 4 & 2 \\\\ 3 & 1 & 11 \\\\ 5 & 9 & 4 \\\\ 2 & 11& 4 \\end{pmatrix}\\) then \\(A^T = \\begin{pmatrix} 1 & 3 & 5 & 2\\\\ 4 & 1 & 9 & 11 \\\\ 2& 11 & 4 & 4 \\end{pmatrix}\\)\nCheck for yourself: What was in the first row (\\(i=1\\)), second column (\\(j=2\\)) is now in the second row (\\(i=2\\)), first column (\\(j=1\\)). That is \\(a_{12} =4 = a'_{21}\\).\nWe can transpose matrices in R using t(). For example, take our matrix A:\n\nA\n\n     [,1] [,2]\n[1,]    3    5\n[2,]    4    6\n[3,]    6    8\n\nt(A)\n\n     [,1] [,2] [,3]\n[1,]    3    4    6\n[2,]    5    6    8\n\n\nIn R, you can find the inverse of a square matrix with solve()\n\nsolve(A)\n\nError in solve.default(A): 'a' (3 x 2) must be square\n\n\nNote, while A is not square A’A is square:\n\nAtA &lt;- t(A) %*% A\n\nsolve(AtA)\n\n          [,1]      [,2]\n[1,]  2.232143 -1.553571\n[2,] -1.553571  1.089286\n\n\n\n\n3.5.2 Additional Matrix Properties and Rules\nThese are a few additional properties and rules that will be useful to us at various points in the course:\n\nSymmetric: Matrix A is symmetric if \\(A = A^T\\)\nIdempotent: Matrix A is idempotent if \\(A^2 = A\\)\nTrace: The trace of a matrix is the sum of its diagonal components \\(Tr(A) = a_{11} + a_{22} + \\dots + a_{mn}\\)\n\nExample of symmetric matrix:\n\\(D = \\begin{pmatrix} 1 & 6 & 22 \\\\ 6 & 4 & 7 \\\\ 22 & 7 & 11 \\end{pmatrix}\\)\n\n## Look at the equivalence\nD &lt;- rbind(c(1,6,22), c(6,4,7), c(22,7,11))\nD\n\n     [,1] [,2] [,3]\n[1,]    1    6   22\n[2,]    6    4    7\n[3,]   22    7   11\n\nt(D)\n\n     [,1] [,2] [,3]\n[1,]    1    6   22\n[2,]    6    4    7\n[3,]   22    7   11\n\n\nWhat is the trace of this matrix?\n\n## diag() pulls out the diagonal of a matrix\nsum(diag(D))\n\n[1] 16\n\n\n\n\n3.5.3 Matrix Rules\nDue to conformability and other considerations, matrix operations are somewhat more restrictive, particularly when it comes to commutativity.\n\nAssociative \\((A + B) + C = A + (B + C)\\) and \\((AB) C = A(BC)\\)\nCommutative \\(A + B = B + A\\)\nDistributive \\(A(B + C) = AB + AC\\) and \\((A + B) C = AC + BC\\)\nCommutative law for multiplication does not hold– the order of multiplication matters: $ AB BA$\n\nRules for Inverses and Transposes\nThese rules will be helpful for simplifying expressions. Treat \\(A\\), \\(B\\), and \\(C\\) as matrices below, and \\(s\\) as a scalar.\n\n\\((A + B)^T = A^T + B^T\\)\n\\((s A)^T\\) \\(= s A^T\\)\n\\((AB)^T = B^T A^T\\)\n\\((A^T)^T = A\\) and \\(( A^{-1})^{-1} = A\\)\n\\((A^T)^{-1} = (A^{-1})^T\\)\n\\((AB)^{-1} = B^{-1} A^{-1}\\)\n\\((ABCD)^{-1} = D^{-1} C^{-1} B^{-1} A^{-1}\\)\n\n\n\n3.5.4 Derivatives with Matrices and Vectors\nLet’s say we have a \\(p \\times 1\\) “column” vector \\(\\mathbf{x}\\) and another \\(p \\times 1\\) vector \\(\\mathbf{a}\\).\nTaking the derivative with respect to vector \\(\\mathbf{x}\\).\nLet’s say we have \\(y = \\mathbf{x}'\\mathbf{a}\\). This process is explained here. Taking the derivative of this is called the gradient.\n\n\\(\\frac{\\delta y}{\\delta x} = \\begin{pmatrix}\\frac{\\delta y}{\\delta x_1} \\\\ \\frac{\\delta y}{\\delta x_2} \\\\ \\vdots \\\\ \\frac{dy}{dx_p} \\end{pmatrix}\\)\n\\(y\\) will have dimensions \\(1 \\times 1\\). \\(y\\) is a scalar.\n\nNote: \\(y = a_1x_1 + a_2x_2 + ... + a_px_p\\). From this expression, we can take a set of “partial derivatives”:\n\\(\\frac{\\delta y}{\\delta x_1} = a_1\\)\n\\(\\frac{\\delta y}{\\delta x_2} = a_2\\), and so on\n\\(\\frac{\\delta y}{\\delta x} = \\begin{pmatrix}\\frac{\\delta y}{\\delta x_1} \\\\ \\frac{\\delta y}{\\delta x_2} \\\\ \\vdots \\\\ \\frac{\\delta y}{\\delta x_p} \\end{pmatrix} = \\begin{pmatrix} a_1 \\\\ a_2 \\\\ \\vdots \\\\ a_p \\end{pmatrix}\\)\n\nWell, this is just vector \\(\\mathbf{a}\\)\n\nAnswer: \\(\\frac{\\delta }{\\delta x} \\mathbf{x}^T\\mathbf{a} = \\mathbf{a}\\). We can apply this general rule in other situations.\nExample 2\nLet’s say we want to differentiate the following where vector \\(\\mathbf{y}\\) is \\(n \\times 1\\), \\(X\\) is \\(n \\times k\\), and \\(\\mathbf{b}\\) is \\(k \\times 1\\). Take the derivative with respect to \\(b\\).\n\n\\(\\mathbf{y}'\\mathbf{y} - 2\\mathbf{b}'X'\\mathbf{y}\\)\nNote that the dimensions of the output are \\(1 \\times 1\\), a scalar quantity.\n\nRemember the derivative of a sum is the sum of derivatives. This allows us to focus on particular terms.\n\nThe first term has no \\(\\mathbf{b}\\) in it, so this will contribute 0.\nThe second term is \\(2\\mathbf{b}'X'\\mathbf{y}\\). We can think about this like the previous example\n\n\\(\\frac{\\delta }{\\delta b} 2\\mathbf{b}'X'\\mathbf{y} = \\begin{pmatrix} \\frac{\\delta }{\\delta b_1}\\\\ \\frac{\\delta }{\\delta b_2} \\\\ \\vdots \\\\ \\frac{\\delta }{\\delta b_k} \\end{pmatrix}\\)\nThe output is needs to be \\(k \\times 1\\) like \\(\\mathbf{b}\\), which is what \\(2 * X'\\mathbf{y}\\) is.\n\nThe derivative is \\(-2X'\\mathbf{y}\\)\n\nExample 3\nAnother useful rule when a matrix \\(A\\) is symmetric: \\(\\frac{\\delta}{\\delta \\mathbf{x}} \\mathbf{x}^TA\\mathbf{x} = (A + A^T)\\mathbf{x} = 2A\\mathbf{x}\\).\nDetails on getting to this result. We are treating the vector \\(\\mathbf{x}\\) as \\(n \\times 1\\) and the matrix \\(A\\) as symmetric.\nWhen we take \\(\\frac{\\delta}{\\delta \\mathbf{x}}\\) (the derivative with respect to \\(\\mathbf{x}\\)), we will be looking for a result with the same dimensions \\(\\mathbf{x}\\).\n\\(\\frac{\\delta }{\\delta \\mathbf{x}} = \\begin{pmatrix}\\frac{\\delta }{\\delta x_1} \\\\ \\frac{\\delta }{\\delta x_2} \\\\ \\vdots \\\\ \\frac{d}{dx_n} \\end{pmatrix}\\)\nLet’s inspect the dimensions of \\(\\mathbf{x}^TA\\mathbf{x}\\). They are \\(1 \\times 1\\). If we perform this matrix multiplication, we would be multiplying:\n\\(\\begin{pmatrix} x_1 & x_2 & \\ldots & x_i \\end{pmatrix} \\times \\begin{pmatrix} a_{11} & a_{12} & \\ldots & a_{1j} \\\\ a_{21} & a_{22} & \\ldots & a_{2j} \\\\ \\vdots & \\vdots & \\vdots & \\vdots \\\\ a_{i1} & a_{i2} & \\ldots & a_{ij} \\end{pmatrix} \\times \\begin{pmatrix}x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_j \\end{pmatrix}\\)\nTo simplify things, let’s say we have the following matrices, where \\(A\\) is symmetric:\n\\(\\begin{pmatrix} x_1 & x_2 \\end{pmatrix} \\times \\begin{pmatrix} a_{11} & a_{12} \\\\ a_{21} & a_{22} \\end{pmatrix} \\times \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix}\\)\nWe can perform the matrix multiplication for the first two quantities, which will result in a \\(1 \\times 2\\) vector. Recall in matrix multiplication we take the sum of the element-wise multiplication of the \\(ith\\) row of the first object by the \\(jth\\) column of the second object. This means multiply the first row of \\(\\mathbf{x}\\) by the first column of \\(A\\) for the entry in cell \\(i=1; j=1\\), and so on.\n\\(\\begin{pmatrix} (x_1a_{11} + x_2a_{21}) & (x_1a_{12} + x_2a_{22}) \\end{pmatrix}\\)\nWe can then multiply this quantity by the last quantity\n\\(\\begin{pmatrix} (x_1a_{11} + x_2a_{21}) & (x_1a_{12} + x_2a_{22}) \\end{pmatrix} \\times \\begin{pmatrix}x_1 \\\\ x_2 \\end{pmatrix}\\)\nThis will results in the \\(1 \\times 1\\) quantity: \\((x_1a_{11} + x_2a_{21})x_1 + (x_1a_{12} + x_2a_{22})x_2 = x_1^2a_{11} + x_2a_{21}x_1 + x_1a_{12}x_2 + x_2^2a_{22}\\)\nWe can now take the derivatives with respect to \\(\\mathbf{x}\\). Because \\(\\mathbf{x}\\) is \\(2 \\times 1\\), our derivative will be a vector of the same dimensions with components:\n\\(\\frac{\\delta }{\\delta \\mathbf{x}} = \\begin{pmatrix}\\frac{\\delta }{\\delta x_1} \\\\ \\frac{\\delta }{\\delta x_2} \\end{pmatrix}\\)\nThese represent the partial derivatives of each component within \\(\\mathbf{x}\\)\nLet’s focus on the first: \\(\\frac{\\delta }{\\delta x_1}\\).\n\\[\\begin{align*}\n\\frac{\\delta }{\\delta x_1} x_1^2a_{11} + x_2a_{21}x_1 +   x_1a_{12}x_2 + x_2^2a_{22} &=\\\\\n&= 2x_1a_{11} + x_2a_{21} + a_{12}x_2\\\\\n\\end{align*}\\]\nWe can repeat this for \\(\\frac{\\delta }{\\delta x_2}\\)\n\\[\\begin{align*}\n\\frac{\\delta }{\\delta x_2} x_1^2a_{11} + x_2a_{21}x_1 +   x_1a_{12}x_2 + x_2^2a_{22} &=\\\\\n&= a_{21}x_1  + x_1a_{12} + 2x_2a_{22}\\\\\n\\end{align*}\\]\nNow we can put the result back into our vector format:\n\\(\\begin{pmatrix}\\frac{\\delta }{\\delta x_1} = 2x_1a_{11} + x_2a_{21} + a_{12}x_2\\\\ \\frac{\\delta }{\\delta x_2} = a_{21}x_1 + x_1a_{12} + 2x_2a_{22}\\end{pmatrix}\\)\nNow it’s just about simplifying to show that we have indeed come back to the rule.\nRecall that for a symmetric matrix, the elements in rows and columns \\(ij\\) = the elements in \\(ji\\). This allows us to read \\(a_{21} = a_{12}\\) and combine those terms (e.g., \\(x_2a_{21} + a_{12}x_2 =2a_{12}x_2\\)) :\n\\(\\begin{pmatrix}\\frac{\\delta }{\\delta x_1} = 2x_1a_{11} + 2a_{12}x_2\\\\ \\frac{\\delta }{\\delta x_2} = 2a_{21}x_1 + 2x_2a_{22}\\end{pmatrix}\\)\nSecond, we can now bring the 2 out front.\n\\(\\begin{pmatrix}\\frac{\\delta }{\\delta x_1} \\\\ \\frac{\\delta }{\\delta x_2} \\end{pmatrix} = 2 * \\begin{pmatrix} x_1a_{11} + a_{12}x_2\\\\ a_{21}x_1 + x_2a_{22}\\end{pmatrix}\\)\nFinally, let’s inspect this and show it is equivalent to this multiplication where we have a \\(2 \\times 2\\) \\(A\\) matrix multiplied by a \\(2 \\times 1\\) \\(\\mathbf x\\) vector. Minor note: Because any individual element of a vector is just a single quantity, we can change the order (e.g., \\(a_{11}*x_1\\) vs. \\(x_1*a_{11}\\)). We just can’t do that for full vectors or matrices\n\\(2A\\mathbf{x} = 2* \\begin{pmatrix} a_{11} & a_{12} \\\\ a_{21} & a_{22} \\end{pmatrix} \\times \\begin{pmatrix}x_1 \\\\ x_2 \\end{pmatrix} = 2 * \\begin{pmatrix} a_{11}x_1 + a_{12}x_2 \\\\ a_{21}x_1 + a_{22}x_2 \\end{pmatrix}\\)\nThe last quantity is the same as the previous step. That’s the rule!\nApplying the rules\n\nThis has a nice analogue to the derivative we’ve seen before \\(q^2 = 2*q\\).\nLet’s say we want to take the derivative of \\(\\mathbf{b}'X'X\\mathbf{b}\\) with respect to \\(\\mathbf{b}\\).\nWe can think of \\(X'X\\) as if it is \\(A\\).\n\nThis gives us \\(2X'X\\mathbf{b}\\) as the result.\n\n\nWhy on earth would we care about this? For one, it helps us understand how we get to our estimates for \\(\\hat \\beta\\) in linear regression. When we have multiple variables, we don’t just want the best estimate for one coefficient, but a vector of coefficients. See more here.\nIn MLE, we will find the gradient of the log likelihood function. We will further go into the second derivatives to arrive at what is called the Hessian. More on that later."
  },
  {
    "objectID": "03-TheMATH.html#practice-problems",
    "href": "03-TheMATH.html#practice-problems",
    "title": "3  The Math",
    "section": "3.6 Practice Problems",
    "text": "3.6 Practice Problems\n\nWhat is \\(24/3 + 5^2 - (8 -4)\\)?\nWhat is \\(\\sum_{i = 1}^5 (i*3)\\)?\nTake the derivative of \\(f(x) =v(4x^2 + 6)^2\\) with respect to \\(x\\).\nTake the derivative of \\(f(x) = e^{2x + 3}\\) with respect to \\(x\\).\nTake the derivative of \\(f(x) = log (x + 3)^2\\) with respect to \\(x\\).\n\nGiven \\(X\\) is an \\(n\\) x \\(k\\) matrix,\n\n\\((X^{T}X)^{-1}X^{T}X\\) can be simplified to?\n\\(((X^{T}X)^{-1}X^{T})^{T} =\\) ?\nIf \\(\\nu\\) is a constant, how does \\((X^{T}X)^{-1}X^{T} \\nu X(X^{T}X)^{-1}\\) simplify?\nIf a matrix \\(P\\) is idempotent, \\(PP =\\) ?\n\n\n3.6.1 Practice Problem Solutions\nBelow are the solutions for the above problems. Remember, try on your own first, then\n\n\nClick for the solution\n\n\nWhat is \\(24/3 + 5^2 - (8 -4)\\)?\n\n\n24/3 + 5^2 - (8 -4)\n\n[1] 29\n\n\n\nWhat is \\(\\sum_{i = 1}^5 (i*3)\\)?\n\nBy hand: \\(1 \\times 3 + 2 \\times 3 + 3 \\times 3 + 4 \\times 3 + 5 \\times 3\\)\n\n\n\n## sol 1\n1*3 + 2*3 + 3*3 + 4*3 + 5*3\n\n[1] 45\n\n## sol 2\ni &lt;- 1:5\nsum(i*3)\n\n[1] 45\n\n\n\nTake the derivative of \\(f(x) =v(4x^2 + 6)^2\\) with respect to \\(x\\).\n\nWe can treat \\(v\\) as a number.\n\n\n\\[\\begin{align*}\nf'(x) &= 2* v(4x^2 + 6) * 8x\\\\\n&= 16vx(4x^2 + 6)\n\\end{align*}\\]\n\nTake the derivative of \\(f(x) = e^{2x + 3}\\) with respect to \\(x\\).\n\n\\[\\begin{align*}\nf'(x) &= 2* e^{2x + 3}\\\\\n&= 2e^{2x + 3}\n\\end{align*}\\]\n\nTake the derivative of \\(f(x) = log (x + 3)^2\\) with respect to \\(x\\).\n\nNote we can re-write this as \\(2 * log (x + 3)\\).\n\n\n\\[\\begin{align*}\nf'(x) &= 2 * \\frac{1}{(x + 3)} * 1\\\\\n&= \\frac{2}{(x + 3)}\n\\end{align*}\\] If we didn’t take that simplifying step, we can still solve:\n\\[\\begin{align*}\n      f'(x) &= \\frac{1}{(x + 3)^2} * 2 * (x + 3) *1\\\\\n      &= \\frac{2}{(x + 3)}\n      \\end{align*}\\]\nGiven \\(X\\) is an \\(n\\) x \\(k\\) matrix,\n\n\\((X^{T}X)^{-1}X^{T}X\\) can be simplified to?\n\n\\(I_k\\) the identity matrix\n\n\\(((X^{T}X)^{-1}X^{T})^{T} =\\) ?\n\nRecall our rule \\((AB)^T = B^TA^T\\)\n\\(X(X^TX)^{-1}\\)\n\nIf \\(\\nu\\) is a constant, how does \\((X^{T}X)^{-1}X^{T} \\nu X(X^{T}X)^{-1}\\) simplify?\n\nWe can pull it out front.\n\n\n\\[\\begin{align*}\n    &= \\nu(X^{T}X)^{-1}X^{T}X(X^{T}X)^{-1} \\\\\n    &= \\nu (X^{T}X)^{-1}\n    \\end{align*}\\]\n\nIf a matrix \\(P\\) is idempotent, \\(PP =\\) ?\n\n\\(P\\) from section 3.5.2"
  },
  {
    "objectID": "04-ReviewofOLS.html#introducing-ols-regression",
    "href": "04-ReviewofOLS.html#introducing-ols-regression",
    "title": "4  Review of OLS",
    "section": "4.1 Introducing OLS Regression",
    "text": "4.1 Introducing OLS Regression\nThe regression method describes how one variable depends on one or more other variables. Ordinary Least Squares regression is a linear model with the matrix representation:\n\\(Y = \\alpha + X\\beta + \\epsilon\\)\nGiven values of variables in \\(X\\), the model predicts the average of an outcome variable \\(Y\\). For example, if \\(Y\\) is a measure of how wealthy a country is, \\(X\\) may contain measures related to the country’s natural resources and/or features of its institutions (things that we think might contribute to how wealthy a country is.) In this equation:\n\n\\(Y\\) is the outcome variable (\\(n \\times 1\\)).1\n\\(\\alpha\\) is a parameter representing the intercept\n\\(\\beta\\) is a parameter representing the slope/marginal effect (\\(k \\times 1\\)), and\n\\(\\epsilon\\) is the error term (\\(n \\times 1\\)).\n\nIn OLS, we estimate a line of best fit to predict \\(\\hat{Y}\\) values for different values of X:\n\n\\(\\hat{Y} = \\hat{\\alpha} + X\\hat{\\beta}\\).\nWhen you see a “\\(\\hat{hat}\\)” on top of a letter, that means it is an estimate of a parameter.\nAs we will see in the next section, in multiple regression, sometimes this equation is represented as just \\(\\hat{Y} = X\\hat{\\beta}\\), where this generally means that \\(X\\) is a matrix that includes several variables and \\(\\hat \\beta\\) is a vector that includes several coefficients, including a coefficient representing the intercept \\(\\hat \\alpha\\)\n\nWe interpret linear regression coefficients as describing how a dependent variable is expected to change when a particular independent variable changes by a certain amount. Specifically:\n\n“Associated with each one unit increase in a variable \\(x_1\\), there is a \\(\\hat{\\beta_1}\\) estimated expected average increase in \\(y\\).”\nIf we have more than one explanatory variable (i.e., a multiple regression), we add the phrase “controlling on/ holding constant other observed factors included in the model.”\n\nWe can think of the interpretation of a coefficient in multiple regression using an analogy to a set of light switches:\n\nWe ask: How much does the light in the room change when we flip one switch, while holding constant the position of all the other switches?\nThis would be a good place to review the Wheelan chapter and Gelman and Hill 3.1 and 3.2 to reinforce what a regression is and how to interpret regression results."
  },
  {
    "objectID": "04-ReviewofOLS.html#diving-deeper-into-ols-matrix-representation",
    "href": "04-ReviewofOLS.html#diving-deeper-into-ols-matrix-representation",
    "title": "4  Review of OLS",
    "section": "4.2 Diving Deeper into OLS Matrix Representation",
    "text": "4.2 Diving Deeper into OLS Matrix Representation\nIn this section, we will review the matrix representation of the OLS regression in more detail and discuss how to derive the estimators for the regression coefficients.2\nOLS in Matrix Form: Let \\(X\\) be an \\(n \\times k\\) matrix where we have observations on k independent variables for n observations. Since our model will usually contain a constant term, one of the columns in the X matrix will contain only ones. This column should be treated exactly the same as any other column in the X matrix.\n\nLet \\(Y\\) be an \\(n \\times 1\\) vector of observations on the dependent variable. Note: because \\(Y\\) is a vector (a matrix with just one column), sometimes it is written in lowercase notation as \\(\\mathbf y\\).\nLet \\(\\epsilon\\) be an \\(n \\times 1\\) vector of disturbances or errors.\nLet \\(\\beta\\) be an \\(k \\times 1\\) vector of unknown population parameters that we want to estimate.\n\n\\(\\begin{pmatrix} y_1 \\\\ y_2 \\\\ y_3 \\\\ y_4 \\\\ ... \\\\ y_n \\end{pmatrix}\\) = \\(\\begin{pmatrix} 1 & x_{11} & x_{12} & x_{13} & ... & x_{1k}\\\\ 1 & x_{21} & x_{22} & x_{23} & ... & x_{2k} \\\\ 1 & x_{31} & x_{32} & x_{33} & ... & x_{3k}\\\\ 1 & x_{41} & x_{42} & x_{43} & ... & x_{4k} \\\\ ... & ... & ... & ... & ... & ... \\\\ 1 & x_{n1} & x_{n2} & x_{n3} & ... & x_{nk}\\end{pmatrix}\\) X \\(\\begin{pmatrix} \\alpha \\\\ \\beta_1 \\\\ \\beta_2 \\\\ \\beta_3 \\\\ ... \\\\ \\beta_k \\end{pmatrix}\\) + \\(\\begin{pmatrix} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\epsilon_3 \\\\ \\epsilon_4 \\\\ ... \\\\ \\epsilon_n \\end{pmatrix}\\)\nOur estimates are then \\(\\mathbf{ \\hat y} = X\\hat \\beta\\). What are the dimensions of this quantity?\nGelman and Hill Section 3.4, pg. 38 provides a nice visual of how this representation maps onto what a typical dataset may look like, where we will try to estimate a set of coefficients that map the relationship between the columns of \\(X\\) and \\(\\mathbf y\\):\n\\\nThis is a good place to review Gelman and Hill 3.4 on different notations for representing the regression model.\n\n4.2.1 Estimating the Coefficients\nModels generally start with some goal. In OLS, our goal is to minimize the sum of squared “residuals.” Here is a video I created to explain why we can represent this as \\(\\mathbf{e'}\\mathbf{e}\\).\n\nNote: at the end of the video it should read \\(X\\hat\\beta\\), not \\(\\hat X \\beta\\)\nWhat is a residual? It’s the difference between y and our estimate of y: \\(y - \\hat y\\). It represents the error in our prediction– how far off our estimate is of the outcome.\nWe can write this in matrix notation in the following way where \\(\\mathbf e\\) is an \\(n \\times 1\\) vector of residuals– a residual for each observation in the data:\n\\[\\begin{align*}\n\\mathbf{e'}\\mathbf{e} &= (Y' - \\hat{\\beta}'X')(Y - X\\hat{\\beta})\\\\\n&=Y'Y - \\hat{\\beta}'X'Y - Y'X\\hat{\\beta} + \\hat{\\beta}'X'X\\hat{\\beta} \\\\\n&= Y'Y - 2\\hat{\\beta}'X'Y + \\hat{\\beta}'X'X\\hat{\\beta}\n\\end{align*}\\]\nRecall we want a line that minimizes this quantity. We minimize the sum of squared residuals by taking the derivative with respect to \\(\\beta\\). (We want to identify the coefficients that help us achieve the goal of minimizing the squared error.) Because we are now deriving an estimate, we will use the hat over \\(\\beta\\):\n\n\\(\\frac{\\delta }{\\delta \\hat \\beta} = -2X'Y + 2X'X\\hat{\\beta}\\)\nSo what is our estimate for \\(\\hat{\\beta}\\)? We take first order conditions\n\n\\[\\begin{align*}\n  0 &=-2X'Y + 2X'X\\hat{\\beta}\\\\\n   \\hat{\\beta} &= (X'X)^{-1}X'Y\n   \\end{align*}\\]\nYou may wonder how we got to these answers. Don’t worry, you will get your chance to solve this! The important thing to note for now, is that we have an analytic solution to our coefficient estimates."
  },
  {
    "objectID": "04-ReviewofOLS.html#ols-regression-in-r",
    "href": "04-ReviewofOLS.html#ols-regression-in-r",
    "title": "4  Review of OLS",
    "section": "4.3 OLS Regression in R",
    "text": "4.3 OLS Regression in R\nTo run a linear regression in R, we use the lm() function.\nThe syntax is lm(y ~ x1, data = mydata) for a regression with y as the name of your dependent variable and there is one explanatory variable x1 where mydata is the name of your data frame.\nlm(y ~ x1 + x2 , data = mydata) is the syntax for a regression with two explanatory variables x1 and x2, where you would add additional variables for larger multivariate regressions. By default, R will include an intercept term in the regression.\n\n4.3.1 Example: Predicting Current Election Votes from Past Election Votes\nIn the American presidential election in 2000, there was an actual controversy in how ballots were cast in the state of Florida. Social scientists used data comparing the election results from 1996 in the state with 2000 as one way to help detect irregularities in the 2000 vote count. For more information on the background of this example, you can watch this video.\nWe will use the data florida.csv available here:\n\n## Load Data\nflorida &lt;- read.csv(\"https://raw.githubusercontent.com/ktmccabe/teachingdata/main/florida.csv\")\n\nThis data set includes several variables described below, where each row represents the voting information for a particular county in Florida.\n\n\n\nName\nDescription\n\n\n\n\ncounty\ncounty name\n\n\nClinton96\nClinton’s votes in 1996\n\n\nDole96\nDole’s votes in 1996\n\n\nPerot96\nPerot’s votes in 1996\n\n\nBush00\nBush’s votes in 2000\n\n\nGore00\nGore’s votes in 2000\n\n\nBuchanan00\nBuchanan’s votes in 2000\n\n\n\nIn 2000, Buchanan was a third party candidate, similar to Perot in 1996. One might think that counties where Perot received a lot of votes in 1996 should also receive a lot in 2000. That is: with a one-vote increase in Perot’s vote, we might expect an average increase in Buchanan’s 2000 vote.\nWe can translate that language into a regression equation:\n\n\\(Buchanan2000 = \\alpha + Perot1996 * \\beta + \\epsilon\\)\n\nIn R, we run this regression the following way. We will save it as an object fit.1. You can name your regression objects anything you want.\n\nfit.1 &lt;- lm(Buchanan00 ~ Perot96, data = florida)\n\n\nsummary(model) provides the summary statistics of the model. In particular, the following statistics are important\n\nEstimate: point estimate of each coefficient\nStd. Error: standard error of each estimate\nt value: indicates the \\(t\\)-statistic of each coefficient under the null hypothesis that it equals zero\nPr(&gt;|t|): indicates the two-sided \\(p\\)-value corresponding to this \\(t\\)-statistic where asterisks indicate the level of statistical significance.\nMultiple R-squared: The coefficient of determination\nAdjusted R-squared: The coefficient of determination adjusting for the degrees of freedom\n\n\nWe will say more to define these quantities in future sections.\n\nsummary(fit.1)\n\n\nCall:\nlm(formula = Buchanan00 ~ Perot96, data = florida)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-612.74  -65.96    1.94   32.88 2301.66 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.34575   49.75931   0.027    0.979    \nPerot96      0.03592    0.00434   8.275 9.47e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 316.4 on 65 degrees of freedom\nMultiple R-squared:  0.513, Adjusted R-squared:  0.5055 \nF-statistic: 68.48 on 1 and 65 DF,  p-value: 9.474e-12\n\n\nR also allows several shortcuts for accessing particular elements of your regression results. Examples:\n\n## Vector of the coefficient estimates only\ncoef(fit.1)\n\n(Intercept)     Perot96 \n 1.34575212  0.03591504 \n\n## Compute confidence intervals for these coefficients\nconfint(fit.1)\n\n                   2.5 %       97.5 %\n(Intercept) -98.03044506 100.72194929\nPerot96       0.02724733   0.04458275\n\n## Table of coefficient results only\nsummary(fit.1)$coefficients\n\n              Estimate   Std. Error    t value     Pr(&gt;|t|)\n(Intercept) 1.34575212 49.759306434 0.02704523 9.785065e-01\nPerot96     0.03591504  0.004340068 8.27522567 9.473505e-12\n\n## Extract standard errors only\nsummary(fit.1)$coefficients[,2]\n\n (Intercept)      Perot96 \n49.759306434  0.004340068 \n\n## Variance-Covariance matrix\nvcov(fit.1)\n\n             (Intercept)       Perot96\n(Intercept) 2475.9885768 -1.360074e-01\nPerot96       -0.1360074  1.883619e-05\n\n## Note that the square root of the diagonal of this matrix provides the standard errors\nsqrt(diag(vcov(fit.1)))\n\n (Intercept)      Perot96 \n49.759306434  0.004340068 \n\n## Degrees of freedom\nfit.1$df.residual\n\n[1] 65\n\n\n\n\n4.3.2 Plotting Regression Results\nWe often don’t want to hide our data under a bushel basket or in complicated regression models. Instead, we might also want to visualize data in R. The function plot() and the function ggplot() from the package ggplot2 are two terrific and flexible functions for visualizing data. We will use the plot() function to visualize the relationship between Perot and Buchanan votes. The example below provides a few arguments you can use within each of these functions, but they are capable of much more.\nAt the core, plotting functions generally work as coordinate systems. You tell R specifically at which x and y coordinates you want your points to be located (e.g., by providing R with a vector of x values and a vector of y values). Then, each function has its own way of allowing you to add bells and whistles to your figure, such as labels (e.g., main, xlab, ylab), point styles (pch), additional lines and points and text (e.g., abline(), lines(), points(), text()), or x and y scales for the dimensions of your axes (e.g., xlim, ylim). You can create a plot without these additional features, but most of the time, you will add them to make your plots look good! and be informative! We will do a lot of plotting this semester.\nNote: feel free to use plot() or ggplot() or both. ggplot has similar capabilities as plot but relies on a different “grammar” of graphics. For example, see the subtle differences in the two plots below.\n\n## Plot\nplot(x = florida$Perot96, # x-values\n     y = florida$Buchanan00, # y-values\n     main = \"Perot and Buchanan Votes\", # label for main title\n     ylab = \"Buchanan Votes\", # y-axis label\n     xlab = \"Perot Votes\", # x-axis label\n     pch = 20) # point type\nabline(fit.1, col = \"red\") # adds a red regression line\n\n\n\n\n\n## ggplot version\nlibrary(ggplot2)\nggplot(data = florida, # which data frame\n       mapping = aes(x = Perot96, y = Buchanan00)) + # x and y coordinates\n  geom_point() +  # tells R we want a scatterplot\n  geom_smooth(method = \"lm\",\n              se = FALSE, colour = \"red\",\n              data = florida, aes(x=Perot96, y=Buchanan00)) + # adds lm regression line\n  ggtitle(\"Perot and Buchanan Votes\") + # main title\n  labs(x = \"Perot Votes\", y = \"Buchanan Votes\") + # x and y labels\n  theme_bw() # changes theme (e.g., color of background)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n## Note: data = florida, aes(x=Perot96, y=Buchanan00) in the geom_smooth line is not necessary if it is the same mapping at the first line. Required if data are different\n\nTip: you might want to save your plots as .pdf or .png after you create it. You can do this straight from your R code. How you do it varies by function. The files will save to your working directory unless you specify a different file path. The code below is the same as above except it has additional lines for saving the plots:\n\n## Plot\npdf(file = \"myfirstmleplot.pdf\", width = 7, height = 5) # play around with the dimensions\nplot(x = florida$Perot96, # x-values\n     y = florida$Buchanan00, # y-values\n     main = \"Perot and Buchanan Votes\", # label for main title\n     ylab = \"Buchanan Votes\", # y-axis label\n     xlab = \"Perot Votes\", # x-axis label\n     pch = 20) # point type\nabline(fit.1, col = \"red\") # adds a red regression line\ndev.off() # this closes your pdf file\n\n## ggplot version\nggplot(data = florida, # which data frame\n       mapping = aes(x = Perot96, y = Buchanan00)) + # x and y coordinates\n  geom_point() +  # tells R we want a scatterplot\n  geom_smooth(method = \"lm\",  \n              se = FALSE, colour = \"red\",\n              data = florida, aes(x=Perot96, y=Buchanan00)) + # adds lm regression line\n  ggtitle(\"Perot and Buchanan Votes\") + # main title\n  labs(x = \"Perot Votes\", y = \"Buchanan Votes\") + # x and y labels\n  theme(plot.title = element_text(hjust = 0.5)) +# centers the title\n  theme_bw() # changes theme (e.g., color of background)\nggsave(\"myfirstmleggplot.png\", device=\"png\", width = 7, height = 5) # saves the last ggplot\n\n\n\n4.3.3 Finding Coefficients without lm\nLet’s put our matrix algebra and R knowledge together. In the previous section, we found that \\(\\hat \\beta = (X'X)^{-1}X'Y\\). If we do that math directly in R, there is no need to use lm() to find those coefficients.\nTo do so, we need \\(X\\) and \\(Y\\).\nRecall \\(Y\\) is an \\(n \\times 1\\) vector representing the outcome of our model. In this case, \\(Y\\) is Buchanan00.\n\nY &lt;- florida$Buchanan00\n\nRecall, \\(X\\) is a \\(n \\times k\\) matrix representing our independent variables and a column of 1’s for the intercept. Let’s build this matrix using cbind which was introduced in section 2.\n\nX &lt;- cbind(1, florida$Perot96)\ndim(X)\n\n[1] 67  2\n\n\nGreat, now we have \\(X\\) and \\(Y\\), so it’s just about a little math. Because \\(Y\\) is a vector, let’s make sure R knows to treat it like an \\(n \\times 1\\) matrix.\n\nY &lt;- cbind(Y)\ndim(Y)\n\n[1] 67  1\n\n\nRecall the solve() and t() functions take the inverse and transpose of matrices.\n\nbetahat &lt;- solve(t(X) %*% X) %*% t(X) %*% Y\n\nFinally, let’s compare the results from our model using lm() with these results.\n\nbetahat\ncoef(fit.1)\n\n              Y\n[1,] 1.34575212\n[2,] 0.03591504\n(Intercept)     Perot96 \n 1.34575212  0.03591504 \n\n\nWe did it! In the problem set, you will get more experience using the analytic solutions to solve for quantities of interest instead of the built-in functions.\n\n\n4.3.4 OLS Practice Problems\nHere are a couple of (ungraded) problems to modify the code above and gain additional practice with data wrangling and visualization in R. As you might have noticed in the example, there is a big outlier in the data. We will see how this observation affects the results.\n\nUsing a linear regression examine the relationship between Perot and Buchanan votes, controlling for Bill Clinton’s 1996 votes.\n\n\nProvide a one sentence summary of the relationship between Perot and Buchanan’s votes.\nIs the relationship significant at the \\(p &lt; 0.05\\) level? What about the relationship between Clinton and Buchanan votes?\nWhat are the confidence intervals for the Perot coefficient results?\nWhat is the residual for the estimate for Palm Beach County– PalmBeach in the county variable?\n\n\nLet’s go back to the bivariate case.\n\n\nSubset the data to remove the county PalmBeach.\nCreate a scatterplot of the relationship between Perot votes and Buchanan votes within this subset. This time make the points blue.\nAdd a regression line based on this subset of data.\nAdd a second regression line in a different color based on the initial bivariate regression we ran in the example, where all data were included.\nDescribe the differences in the regression lines.\n\n\n\n4.3.5 Code for solutions\n\nfit.multiple &lt;- lm(Buchanan00 ~ Perot96 + Clinton96, data = florida)\nsummary(fit.multiple)\n\n\nCall:\nlm(formula = Buchanan00 ~ Perot96 + Clinton96, data = florida)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-705.06  -49.17   -4.71   27.34 2254.89 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept) 14.110353  51.644141   0.273  0.78556   \nPerot96      0.027394   0.010095   2.714  0.00854 **\nClinton96    0.001283   0.001372   0.935  0.35325   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 316.7 on 64 degrees of freedom\nMultiple R-squared:  0.5196,    Adjusted R-squared:  0.5046 \nF-statistic: 34.61 on 2 and 64 DF,  p-value: 6.477e-11\n\nconfint(fit.multiple)[2,]\n\n      2.5 %      97.5 % \n0.007228254 0.047560638 \n\nflorida$res &lt;- residuals(fit.multiple)\nflorida$res[florida$county == \"PalmBeach\"]\n\n[1] 2254.893\n\nflorida.pb &lt;- subset(florida, subset = (county != \"PalmBeach\"))\nfit2 &lt;- lm(Buchanan00 ~ Perot96, data = florida.pb)\n\nggplot(data = florida.pb, # which data frame\n       mapping = aes(x = Perot96, y = Buchanan00)) + # x and y coordinates\n  geom_point(color=\"blue\") +  # tells R we want a scatterplot\n  geom_smooth(method = \"lm\", \n              se = FALSE, colour = \"green\",\n              data = florida.pb, aes(x=Perot96, y=Buchanan00)) + # adds lm regression line\n    geom_smooth(method = \"lm\", \n              se = FALSE, colour = \"red\",\n              data = florida, aes(x=Perot96, y=Buchanan00)) + # adds lm regression line\n  ggtitle(\"Perot and Buchanan Votes\") + # main title\n  labs(x = \"Perot Votes\", y = \"Buchanan Votes\") + # x and y labels\n  theme(plot.title = element_text(hjust = 0.5)) +# centers the title\n  theme_bw() # changes theme (e.g., color of background)\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "04-ReviewofOLS.html#extra-practice-building-and-breaking-regression",
    "href": "04-ReviewofOLS.html#extra-practice-building-and-breaking-regression",
    "title": "4  Review of OLS",
    "section": "4.4 Extra practice building and breaking regression",
    "text": "4.4 Extra practice building and breaking regression\nBelow is additional practice with regression and R, showing how to work with different variables and diagnosing errors.\nWhat is the association between race and income?\nLet’s say we want to explore the relationship between race and income, where the people in our sample take on the values white, Black, Asian, and Hispanic for race. We can write this as:\n\\(Income_i = \\alpha + \\beta*race_i + \\epsilon\\)\nHowever, race is not a numeric variable. This complicates our regression equation because what exactly is a 1-unit change in race? Sure, we could assign numeric values to each racial category in our data (e.g., white = 1, Black = 2, Hispanic = 3, Asian = 4), but we would have no reason to assume that the change in income would be linear as you change in race by units. Why should the difference in income between white and Black individuals be estimated as the same difference between Black and Hispanic individuals?\nIn a linear regression, when you have categorical independent variables, what should you typically do?\n\n4.4.1 Categorical variables\nLet’s build some data\nBuild a matrix with dummy variables for each race\nRun the code below and see what is in X.\n\n## Dummy variable example\nresprace &lt;- c(\"white\", \"white\", \"asian\", \"black\", \"hispanic\",\n              \"hispanic\", \"hispanic\", \"asian\", \"white\", \"black\", \n              \"black\", \"black\", \"asian\", \"asian\", \"white\", \"white\")\n\n## \"Dummy variables\"\nwhite &lt;- rep(0, length(resprace))\nwhite[resprace == \"white\"] &lt;- 1\nasian &lt;- rep(0, length(resprace))\nasian[resprace == \"asian\"] &lt;- 1\nblack &lt;- rep(0, length(resprace))\nblack[resprace == \"black\"] &lt;- 1\nhispanic &lt;- rep(0, length(resprace))\nhispanic[resprace == \"hispanic\"] &lt;- 1\n\n## Matrix\nX &lt;- cbind(white, asian, black, hispanic)\nX\n\n      white asian black hispanic\n [1,]     1     0     0        0\n [2,]     1     0     0        0\n [3,]     0     1     0        0\n [4,]     0     0     1        0\n [5,]     0     0     0        1\n [6,]     0     0     0        1\n [7,]     0     0     0        1\n [8,]     0     1     0        0\n [9,]     1     0     0        0\n[10,]     0     0     1        0\n[11,]     0     0     1        0\n[12,]     0     0     1        0\n[13,]     0     1     0        0\n[14,]     0     1     0        0\n[15,]     1     0     0        0\n[16,]     1     0     0        0\n\n\nLet’s build toward a regression model\nLet’s create a Y variable representing our outcome for income. Let’s also add an intercept to our X matrix. Take a look into our new X.\n\n## Dependent variable\nY &lt;- cbind(c(10, 11, 9, 8, 9, 7, 7, 13, 12, 11, 8, 7, 4, 13, 8, 7))\n\nX &lt;- cbind(1, X)\n\nLet’s now apply the formula \\((X'X)^{-1}X'Y\\) to estimate our coefficients.\nAssign the output to an object called betahat\n\nbetahat &lt;- solve(t(X) %*% X) %*% t(X) %*% Y\nbetahat\n\n\n\n4.4.2 Formalizing Linear Dependence\nWhy were our dummy variables linear dependent?\nIf we inspect X we can see that taking each column \\(\\mathbf{x_1} - \\mathbf{x_2} - \\mathbf{x_3}-\\mathbf{x_4}=0\\). There is a linear relationship between the variables.\nFormally, a set of vectors (e.g.,\\(\\mathbf{x_1}, \\mathbf{x_2}, ...\\mathbf{x_k}\\)) is linearly independent if the equation \\(\\mathbf{x_1}*a_1 + \\mathbf{x_2}*a_2 +... + \\mathbf{x_k}*a_k= 0\\) only in the trivial case where \\(a_1\\) and \\(a_2\\) through \\(a_k\\) are 0. A set of vectors has a linearly dependent relation if there is a solution \\(\\mathbf{x_1}*a_1 + \\mathbf{x_2}*a_2 +... + \\mathbf{x_k}*a_k = 0\\) where not all \\(a_1, a_2\\) through \\(a_k\\) are 0.\nFor OLS, we must assume no perfect collinearity.\n\nNo independent variable is constant\nNo exactly linear relationships among the independent variables\nThe rank of X is \\(k\\) where the rank of a matrix is the maximum number of linearly independent columns.\n\nAs discussed in the course notes, a square matrix is only invertible if its columns are linearly independent. In OLS, in order to estimate unique solutions for \\(\\hat \\beta\\), we need to invert \\((X'X)^{-1}\\). When we have perfect collinearity, we cannot do this.\nNote the linear dependence in X\n\n## Matrix\nX[, 1] - X[, 2] - X[, 3] - X[, 4] - X[,5]\n\n [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n\n\nTry to take \\((X'X)^{-1}\\)\n\nsolve(t(X) %*% X)\n\nHow do we correct this?\nTo address this, we are going to drop one of the categorical variables when we run the regression. Consequently, our coefficients will now be interpreted as differences between this reference category (the category left out, e.g., white) and the particular group (e.g., white vs. Asian or white vs. Black or white vs. Hispanic).\nRedefine X removing the white column, and calculate \\((X'X)^{-1}X'Y\\)\n\nX &lt;- cbind(1, asian, black, hispanic)\nbetahat &lt;- solve(t(X) %*% X) %*% t(X) %*% Y\nbetahat \n\n              [,1]\n          9.600000\nasian     0.150000\nblack    -1.100000\nhispanic -1.933333\n\n\nCheck this with the lm() function\n\nsummary(lm(Y ~ asian + black + hispanic))\n\n\nCall:\nlm(formula = Y ~ asian + black + hispanic)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.7500 -0.9375 -0.5000  1.6500  3.2500 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    9.600      1.181   8.132 3.18e-06 ***\nasian          0.150      1.771   0.085    0.934    \nblack         -1.100      1.771  -0.621    0.546    \nhispanic      -1.933      1.928  -1.003    0.336    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.64 on 12 degrees of freedom\nMultiple R-squared:  0.1105,    Adjusted R-squared:  -0.1119 \nF-statistic: 0.4967 on 3 and 12 DF,  p-value: 0.6914\n\n\nIn R and many other statistical softwares, the regression function will forcibly drop one of your variables if it encounters this type of linear dependence. See below when we include all four race dummies in the model.\nCheck what happens with the lm() function\n\nsummary(lm(Y ~ white + asian + black + hispanic))\n\n\nCall:\nlm(formula = Y ~ white + asian + black + hispanic)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.7500 -0.9375 -0.5000  1.6500  3.2500 \n\nCoefficients: (1 not defined because of singularities)\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   7.6667     1.5240   5.031 0.000294 ***\nwhite         1.9333     1.9278   1.003 0.335711    \nasian         2.0833     2.0161   1.033 0.321820    \nblack         0.8333     2.0161   0.413 0.686650    \nhispanic          NA         NA      NA       NA    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.64 on 12 degrees of freedom\nMultiple R-squared:  0.1105,    Adjusted R-squared:  -0.1119 \nF-statistic: 0.4967 on 3 and 12 DF,  p-value: 0.6914\n\n\nAn alternative way to enter categorical variables in a regression is to let the function create the dummy variables for you using factor(var, levels = ) to make sure R knows it is a factor variables.\n\nresprace &lt;- factor(resprace, levels = c(\"white\", \"asian\", \"black\", \"hispanic\"))\nsummary(lm(Y ~ resprace))\n\n\nCall:\nlm(formula = Y ~ resprace)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.7500 -0.9375 -0.5000  1.6500  3.2500 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         9.600      1.181   8.132 3.18e-06 ***\nrespraceasian       0.150      1.771   0.085    0.934    \nrespraceblack      -1.100      1.771  -0.621    0.546    \nrespracehispanic   -1.933      1.928  -1.003    0.336    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.64 on 12 degrees of freedom\nMultiple R-squared:  0.1105,    Adjusted R-squared:  -0.1119 \nF-statistic: 0.4967 on 3 and 12 DF,  p-value: 0.6914\n\n\n\n\n4.4.3 Other examples of breaking the no perfect collinearity rule\nRecall, for OLS, we must assume no perfect collinearity.\n\nNo independent variable is constant\nNo exactly linear relationships among the independent variables\nThe rank of X is \\(k\\) where the rank of a matrix is the maximum number of linearly independent columns.\n\nLet’s say we wanted to control for age, but all of our sample was 18 years old. Let’s try to add this to the X matrix.\nRedefine X adding age column, and calculate \\((X'X)^{-1}X'Y\\)\n\nage &lt;- rep(18, length(resprace))\nX &lt;- cbind(1, asian, black, hispanic, age)\nbetahat &lt;- solve(t(X) %*% X) %*% t(X) %*% Y\nbetahat \n\nLet’s visit the Florida example. Let’s say we had two Perot96 variables– one the raw votes and one where votes were multiplied by 1000 to adjust the order of magnitude.\nRegress Buchanan’s votes on Perot and Perot adjusted values\n\nflorida &lt;- read.csv(\"https://raw.githubusercontent.com/ktmccabe/teachingdata/main/florida.csv\")\n\nflorida$perotadjusted &lt;- florida$Perot96 * 1000\n\nY &lt;- florida$Buchanan00\nX &lt;- cbind(1, florida$Perot96, florida$perotadjusted)\nbetahat &lt;- solve(t(X) %*% X) %*% t(X) %*% Y\nbetahat"
  },
  {
    "objectID": "04-ReviewofOLS.html#uncertainty-and-regression",
    "href": "04-ReviewofOLS.html#uncertainty-and-regression",
    "title": "4  Review of OLS",
    "section": "4.5 Uncertainty and Regression",
    "text": "4.5 Uncertainty and Regression\nWe have now gone through the process of minimizing the sum of squared errors (\\(\\mathbf{e'e}\\)) and deriving estimates for the OLS coefficients \\(\\hat \\beta = (X'X)^{-1}X'Y\\). In this section, we will discuss how to generate estimates of the uncertainty around these estimates.\nWhere we are going:\n\nIn the last section, we visited an example related to the 2000 election in Florida. We regressed county returns for Buchanan in 2000 (Y) on county returns for Perot in 1996 (X).\n\n\n## Load Data\nflorida &lt;- read.csv(\"https://raw.githubusercontent.com/ktmccabe/teachingdata/main/florida.csv\")\nfit.1 &lt;- lm(Buchanan00 ~ Perot96, data = florida)\nsummary(fit.1)\n\n\nCall:\nlm(formula = Buchanan00 ~ Perot96, data = florida)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-612.74  -65.96    1.94   32.88 2301.66 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.34575   49.75931   0.027    0.979    \nPerot96      0.03592    0.00434   8.275 9.47e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 316.4 on 65 degrees of freedom\nMultiple R-squared:  0.513, Adjusted R-squared:  0.5055 \nF-statistic: 68.48 on 1 and 65 DF,  p-value: 9.474e-12\n\n\nThe summary output of the model shows many different quantities in addition to the coefficient estimates. In particular, in the second column of the summary, we see the standard errors of the coefficients. Like many statistical software programs, the lm() function neatly places these right next to the coefficients. We will now discuss how we get to these values.\n\n4.5.1 Variance of the Coefficients\nThe standard error is the square root of the variance, representing the typical deviation we would expect to see between our estimates \\(\\hat \\beta\\) of the parameter \\(\\beta\\) across repeated samples. So to get to the standard error, we just need to get to an estimate of the variance.\nLet’s take the journey. First the math. As should start becoming familiar, we have our initial regression equation, which describes the relationship between the independent variables and dependent variables.\n\nStart with the model: \\(Y = X\\beta + \\epsilon\\)\n\nWe want to generate uncertainty for our estimate of \\(\\hat \\beta =(X'X)^{-1}X'Y\\)\n\nNote: Conditional on fixed values of \\(X\\) (I say fixed values because this is our data. We know \\(X\\) from our dataset.), the only random component is \\(\\epsilon\\).\n\nWhat does that mean? Essentially, the random error term in our regression equation is what is giving us the uncertainty. If \\(Y\\) was a deterministic result of \\(X\\), we would have no need for it, but it’s not. The relationship is not exact, varies sample to sample, subject to random perturbations, represented by \\(\\epsilon\\).\n\n\nBelow we go through how to arrive at the mathematical quantity representing the variance of \\(\\hat \\beta\\) which we will notate as \\(\\mathbf{V}(\\hat\\beta)\\). The first part of the math below is just substituting terms:\n\\[\\begin{align*}\n\\mathbf{V}(\\widehat{\\beta}) &=\n\\mathbf{V}( (X^T X) ^{-1} X^T Y))  \\\\\n&= \\underbrace{\\mathbf{V}( (X^T X) ^{-1} X^T (X\\beta + \\epsilon))}_\\text{Sub in the expression for Y from above}  \\\\\n&= \\underbrace{\\mathbf{V}((X^T X) ^{-1} X^T X \\beta + (X^T X) ^{-1} X^T \\epsilon)}_\\text{Distribute the term to the items in the parentheses}  \\\\\n&= \\underbrace{\\mathbf{V}(\\beta + (X^T X) ^{-1} X^T \\epsilon)}_\\text{Using the rules of inverses, the two terms next to $\\beta$ canceled each other out}  \n\\end{align*}\\]\nThe next part of the math requires us to use knowledge of the definition of variance and the rules associated. We draw on two in particular:\n\nThe variance of a constant is zero.\nWhen you have a constant multipled by a random variable, e.g., \\(\\mathbf{V}(4d)\\), it can come out of the variance operator, but must be squared: \\(16\\mathbf{V}(d)\\)\nPutting these together: \\(\\mathbf{V}(2 + 4d)= 16\\mathbf{V}(d)\\)\n\nKnowing these rules, we can proceed: \\[\\begin{align*}\n\\mathbf{V}(\\widehat{\\beta}) &= \\mathbf{V}(\\beta + (X^T X) ^{-1} X^T \\epsilon) \\\\\n&=\\underbrace{ \\mathbf{V}((X^T X) ^{-1} X^T \\epsilon)}_\\text{$\\beta$ drops out because in a regression it is an unkown \"parameter\"-- it's constant, which means its variance is zero.}\\\\\n&= \\underbrace{(X^T X)^{-1}X^T \\mathbf{V}( \\epsilon) ((X^T X)^{-1}X^T)^T}_\\text{We can move $(X^T X)^{-1}X^T$ out front because our data are fixed quantities, but in doing so, we have to \"square\" the matrix.}\\\\\n&= (X^T X)^{-1}X^T \\mathbf{V}( \\epsilon) X (X^T X)^{-1}\n\\end{align*}\\]\nThe resulting quantity is our expression for the \\(\\mathbf{V}(\\hat \\beta)\\). However, in OLS, we make an additional assumption that allows us to further simplify the expression. We assume homoscedasticity aka “constant” or “equal error variance” which says that the variance of the errors are the same across observations: \\(\\mathbf{V}(\\epsilon) = \\sigma^2 I_n\\).\n\nIf we assume homoscedastic errors, then Var\\((\\epsilon) = \\sigma^2 I_n\\)\n\n\\[\\begin{align*}\n\\mathbf{V}(\\widehat{\\beta})  &= (X^T X)^{-1}X^T \\mathbf{V}( \\epsilon) X (X^T X)^{-1}\\\\\n&= \\underbrace{(X^T X)^{-1}X^T \\sigma^2I_n X (X^T X)^{-1}}_\\text{Assume homoskedasticity}\\\\\n&= \\underbrace{\\sigma^2(X^T X)^{-1} X^T X (X^T X)^{-1}}_\\text{Because it is a constant, we can move it out in front of the matrix multiplication, and then simplify the terms.}  \\\\\n&= \\sigma^2(X^T X)^{-1}\n\\end{align*}\\]\nAll done! This expression: \\(\\sigma^2(X^T X)^{-1}\\) represents the variance of our coefficient estimates. Note its dimensions: \\(k \\times k\\). It has the same number of rows and columns as the number of our independent variables (plus the intercept).\nThere is one catch, though. How do we know what \\(\\sigma^2\\) is? Well, we don’t. Just like the unknown parameter \\(\\beta\\), we have to estimate it in our regression model.\nJust like with the coefficients, we notate our estimate as \\(\\widehat{\\sigma}^2\\). Our estimate is based on the observed residual errors in the model and is as follows:\n\n\\(\\widehat{\\sigma}^2 = \\frac{1}{N-K}\\sum_{i=1}^N \\widehat{\\epsilon_i^2} = \\frac{1}{N-K} \\mathbf{e'e}\\)\n\nThat means our estimate of the variance of the coefficients is found within: \\(\\hat \\sigma^2(X^T X)^{-1}\\)\nAgain, this is a \\(k \\times k\\) matrix and is often called the variance covariance matrix. We can extract this quantity from our linear models in R using vcov().\n\nvcov(fit.1)\n\n             (Intercept)       Perot96\n(Intercept) 2475.9885768 -1.360074e-01\nPerot96       -0.1360074  1.883619e-05\n\n\nThis is the same that we would get if manually we took the residuals and multiplied it by our \\(X\\) matrix according to the formula above:\n\nX &lt;- cbind(1, florida$Perot96)\ne &lt;- cbind(residuals(fit.1))\nsigmahat &lt;- ((t(e) %*% e) / (nrow(florida) -2)) \n## tell r to stop treating sigmahat as a matrix\nsigmahat &lt;-as.numeric(sigmahat)\nXtX &lt;- solve(t(X) %*%X)\nsigmahat * XtX\n\n             [,1]          [,2]\n[1,] 2475.9885768 -1.360074e-01\n[2,]   -0.1360074  1.883619e-05\n\n\nThe terms on the diagonal represent the variance of a particular coefficient in the model.The standard error of a particular coefficient \\(k\\) is: s.e.(\\(\\hat{\\beta_k}) = \\sqrt{\\widehat{\\sigma}^2 (X'X)^{-1}}_{kk}\\). The off-diagonal components represent the covariances between the coefficients.\nRecall that the standard error is just the square root of the variance. So, to get the nice standard errors we saw in the summary output, we can take the square root of the quantities on the diagonal of this matrix.\n\nsqrt(diag(vcov(fit.1)))\n\n (Intercept)      Perot96 \n49.759306434  0.004340068 \n\nsummary(fit.1)$coefficients[,2]\n\n (Intercept)      Perot96 \n49.759306434  0.004340068 \n\n\nWhy should I care?\n\nWell R actually doesn’t make it that easy to extract standard errors from the summary output. You can see above that the code for extracting the standard errors using what we know about them being the square root of the variance is about as efficient as extracting the second column of the coefficient component of the summary of the model.\nSometimes, we may think that the assumption of equal error variance is not feasible and that we have unequal error variance or “heteroscedasticity.” Researchers have developed alternative expressions to model unequal error variance. Generally, what this means is they can no longer make that simplifying assumption, have to stop at the step with the uglier expression \\((X^T X)^{-1}X^T \\mathbf{V}( \\epsilon) X (X^T X)^{-1}\\) and then assume something different about the structure of the errors in order to estimate the coefficients. These alternative variance estimators are generally what are referred to as “robust standard errors.” There are many different robust estimators, and you will likely come across them in your research.\n\nSome of you may have learned the formal, general definition for variance as defined in terms of expected value: \\(\\mathbb{E}[(\\widehat{m} - \\mathbb{E}(\\widehat{m}))^2 ]\\). We could also start the derivation there. This is not required for the course, but it is below if you find it useful. In particular, it can help show why we wend up needing to square a term when we move it outside the variance operator:\n\\[\\begin{align*}\n\\mathbf{V}(\\widehat{\\beta}) &= \\mathbb{E}[(\\widehat{\\beta} - \\mathbb{E}(\\hat \\beta))^2)] \\\\\n&= \\mathbb{E}[(\\widehat{\\beta} - \\beta)^2]\\\\\n&= \\mathbb{E}[(\\widehat{\\beta} - \\beta)(\\widehat{\\beta} - \\beta)^T ] \\\\ &=\n\\mathbb{E}[(X^T X) ^{-1} X^TY - \\beta)(X^T X) ^{-1} X^TY - \\beta)^T]  \\\\\n&= \\mathbb{E}[(X^T X) ^{-1} X^T(X\\beta + \\epsilon) - \\beta)(X^T X) ^{-1} X^T(X\\beta + \\epsilon) - \\beta)^T]\\\\\n&=  \\mathbb{E}[(X^T X) ^{-1} X^TX\\beta + (X^T X) ^{-1} X^T\\epsilon - \\beta)(X^T X) ^{-1} X^TX\\beta + (X^T X) ^{-1} X^T\\epsilon - \\beta)^T]\\\\\n&=  \\mathbb{E}[(\\beta + (X^T X) ^{-1} X^T\\epsilon - \\beta)(\\beta + (X^T X) ^{-1} X^T\\epsilon - \\beta)^T]\\\\\n&=  \\mathbb{E}[(X^T X) ^{-1} X^T\\epsilon)(X^T X) ^{-1} X^T\\epsilon)^T]\\\\\n&= (X^T X) ^{-1} X^T\\mathbb{E}(\\epsilon\\epsilon^T)X(X^T X) ^{-1}\\\\\n&= \\underbrace{(X^T X) ^{-1} X^T\\sigma^2I_nX(X^T X) ^{-1}}_\\text{Assume homoskedasticity}\\\\\n&= \\sigma^2(X^T X) ^{-1} X^TX(X^T X) ^{-1}\\\\\n&= \\sigma^2(X^T X) ^{-1}\n\\end{align*}\\]\nNote: Along the way, in writing \\(\\mathbb{E}(\\hat \\beta) = \\beta\\), we have implicitly assumed that \\(\\hat \\beta\\) is an “unbiased” estimator of \\(\\beta\\). This is not free. It depends on an assumption that the error term in the regression \\(\\epsilon\\) is independent of our independent variables. This can be violated in some situations, such as when we have omitted variable bias, which is discussed at the end of our OLS section.\n\n\n4.5.2 Hypothesis Testing\nMost of the time in social science, we run a regression because we have some hypothesis about how a change in our independent variable affects the change in our outcome variable.\nIn OLS, we can perform a hypothesis test for each independent variable in our data. The structure of the hypothesis test is:\n\nNull hypothesis: \\(\\beta_k = 0\\)\n\nThis essentially means that we don’t expect a particular \\(x_k\\) independent variable to have a relationship with our outcome variable.\n\nAlternative hypothesis: \\(\\beta_k \\neq 0\\)\n\nWe do expect a positive or negative relationship between a particular \\(x_k\\) and the dependent variable.\n\n\nWe can use our estimates for \\(\\hat \\beta\\) coefficients and their standard errors to come to a conclusion about rejecting or failing to reject the null hypothesis of no relationship by using a t-test.\nIn a t-test, we take our coefficient estimates and divide them by the standard error in order to “standardize” them on a scale that we can use to determine how likely it is we would have observed a value for \\(\\hat \\beta\\) as extreme or more extreme as the one we observed in a world where the true \\(\\beta = 0\\). This is just like a t-test you might have encountered before for a difference in means between groups, except this time our estimate is \\(\\hat \\beta\\).\n\\[\\begin{align*}\nt_{\\hat \\beta_k} &= \\frac{\\hat \\beta_k}{s.e.(\\hat \\beta_k)}\n\\end{align*}\\]\nGenerally speaking, when \\(t\\) is about +/-2 or greater in magnitude, the coefficient will be “significant” at conventional levels (i.e., \\(p &lt;0.05\\)), meaning that we are saying that it is really unlikely we would have observed a value as big as \\(\\hat \\beta_k\\) if the null hypothesis were true. Therefore, we can reject the null hypothesis.\nHowever, to get a specific quantity, we need to calculate the p-value, which depends on the t-statistic and the degrees of freedom in the model. The degrees of freedom in a regression model are \\(N-k\\), the number of observations in the model minus the number of independent variables plus the intercept.\nIn R, we can calculate p-values using the pt() function. By default, most people use two-sided hypothesis tests for regression. So to do that, we are going to find the area on each side of the t values, or alternatively, multiply the area to the right of our positive t-value by 2.\n\n## Let's say t was 2.05 and \n## And there were 32 observations and 3 variables in the regression plus an intercept\nt &lt;- 2.05\ndf.t &lt;- 32 -4\np.value &lt;- 2 * (pt(abs(t), df=df.t, lower.tail = FALSE))\np.value\n\n[1] 0.04983394\n\n\nLet’s do this for the florida example. First, we can find t by dividing our coefficients by the standard errors.\n\nt &lt;- coef(fit.1) / (sqrt(diag(vcov(fit.1))))\nt \n\n(Intercept)     Perot96 \n 0.02704523  8.27522567 \n\n## Compare with output\nsummary(fit.1)$coefficients[, 3]\n\n(Intercept)     Perot96 \n 0.02704523  8.27522567 \n\n\nWe can then find the p-values.\n\nt &lt;- coef(fit.1) / (sqrt(diag(vcov(fit.1))))\ndf.t &lt;- fit.1$df.residual\np.value &lt;- 2 * (pt(abs(t), df=df.t, lower.tail = FALSE))\np.value\n\n (Intercept)      Perot96 \n9.785065e-01 9.473505e-12 \n\nsummary(fit.1)$coefficients[, 4]\n\n (Intercept)      Perot96 \n9.785065e-01 9.473505e-12 \n\n\nWe see that the coefficient for Perot96 is significant. The p-value is tiny. In R, for small numbers, R automatically shifts to scientific notation. The 9.47e-12 means the p-value is essentially zero, with the stars in the summary output indicating the p-value is \\(p &lt; 0.001\\). R will also output a test of the significance of the intercept using the same formula as all other coefficients. This generally does not have much interpretive value, so you are usually safe to ignore it.\nConfidence Intervals\nInstead of representing the significance using p-values, sometimes it is helpful to report confidence intervals around the coefficients. This can be particularly useful when visualizing the coefficients. The 95% confidence interval represents roughly 2 standard errors above and below the coefficient. The key thing to look for is whether it overlaps with zero (not significant) or does not (in which case the coefficient is significant).\nThe precise formula is\n\\(\\widehat{\\beta}_k\\) Confidence intervals: \\(\\widehat{\\beta}_k - t_{crit.value} \\times s.e._{\\widehat{\\beta}_k}, \\widehat{\\beta}_k + t_{crit.value} \\times s.e_{\\widehat{\\beta}_k}\\)\nIn R, we can use qt() to get the specific critical value associated with a 95% confidence interval. This will be around 2, but fluctuates depending on the degrees of freedom in your model (which are function of your sample size and how many variables you have in the model.) R also has a shortcut confint() function to extract the coefficients from the model. Below we do this for the Perot96 coefficient.\n\n## Critical values from t distribution at .95 level\nqt(.975, df = fit.1$df.residual) # n- k degrees of freedom\n\n[1] 1.997138\n\n## Shortcut\nconfint(fit.1)[2,]\n\n     2.5 %     97.5 % \n0.02724733 0.04458275 \n\n## By hand\ncoef(fit.1)[2] - qt(.975, df = fit.1$df.residual)*sqrt(diag(vcov(fit.1)))[2]\n\n   Perot96 \n0.02724733 \n\ncoef(fit.1)[2] + qt(.975, df = fit.1$df.residual)*sqrt(diag(vcov(fit.1)))[2]\n\n   Perot96 \n0.04458275 \n\n\n\n\n4.5.3 Goodness of Fit\nA last noteworthy component to the standard regression output is the goodness of fit statistics. For this class, we can put less attention on these, though there will be some analogues when we get into likelihood.\nThese are measures of how much of the total variation in our outcome measure can be explained by our model, as well as how far off are our estimates from the truth.\nFor the first two measures R-squared and Adjusted R-squared, we draw on three quantities:\n\nTotal Sum of Squares–how much variance in \\(Y_i\\) is there to explain?\n\n\\(TSS: \\sum_{i=1}^N (Y_i -\\overline Y_i)^2\\)\n\nEstimated Sum of Squares–how much of this variance do we explain?\n\n\\(ESS: \\sum_{i=1}^N (\\widehat Y_i -\\overline Y_i)^2\\)\n\nResidual Sum of Squares–how much variance is unexplained?\n\n\\(RSS: \\sum_{i=1}^N ( Y_i -\\widehat Y_i)^2\\)\n\n\\(TSS = ESS + RSS\\)\nMultiple R-squared: \\(\\frac{ESS}{TSS}\\)\n\nThis is a value from 0 to 1, representing the proportion of the variance in the outcome that can be explained by the model. Higher values are generally considered better, but there are many factors that can affect R-squared values. In most social science tasks where the goal is to engage in hypothesis testing of coefficients, this measure is of less value.\n\nAdjusted R-squared: \\(1 - \\frac{\\frac{RSS}{n - k}}{\\frac{TSS}{n - 1}}\\)\n\nThis is essentially a penalized version of R-squared. When you add additional predictors to a model, the R-squared value can never decrease, even if the predictors are useless. The Adjusted R-squared adds a consideration for the degrees of freedom into the equation, creating a penalty for adding more and more predictors.\n\nResidual standard error aka root mean squared error aka square root of the mean squared residual: \\(r.s.e = \\sqrt{\\frac{RSS}{n-k}}\\)\n\nThis represents the typical deviation of an estimate of the outcome from the actual outcome. This quantity is often used to assess the quality of prediction exercises. It is used less often in social science tasks where the goal is hypothesis testing of the relationship between one or more independent variables and the outcome.\n\n\nF-Statistic\nSo far we have conducted hypothesis tests for each individual coefficient. We can also conduct a global hypothesis test, where the null hypothesis is that all coefficients are zero, with the alternative being that at least one coefficient is nonzero. This is the test represented by the F-statistic in the regression output.\nThe F-statistic helps us test the null hypothesis that all of the regression slopes are 0: \\(H_0 = \\beta_1 = \\beta_2 = \\dots = \\beta_k = 0\\)\n\n\\(F_0 = \\frac{ESS/(k - 1)}{RSS/(n - k)}\\)\nThe F-Statistic has two separate degrees of freedom.\n\nThe model sum of squares degrees of freedom (ESS) are \\(k - 1\\).\nThe residual error degrees of freedom (RSS) are \\(n - k\\).\nIn a regression output, the model degrees of freedom are generally the first presented: “F-statistic: 3.595 on \\((k - 1) = 1\\) and \\((n - k) = 48\\) DF.”\n\n\nNote: This test is different from our separate hypothesis tests that a \\(k\\) regression slope is 0. For that, we use the t-tests discussed above."
  },
  {
    "objectID": "04-ReviewofOLS.html#generating-predictions-from-regression-models",
    "href": "04-ReviewofOLS.html#generating-predictions-from-regression-models",
    "title": "4  Review of OLS",
    "section": "4.6 Generating predictions from regression models",
    "text": "4.6 Generating predictions from regression models\nThe regression coefficients tell us how much \\(Y\\) is expected to change for a one-unit change in \\(x_k\\). It does not immediately tell us the values we estimate our outcome (\\(\\hat Y\\)) to take conditional on particular values of \\(x_k\\). While often knowing our independent variables have a significant effect on the outcome and the size of the coefficient is sufficient for testing our hypotheses, it can be helpful for interpretation’s sake, to see the estimated values for the outcome. This is going to be particularly important once we get into models like logistic regression, where the coefficients won’t be immediately interpretable.\nRecall that our equation for estimating values of our outcomes is:\n\n\\(\\hat Y = X\\hat \\beta\\) This can also be written out in long form for any particular observation \\(i\\):\n\\(\\hat y_i = \\hat \\alpha + \\hat \\beta_1*x_1i + \\hat \\beta_2*x_2i + ... \\hat\\beta_k*x_ki\\)\n\nThe estimated values of our regression \\(\\hat Y\\) are often called the “fitted values.” In R, you can identify the estimated values for each observation using the fitted() command.\n\n## Y hat for the first observation in the data\nfitted(fit.1)[1]\n\n      1 \n291.252 \n\n\nAgain, this is just the multiplication of the matrix \\(X\\) and \\(\\hat \\beta\\). If we have already run a regression model in R, one shortcut for getting the \\(X\\) matrix, is to use the model.matrix command. We can get \\(\\hat \\beta\\) using the coef() command.\n\nX &lt;- model.matrix(fit.1)\nhead(X) # head() shows about the first six values of an object\n\n  (Intercept) Perot96\n1           1    8072\n2           1     667\n3           1    5922\n4           1     819\n5           1   25249\n6           1   38964\n\nbetahat &lt;- coef(fit.1)\n\nOur fitted values are then just\n\nyhat &lt;- X %*% betahat\nhead(yhat)\n\n        [,1]\n1  291.25196\n2   25.30108\n3  214.03462\n4   30.76017\n5  908.16461\n6 1400.73939\n\n\nIf I want to generate an estimate for any particular observation, I could also just extract its specific value for Perot96.\n\nflorida$Perot96[1]\n\n[1] 8072\n\n\nLet’s estimate the Buchanan 2000 votes for the first county in the data with Perot 96 votes of 8072. We can write it out as \\(\\hat Buchanan00_1 =\\hat \\alpha + \\hat \\beta*Perot96_1\\)\n\nbuch00hat &lt;- coef(fit.1)[1] + coef(fit.1)[2]*florida$Perot96[1]\nbuch00hat\n\n(Intercept) \n    291.252 \n\n\nWhat is useful about this is that now we have the coefficient estimates, we can apply them to any values of \\(X\\) we wish in order to generate estimates/predictions of the values \\(Y\\) will take given particular values of our independent variables.\nOne function that is useful for this (as a shortcut) is the predict(fit, newdata=newdataframe) function in R. It allows you to enter in “newdata”– meaning values of the \\(X\\) variables for which you want to generate estimates of \\(Y\\) based on the coefficient estimates of your regression model.\nFor example, let’s repeat the calculation from above for Perot96 = 8072.\n\npredict(fit.1, newdata = data.frame(Perot96 = 8072))\n\n      1 \n291.252 \n\n\nWe can also generate confidence intervals around these estimates by adding interval = \"confidence\" in the command.\n\npredict(fit.1, newdata = data.frame(Perot96 = 8072), interval=\"confidence\")\n\n      fit      lwr      upr\n1 291.252 213.7075 368.7964\n\n\nWe can also simultaneously generate multiple predictions by supplying a vector of values in the predict() command. For example, let’s see the estimated Buchanan votes for when the Perot 1996 votes took values of 1000 to 10,000 by intervals of 1,000.\n\npredict(fit.1, newdata = data.frame(Perot96 = c(1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000, 9000, 10000)))\n\n        1         2         3         4         5         6         7         8 \n 37.26079  73.17583 109.09087 145.00591 180.92095 216.83599 252.75104 288.66608 \n        9        10 \n324.58112 360.49616 \n\n\nThe important thing to note about the predict() command is that if you have multiple independent variables, you have to specify the values you want each of them to take when generating the estimated values of y.\n\nfit.2 &lt;- lm(Buchanan00 ~ Perot96 + Clinton96, data = florida)\n\nFor example, let’s build a second model with Clinton96 as an additional predictor. In order to generate the same prediction for different values of Perot 1996 votes, we need to tell R at what values we should “hold constant” Clinton96. I.e., we want to see how hypothetical changes in Perot96 votes influence changes in Buchanan 2000 votes while also leaving the Clinton votes identical. This is that lightswitch metaphor– flipping one switch, while keeping the rest untouched.\nThere are two common approaches to doing this. 1) We can hold constant Clinton96 votes at its mean value in the data 2) We can keep Clinton96 at its observed values in the data. In linear regression, it’s not going to matter which approach you take. In other models we will talk about later, this distinction may matter more substantially because of how our quantities of interest change across different values of \\(X\\hat \\beta\\).\nThe first approach is easily implemented in predict.\n\npredict(fit.2, newdata = data.frame(Perot96 = 8072, Clinton96 = mean(florida$Clinton96)))\n\n      1 \n283.997 \n\n\nFor the second approach, what we will do is generate an estimate for Buchanan’s votes in 2000 when Perot96 takes 8072 votes, and we keep Clinton96’s votes at whatever value it currently is in the data. That is, we will generate \\(n\\) estimates for Buchanan’s votes when Perot takes 8072. Then, we will take the mean of this as our “average estimate” of Buchanan’s votes in 2000 based on Perot’s votes at a level of 8072. We can do this in one of two ways:\n\n## Manipulating the X matrix\nX &lt;- model.matrix(fit.2)\n## Replace Perot96 column with all 8072 values\nX[, \"Perot96\"] &lt;- 8072\nhead(X) #take a peek\n\n  (Intercept) Perot96 Clinton96\n1           1    8072     40144\n2           1    8072      2273\n3           1    8072     17020\n4           1    8072      3356\n5           1    8072     80416\n6           1    8072    320736\n\n## Generate yhat\nyhats &lt;-X %*% coef(fit.2)\n\n## take the mean\nmean(yhats)\n\n[1] 283.997\n\n\n\n## Use predict\nyhats &lt;- predict(fit.2, newdata = data.frame(Perot96=8072, Clinton96=florida$Clinton96))\nmean(yhats)\n\n[1] 283.997\n\n\nNow often, after we generate these predicted values, we want to display them for the whole world to see. You will get a chance to visualize values like this using the plotting functions in the problem sets. We have already seen one example of this in the simple bivariate case, when R plotted the bivariate regression line in section 4.3.2. However, the predict function extends are capabilities to plot very specific values of \\(X\\) and \\(\\hat Y\\) for bivariate or multiple regressions.\n\nThe predict() function is also very relevant when we move to logistic, probit, etc. regressions. This is just the start of a beautiful friendship between you and predict() and associated functions."
  },
  {
    "objectID": "04-ReviewofOLS.html#wrapping-up-ols",
    "href": "04-ReviewofOLS.html#wrapping-up-ols",
    "title": "4  Review of OLS",
    "section": "4.7 Wrapping up OLS",
    "text": "4.7 Wrapping up OLS\nLinear regression is a great way to explain the relationship between one or more independent variables and an outcome variables. However, there is no free lunch. We have already mentioned a couple of assumptions along the way. Below we will summarize these and other assumptions. These are things you should be mindful of when you use linear regression in your own work. Some conditions that generate violations of these assumptions can also motivate why we will seek out alternative methods, such as those that rely on maximum likelihood estimation.\nThis is a good place to review Gelman section 3.6.\n\nExogeneity. This one we haven’t discussed yet, but is an important assumption for letting us interpret our coefficients \\(\\hat \\beta\\) as “unbiased” estimates of the true parameters \\(\\beta\\). We assume that the random error in the regression model \\(\\epsilon\\) is indeed random, and uncorrelated with and independent of our independent variables \\(X\\). Formally:\n\n\\(\\mathbb{E}(\\epsilon| X) = \\mathbb{E}(\\epsilon) = 0\\).\nThis can be violated, for example, when we suffer from Omitted Variable Bias due to having an “endogenous explanatory variable” that is correlated with some unobserved or unaccounted for factor. This bias comes from a situation where there is some variable that we have left out of the model (\\(Z\\)), and is therefore a part of the unobserved error term. Moreover this variable which is correlated with–and a pre-cursor of– our independent variables and is a cause of our dependent variable. A failure to account for omitted variables can create bias in our coefficient estimates. Concerns about omitted variable bias often prompt people to raise their hands in seminars and ask questions like, “Well have you accounted for this? Have you accounted for that? How do you know it is \\(X\\) driving your results and not \\(Z\\)?” If we omit important covariates, we may wrongly attribute an effect to \\(X\\) when it was really the result of our omitted factor \\(Z\\). Messing discusses this here.\nThis is a really tough assumption. The only real way to guarantee the independence of your error term and the independent variables is if you have randomly assigned values to the independent variables (such as what you do when you randomly assign people to different treatment conditions in an experiment). Beyond random assignment, you have to rely on theory to understand what variables you need to account for in the regression model to be able to plausibly claim your estimate of the relationship between a given independent variable and the dependent variable is unbiased. Failing to control for important factors can lead to misleading results, such as what happens in Simpson’s paradox, referenced in the Messing piece.\nDanger Note 1: The danger here, though, is that the motivation for avoiding omitted variable bias might be to keep adding control after control after control into the regression model. However, model building in this way can sometimes be atheoretical and result in arbitrary fluctuations in the size of your coefficients and their significance. At its worse, it can lead to “p-hacking” where researchers keep changing their models until they find the results they like. The Lenz and Sahn article on Canvas talks more about the dangers of arbitrarily adding controls to the model.\nDanger Note 2: We also want to avoid adding “bad controls” to the model. Messing talks about this in the medium article as it relates to collider bias. We want to avoid adding controls to our model, say \\(W\\) that are actually causes of \\(Y\\) and causes of \\(X\\) instead of the other way around.\nModel building is a delicate enterprise that depends a lot on having a solid theory that guides the choice of variables.\n\nHomoscedasticity. We saw this when defining the variance estimator for the OLS coefficients. We assume constant error variance. This can be violated when we think observations at certain values of our independent variables may have different magnitudes of error than observations at other values of our independent variables.\n\nNo correlation in the errors. The error terms are not correlated with each other. This can be violated in time series models (where we might think past, present, and future errors are correlated) or in cases where our observations are nested in some hierarchical structures (e.g., perhaps students in a school) and the errors are correlated.\n\nNo perfect collinearity. The \\(X\\) matrix must be full rank: We cannot have linear dependence between columns in our X matrix. We saw this in the tutorial when we tried to add the dummy variables for all of our racial groups into a regression at once. When there is perfect collinearity between variables, our regression will fail.\n\nWe should also avoid situations where we have severe multicollinearity. This can happen when we include two or more variables in a regression model that are highly correlated (just not perfectly correlated). While the regression will still run in this case, it can inflate the standard errors of the coefficients, making it harder to detect significant effects. This is particularly problematic in smaller samples.\n\nLinearity. The relationship between the independent and dependent variables needs to be linear in the parameters. It should be modeled as the addition of constants or parameters multiplied by the independent variables. If instead the model requires the multiplication of parameters, this is no longer linear (e.g., \\(\\beta^2\\)). Linearity also often refers to the shape of the model. Our coefficients tell us how much change we expect in the outcome for each one-unit change in an independent variable. We might think some relationships are nonlinear– meaning this rate of change varies across values of the independent variables. If that is the case, we need to shift the way our model is specified to account for this or change modeling approaches.\n\nFor example, perhaps as people get older (one-unit changes in age), they become more politically engaged, but at some age level, their political engagement starts to decline. This would mean the slope (that expected change in political engagement for each one-unit change in age) is not constant across all levels of age. There, we might be violating linearity in the curvature of the relationship between the independent and dependent variables. This is sometimes why you might see \\(age^2\\) or other nonlinear terms in regression equations to better model this curvature.\nLikewise, perhaps each additional level of education doesn’t result in the same average increase in \\(y\\). If not, you could consider including categorical dummy variables for different levels of education instead of treating education as a numeric variable.\n\nNormality. We assume that the errors are normally distributed. As Gelman 3.6 notes, this is a less important assumption and is generally not required.\n\nOLS Properties\nWhy we like OLS. When we meet our assumptions, OLS produces the best linear unbiased estimates (BLUE). A discussion of this here. We have linearity in our parameters (e.g., \\(\\beta\\) and not \\(\\beta^2\\) for example). The unbiasedness means that the expected value (aka the average over repeated samples) of our estimates \\(\\mathbb{E}(\\hat \\beta)= \\beta\\) is the true value. Our estimates are also efficient, which has to do with the variance, not only are our estimates true in expectation, but we also have lower variance than an alternative linear unbiased estimator could get us. If our assumptions fail, then we might no longer have BLUE. OLS estimates are also consistent, meaning that as the sample gets larger and larger, the estimates start converging to the truth.\nNow, a final hidden assumption in all of this is that the sample of our data is representative of the population we are trying to make inferences about. If that is not the case, then we may no longer be making unbiased observations to that population level. Further adjustments may be required (e.g., analyses of survey data sometimes use weights to adjust estimates to be more representative).\nWhen we violate these assumptions, OLS may no longer be best, and we may opt for other approaches. More soon!\n\n4.7.1 Practice Problems\n\nLet’s use the florida data. Run a regression according to the following formula:\n\n\\(Buchanan00_i = \\alpha + \\beta_1*Perot96_i + \\beta_2*Dole96 + \\beta_3*Gore00 + \\epsilon\\)\n\nReport the coefficient for Perot96. What do you conclude about the null hypothesis that there is no relationship between 1996 Perot votes and 2000 Buchanan votes?\nWhat is the confidence interval for the Perot96 coefficient estimate?\nWhen Perot 1996 vote is 5500, what is the expected 2000 Buchanan vote?\n\n\n\n4.7.2 Practice Problem Code for Solutions\n\nfit.practice &lt;- lm(Buchanan00 ~ Perot96 + Dole96 + Gore00, data = florida)\n\ncoef(fit.practice)[\"Perot96\"]\n\n   Perot96 \n0.02878927 \n\nconfint(fit.practice)[\"Perot96\", ]\n\n      2.5 %      97.5 % \n0.004316382 0.053262150 \n\nexpbuch &lt;- model.matrix(fit.practice)\nexpbuch[,\"Perot96\"] &lt;- 5500\nmean(expbuch %*% as.matrix(coef(fit.practice)))\n\n[1] 211.1386"
  },
  {
    "objectID": "04-ReviewofOLS.html#week-2-example",
    "href": "04-ReviewofOLS.html#week-2-example",
    "title": "4  Review of OLS",
    "section": "4.8 Week 2 Example",
    "text": "4.8 Week 2 Example\nThis example is based on Dancygier, Rafaela; Egami, Naoki; Jamal, Amaney; Rischke, Ramona, 2020, “Hate Crimes and Gender Imbalances: Fears over Mate Competition and Violence against Refugees”, published in the American Journal of Political Science. Replication data is available here. We will draw on the survey portion of the article and replicate Table 1 in the paper. The pre-print is available here.\nThe abstract is: As the number of refugees rises across the world, anti-refugee violence has become a pressing concern. What explains the incidence and support of such hate crime? We argue that fears among native men that refugees pose a threat in the competition for female partners is a critical but understudied factor driving hate crime. Employing a comprehensive dataset on the incidence of hate crime across Germany, we first demonstrate that hate crime rises where men face disadvantages in local mating markets. Next, we complement this ecological evidence with original survey measures and confirm that individual-level support for hate crime increases when men fear that the inflow of refugees makes it more difficult to find female partners. Mate competition concerns remain a robust predictor even when controlling for antirefugee views, perceived job competition, general frustration, and aggressiveness. We conclude that a more complete understanding of hate crime and immigrant conflict must incorporate marriage markets and mate competition.\nThe authors summarize their hypotheses as, “the notion that male refugees are engaged in romantic relationships with German women has received considerable media attention from a variety of sources, with coverage ranging from the curious to the outright hostile. We argue that the prospect of refugee-native mate competition can trigger or compound resentment against refugees, including support for hate crime” pg. 14\n\nlibrary(foreign)\ndat_use &lt;- read.dta(\"https://github.com/ktmccabe/teachingdata/blob/main/dat_use.dta?raw=true\")\n\nThe data include wave 4 of an online survey fielded in Germany through Respondi from September 2016 to December 2017). Each wave was designed to be nationally representative on age (starting at 18), gender, and state (Bundesland) with a sample of about 3,000 respondents in each wave.\nKey variables include\n\nhate_violence_means representing respondents’ agreement or disagreement to the Only Means question: “When it comes to the refugee problem, violence is sometimes the only means that citizens have to get the attention of German politicians.” from (1) disagree strongly to (4) agree strongly.\nMateComp_cont, Mate Competition. “The inflow of refugees makes it more difficult for native men to find female partners.” from (1) disagree strongly to (4) agree strongly.\nThe data include several other variables related to the demographics of the respondents and measures representing potential alternative explanations, such as JobComp_cont (agreement with “the inflow of young male refugees makes it more difficult for young native men to find apprenticeships and jobs”) and LifeSatis_cont (0-10 scale, ranging from extremely dissatisfied to extremely satisfied).\n\nLet’s pause here to ask a few questions about research design.\n\nWhat is the outcome? What is the independent variable of interest?\n\nHow would we write out the bivariate regression model?\n\nWhy OLS? (e.g., why not experiment?)\nWhat types of alternative explanations might exist?\n\nOk let’s move to replication of the first two regression models in the table:\n\n\n\nTry to code these on your own, then click for the solution\n\n\nlm1 &lt;- lm(hate_violence_means ~ MateComp_cont, data=dat_use)\n\nlm2 &lt;- lm(hate_violence_means ~ MateComp_cont + JobComp_cont + LifeSatis_cont, data=dat_use)\n\n\nNow, let’s compare the summary output of each output.\n\n\nTry on your own, then click for the solution\n\n\nsummary(lm1)\n\n\nCall:\nlm(formula = hate_violence_means ~ MateComp_cont, data = dat_use)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.6804 -0.3694 -0.3694  0.6306  2.6306 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    0.93235    0.03302   28.24   &lt;2e-16 ***\nMateComp_cont  0.43702    0.01635   26.73   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7993 on 3017 degrees of freedom\nMultiple R-squared:  0.1915,    Adjusted R-squared:  0.1912 \nF-statistic: 714.6 on 1 and 3017 DF,  p-value: &lt; 2.2e-16\n\nsummary(lm2)\n\n\nCall:\nlm(formula = hate_violence_means ~ MateComp_cont + JobComp_cont + \n    LifeSatis_cont, data = dat_use)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.8275 -0.4783 -0.1842  0.3171  2.8452 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     0.788623   0.057849   13.63   &lt;2e-16 ***\nMateComp_cont   0.263437   0.020261   13.00   &lt;2e-16 ***\nJobComp_cont    0.249956   0.018672   13.39   &lt;2e-16 ***\nLifeSatis_cont -0.014725   0.006292   -2.34   0.0193 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7751 on 3015 degrees of freedom\nMultiple R-squared:  0.2403,    Adjusted R-squared:  0.2395 \nF-statistic: 317.9 on 3 and 3015 DF,  p-value: &lt; 2.2e-16\n\n\n\nQuestions about the output\n\nHow should we interpret the coefficients?\n\nDo they support the researchers’ hypotheses?\n\nHow would we extract confidence intervals from the coefficients?\nHow should we interpret the goodness of fit statistics at the bottom of the output?\n\nAdditional Models We can also run regressions with even more covariates, as the authors do in models 3-6 in the paper.\n\n\nClick to reveal regression code below.\n\n\nlm3 &lt;- lm(hate_violence_means ~ MateComp_cont + JobComp_cont + LifeSatis_cont + \n            factor(age_group) +     # age group\n            factor(gender) +     # gender \n            factor(state) +     # state  \n            factor(citizenship) +    # german citizen\n            factor(marital) +    # marital status\n            factor(religion) +    # religious affiliation\n            eduyrs +    # education\n            factor(occupation) +    # main activity\n            factor(income) +   # income\n            factor(household_size) +   # household size\n            factor(self_econ),    # subjective social status\n          data=dat_use)\n\nlm4 &lt;- lm(hate_violence_means ~ MateComp_cont + JobComp_cont + LifeSatis_cont + \n            factor(age_group) +   # age group\n            factor(gender) +   # gender \n            factor(state) +   # state  \n            factor(citizenship) +  # german citizen\n            factor(marital) +  # marital status\n            factor(religion) +  # religious affiliation\n            eduyrs +  # education\n            factor(occupation) +  # main activity\n            factor(income) + # income\n            factor(household_size) + # household size\n            factor(self_econ) + # subjective social status\n            factor(ref_integrating) + # Refugee Index (National-level; Q73) 8 in total\n            factor(ref_citizenship) + factor(ref_reduce) + factor(ref_moredone) + factor(ref_cultgiveup) + \n            factor(ref_economy) + factor(ref_crime) + factor(ref_terror),\n          data=dat_use)\n\nlm5 &lt;- lm(hate_violence_means ~ MateComp_cont + JobComp_cont + LifeSatis_cont + \n            factor(age_group) +      # age group\n            factor(gender) +      # gender \n            factor(state) +      # state  \n            factor(citizenship) +     # german citizen\n            factor(marital) +     # marital status\n            factor(religion) +     # religious affiliation\n            eduyrs + # education\n            factor(occupation) +     # main activity\n            factor(income) +    # income\n            factor(household_size) +    # household size\n            factor(self_econ) +    # subjective social status\n            factor(ref_integrating) + # Refugee Index (National-level; Q73) 8 in total\n            factor(ref_citizenship) + factor(ref_reduce) + factor(ref_moredone) + factor(ref_cultgiveup) + \n            factor(ref_economy) + factor(ref_crime) + factor(ref_terror) + \n            factor(ref_loc_services) +    # Refugee Index (Local, Q75)\n            factor(ref_loc_economy) + factor(ref_loc_crime) + factor(ref_loc_culture) + factor(ref_loc_islam) + \n            factor(ref_loc_schools) + factor(ref_loc_housing) + factor(ref_loc_wayoflife), ## end\n          data=dat_use)\n\nformula.5 &lt;- \n  as.character(\"hate_violence_means ~ MateComp_cont + JobComp_cont + \n               LifeSatis_cont +  factor(age_group) + factor(gender) + \n               factor(state) + factor(citizenship) + factor(marital) + \n               factor(religion) + eduyrs + factor(occupation) + \n               factor(income) + factor(household_size) + factor(self_econ) + \n               factor(ref_integrating) + factor(ref_citizenship) + factor(ref_reduce) + \n               factor(ref_moredone) + factor(ref_cultgiveup) + \n               factor(ref_economy) + factor(ref_crime) + factor(ref_terror)  + \n               factor(ref_loc_services) +  factor(ref_loc_economy) + factor(ref_loc_crime) + \n               factor(ref_loc_culture) + factor(ref_loc_islam) + \n               factor(ref_loc_schools) + factor(ref_loc_housing) + factor(ref_loc_wayoflife)\")\n\nformula.6 &lt;- paste(formula.5, \"factor(distance_ref) + factor(settle_ref)\", \n                   \"lrscale + afd + muslim_ind + afd_ind + contact_ind\", \n                   sep=\"+\", collapse=\"+\") \n\nlm6 &lt;- lm(as.formula(formula.6), data=dat_use)\n\n\n\n\nTable 1: Mate Competition Predicts Support for Hate Crime.\n\n\n\n\n \n\n\nModel 1\n\n\nModel 2\n\n\nModel 3\n\n\nModel 4\n\n\nModel 5\n\n\nModel 6\n\n\n\n\n\n\n(Intercept)\n\n\n0.9323***\n\n\n0.7886***\n\n\n1.3982***\n\n\n1.4437***\n\n\n1.4372***\n\n\n1.3878***\n\n\n\n\n \n\n\n(0.0330)\n\n\n(0.0578)\n\n\n(0.2293)\n\n\n(0.2296)\n\n\n(0.2388)\n\n\n(0.2372)\n\n\n\n\nMateComp_cont\n\n\n0.4370***\n\n\n0.2634***\n\n\n0.2361***\n\n\n0.2064***\n\n\n0.1848***\n\n\n0.1550***\n\n\n\n\n \n\n\n(0.0163)\n\n\n(0.0203)\n\n\n(0.0206)\n\n\n(0.0194)\n\n\n(0.0195)\n\n\n(0.0189)\n\n\n\n\nJobComp_cont\n\n\n \n\n\n0.2500***\n\n\n0.2358***\n\n\n0.0772***\n\n\n0.0650***\n\n\n0.0559***\n\n\n\n\n \n\n\n \n\n\n(0.0187)\n\n\n(0.0189)\n\n\n(0.0195)\n\n\n(0.0196)\n\n\n(0.0189)\n\n\n\n\nLifeSatis_cont\n\n\n \n\n\n-0.0147**\n\n\n-0.0136*\n\n\n-0.0034\n\n\n-0.0020\n\n\n-0.0001\n\n\n\n\n \n\n\n \n\n\n(0.0063)\n\n\n(0.0070)\n\n\n(0.0065)\n\n\n(0.0065)\n\n\n(0.0062)\n\n\n\n\nfactor(age_group)30-39\n\n\n \n\n\n \n\n\n-0.1323**\n\n\n-0.1800***\n\n\n-0.1821***\n\n\n-0.1957***\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.0525)\n\n\n(0.0489)\n\n\n(0.0488)\n\n\n(0.0471)\n\n\n\n\nfactor(age_group)40-49\n\n\n \n\n\n \n\n\n-0.2088***\n\n\n-0.2771***\n\n\n-0.2709***\n\n\n-0.2808***\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.0525)\n\n\n(0.0490)\n\n\n(0.0490)\n\n\n(0.0474)\n\n\n\n\nfactor(age_group)50-59\n\n\n \n\n\n \n\n\n-0.2876***\n\n\n-0.3621***\n\n\n-0.3480***\n\n\n-0.3580***\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.0535)\n\n\n(0.0501)\n\n\n(0.0502)\n\n\n(0.0486)\n\n\n\n\nfactor(age_group)60 and older\n\n\n \n\n\n \n\n\n-0.3362***\n\n\n-0.3427***\n\n\n-0.3199***\n\n\n-0.3073***\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.0678)\n\n\n(0.0631)\n\n\n(0.0631)\n\n\n(0.0610)\n\n\n\n\nfactor(gender)Female\n\n\n \n\n\n \n\n\n-0.0247\n\n\n-0.0528*\n\n\n-0.0451\n\n\n-0.0233\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.0299)\n\n\n(0.0281)\n\n\n(0.0282)\n\n\n(0.0272)\n\n\n\n\nfactor(state)Bayern\n\n\n \n\n\n \n\n\n0.0097\n\n\n-0.0168\n\n\n-0.0148\n\n\n-0.0229\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.0531)\n\n\n(0.0494)\n\n\n(0.0491)\n\n\n(0.0474)\n\n\n\n\nfactor(state)Berlin\n\n\n \n\n\n \n\n\n0.0106\n\n\n-0.0023\n\n\n-0.0259\n\n\n0.0037\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.0776)\n\n\n(0.0722)\n\n\n(0.0720)\n\n\n(0.0706)\n\n\n\n\nfactor(state)Brandenburg\n\n\n \n\n\n \n\n\n-0.1572*\n\n\n-0.1023\n\n\n-0.0949\n\n\n-0.1082\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.0896)\n\n\n(0.0833)\n\n\n(0.0834)\n\n\n(0.0805)\n\n\n\n\nfactor(state)Bremen\n\n\n \n\n\n \n\n\n-0.1266\n\n\n-0.1252\n\n\n-0.1750\n\n\n-0.0508\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.1531)\n\n\n(0.1423)\n\n\n(0.1415)\n\n\n(0.1365)\n\n\n\n\nfactor(state)Hamburg\n\n\n \n\n\n \n\n\n-0.0208\n\n\n-0.0140\n\n\n-0.0255\n\n\n-0.0269\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.1016)\n\n\n(0.0946)\n\n\n(0.0941)\n\n\n(0.0914)\n\n\n\n\nfactor(state)Hessen\n\n\n \n\n\n \n\n\n-0.1207*\n\n\n-0.0931\n\n\n-0.0766\n\n\n-0.0853\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.0647)\n\n\n(0.0604)\n\n\n(0.0601)\n\n\n(0.0578)\n\n\n\n\nfactor(state)Mecklenburg-Vorpommern\n\n\n \n\n\n \n\n\n-0.0849\n\n\n-0.1008\n\n\n-0.1015\n\n\n-0.1572*\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.1035)\n\n\n(0.0961)\n\n\n(0.0959)\n\n\n(0.0928)\n\n\n\n\nfactor(state)Niedersachsen\n\n\n \n\n\n \n\n\n-0.0993\n\n\n-0.1052*\n\n\n-0.1055*\n\n\n-0.1190**\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.0607)\n\n\n(0.0564)\n\n\n(0.0561)\n\n\n(0.0543)\n\n\n\n\nfactor(state)Nordrhein-Westfalen\n\n\n \n\n\n \n\n\n-0.0299\n\n\n-0.0277\n\n\n-0.0414\n\n\n-0.0414\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.0501)\n\n\n(0.0465)\n\n\n(0.0465)\n\n\n(0.0450)\n\n\n\n\nfactor(state)Rheinland-Pfalz\n\n\n \n\n\n \n\n\n-0.1178\n\n\n-0.1137\n\n\n-0.1089\n\n\n-0.1407**\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.0750)\n\n\n(0.0700)\n\n\n(0.0697)\n\n\n(0.0675)\n\n\n\n\nfactor(state)Saarland\n\n\n \n\n\n \n\n\n-0.0264\n\n\n0.0227\n\n\n0.0353\n\n\n-0.0250\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.1293)\n\n\n(0.1203)\n\n\n(0.1199)\n\n\n(0.1162)\n\n\n\n\nfactor(state)Sachsen\n\n\n \n\n\n \n\n\n-0.0357\n\n\n-0.0813\n\n\n-0.1118\n\n\n-0.1470**\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.0734)\n\n\n(0.0683)\n\n\n(0.0684)\n\n\n(0.0662)\n\n\n\n\nfactor(state)Sachsen-Anhalt\n\n\n \n\n\n \n\n\n-0.0193\n\n\n-0.0811\n\n\n-0.0765\n\n\n-0.1024\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.0927)\n\n\n(0.0862)\n\n\n(0.0863)\n\n\n(0.0836)\n\n\n\n\nfactor(state)Schleswig-Holstein\n\n\n \n\n\n \n\n\n-0.2402***\n\n\n-0.1693**\n\n\n-0.1725**\n\n\n-0.1839**\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.0862)\n\n\n(0.0806)\n\n\n(0.0802)\n\n\n(0.0773)\n\n\n\n\nfactor(state)Thringen\n\n\n \n\n\n \n\n\n0.0090\n\n\n-0.0076\n\n\n-0.0081\n\n\n-0.0654\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.0957)\n\n\n(0.0889)\n\n\n(0.0887)\n\n\n(0.0858)\n\n\n\n\nfactor(citizenship)1\n\n\n \n\n\n \n\n\n-0.0621\n\n\n-0.0831\n\n\n-0.0739\n\n\n-0.0314\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.1064)\n\n\n(0.0990)\n\n\n(0.0983)\n\n\n(0.0946)\n\n\n\n\nfactor(marital)With partner, not living together\n\n\n \n\n\n \n\n\n0.0825\n\n\n0.0323\n\n\n0.0145\n\n\n0.0099\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.0572)\n\n\n(0.0532)\n\n\n(0.0529)\n\n\n(0.0509)\n\n\n\n\nfactor(marital)With partner, living together\n\n\n \n\n\n \n\n\n0.0968*\n\n\n0.0570\n\n\n0.0582\n\n\n0.0342\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.0562)\n\n\n(0.0524)\n\n\n(0.0521)\n\n\n(0.0501)\n\n\n\n\nfactor(marital)Married\n\n\n \n\n\n \n\n\n0.0884\n\n\n0.0487\n\n\n0.0509\n\n\n0.0165\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.0538)\n\n\n(0.0500)\n\n\n(0.0497)\n\n\n(0.0479)\n\n\n\n\nfactor(marital)Registered partnership\n\n\n \n\n\n \n\n\n0.0982\n\n\n0.1345\n\n\n0.1601\n\n\n0.1976\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.1880)\n\n\n(0.1753)\n\n\n(0.1744)\n\n\n(0.1679)\n\n\n\n\nfactor(marital)Divorced / separated\n\n\n \n\n\n \n\n\n0.1150*\n\n\n0.0938\n\n\n0.0853\n\n\n0.0877\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.0616)\n\n\n(0.0573)\n\n\n(0.0569)\n\n\n(0.0549)\n\n\n\n\nfactor(marital)Widowed\n\n\n \n\n\n \n\n\n0.1612*\n\n\n0.1556*\n\n\n0.1309\n\n\n0.1243\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.0920)\n\n\n(0.0855)\n\n\n(0.0855)\n\n\n(0.0824)\n\n\n\n\nfactor(religion)Roman Catholic\n\n\n \n\n\n \n\n\n-0.0034\n\n\n-0.0300\n\n\n-0.0333\n\n\n-0.0713*\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.0407)\n\n\n(0.0378)\n\n\n(0.0377)\n\n\n(0.0364)\n\n\n\n\nfactor(religion)Protestant\n\n\n \n\n\n \n\n\n-0.0640*\n\n\n-0.0396\n\n\n-0.0249\n\n\n-0.0556*\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.0375)\n\n\n(0.0348)\n\n\n(0.0349)\n\n\n(0.0337)\n\n\n\n\nfactor(religion)Protestant Free Church\n\n\n \n\n\n \n\n\n-0.0225\n\n\n-0.0240\n\n\n-0.0170\n\n\n-0.0780\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.1022)\n\n\n(0.0951)\n\n\n(0.0945)\n\n\n(0.0911)\n\n\n\n\nfactor(religion)Other Protestant\n\n\n \n\n\n \n\n\n0.7822**\n\n\n0.9286***\n\n\n0.9234***\n\n\n0.8768**\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.3855)\n\n\n(0.3587)\n\n\n(0.3565)\n\n\n(0.3441)\n\n\n\n\nfactor(religion)Eastern Orthodox\n\n\n \n\n\n \n\n\n0.2751\n\n\n0.1666\n\n\n0.1350\n\n\n0.1455\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.1869)\n\n\n(0.1744)\n\n\n(0.1735)\n\n\n(0.1672)\n\n\n\n\nfactor(religion)Other Christian\n\n\n \n\n\n \n\n\n-0.0119\n\n\n0.0406\n\n\n0.0645\n\n\n0.0954\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.1633)\n\n\n(0.1518)\n\n\n(0.1514)\n\n\n(0.1458)\n\n\n\n\nfactor(religion)Jewish\n\n\n \n\n\n \n\n\n0.0827\n\n\n-0.1329\n\n\n-0.0855\n\n\n-0.2074\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.3460)\n\n\n(0.3217)\n\n\n(0.3202)\n\n\n(0.3081)\n\n\n\n\nfactor(religion)Muslim\n\n\n \n\n\n \n\n\n-0.0578\n\n\n0.0586\n\n\n0.0046\n\n\n0.0906\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.1667)\n\n\n(0.1554)\n\n\n(0.1549)\n\n\n(0.1509)\n\n\n\n\nfactor(religion)Eastern religion (Buddhism, Hinduism, Sikhism, Shinto, Tao, etc.)\n\n\n \n\n\n \n\n\n-0.0026\n\n\n0.0043\n\n\n0.0289\n\n\n0.0138\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.1215)\n\n\n(0.1130)\n\n\n(0.1129)\n\n\n(0.1086)\n\n\n\n\nfactor(religion)Other non-Christian religion\n\n\n \n\n\n \n\n\n0.3904*\n\n\n0.4759**\n\n\n0.4412**\n\n\n0.2675\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.2333)\n\n\n(0.2175)\n\n\n(0.2168)\n\n\n(0.2089)\n\n\n\n\nfactor(religion)Christian, but not close to a particular religious community\n\n\n \n\n\n \n\n\n-0.0270\n\n\n0.0102\n\n\n0.0083\n\n\n-0.0177\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.0611)\n\n\n(0.0567)\n\n\n(0.0566)\n\n\n(0.0544)\n\n\n\n\nfactor(religion)No answer\n\n\n \n\n\n \n\n\n0.1273\n\n\n0.2257**\n\n\n0.2106**\n\n\n0.1926*\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.1116)\n\n\n(0.1039)\n\n\n(0.1036)\n\n\n(0.0999)\n\n\n\n\neduyrs\n\n\n \n\n\n \n\n\n-0.0179***\n\n\n-0.0139***\n\n\n-0.0121***\n\n\n-0.0088**\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.0042)\n\n\n(0.0039)\n\n\n(0.0039)\n\n\n(0.0038)\n\n\n\n\nfactor(occupation)Parental leave\n\n\n \n\n\n \n\n\n0.1940\n\n\n0.1368\n\n\n0.1319\n\n\n0.1606\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.1589)\n\n\n(0.1478)\n\n\n(0.1473)\n\n\n(0.1417)\n\n\n\n\nfactor(occupation)In schooling / vocational training, student\n\n\n \n\n\n \n\n\n-0.0690\n\n\n-0.1196\n\n\n-0.1279\n\n\n-0.1244\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.0926)\n\n\n(0.0862)\n\n\n(0.0859)\n\n\n(0.0828)\n\n\n\n\nfactor(occupation)Unemployed / seeking work\n\n\n \n\n\n \n\n\n0.0088\n\n\n-0.0339\n\n\n-0.0420\n\n\n-0.0451\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.1080)\n\n\n(0.1004)\n\n\n(0.0999)\n\n\n(0.0961)\n\n\n\n\nfactor(occupation)Retired\n\n\n \n\n\n \n\n\n0.1055\n\n\n0.0493\n\n\n0.0470\n\n\n0.0298\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.0816)\n\n\n(0.0758)\n\n\n(0.0755)\n\n\n(0.0726)\n\n\n\n\nfactor(occupation)Permanently sick or disabled\n\n\n \n\n\n \n\n\n-0.1574\n\n\n-0.1490\n\n\n-0.1458\n\n\n-0.1507\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.1287)\n\n\n(0.1198)\n\n\n(0.1194)\n\n\n(0.1149)\n\n\n\n\nfactor(occupation)Unskilled worker\n\n\n \n\n\n \n\n\n0.1800*\n\n\n0.0893\n\n\n0.0642\n\n\n0.0221\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.0934)\n\n\n(0.0870)\n\n\n(0.0865)\n\n\n(0.0833)\n\n\n\n\nfactor(occupation)Skilled worker\n\n\n \n\n\n \n\n\n0.2026**\n\n\n0.1379*\n\n\n0.1393*\n\n\n0.1073\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.0871)\n\n\n(0.0812)\n\n\n(0.0808)\n\n\n(0.0778)\n\n\n\n\nfactor(occupation)Employee in low / medium position\n\n\n \n\n\n \n\n\n0.1065\n\n\n0.0560\n\n\n0.0524\n\n\n0.0653\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.0740)\n\n\n(0.0688)\n\n\n(0.0685)\n\n\n(0.0659)\n\n\n\n\nfactor(occupation)Employee in high position\n\n\n \n\n\n \n\n\n0.0536\n\n\n-0.0116\n\n\n-0.0165\n\n\n-0.0292\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.0824)\n\n\n(0.0766)\n\n\n(0.0763)\n\n\n(0.0735)\n\n\n\n\nfactor(occupation)Civil servant\n\n\n \n\n\n \n\n\n-0.0009\n\n\n-0.1061\n\n\n-0.1342\n\n\n-0.1687\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.1307)\n\n\n(0.1214)\n\n\n(0.1209)\n\n\n(0.1166)\n\n\n\n\nfactor(occupation)Senior civil servant\n\n\n \n\n\n \n\n\n0.0173\n\n\n0.0085\n\n\n0.0262\n\n\n-0.0256\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.1481)\n\n\n(0.1380)\n\n\n(0.1372)\n\n\n(0.1325)\n\n\n\n\nfactor(occupation)Senior civil servant &lt;96&gt; highest level\n\n\n \n\n\n \n\n\n-0.0083\n\n\n-0.1016\n\n\n-0.0793\n\n\n-0.0611\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.1262)\n\n\n(0.1174)\n\n\n(0.1169)\n\n\n(0.1127)\n\n\n\n\nfactor(occupation)Self-employed / freelancer\n\n\n \n\n\n \n\n\n0.1171\n\n\n0.0323\n\n\n0.0396\n\n\n0.0693\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.0889)\n\n\n(0.0828)\n\n\n(0.0824)\n\n\n(0.0794)\n\n\n\n\nfactor(occupation)Other\n\n\n \n\n\n \n\n\n0.2269\n\n\n0.0467\n\n\n0.0232\n\n\n0.0080\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.1724)\n\n\n(0.1607)\n\n\n(0.1601)\n\n\n(0.1540)\n\n\n\n\nfactor(income)500 to below 1,000 &lt;80&gt;\n\n\n \n\n\n \n\n\n0.0260\n\n\n0.0768\n\n\n0.0750\n\n\n-0.0028\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.1024)\n\n\n(0.0953)\n\n\n(0.0948)\n\n\n(0.0914)\n\n\n\n\nfactor(income)1,000 to below 1,500 &lt;80&gt;\n\n\n \n\n\n \n\n\n0.0677\n\n\n0.0714\n\n\n0.0642\n\n\n-0.0274\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.1011)\n\n\n(0.0943)\n\n\n(0.0937)\n\n\n(0.0904)\n\n\n\n\nfactor(income)1,500 to below 2,000 &lt;80&gt;\n\n\n \n\n\n \n\n\n0.1360\n\n\n0.1289\n\n\n0.1319\n\n\n0.0564\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.1024)\n\n\n(0.0955)\n\n\n(0.0949)\n\n\n(0.0914)\n\n\n\n\nfactor(income)2,000 to below 2,500 &lt;80&gt;\n\n\n \n\n\n \n\n\n0.1320\n\n\n0.1155\n\n\n0.1146\n\n\n0.0028\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.1045)\n\n\n(0.0974)\n\n\n(0.0969)\n\n\n(0.0935)\n\n\n\n\nfactor(income)2,500 to below 3,000 &lt;80&gt;\n\n\n \n\n\n \n\n\n0.0479\n\n\n0.0606\n\n\n0.0615\n\n\n-0.0466\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.1071)\n\n\n(0.0998)\n\n\n(0.0992)\n\n\n(0.0956)\n\n\n\n\nfactor(income)3,000 to below 3,500 &lt;80&gt;\n\n\n \n\n\n \n\n\n0.1659\n\n\n0.1531\n\n\n0.1557\n\n\n0.0384\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.1108)\n\n\n(0.1031)\n\n\n(0.1025)\n\n\n(0.0989)\n\n\n\n\nfactor(income)3,500 to below 4,000 &lt;80&gt;\n\n\n \n\n\n \n\n\n0.2256**\n\n\n0.2133**\n\n\n0.2101**\n\n\n0.0785\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.1138)\n\n\n(0.1059)\n\n\n(0.1054)\n\n\n(0.1017)\n\n\n\n\nfactor(income)4,000 to below 4,500 &lt;80&gt;\n\n\n \n\n\n \n\n\n0.0770\n\n\n0.0396\n\n\n0.0271\n\n\n-0.0996\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.1211)\n\n\n(0.1127)\n\n\n(0.1121)\n\n\n(0.1081)\n\n\n\n\nfactor(income)4,500 to below 5,000 &lt;80&gt;\n\n\n \n\n\n \n\n\n0.2446*\n\n\n0.1782\n\n\n0.1755\n\n\n0.0431\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.1274)\n\n\n(0.1188)\n\n\n(0.1182)\n\n\n(0.1140)\n\n\n\n\nfactor(income)5,000 or more &lt;80&gt;\n\n\n \n\n\n \n\n\n0.2017\n\n\n0.1350\n\n\n0.1128\n\n\n0.0250\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.1227)\n\n\n(0.1143)\n\n\n(0.1136)\n\n\n(0.1095)\n\n\n\n\nfactor(income)No answer\n\n\n \n\n\n \n\n\n0.0325\n\n\n0.0498\n\n\n0.0501\n\n\n-0.0453\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.1062)\n\n\n(0.0990)\n\n\n(0.0984)\n\n\n(0.0949)\n\n\n\n\nfactor(household_size)2\n\n\n \n\n\n \n\n\n0.0362\n\n\n0.0390\n\n\n0.0316\n\n\n0.0617\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.0511)\n\n\n(0.0475)\n\n\n(0.0473)\n\n\n(0.0457)\n\n\n\n\nfactor(household_size)3\n\n\n \n\n\n \n\n\n0.0404\n\n\n0.0374\n\n\n0.0403\n\n\n0.0675\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.0574)\n\n\n(0.0535)\n\n\n(0.0533)\n\n\n(0.0515)\n\n\n\n\nfactor(household_size)4\n\n\n \n\n\n \n\n\n0.0289\n\n\n0.0129\n\n\n0.0114\n\n\n0.0516\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.0647)\n\n\n(0.0602)\n\n\n(0.0599)\n\n\n(0.0578)\n\n\n\n\nfactor(household_size)5\n\n\n \n\n\n \n\n\n0.0161\n\n\n0.0255\n\n\n0.0347\n\n\n0.0345\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.1046)\n\n\n(0.0972)\n\n\n(0.0968)\n\n\n(0.0934)\n\n\n\n\nfactor(household_size)6\n\n\n \n\n\n \n\n\n0.3162*\n\n\n0.3629**\n\n\n0.4030**\n\n\n0.3646**\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.1793)\n\n\n(0.1666)\n\n\n(0.1658)\n\n\n(0.1599)\n\n\n\n\nfactor(household_size)7\n\n\n \n\n\n \n\n\n0.0387\n\n\n0.0311\n\n\n0.0495\n\n\n-0.0145\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.3181)\n\n\n(0.2957)\n\n\n(0.2939)\n\n\n(0.2838)\n\n\n\n\nfactor(household_size)8\n\n\n \n\n\n \n\n\n0.7654**\n\n\n0.9534***\n\n\n0.8352**\n\n\n0.7004**\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.3876)\n\n\n(0.3615)\n\n\n(0.3615)\n\n\n(0.3479)\n\n\n\n\nfactor(household_size)12\n\n\n \n\n\n \n\n\n-0.0289\n\n\n0.0946\n\n\n-0.0435\n\n\n-0.1011\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.7761)\n\n\n(0.7217)\n\n\n(0.7188)\n\n\n(0.6915)\n\n\n\n\nfactor(self_econ)2\n\n\n \n\n\n \n\n\n-0.0271\n\n\n-0.1166\n\n\n-0.1141\n\n\n-0.0368\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.1629)\n\n\n(0.1517)\n\n\n(0.1511)\n\n\n(0.1457)\n\n\n\n\nfactor(self_econ)3\n\n\n \n\n\n \n\n\n-0.2019\n\n\n-0.2069\n\n\n-0.2058\n\n\n-0.1859\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.1513)\n\n\n(0.1408)\n\n\n(0.1405)\n\n\n(0.1353)\n\n\n\n\nfactor(self_econ)4\n\n\n \n\n\n \n\n\n-0.1501\n\n\n-0.1394\n\n\n-0.1358\n\n\n-0.1343\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.1510)\n\n\n(0.1405)\n\n\n(0.1403)\n\n\n(0.1351)\n\n\n\n\nfactor(self_econ)5\n\n\n \n\n\n \n\n\n-0.2279\n\n\n-0.1700\n\n\n-0.1705\n\n\n-0.1569\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.1491)\n\n\n(0.1389)\n\n\n(0.1386)\n\n\n(0.1335)\n\n\n\n\nfactor(self_econ)6\n\n\n \n\n\n \n\n\n-0.2812*\n\n\n-0.2191\n\n\n-0.2186\n\n\n-0.2051\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.1503)\n\n\n(0.1400)\n\n\n(0.1397)\n\n\n(0.1345)\n\n\n\n\nfactor(self_econ)7\n\n\n \n\n\n \n\n\n-0.3444**\n\n\n-0.2527*\n\n\n-0.2484*\n\n\n-0.2383*\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.1518)\n\n\n(0.1415)\n\n\n(0.1414)\n\n\n(0.1360)\n\n\n\n\nfactor(self_econ)8\n\n\n \n\n\n \n\n\n-0.2107\n\n\n-0.1598\n\n\n-0.1765\n\n\n-0.1973\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.1573)\n\n\n(0.1466)\n\n\n(0.1462)\n\n\n(0.1407)\n\n\n\n\nfactor(self_econ)9\n\n\n \n\n\n \n\n\n-0.1747\n\n\n-0.0476\n\n\n-0.0684\n\n\n-0.0658\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.1933)\n\n\n(0.1804)\n\n\n(0.1801)\n\n\n(0.1731)\n\n\n\n\nfactor(self_econ)10 ( TOP )\n\n\n \n\n\n \n\n\n0.3679\n\n\n0.2960\n\n\n0.2701\n\n\n0.2253\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.2349)\n\n\n(0.2192)\n\n\n(0.2183)\n\n\n(0.2100)\n\n\n\n\nfactor(self_econ)0 ( BOTTOM )\n\n\n \n\n\n \n\n\n-0.0023\n\n\n-0.0450\n\n\n-0.0278\n\n\n0.0017\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.2077)\n\n\n(0.1933)\n\n\n(0.1925)\n\n\n(0.1853)\n\n\n\n\nfactor(ref_integrating)2\n\n\n \n\n\n \n\n\n \n\n\n-0.0585\n\n\n-0.0390\n\n\n-0.0304\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0913)\n\n\n(0.0924)\n\n\n(0.0890)\n\n\n\n\nfactor(ref_integrating)3\n\n\n \n\n\n \n\n\n \n\n\n-0.0921\n\n\n-0.0692\n\n\n-0.0772\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0935)\n\n\n(0.0951)\n\n\n(0.0918)\n\n\n\n\nfactor(ref_integrating)4\n\n\n \n\n\n \n\n\n \n\n\n0.0787\n\n\n0.0928\n\n\n0.0585\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0998)\n\n\n(0.1015)\n\n\n(0.0980)\n\n\n\n\nfactor(ref_citizenship)2\n\n\n \n\n\n \n\n\n \n\n\n0.0020\n\n\n-0.0202\n\n\n-0.0245\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0444)\n\n\n(0.0447)\n\n\n(0.0429)\n\n\n\n\nfactor(ref_citizenship)3\n\n\n \n\n\n \n\n\n \n\n\n0.0893*\n\n\n0.0720\n\n\n0.0349\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0493)\n\n\n(0.0497)\n\n\n(0.0480)\n\n\n\n\nfactor(ref_citizenship)4\n\n\n \n\n\n \n\n\n \n\n\n0.1626***\n\n\n0.1425**\n\n\n0.1000*\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0571)\n\n\n(0.0581)\n\n\n(0.0561)\n\n\n\n\nfactor(ref_reduce)2\n\n\n \n\n\n \n\n\n \n\n\n-0.0253\n\n\n-0.0103\n\n\n-0.0041\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0595)\n\n\n(0.0599)\n\n\n(0.0576)\n\n\n\n\nfactor(ref_reduce)3\n\n\n \n\n\n \n\n\n \n\n\n0.0162\n\n\n0.0326\n\n\n-0.0106\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0636)\n\n\n(0.0644)\n\n\n(0.0623)\n\n\n\n\nfactor(ref_reduce)4\n\n\n \n\n\n \n\n\n \n\n\n0.0354\n\n\n0.0465\n\n\n-0.1045\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0744)\n\n\n(0.0754)\n\n\n(0.0736)\n\n\n\n\nfactor(ref_moredone)2\n\n\n \n\n\n \n\n\n \n\n\n0.0930*\n\n\n0.0802*\n\n\n0.0559\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0482)\n\n\n(0.0484)\n\n\n(0.0467)\n\n\n\n\nfactor(ref_moredone)3\n\n\n \n\n\n \n\n\n \n\n\n0.1947***\n\n\n0.1834***\n\n\n0.0920*\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0528)\n\n\n(0.0532)\n\n\n(0.0520)\n\n\n\n\nfactor(ref_moredone)4\n\n\n \n\n\n \n\n\n \n\n\n0.3050***\n\n\n0.2874***\n\n\n0.1561**\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0618)\n\n\n(0.0623)\n\n\n(0.0609)\n\n\n\n\nfactor(ref_cultgiveup)2\n\n\n \n\n\n \n\n\n \n\n\n-0.0125\n\n\n-0.0212\n\n\n-0.0309\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0586)\n\n\n(0.0590)\n\n\n(0.0569)\n\n\n\n\nfactor(ref_cultgiveup)3\n\n\n \n\n\n \n\n\n \n\n\n0.0145\n\n\n-0.0054\n\n\n-0.0505\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0582)\n\n\n(0.0587)\n\n\n(0.0568)\n\n\n\n\nfactor(ref_cultgiveup)4\n\n\n \n\n\n \n\n\n \n\n\n0.1507**\n\n\n0.1281*\n\n\n0.0733\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0652)\n\n\n(0.0656)\n\n\n(0.0636)\n\n\n\n\nfactor(ref_economy)2\n\n\n \n\n\n \n\n\n \n\n\n0.0237\n\n\n0.0456\n\n\n0.0510\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0549)\n\n\n(0.0582)\n\n\n(0.0561)\n\n\n\n\nfactor(ref_economy)3\n\n\n \n\n\n \n\n\n \n\n\n-0.0004\n\n\n0.0343\n\n\n0.0145\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0606)\n\n\n(0.0663)\n\n\n(0.0638)\n\n\n\n\nfactor(ref_economy)4\n\n\n \n\n\n \n\n\n \n\n\n0.1657**\n\n\n0.2379***\n\n\n0.1524**\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0702)\n\n\n(0.0778)\n\n\n(0.0750)\n\n\n\n\nfactor(ref_crime)2\n\n\n \n\n\n \n\n\n \n\n\n0.0183\n\n\n-0.0033\n\n\n-0.0037\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0606)\n\n\n(0.0645)\n\n\n(0.0620)\n\n\n\n\nfactor(ref_crime)3\n\n\n \n\n\n \n\n\n \n\n\n0.0794\n\n\n-0.0061\n\n\n-0.0290\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0661)\n\n\n(0.0715)\n\n\n(0.0688)\n\n\n\n\nfactor(ref_crime)4\n\n\n \n\n\n \n\n\n \n\n\n0.2506***\n\n\n0.1343\n\n\n0.0431\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0774)\n\n\n(0.0835)\n\n\n(0.0806)\n\n\n\n\nfactor(ref_terror)2\n\n\n \n\n\n \n\n\n \n\n\n-0.0689\n\n\n-0.0975*\n\n\n-0.1060*\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0568)\n\n\n(0.0578)\n\n\n(0.0556)\n\n\n\n\nfactor(ref_terror)3\n\n\n \n\n\n \n\n\n \n\n\n-0.0330\n\n\n-0.0818\n\n\n-0.1054*\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0608)\n\n\n(0.0617)\n\n\n(0.0595)\n\n\n\n\nfactor(ref_terror)4\n\n\n \n\n\n \n\n\n \n\n\n-0.0338\n\n\n-0.0865\n\n\n-0.1144*\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0707)\n\n\n(0.0715)\n\n\n(0.0689)\n\n\n\n\nfactor(ref_loc_services)2\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n0.0852\n\n\n0.0888\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0680)\n\n\n(0.0653)\n\n\n\n\nfactor(ref_loc_services)3\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n0.0765\n\n\n0.0788\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0681)\n\n\n(0.0655)\n\n\n\n\nfactor(ref_loc_services)4\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n0.0577\n\n\n0.0699\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0761)\n\n\n(0.0732)\n\n\n\n\nfactor(ref_loc_economy)2\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n-0.1186*\n\n\n-0.1209*\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0718)\n\n\n(0.0690)\n\n\n\n\nfactor(ref_loc_economy)3\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n-0.1420*\n\n\n-0.1562**\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0767)\n\n\n(0.0738)\n\n\n\n\nfactor(ref_loc_economy)4\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n-0.2892***\n\n\n-0.2972***\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0864)\n\n\n(0.0832)\n\n\n\n\nfactor(ref_loc_crime)2\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n0.0813\n\n\n0.0727\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0582)\n\n\n(0.0560)\n\n\n\n\nfactor(ref_loc_crime)3\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n0.2474***\n\n\n0.2050***\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0662)\n\n\n(0.0640)\n\n\n\n\nfactor(ref_loc_crime)4\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n0.3110***\n\n\n0.2766***\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0800)\n\n\n(0.0774)\n\n\n\n\nfactor(ref_loc_culture)2\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n0.0051\n\n\n0.0068\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0524)\n\n\n(0.0507)\n\n\n\n\nfactor(ref_loc_culture)3\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n0.0297\n\n\n0.0078\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0612)\n\n\n(0.0594)\n\n\n\n\nfactor(ref_loc_culture)4\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n0.1232*\n\n\n0.0013\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0737)\n\n\n(0.0718)\n\n\n\n\nfactor(ref_loc_islam)2\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n-0.0128\n\n\n-0.0085\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0531)\n\n\n(0.0511)\n\n\n\n\nfactor(ref_loc_islam)3\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n-0.0085\n\n\n-0.0453\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0550)\n\n\n(0.0531)\n\n\n\n\nfactor(ref_loc_islam)4\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n-0.0281\n\n\n-0.1068*\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0660)\n\n\n(0.0638)\n\n\n\n\nfactor(ref_loc_schools)2\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n0.1254\n\n\n0.1084\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0887)\n\n\n(0.0853)\n\n\n\n\nfactor(ref_loc_schools)3\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n0.0552\n\n\n0.0573\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0832)\n\n\n(0.0801)\n\n\n\n\nfactor(ref_loc_schools)4\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n-0.0806\n\n\n-0.0809\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0845)\n\n\n(0.0814)\n\n\n\n\nfactor(ref_loc_housing)2\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n0.0134\n\n\n0.0095\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0586)\n\n\n(0.0566)\n\n\n\n\nfactor(ref_loc_housing)3\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n0.0068\n\n\n0.0008\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0564)\n\n\n(0.0547)\n\n\n\n\nfactor(ref_loc_housing)4\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n0.0432\n\n\n0.0433\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0608)\n\n\n(0.0590)\n\n\n\n\nfactor(ref_loc_wayoflife)2\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n-0.0586\n\n\n-0.0653\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0621)\n\n\n(0.0597)\n\n\n\n\nfactor(ref_loc_wayoflife)3\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n-0.0341\n\n\n-0.0515\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0609)\n\n\n(0.0586)\n\n\n\n\nfactor(ref_loc_wayoflife)4\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n0.0694\n\n\n0.0550\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0707)\n\n\n(0.0682)\n\n\n\n\nfactor(distance_ref)3-5 kilometers\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n-0.0375\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0362)\n\n\n\n\nfactor(distance_ref)6-10 kilometers\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n0.0256\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0405)\n\n\n\n\nfactor(distance_ref)11-20 kilometers\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n0.0165\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0494)\n\n\n\n\nfactor(distance_ref)21-50 kilometers\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n0.0644\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0534)\n\n\n\n\nfactor(distance_ref)More than 50 kilometer\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n0.0568\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0794)\n\n\n\n\nfactor(distance_ref)Don’t know\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n-0.0389\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0418)\n\n\n\n\nfactor(settle_ref)1 &lt;96&gt; 49\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n-0.0133\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0668)\n\n\n\n\nfactor(settle_ref)50 &lt;96&gt; 249\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n-0.0178\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0661)\n\n\n\n\nfactor(settle_ref)250 &lt;96&gt; 499\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n-0.0455\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0694)\n\n\n\n\nfactor(settle_ref)500 &lt;96&gt; 999\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n-0.0213\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0732)\n\n\n\n\nfactor(settle_ref)1000 and more\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n-0.0536\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0687)\n\n\n\n\nlrscale\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n0.0235***\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0078)\n\n\n\n\nafd\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n0.0044***\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0006)\n\n\n\n\nmuslim_ind\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n0.3152***\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0701)\n\n\n\n\nafd_ind\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n0.3390***\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0489)\n\n\n\n\ncontact_ind\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n0.0741\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0522)\n\n\n\n\nR2\n\n\n0.1915\n\n\n0.2403\n\n\n0.2883\n\n\n0.3942\n\n\n0.4097\n\n\n0.4592\n\n\n\n\nAdj. R2\n\n\n0.1912\n\n\n0.2395\n\n\n0.2673\n\n\n0.3712\n\n\n0.3821\n\n\n0.4308\n\n\n\n\nNum. obs.\n\n\n3019\n\n\n3019\n\n\n3008\n\n\n3008\n\n\n3008\n\n\n3008\n\n\n\n\n\n\n***p &lt; 0.01; **p &lt; 0.05; *p &lt; 0.1\n\n\n\n\n\n\nFinal Questions\nEven with all these covariates accounted for, the authors still engage in a discussion about possible violations of the OLS assumptions that could bias their results, as well as potential alternative modelling strategies.\n\nIs their survey representative? They replicate using another polling firm.\nAre there even more alternative explanations?\nIs OLS the right choice?\nValidity (discussed in Gelman and Hill). Does the outcome accurately measure the concept? They consider alternative outcomes and visualize the coefficient results in Figure 4.\n\nMessage: Attacks against refugee homes are sometimes necessary to make it clear to politicians that we have a refugee problem.\nJustified : Hostility against refugees is sometimes justified, even when it ends up in violence.\nPrevent : Xenophobic acts of violence are defensible if they result in fewer refugees settling in town.\nCondemn: Politicians should condemn attacks against refugees more forcefully.\n\n\n\nAdditional Practice Questions.\n\nFind the average expected level of “Only Means” agreement at each level of mate competition. Plot the results. Base these results on lm2.\nFit lm2 using the generalized linear model glm approach (with a normal distribution) instead of the lm\nWhat are some of the conceptual differences between ordinary least squares and maximum likelihood estimation?"
  },
  {
    "objectID": "04-ReviewofOLS.html#footnotes",
    "href": "04-ReviewofOLS.html#footnotes",
    "title": "4  Review of OLS",
    "section": "",
    "text": "Recall this notation means rows by columns, \\(Y\\) is a vector of length \\(n\\) (the number of observations), and since there is only 1 outcome measure, it is 1 column.↩︎\nThis video from Ben Lambert provides additional intuition for understanding OLS in a matrix form and how it can be useful.↩︎"
  },
  {
    "objectID": "05-IntrotoMaximumLikelihood.html#what-is-likelihood",
    "href": "05-IntrotoMaximumLikelihood.html#what-is-likelihood",
    "title": "5  Introduction to MLE",
    "section": "5.1 What is likelihood?",
    "text": "5.1 What is likelihood?\nJust like the derivation of the OLS coefficient estimators \\(\\hat \\beta\\) for the \\(\\beta\\) parameters started with a goal (minimizing least squared errors) in describing the relationship between variables, with likelihood we also start with a goal.\nThe “likelihood” is going to ask the question: What values of the unknown parameters make the data we see least surprising?\nWhen we get into likelihood, we will be drawing more directly on concepts within probability. We start by making a choice about what type of data generating process best describes our outcome data. Eventually, our likelihood function represents the “probability density of the data given the parameters and predictors.” (Definition taken from Gelman et al. 2020, pg. 105).\n\nIn MLE, we are going to choose parameter estimates \\(\\widehat{\\theta}\\) for \\(\\theta\\) that maximize the likelihood that our data came from the particular distribution.\n\nAlready, we are placing a lot of emphasis on the nature of our outcome data. The nature of our likelihood will change depending on if the data are dichotomous (e.g, similar to a set of coin flips that could be head or tails) or a count (e.g., similar to the number of events expected to occur over a certain interval) or more of a continuous numeric distribution with (e.g., where the probabilities of certain levels can be visualized similar to a bell curve). Each of these sets of data are generated through a different process, which is described by a particular probability function.\nExample\nThis introduction is based on Ben Lambert’s video. I highly recommend watching this 8-9 minute video. Below we highlight a few of the key concepts and definitions.\nTake the UK population of 70 million. We have a sample of this, and in our sample some observations are male and some female. How can we use what we have, a sample, to estimate the probability that an individual is male?1\n\nFirst, we can make a judgment about the data generating process. We can suppose there is some probability distribution function that determines the probability is a male or female. A probability density function (PDF for continuous data or PMF for discrete data) tells you the relative probability or likelihood of observing a particular value given the parameters of the distribution.\n\nLet’s call this \\(f(y_i | p)\\) where \\(y_i = 1\\) if male, 0 if female.\nWe will say \\(p\\) is the probability an individual is male. \\(p^{y_i}(1 - p)^{1-y_i}\\)\n\nWe are going to treat this like a toss of a coin, which has a Bernouilli distribution (every probability distribution we are dealing with is going to have an associated formula. You don’t need to memorize these. You can look them up on the internet when needed.) So, \\(f()\\) tells us the probability we would have gotten the value of the observation if we think of our observation as this Bernouilli toss of coin. This is the likelihood for a single observation.\nFor example, we can now plug this into our Bernoulli function for the two possible cases of values for \\(y_i\\).\n\n\\(f(1|p) = p^1(1-p)^{1-1} = p\\) probability an individual is male\n\\(f(0 |p)= p ^0(1-p)^{1-0} = 1-p\\) probability individual is female\n\nNow we want an estimate using our entire sample of observations, not just a single \\(i\\). What if we have \\(n\\) observations? \\(f(y_1, y_2, ... y_n | p)\\). We can write the joint probability as all individual probabilities multiplied together if we assume our observations are independent. This will now represent the likelihood for all observations.\n\n\\(L = P(Y_1 = y_1, Y_2=y_2, ..., Y_n = y_n)= \\prod_{i=1}^n p^y_i(1 - p)^{1-y_i}\\)\nThis answers what is the probability that \\(Y_1\\) took on the particular value \\(y_1\\)\nOk, now we have the statement \\(L = P(Y_1 = y_1, Y_2=y_2, ..., Y_n = y_n)\\). This joint probability is the likelihood (Technically it is proportionate to the likelihood, but this detail will not matter for us. What this means is there is a hidden constant \\(k(y)\\) multiplied by the joint probability. Because likelihood is a relative concept, this constant can fall out.)\n\nGenerally, we don’t know \\(p\\). We are trying to estimate it. What we want to do is choose the \\(\\hat p\\) to maximize the likelihood that we would have gotten this set of observations given that \\(Y_i\\) has a probability distribution as specified.\n\nWe have used a buzz word: “maximize.” Just as in OLS, that should be our signal that a derivative should be taken so that we can find the quantities that represent the maximum.\nWe differentiate L with respect to p, set it to 0, to give us \\(\\hat{p}\\).\n\nOur issue (or at least one of our issues) is that products are tough to differentiate. A chain rule disaster. Instead, we use a trick of taking the log of the likelihood: log \\(\\prod ()\\).2 Benefit: it turns it into a sum, much easier. \\(\\log ab = \\log a + \\log b\\). So we will actually differentiate the \\(\\log\\) of the likelihood. Yes, this is why we had logs as part of Section 3.\n\n\n5.1.1 Summarizing Steps for Maximum Likelihood\nInitial Setup\n\nWhat is the data generating process? This means think about the structure of the dependent variable. Is it continuous, is it a count, is it binary, is it ordered? Based on this, describe the probability distribution for \\(Y_i\\).\nDefine the likelihood for a single observation\nDefine the likelihood for all observations\nFind the log-likelihood\n\nThen, the derivation begins! Yes, we’ve only just begun, but that first step of deciding the data generating process is huge.\nExample: Count Data\nLet’s say we are trying to understand the relationship between an independent variable and the number of news articles reported on a topic. This is a count of data. It goes from 0 to some positive number, never extending below 0. A distribution that is a good fit for this is the Poisson. We start by specifying this as our data generating process and looking up the Poisson probability density function, which has the parameter \\(\\lambda\\).\n\nData Generating Process and probability density function.\n\n\\[\\begin{align*}\n&Y_i \\stackrel{\\rm i.i.d.}{\\sim} Pois(\\lambda)\\Rightarrow\\\\\n&\\Pr(Y=Y_i|\\lambda)=\\lambda \\frac{exp(-\\lambda) \\lambda^{Y_i}}{Y_i!}\n\\end{align*}\\]\nNote: we assume our observations are iid (independently and identically distributed (For definition.) This assumption can be relaxed.\n\nWhat is the likelihood for a single observation?\n\n\\[\\begin{align*}\n\\mathcal L(\\lambda|Y_i)=\\Pr(Y=Y_i|\\lambda)\n\\end{align*}\\]\n\nWhat is the likelihood for all observations?\n\n\\[\\begin{align*}\n\\mathcal L(\\lambda|Y)&=\\mathcal L(\\lambda|Y_1)\\times\\mathcal  L(\\lambda|Y_2)\\times \\ldots \\times \\mathcal L(\\lambda|Y_{N})\\\\\n\\mathcal L(\\lambda|Y)&=\\prod_{i=1}^N\\mathcal L(\\lambda|Y_i)\\\\\n\\end{align*}\\]\n\nEasier to work with log-likelihood\n\n\\[\\begin{align*}\n\\ell(\\lambda|Y)&=\\sum_{i=1}^N\\mathcal \\log(\\mathcal L(\\lambda|Y_i))\\\\\n\\end{align*}\\]\nGiven observed data \\(Y\\), what is the likelihood it was generated from \\(\\lambda\\)? We will be choosing estimates of the parameters that maximize the likelihood we would have seen these data. Generally, we will also consider parameters like \\(\\lambda\\) to be functions of our covariates– the things we think help explain our otucome.\nFor additonal practice, try to write down the likelihood of a single observation, the likelihood for all observations, and the log likelihood for an outcome we believe is normally distributed. We have \\(Y_i \\sim N(\\mu, \\sigma^2)\\). Our PDF is:\n\\[\\begin{align*}\nf(Y_i | \\theta) &=  \\frac{1}{\\sigma\\sqrt{2\\pi}}e^{\\frac{-(Y_i-\\mu)^2}{2\\sigma^2}}\n\\end{align*}\\]\nYou can take it from here.\n\n\nTry on your own, then expand for the solution.\n\nWe have the PDF.\n\nLet’s write the likelihood for a single observation.\n\n\\[\\begin{align*}\nL(\\theta | Y_i) = L(\\mu, \\sigma^2 | Y_i) &= \\frac{1}{\\sigma\\sqrt{2\\pi}}e^{\\frac{-(Y_i-\\mu)^2}{2\\sigma^2}}\n\\end{align*}\\]\n\nLet’s write the likelihood for all observations.\n\n\\[\\begin{align*}\nL(\\mu, \\sigma^2 | Y) &=  \\prod_{i=1}^{N} \\frac{1}{\\sigma\\sqrt{2\\pi}}e^{\\frac{-(Y_i-\\mu)^2}{2\\sigma^2}}\n\\end{align*}\\]\n\nLet’s write the log likelihood.\n\n\\[\\begin{align*}\n\\ell(\\mu, \\sigma^2 | Y) &= \\sum_{i = 1}^N \\log \\Bigg( \\frac{1}{\\sigma\\sqrt{2\\pi}}e^{\\frac{-(Y_i-\\mu)^2}{2\\sigma^2}}\\Bigg)\\\\\n&= \\sum_{i = 1}^N \\underbrace{\\log \\Bigg( \\frac{1}{\\sigma\\sqrt{2\\pi}}\\Bigg) + \\log e^{\\frac{-(Y_i-\\mu)^2}{2\\sigma^2}}}_\\text{Using the rule $\\log ab = \\log a + \\log b$}\\\\\n&= \\underbrace{\\sum_{i = 1}^N \\log  \\frac{1}{\\sigma\\sqrt{2\\pi}} - \\frac{(Y_i-\\mu)^2}{2\\sigma^2}}_\\text{The second term was of the form $\\log e ^ a$, we can re-write as $a * \\log e$. $\\log e$ cancels to 1, leaving us with just $a$.}\\\\\n&= \\sum_{i = 1}^N \\log  \\frac{1}{\\sigma\\sqrt{2\\pi}} - \\frac{(Y_i-\\mathbf{x}_i'\\beta)^2}{2\\sigma^2}\n\\end{align*}\\]\nNote how we usually will sub in \\(X\\beta\\) or \\(\\mathbf{x_i'\\beta}\\) for the parameter because we think these will vary according to covariates in our data. The \\(e\\) in the Normal PDF is the base of the natural log. It is a mathematical constant. Sometimes you might see this written as \\(exp\\) instead of \\(e\\). In R, you can use exp() to get this constant. For example, \\(\\log e^2\\) in R would be log(exp(2))."
  },
  {
    "objectID": "05-IntrotoMaximumLikelihood.html#generalized-linear-models",
    "href": "05-IntrotoMaximumLikelihood.html#generalized-linear-models",
    "title": "5  Introduction to MLE",
    "section": "5.2 Generalized Linear Models",
    "text": "5.2 Generalized Linear Models\nBefore we get into the details of deriving the estimators, we are going to discuss another connection between linear models and the types of models we will work with when we are using common maximum likelihood estimators.\nRecall our linear model: \\(y_i = \\beta_o + \\beta_1x_{i1} + ... \\beta_kx_{ik} + \\epsilon\\)\n\n\\(Y\\) is modelled by a linear function of explanatory variables \\(X\\)\n\\(\\hat \\beta\\) is our estimate of how much \\(X\\) influences \\(Y\\) (the slope of the line)\nOn average, a one-unit change in \\(X_{ik}\\) is associated with a \\(\\hat \\beta_{k}\\) change in \\(Y_i\\)\nSlope/rate of change is linear, does not depend on where you are in \\(X\\). Every one-unit change has the same expected increase or decrease\n\nSometimes we are dealing with outcome data that are restricted or “limited” in some way such that this standard linear predictor will no longer make sense. If we keep changing \\(X\\) we may eventually generate estimates of \\(\\hat y\\) that extend above or below the plausible range of values for our actual observed outcomes.\nThe generalized linear model framework helps address this problem by adding two components: a nonlinear transformation and a probability model. This allows us to make predictions of our outcomes that retain the desired bounded qualities of our observed data. Generalized linear models include linear regression as a special case (a case where no nonlinear transformation is required), but as its name suggests, is much more general and can be applied to many different outcome structures.\n\n5.2.1 GLM Model.\nIn a GLM, we still have a “linear predictor”: \\(\\eta_i = \\beta_o + \\beta_1x_{i1} + ... + \\beta_kx_{ik}\\)\n\nBut our \\(Y_i\\) might be restricted in some way (e.g., might be binary).\nSo, now we require a “link” function which tells us how \\(Y\\) depends on the linear predictor. This is the key to making sure our linear predictor, when transformed, will map into sensible units of Y.\n\nOur \\(Y_i\\) will also now be expressed in terms of a probability model, and it is this probability distribution that generates the randomness (the stochastic component of the model). For example, when we have binary outcome data, such as \\(y_i =\\) 1 or 0 for someone turning out to vote or not, we may try to estimate the probability that someone turns out to vote given certain explanatory variables. We can write this as \\(Pr(Y_i = 1 | x_i\\)).\nIn a GLM, we need a way to transform our linear predictor such that as we shift in values of \\(X\\hat \\beta\\), we stay within plausible probability ranges.\n\nTo do so we use a “link” function that is used to model the data.\n\nFor example, in logistic regression, our link function will be the “logit”:\n\n\\[\\begin{align*}\nPr(Y_i = 1 | x_i) &= \\pi_i\\\\\n\\eta_i &= \\text{logit}(\\pi_i) = \\log \\frac{\\pi_i}{1-\\pi_i} &= \\beta_o + \\beta_1x_{i1} + ... + \\beta_kx_{ik}\n\\end{align*}\\]\nOne practical implication of this is that when we generate our coefficient estimates \\(\\hat \\beta\\), these will no longer be in units if \\(y_i\\) or even in units of probability. Instead, they will be in units as specified by the link function. In logistic regression, this means they will be in “logits.”\n\nFor every one-unit change in \\(x_k\\), we get a \\(\\hat \\beta_k\\) change in logits of \\(y\\)\n\nHowever, the nice thing is that because we know the link function, with a little bit of work, we can use the “response” function to transform our estimates back into the units of \\(y_i\\) that we care about.\n\n\\[\\begin{align*}\nPr(Y_i = 1 | x_i) &= \\pi_i = g^{-1}(\\eta_i) \\\\\n&= \\text{logit}^{-1}(\\pi_i) \\\\\n&= \\frac{exp^{x_i'\\beta}}{1 + exp^{x_i'\\beta}}\n\\end{align*}\\]\n\n\n5.2.2 Linking likelihood and the GLM\nLet’s use \\(\\theta\\) to represent the parameters of the pdf/pmf that we have deemed appropriate for our outcome data. As discussed before, we can write the likelihood for an observation as a probability statement.\n\n\\(\\mathcal L (\\theta | Y_i) = \\Pr(Y=Y_i | \\theta)\\)\n\nIn social science, instead of thinking of these parameters as just constants (e.g., \\(p\\) or \\(\\mu\\)), we generally believe that they vary according to our explanatory variables in \\(X\\). We think \\(Y_i\\) is distributed according to a particular probability function and that the parameters that shape that distribution are a function of the covariates.\n\n\\(Y_i \\sim f(y_i | \\theta_i)\\) and \\(\\theta_i = g(X_i, \\beta)\\)\n\nEach type of model we come across–guided by the structure of the dependent variable– is just going to have different formulas for each of these components.\nExamples\n\n\n\n\n\n\n\n\n\n\nModel\nPDF\n\\(\\theta_i\\) ; Link\\(^{-1}\\)\n\\(\\eta_i\\)\n\n\n\n\n\nLinear\n\\(Y_i \\sim \\mathcal{N}(\\mu_i,\\sigma^2)\\)\n\\(\\mu_i = X_i^\\prime\\beta\\)\n\\(\\mu_i\\)\n\n\n\nLogit\n\\(Y_i \\sim \\rm{Bernoulli}(\\pi_i)\\)\n\\(\\pi_i=\\frac{\\exp(X_i^\\prime\\beta)}{(1+\\exp(X_i^\\prime\\beta))}\\)\nlogit\\((\\pi_i)\\)\n\n\n\nProbit\n\\(Y_i \\sim \\rm{Bernoulli}(\\pi_i)\\)\n\\(\\pi_i = \\Phi(X_i^\\prime\\beta)\\)\n\\(\\Phi^{-1}(\\pi_i)\\)\n\n\n\n\nThese generalized linear models are then fit through maximum likelihood estimation, through an approach discussed in the next section where we use algorithms to choose the most likely values of the \\(\\beta\\) parameters given the observed data.\nNote: not all ML estimators can be written as generalized linear models, though many we use in political science are indeed GLMs. To be a GLM, the distribution we specify for the data generating process has to be a part of the exponential family of probability distributions (fortunately the gaussian normal, poisson, bernouilli, binomial, gamma, and negative binomial are), and after that, we need the linear predictor and link function.\n\n\n5.2.3 GLM in R\nThe way generalized linear models work in R is very similar to lm.\nBelow is a simple example where we will specify a linear model in lm() and glm() to compare.\n\n## Load Data\nflorida &lt;- read.csv(\"https://raw.githubusercontent.com/ktmccabe/teachingdata/main/florida.csv\")\n\nfit.lm &lt;- lm(Buchanan00 ~ Perot96, data=florida)\nfit.glm &lt;- glm(Buchanan00 ~ Perot96, data=florida, \n               family=gaussian(link = \"identity\"))\n\nFor the glm, we just need to tell R the family of distributions we are using and the appropriate link function. In this example, we are going to use the normal gaussian distribution to describe the data generating process for Buchanan00. This is appropriate for nice numeric continuous data, even if it isn’t perfectly normal. The normal model has a link function, but it is the special case where the link function is just the identity. There is no nonlinear transformation that takes place. Therefore, we can still interpret the \\(\\hat \\beta\\) results in units of \\(Y\\) (votes in this case).\nIn this special case, the \\(\\hat \\beta\\) estimates from lm() and glm() will be the same.\n\ncoef(fit.lm)\n\n(Intercept)     Perot96 \n 1.34575212  0.03591504 \n\ncoef(fit.glm)\n\n(Intercept)     Perot96 \n 1.34575212  0.03591504 \n\n\nThere are some differences in the mechanics of how we get to the results in each case, but we will explore those more in the next section. I.e., these coefficients do not come out of thin air. Just like in OLS, we have to work for them."
  },
  {
    "objectID": "05-IntrotoMaximumLikelihood.html#mle-estimation",
    "href": "05-IntrotoMaximumLikelihood.html#mle-estimation",
    "title": "5  Introduction to MLE",
    "section": "5.3 MLE Estimation",
    "text": "5.3 MLE Estimation\nThis section will discuss the general process for deriving maximum likelihood estimators. It’s all very exciting. It builds on the resources from the previous sections. In the next section, we will go through this process for a binary dependent variable. Here, we lay out the overview.\n\n5.3.1 Deriving Estimators\nRecall, we’ve already gone through a few steps of maximum likelihood estimation.\nInitial Setup\n\nWhat is the data generating process? This means think about the structure of the dependent variable. Is it continuous, is it a count, is it binary, is it ordered? Based on this, describe the probability distribution for \\(Y_i\\).\nDefine the likelihood for a single observation\nDefine the likelihood for all observations\nFind the log-likelihood\n\nNow we add steps building on the log-likelihood.\n\nMaximize the function with respect to (wrt) \\(\\theta\\)\n\nTake the derivative wrt \\(\\theta\\). We call this the “score”\nSet \\(S(\\theta) = 0\\) and solve for \\(\\hat \\theta\\) (if possible)\nIf not possible (often the case), we use an optimization algorithm to maximize the log likelihood.\n\nTake the second derivative of the log likelihood to get the “hessian” and help estimate the uncertainty of the estimates.\n\n\n\n5.3.2 Score function\nThe first derivative of the log-likelihood is called the score function: \\(\\frac{\\delta \\ell}{\\delta \\theta} = S(\\theta)\\). This will tell us how steep the slope of the log likelihood is given certain values of the parameters. What we are looking for as we sift through possible values of the parameters, is the set of values that will make the slope zero, signalling that the function has reached a peak (maximizing the likelihood.)\nWe set the \\(S(\\theta) = 0\\) and solve for \\(\\hat \\theta\\) (if possible).\n\n\\(\\hat \\theta\\) are the slopes/gradient, which we use as estimates (e.g., \\(\\hat \\beta\\)).\nWe can interpret the sign and significance just as we do in OLS.\nBut, unlike OLS, most of the time, these are not linear changes in units of \\(Y\\)\nWe have to transform them into interpretable quantities\n\nExample: Normally distributed outcome\nStart with the log-likelihood\n\\[\\begin{align*}\n\\ell(\\theta | Y) &= \\sum_{i = 1}^N \\log \\Bigg( \\frac{1}{\\sigma\\sqrt{2\\pi}}e^{\\frac{-(Y_i-\\mu)^2}{2\\sigma^2}}\\Bigg)\\\\\n&= \\sum_{i = 1}^N \\underbrace{\\log \\Bigg( \\frac{1}{\\sigma\\sqrt{2\\pi}}\\Bigg) + \\log e^{\\frac{-(Y_i-\\mu)^2}{2\\sigma^2}}}_\\text{Using the rule $\\log ab = \\log a + \\log b$}\\\\\n&= \\underbrace{\\sum_{i = 1}^N \\log  \\frac{1}{\\sigma\\sqrt{2\\pi}} - \\frac{(Y_i-\\mu)^2}{2\\sigma^2}}_\\text{The second term was of the form $\\log e ^ a$, we can re-write as $a * \\log e$. $\\log e$ cancels to 1, leaving us with just $a$.}\\\\\n&= \\sum_{i = 1}^N \\log  \\frac{1}{\\sigma\\sqrt{2\\pi}} - \\frac{(Y_i-\\mathbf{x}_i'\\beta)^2}{2\\sigma^2}\n\\end{align*}\\]\n\nNote: when you see \\(\\mathbf{x}_i'\\beta\\), usually that is the representation of the multiplication of \\(k\\) covariates (a \\(1 \\times k\\) vector) for a particular observation \\(i\\) by \\(k \\times 1\\) coefficient values \\(\\beta\\). You can contrast this with \\(X\\beta\\), which represents \\(n \\times k\\) rows of observations with \\(k\\) covariates multiplied by the \\(k \\times 1\\) coefficients. You will see both notations depending on if notation is indexed by \\(i\\) or represented fully in matrix form. The \\(\\mathbf{x_i'}\\) representation tends to come up more when we are dealing with likelihood equations. Here is a short video relating these notations.\n\n\nTake the derivative wrt \\(\\theta\\). Note: we have to take two derivatives- one for \\(\\mu\\) (\\(\\beta\\)) and one for \\(\\sigma^2\\). For this example we will focus only on the derivative wrt to \\(\\beta\\), as that it what gets us the coefficient estimates.3\nNote: Below, we can simplify the expression of the log likelihood for taking the derivative with respect to \\(\\beta\\) because any term (i.e., the first term in the log likelihood in this case) that does not have a \\(\\beta\\) will fall out of the derivative expression. This is because when we take the derivative with respect to \\(\\beta\\) we treat all other terms as constants, and the slope of a constant (the rate of change of a constant) is zero. The curly \\(\\delta\\) in the expression below means “the derivative of …” with respect to \\(\\beta\\).\n\\[\\begin{align*}\n\\delta_\\beta \\ell(\\theta | Y) &= -\\frac{1}{2\\sigma^2}\\sum_{i = 1}^N \\delta_\\beta (Y_i-\\mathbf{x}_i'\\hat \\beta)^2\n\\end{align*}\\] The right term should look familiar! It is the same derivative we take when we are minimizing the least squares. Therefore, we will end up with \\(S(\\hat \\theta)_\\beta = \\frac{1}{\\sigma^2}X'(Y - X\\hat \\beta)\\). We set this equal to 0. \\[\\begin{align*}\n\\frac{1}{\\sigma^2}X'(Y - X\\hat \\beta) &= 0\\\\\n\\frac{1}{\\sigma^2}X'Y &= \\frac{1}{\\sigma^2}X'X\\hat \\beta \\\\\n(X'X)^{-1}X'Y = \\hat \\beta\n\\end{align*}\\]\n\n\n5.3.3 Hessian and Information Matrix\nThe second derivative of the log-likelihood is the Hessian \\((H(\\theta))\\).\n\nThe second derivative is a measure of the curvature of the likelihood function. This will help us confirm that we are at a maximum, and it will also help us calculate the uncertainty.\nThe more curved (i.e., the steeper the curve), the more certainty we have.\nThe \\(I\\) stands for the information matrix. The \\(H\\) stands for Hessian. \\(I(\\theta) = - \\mathbb{E}(H)\\)\n\n\\(var(\\theta) = [I(\\theta)]^{-1} = ( - \\mathbb{E}(H))^{-1}\\)\nStandard errors are the square roots of the diagonals of this \\(k \\times k\\) matrix (like vcov() in OLS)\n\n\nExample: Normal\nStart with the log-likelihood\n\\[\\begin{align*}\n\\ell(\\theta | Y) &= \\sum_{i = 1}^N \\log  \\frac{1}{\\sigma\\sqrt{2\\pi}} - \\frac{(Y_i-x_i'\\beta)^2}{2\\sigma^2}\n\\end{align*}\\]\nBecause our \\(\\theta\\) has two parameters, the Hessian actually has four components. For this example, we will focus on one: the first and second derivatives wrt \\(\\beta\\).\n\nRecall the first derivative = \\(\\frac{1}{\\sigma^2}X'(Y - X\\hat \\beta)\\).\nWe now take the second derivative with respect to \\(\\hat \\beta\\)\n\\[\\begin{align*}\n\\frac{\\delta^2}{\\delta \\hat \\beta} \\frac{1}{\\sigma^2}X'(Y - X\\hat \\beta)&= -\\frac{1}{\\sigma^2}X'X\n\\end{align*}\\]\nTo get our variance, we take the inverse of the negative (-) of this:\n\n\\(\\sigma^2(X'X)^{-1}\\) Should look familiar!\n\n\nWith this example, we can start to see why lm and glm for a normally distributed outcome generate the same estimates. The maximum likelihood estimator is the same as the least squares estimator.\n\n\n5.3.4 MLE Estimation Algorithm\nSuppose we are interested in finding the true probability \\(p\\) that a comment made on twitter is toxic, and we have a small sample of hand-coded data. Let’s say we have \\(n=8\\) observations where we could observe a \\(y_i = 1\\) or \\(0\\). For example, let’s say we read an online sample of tweets and we classified tweets as “toxic=1” or “nontoxic=0.” In our sample of \\(n=8\\), we coded 6 of them as toxic and 2 as nontoxic.\n\nWe can write down the likelihood for a single observation using the Bernouilli pmf:\n\\(L(p | y_i) = p^{y_i}*(1-p)^{(1-y_i)}\\)\nWe could then write out the likelihood for all 8 observations as follows:\n\nWhere the equation simplifies to \\(p\\) for observations where \\(y_i\\) = 1 and (1-p) for observations where \\(y_i\\) = 0. For simplicity, let’s say \\(i=1\\) to \\(6\\) were toxic, and \\(i=7\\) to \\(8\\) were nontoxic.\n\\(L(p | \\mathbf{y}) = p * p * p * p * p * p * (1-p) * (1-p)\\)\n\nNow a naive way to maximize the likelihood would be to just try out different quantities for \\(p\\) and see which give us the maximum.\n\n## Let's try this for different p's\np &lt;- seq(.1, .9, .05)\nL &lt;- p * p * p * p * p * p * (1-p) * (1-p)\n\nWe can then visualize the likelihood results and figure out about at which value for \\(\\hat p\\) we have maximized the likelihood.\n\nplot(x=p, y=L, type=\"b\",\n     xaxt=\"n\")\naxis(1, p, p)\n\n\n\n\nWhen we have more complicated models, we are taking a similar approach–trying out different values and comparing the likelihood (or log likelihood), but we will rely on a specific algorithm(s) that will help us get to the maximum a bit faster than a naive search would allow.\nDon’t worry the built-in functions in R will do this for you (e.g., what happens under the hood of glm()), but if you were to need to develop your own custom likelihood function for some reason, you could directly solve it through an optimization algorithm if no such built-in function is appropriate.\nYou can skip the details below if you wish and jump to the MLE Properties section. This content will only be involved in problem sets as extra credit, as you may not have to use optim in your own research.\nThe optim function in R provides one such approach. For this optimization approach, we will need to.\n\nDerive the likelihood and/or log likelihood function and score\nCreate an R function for the quantity to want to optimize (often the log likelihood) where given we provide the function certain values, the function returns the resulting quantity. (Kind of like when we supply the function mean() with a set of values, it returns the average of the values by computing the average under the hood of the function.)\nUse optim() to maximize\n\noptim(par, fn, ..., gr, method, control, hessian,...), where\npar: initial values of the parameters\nfn: function to be maximized (minimized)\ngr: optional argument, can include the gradient to help with optimization\n...: (specify other variables in fn)\nmethod: optimization algorithm\ncontrol: parameters to fine-tune optimization\nhessian: returns the Hessian matrix if TRUE\n\n\nBy default, optim performs minimization. Make sure to set control = list(fnscale=-1) for maximization\n\nFor starting values par, least squares estimates are often used. More sensible starting values help your optimize more quickly. You may need to adjust the maxit control parameter to make sure the optimization converges.\nA commonly used method is BFGS (a variant of Newton-Raphson), similar to what glm() uses, but there are other methods available.\n\nExample 1: estimating p\nLet’s take our relatively simple example about toxic tweets above and optimize the likelihood. First, we create a function for the likelihood that will calculate the likelihood for the values supplied. In the future, our models will be complicated enough, we will stick with the log likelihood, which allows us to take a sum instead of a product.\nOne benefit of R is that you can write your own functions, just like mean() is a built-in function in R. For more information on writing functions, you can review Imai QSS Chapter 1 pg. 19..\n\nlik.p &lt;- function(p){\n  lh &lt;- p * p * p * p * p * p * (1-p) * (1-p)\n  return(lh)\n}\n\nOk, now that we have our likelihood function, we can optimize. We just have to tell R a starting parameter for \\(\\hat p\\). Let’s give it a (relatively) bad one just to show how it works (i.e., can optim find the sensible .75 value. If you give the function too bad of a value, it might not converge before it maxes out and instead return a local min/max instead of a global one.\n\nstartphat &lt;- .25\nopt.fit &lt;- optim(par = startphat, fn=lik.p, method=\"BFGS\",\n                 control=list(fnscale=-1))\n\n## This should match our plot\nopt.fit$par\n\n[1] 0.7500035\n\n## you should check convergence. Want this to be 0 to make sure it converged\nopt.fit$convergence\n\n[1] 0\n\n\nExample 2: Linear Model\nWe can use optim to find a solution for a linear model by supplying R with our log likelihood function.\nFor the MLE of the normal linear model, our log likelihood equation is:\n\\[\\begin{align*}\n\\ell(\\theta | Y) &= \\sum_{i = 1}^N \\log  \\frac{1}{\\sigma\\sqrt{2\\pi}} - \\frac{(Y_i-\\mathbf{x}_i'\\beta)^2}{2\\sigma^2}\n\\end{align*}\\]\nNow that we have our log likelihood, we can write a function that for a given set of \\(\\hat \\beta\\) and \\(\\hat \\sigma^2\\) parameter values, \\(X\\), and \\(Y\\), it will return the log likelihood.\n\nBelow we indicate we will supply an argument par (an arbitrary name) that will inclue our estimates for the parameters: \\(k\\) values for the set of \\(\\hat \\beta\\) estimates and a \\(k + 1\\) value for the \\(\\hat \\sigma^2\\) estimate. Many models with only have one set of parameters. This is actually a slightly more tricky example.\nThe lt line is the translation of the equation above into R code\n\n\n## Log Likelihood function for the normal model\nl_lm &lt;- function(par, Y, X){\n  k &lt;- ncol(X)\n  beta &lt;- par[1:k]\n  sigma2 &lt;- par[(k+1)]\n  lt &lt;- sum(log(1/(sqrt(sigma2)*sqrt(2*pi))) - ((Y - X %*% beta)^2/(2*sigma2)))\n  return(lt)\n}\n\nNow that we have our function, we can apply it to a problem.\nLet’s use an example with a sample of Democrats from the 2016 American National Election Study dataset. This example is based on the article “Hostile Sexism, Racial Resentment, and Political Mobilization” by Kevin K. Banda and Erin C. Cassese published in Political Behavior in 2020. We are not replicating their article precisely, but we use similar data and study similar relationships.\nThe researchers were interested in how cross-pressures influence the political participation of different partisan groups. In particular, they hypothesized that Democrats in the U.S. who held more sexist views would be demobilized from political participation in 2016, a year in which Hillary Clinton ran for the presidency.\nThe data we are using are available anesdems.csv and represent a subset of the data for Democrats (including people who lean toward the Democratic party). We have a few variables of interest\n\nparticipation: a 0 to 8 variable indicating the extent of a respondent’s political participation\nfemale: a 0 or 1 variable indicating if the respondent is female\nedu: a numeric variable indicating a respondent’s education level\nage: a numeric variable indicating a respondent’s age.\nsexism: a numeric variable indicating a respondent’s score on a battery of questions designed to assess hostile sexism, where higher values indicate more hostile sexism.\n\nLet’s regress participation on these variables and estimate it using OLS, GLM, and optim. Note, OLS and GLM fit through their functions in R will automatically drop any observations that have missing data on these variables. To make it comparable with optim, we will manually eliminate missing data.\n\nanes &lt;- read.csv(\"https://raw.githubusercontent.com/ktmccabe/teachingdata/main/anesdems.csv\")\n## choose variables we will use\nanes &lt;- subset(anes, select=c(\"participation\", \"age\", \"edu\", \"sexism\", \"female\"))\n## omit observations with missing data on these variables\nanes &lt;- na.omit(anes)\n\n## OLS and GLM regression\nfit &lt;- lm(participation ~ female + edu + age + sexism, data=anes)\nfit.glm &lt;- glm(participation ~ female + edu + age + sexism, data=anes,\n               family=gaussian(link=\"identity\"))\n\nNow we will build our data for optim. We need \\(X\\), \\(Y\\), and a set of starting \\(\\hat \\beta\\) and \\(\\hat \\sigma^2\\) values.\n\n## X and Y data\nX.anes &lt;- model.matrix(fit)\nY.anes &lt;- as.matrix(anes$participation)\n## make sure dimensions are the same\nnrow(X.anes)\n\n[1] 1585\n\nnrow(Y.anes)\n\n[1] 1585\n\n## Pick starting values for parameters\nstartbetas &lt;- coef(fit)\n## Recall our estimate for sigma-squared based on the residuals\nk &lt;- ncol(X.anes)\nstartsigma &lt;- sum(fit$residuals^2) / (nrow(X.anes) - k )\nstartpar &lt;- c(startbetas, startsigma)\n\n## Fit model\n## But let's make it harder on the optimization by providing arbitrary starting values\n## (normally you wouldn't do this)\nstartpar &lt;- c(1,1,1,1,1,1)\nopt.fit &lt;- optim(par = startpar, fn=l_lm, X = X.anes,\n                 Y=Y.anes, method=\"BFGS\",\n                 control=list(fnscale=-1),\n                   hessian=TRUE)\n\nWe can compare this optimization approach to the output in glm().\nWe can first compare the log likelihoods\n\nlogLik(fit.glm)\n\n'log Lik.' -2661.428 (df=6)\n\nopt.fit$value\n\n[1] -2661.428\n\n\nWe can compare the coefficients.\n\n## Coefficients\nround(coef(fit), digits=4)\nround(coef(fit.glm), digits=4)\nround(opt.fit$par, digits=4)[1:k]\n\n(Intercept)      female         edu         age      sexism \n     0.9293     -0.2175      0.1668      0.0088     -0.9818 \n(Intercept)      female         edu         age      sexism \n     0.9293     -0.2175      0.1668      0.0088     -0.9818 \n[1]  0.9294 -0.2175  0.1668  0.0088 -0.9819\n\n\nWe can add the gradient of the log likelihood to help improve optimization. This requires specifying the first derivative (the score) of the parameters. Unfortunately this means taking the derivative of that ugly normal log likelihood above. Again, with the normal model, we have two scores because of \\(\\hat \\beta\\) and \\(\\hat \\sigma^2\\). For others, we may just have one.\n\n## first derivative function\nscore_lm &lt;- function(par, Y, X){\n  k &lt;- ncol(X)\n  beta &lt;- as.matrix(par[1:k])\n  scorebeta &lt;- (1/par[k+1]) * (t(X) %*% (Y - X %*% beta))\n  scoresigma &lt;- -nrow(X)/(par[k+1]*2) + sum((Y - X %*% beta)^2)/(2 * par[k+1]^2)\n  return(c(scorebeta, scoresigma))\n}\n\n## Fit model\nopt.fit &lt;- optim(par = startpar, fn=l_lm, gr=score_lm, X = X.anes,\n                 Y=Y.anes, method=\"BFGS\",\n                 control=list(fnscale=-1),\n                   hessian=TRUE)\n\nIn addition to using optim, we can program our own Newton-Raphson algorithm, which is a method that continually updates the coefficient estimates \\(\\hat \\beta\\) until it converges on a set of estimates. We will see this in a future section. The general algorithm involves the components we’ve seen before: values for \\(\\hat \\beta\\), the score, and the Hessian.\n\nNewton-Raphson: \\(\\hat \\beta_{new} = \\hat \\beta_{old} - H(\\beta_{old})^{-1}S(\\hat \\beta_{old})\\)"
  },
  {
    "objectID": "05-IntrotoMaximumLikelihood.html#mle-properties",
    "href": "05-IntrotoMaximumLikelihood.html#mle-properties",
    "title": "5  Introduction to MLE",
    "section": "5.4 MLE Properties",
    "text": "5.4 MLE Properties\nJust like OLS had certain properties (BLUE) that made it worthwhile, models using MLE also have desirable features under certain assumptions and regularity conditions.\nLarge sample properties\n\nMLE is consistent: \\(p\\lim \\hat \\theta^{ML} = \\theta\\)\nIt is also asymptotically normal: \\(\\hat \\theta^{ML} \\sim N(\\theta, [I(\\theta)]^{-1})\\)\n\nThis will allow us to use the normal approximation to calculate z-scores and p-values\n\nAnd it is asymptotically efficient. “In other words, compared to any other consistent and uniformly asymptotically Normal estimator, the ML estimator has a smaller asymptotic variance” (King 1998, 80).\n\nNote on consistency\nWhat does it mean to say an estimator is consistent? As samples get larger and larger, we converge to the truth.\nConsistency: \\(p\\lim \\hat{\\theta} =\\beta\\) As \\(n \\rightarrow \\infty P(\\hat{\\theta} - \\theta&gt; e) \\rightarrow 0\\).\n\nConvergence in probability: the probability that the absolute difference between the estimate and parameter being larger than \\(e\\) goes to zero as \\(n\\) gets bigger.\n\nNote that bias and consistency are different: Consistency means that as the sample size (\\(n\\)) gets large the estimate gets closer to the true value. Unbiasedness is not affected by sample size. An estimate is unbiased if over repeated samples, its expected value (average) is the true parameter.\nIt is possible for an estimator to be unbiased and consistent, biased and not consistent, or consistent yet biased.\n; \n\nImages taken from here\nWhat is a practical takeaway from this? The desirable properties of MLE kick in with larger samples. When you have a very small sample, you might use caution with your estimates.\nNo Free Lunch\nWe have hinted at some of the assumptions required, but below we can state them more formally.\n\nFirst, we assume a particular data generating process: the probability model.\nWe are generally assuming that observations are independent and identically distributed (allowing us to write the likelihood as a product)– unless we explicitly write the likelihood in a way that takes this into account.\n\nWhen we have complicated structures to our data, this assumption may be violated, such as data that is clustered in particular hierarchical entities.\n\nWe assume the model (i.e., the choice of covariates and how they are modeled) is correctly specified. (e.g., no omitted variables.)\nWe have to meet certain technical regularity conditions–meaning that our problem is a “regular” one. These, in the words of Gary King are “obviously quite technical” (1998, 75). We will not get into the details, but you can see pg. 75 of Unifying Political Methodology for the formal mathematical statements. In short, our paramaters have to be identifiable and within the parameter space of possible values (this identifiability can be violated, for example, when we have too many parameters relative to the number of observations in the sample), we have to be able to differentiate the log-likelihood (in fact, it needs to be twice continuously differentiable) along the support (the range of values) in the data. The information matrix, which we get through the second derivative, must be positive definite and finitely bounded. This helps us know we are at a maximum, and the maximum exists and is finite. You can visualize this as a smooth function, that is not too sharp (which would make it non differentiable), but has a peak (a maximum) that we can identify.\n\n\n5.4.1 Hypothesis Tests\nWe can apply the same hypothesis testing framework to our estimates here as we did in linear regression. First, we can standardize our coefficient estimates by dividing them by the standard error. This will generate a “z score.” Just like when we had the t value in OLS, we can use the z score to calculate the p-value and make assessments about the null hypothesis that a given \\(\\hat \\beta_k\\) = 0.\n\\[\\begin{align*}\nz &= \\frac{\\hat \\theta_k}{\\sqrt{Var(\\hat \\theta)_k}} \\sim N(0,1)\n\\end{align*}\\]\nNote: to get p-values, we typically now use, 2 * pnorm(abs(z), lower.tail=F) instead of pt() and our critical values are based on qnorm() instead of qt(). R will follow the same in most circumstances. In large samples, these converge to the same quantities.\n\n\n5.4.2 Model Output in R\nAs discussed, we can fit a GLM in R using the glm function:\n\nglm(formula, data, family = XXX(link = \"XXX\", ...), ...)\n\nformula: The model written in the form similar to lm()\ndata: Data frame\nfamily: Name of PDF for \\(Y_i\\) (e.g. binomial, gaussian)\nlink: Name of the link function (e.g. logit, `probit, identity, log)\n\n\n\n## Load Data\nfit.glm &lt;- glm(participation ~ female + edu + age + sexism, data=anes,\n               family=gaussian(link=\"identity\"))\n\nWe’ve already discussed the coefficient output. Like lm(), GLM wiil also display the standard errors, z-scores / t-statistics, and p-values of the model in the model summary.\n\nsummary(fit.glm)$coefficients\n\n                Estimate  Std. Error   t value     Pr(&gt;|t|)\n(Intercept)  0.929302597 0.154033186  6.033132 1.999280e-09\nfemale      -0.217453631 0.067415200 -3.225588 1.282859e-03\nedu          0.166837062 0.022402469  7.447262 1.559790e-13\nage          0.008795138 0.001874559  4.691843 2.941204e-06\nsexism      -0.981808431 0.154664950 -6.347970 2.844067e-10\n\n\nFor this example, R reverts to the t-value instead of the z-score given that we are using the linear model. In other examples, you may see z in place of t. There are only small differences in these approximations because as your sample size gets larger, the degrees of freedom (used in the calculation of p-calues for the t distribution) are big enough that the t distribution converges to the normal distribution.\nGoodness of fit\nThe glm() model has a lot of summary output.\n\nsummary(fit.glm)\n\n\nCall:\nglm(formula = participation ~ female + edu + age + sexism, family = gaussian(link = \"identity\"), \n    data = anes)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.3001  -0.8343  -0.3013   0.3651   7.2887  \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.929303   0.154033   6.033 2.00e-09 ***\nfemale      -0.217454   0.067415  -3.226  0.00128 ** \nedu          0.166837   0.022402   7.447 1.56e-13 ***\nage          0.008795   0.001875   4.692 2.94e-06 ***\nsexism      -0.981808   0.154665  -6.348 2.84e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 1.688012)\n\n    Null deviance: 2969.3  on 1584  degrees of freedom\nResidual deviance: 2667.1  on 1580  degrees of freedom\nAIC: 5334.9\n\nNumber of Fisher Scoring iterations: 2\n\n\nSome of the output represents measures of the goodness of fit of the model. However, their values are not directly interpretable from a single model.\n\nLarger (less negative) likelihood, the better the model fits the data. (logLik(mod)). The becomes relevant when comparing two or more models.\nDeviance is calculated from the likelihood. This is a measure of discrepancy between observed and fitted values. (Smaller values, better fit.)\n\nNull deviance: how well the outcome is predicted by a model that includes only the intercept. (\\(df = n - 1\\))\nResidual deviance: how well the outcome is predicted by a model with our parameters. (\\(df = n-k\\))\n\nAIC- used for model comparison. Smaller values indicate a more parsimonious model. Accounts for the number of parameters (\\(K\\)) in the model (like Adjusted R-squared, but without the ease of interpretation). Sometimes used as a criteria in prediction exercises (using a model on training data to predict test data). For more information on how AIC can be used in prediction exercises, see here.\n\nLikelihood Ratio Test\nThe likelihood ratio test compares the fit of two models, with the null hypothesis being that the full model does not add more explanatory power to the reduced model. Note: You should only compare models if they have the same number of observations.\nThis type of test is used in a similar way that people compare R-squared values across models.\n\nfit.glm2 &lt;- glm(participation ~ female + edu + age + sexism, data=anes,\n               family=gaussian(link=\"identity\"))\n\nfit.glm1 &lt;- glm(participation ~ female + edu + age, data=anes, \n               family=gaussian(link = \"identity\"))\n               \nanova(fit.glm1, fit.glm2, test = \"Chisq\") #  reject the null\n\nAnalysis of Deviance Table\n\nModel 1: participation ~ female + edu + age\nModel 2: participation ~ female + edu + age + sexism\n  Resid. Df Resid. Dev Df Deviance  Pr(&gt;Chi)    \n1      1581     2735.1                          \n2      1580     2667.1  1   68.021 2.182e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nPseudo-R-squared\nWe don’t have an exact equivalent to the R-squared in OLS, but people have developed “pseudo” measures.\nExample: McFadden’s R-squared\n\n\\(PR^2 = 1 - \\frac{\\ell(M)}{\\ell(N)}\\)\n\nwhere \\(\\ell(M)\\) is the log-likelihood for your fitted model and \\(\\ell(N)\\) is the log-likelihood for a model with only the intercept\n\nRecall, greater (less negative) values of the log-likelihood indicate better fit\nMcFadden’s values range from 0 to close to 1\n\n\n# install.packages(\"pscl\")\nlibrary(pscl)\nfit.glm.null &lt;- glm(participation ~ 1, anes, family = gaussian(link = \"identity\"))\npr &lt;- pR2(fit.glm1)\n\nfitting null model for pseudo-r2\n\npr[\"McFadden\"]\n\n  McFadden \n0.02370539 \n\n## Or, by hand:\n1 - (logLik(fit.glm1)/logLik(fit.glm.null))\n\n'log Lik.' 0.02370539 (df=5)"
  },
  {
    "objectID": "05-IntrotoMaximumLikelihood.html#footnotes",
    "href": "05-IntrotoMaximumLikelihood.html#footnotes",
    "title": "5  Introduction to MLE",
    "section": "",
    "text": "In the future, we may want to know the probability of turning out to vote, or of going to war, or voting “yes” on a particular policy question, etc.↩︎\nWhy does this work? It has to do with the shape of the log (always increasing). Details are beyond the scope.↩︎\nEssentially, you need to take derivatives with respect to each of the parameters. Some models we use will have only one parameter, which is easier.↩︎"
  },
  {
    "objectID": "06-BinaryOutcomes.html#data-generating-process",
    "href": "06-BinaryOutcomes.html#data-generating-process",
    "title": "6  Binary Dependent Variables",
    "section": "6.1 Data Generating Process",
    "text": "6.1 Data Generating Process\nLet’s say \\(Y_i\\) is a set of 0’s and 1’s for whether two states have experienced a dispute, an outcome common in IR studies.\n\\[\\begin{gather*}\nY_i = \\begin{cases}1, \\;\\text{a dispute happened}\\\\ 0,\\;\n\\text{a dispute did not happen}\\end{cases}\n\\end{gather*}\\]\n\nOutside of political science, for example, in the field of higher education, we might instead think about an outcome related to whether a student has (= 1) or has not (= 0) had a negative advising experience.\n\n\n# Example of first 20 observations (Y1, Y2, ..., Y20)\n1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n\nWe need to align these data with a data generating process and distribution.\n\nFor each \\(Y_i\\), it is like a single trial, where you have a dispute with some probability \\((\\pi)\\)\n\nThis sounds like the Bernoulli distribution! \\(Y_i \\sim Bernouli(\\pi)\\)\n\n\n\n6.1.1 MLE Estimation\nLet’s do the steps we saw in the previous section.\n\nWhat is the data generating process? Based on this, describe the probability distribution for \\(Y_i\\).\n\nNote: if you are using a function like glm() you can proceed directly there after this step. However, let’s work under the hood for a bit.\n\n\\[\\begin{align*}\nY_i \\sim f(Y_i | \\pi) &= Pr(Y_i = y_i |\\pi_i) = \\underbrace{\\pi^{y_i}(1 -\\pi)^{(1-y_i)}}_\\text{pmf for Bernoulli}\n\\end{align*}\\] for \\(y = 0,1\\)\nDefine the likelihood for a single observation\nDefine the likelihood for all observations\nFind the log-likelihood\n\n\\[\\begin{align*}\n\\mathcal L( \\pi | Y_i) &= \\underbrace{\\pi^{y_i}(1 -\\pi)^{(1-y_i)}}_\\text{Likelihood for single observation}\\\\\n\\mathcal L( \\pi | Y) &= \\underbrace{\\prod_{i=1}^n\\pi^{y_i}(1 -\\pi)^{(1-y_i)}}_\\text{Likelihood for all observations}\\\\\n\\ell( \\pi | Y) &= \\underbrace{\\sum_{i=1}^n\\log \\pi^{y_i}(1 -\\pi)^{(1-y_i)}}_\\text{Log likelihood}\\\\\n\\hat \\pi &= \\text{Next step: arg max } \\ell( \\pi | Y) \\text{ wrt $\\pi$}\n\\end{align*}\\]\nAdd step: nonlinear transformation to \\(X\\beta\\)\n\nNote that because we are likely using covariates, we need to express our parameter as a function of \\(X\\beta\\). Why? Because we don’t think there is a constant probability for a dispute. Instead, we think the probability of a dispute varies according to different independent variables, which are included in the \\(X\\) matrix and everntually will each have their own \\(\\beta_k\\) relationship with the probability of dispute.\n\nNow that we are outside of linear territory, we cannot just simply replace \\(\\pi\\) with \\(X\\beta\\) in the equation. Instead, \\(\\pi\\) is a function of \\(X\\beta\\). \\(\\pi = g(X_i, \\beta) \\neq \\mathbf{x}_i'\\beta\\)\n\nThis is because we need a transformation, such as the logit or probit to map our linear predictor into the outcome to make sure the linear predictor can be transformed back into sensible units of the outcome. The logit is one variety, as is the probit. Like two roads diverged in a yellow wood, this is the point in the process where we choose the transformation. For this first example, let’s apply a logit transformation, which restricts our estimates to between 0 and 1 (a good thing for probability!) where:\n\\(\\pi_i = \\text{logit}^{-1}(\\eta_i) = \\frac{exp^{\\eta_i}}{1 + exp^{\\eta_i}} = \\frac{exp^{\\mathbf{x}_i'\\beta}}{1 + exp^{\\mathbf{x}_i'\\beta}}\\)\n\\(\\eta_i = \\text{logit}(\\pi_i) = \\log\\frac{\\pi_i}{1-\\pi_i} = \\mathbf{x}_i'\\beta\\)\n\n\nMaximize the function with respect to (wrt) \\(\\theta\\)\n\nWhere \\(\\pi_i = \\frac{exp^{\\mathbf{x}_i'\\beta}}{1 + exp^{\\mathbf{x}_i'\\beta}}\\)\n\\[\\begin{align*}\n\\hat \\pi &= \\text{arg max } \\ell( \\pi | Y) \\text{ wrt $\\pi$} \\\\\n&= \\text{arg max} \\sum_{i=1}^n\\log \\pi_i^{y_i}(1 -\\pi_i)^{(1-y_i)}\\\\\n&= \\text{arg max} \\sum_{i=1}^n \\underbrace{y_i \\log \\Big( \\frac{exp^{\\mathbf{x}_i'\\beta}}{1 + exp^{\\mathbf{x}_i'\\beta}}\\Big) + (1-y_i)\\log \\Big(1-\\frac{exp^{\\mathbf{x}_i'\\beta}}{1 + exp^{\\mathbf{x}_i'\\beta}}\\Big)}_\\text{We replaced $\\pi_i$ and used the rule $\\log a^b = b \\log a$ to bring down the $y_i$ terms.}\n\\end{align*}\\]\nAt this point, we take the derivative with respect to \\(\\beta\\). You can try this on your own, and, if you’re lucky, it may show up on a problem set near you. With a bit of work, we should get something that looks like the below, which we can represent in terms of a sum or in matrix notation.\n\\[\\begin{align*}\nS(\\theta) &=  \\sum_{i=1}^n (Y_i - \\pi_i)\\mathbf{x}^T_i\\\\\n&= X^T(Y - \\mathbf{\\pi})\n\\end{align*}\\]\nYou can note that the matrix notation retains the dimensions \\(k \\times 1\\), which we would expect because we want to choose a set of \\(k\\) coefficients in our \\(k \\times 1\\) vector \\(\\beta\\). The score, as written in summation notation, also has length \\(k\\) but here, we use a convention as writing \\(\\mathbf{x}_i'\\) in row vector representation instead of a column vector. You could instead represent this as multiplied by \\(\\mathbf{x}_i\\), which would give us the \\(k \\times 1\\) dimensions. Either way we have \\(k\\) coefficients. These are just different notations.\n\nTake the second derivative of the log likelihood to get the “hessian” and help estimate the uncertainty of the estimates. Again, we can represent this as a sum or in matrix notation, which might be easier when translating this into R code where our data is more naturally inside a matrix.\n\n\\[\\begin{align*}\nH(\\theta) &=  - \\sum_{i=1}^n \\mathbf{x}_i\\mathbf{x}^T_i(\\pi_i)(1 - \\pi_i)\\\\\n&= -X^TVX\n\\end{align*}\\] where \\(V\\) is \\(n \\times n\\) diagonal matrix with weights that are the ith element of \\((\\pi)(1 - \\pi)\\)\nOnce we have these quantities, we can optimize the function with an algorithm or go to glm in R, which will do that for us."
  },
  {
    "objectID": "06-BinaryOutcomes.html#r-code-for-fitting-logistic-regression",
    "href": "06-BinaryOutcomes.html#r-code-for-fitting-logistic-regression",
    "title": "6  Binary Dependent Variables",
    "section": "6.2 R code for fitting logistic regression",
    "text": "6.2 R code for fitting logistic regression\nWe can fit logistic regressions in R through glm(). Let’s build on the ANES example from section 5.3 and analyze a dichotomized measure of participation where 1=participated in at least some form and 0=did not participate.\n\nanes &lt;- read.csv(\"https://raw.githubusercontent.com/ktmccabe/teachingdata/main/anesdems.csv\")\nanes$partbinary &lt;- ifelse(anes$participation &gt; 0, 1, 0)\n\nWe can then fit using glm where family = binomial(link=\"logit\")\n\nout.logit &lt;- glm(partbinary ~ female + edu + age + sexism, data=anes,\n                 family = binomial(link=\"logit\"))\n\nThe summary output includes the logit coefficients, standard errors, z-scores, and p-values.\n\nsummary(out.logit)\n\n\nCall:\nglm(formula = partbinary ~ female + edu + age + sexism, family = binomial(link = \"logit\"), \n    data = anes)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.5668   0.3328   0.4475   0.6287   1.2936  \n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  1.016734   0.334656   3.038  0.00238 ** \nfemale      -0.382087   0.151516  -2.522  0.01168 *  \nedu          0.321190   0.050945   6.305 2.89e-10 ***\nage          0.008682   0.004046   2.146  0.03188 *  \nsexism      -1.593694   0.336373  -4.738 2.16e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1361.5  on 1584  degrees of freedom\nResidual deviance: 1252.9  on 1580  degrees of freedom\n  (355 observations deleted due to missingness)\nAIC: 1262.9\n\nNumber of Fisher Scoring iterations: 5\n\n\n\n6.2.1 Writing down the regression model\nIn the articles you write, you will describe the methods you use in detail, including the variables in the model and the type of regression (e.g., logistic regression). Sometimes you may want to go a step further and be very explicit about the model that you ran. We’ve already seen the regression equations for linear models. For the GLMs, they will look very similar, but we need to make the link/response function an explicit part of the equation.\nFor example, for logistic regression we have a few ways of writing it, including:\n\n\\(\\log \\frac{\\pi_i}{1-\\pi_i} = \\mathbf{x_i'}\\beta\\), or alternatively\n\\(Pr(Y_i = 1 | \\mathbf{x}_i) = logit^{-1}(\\mathbf{x}_i'\\beta) = \\frac{exp(\\mathbf{x_i'}\\beta)}{(1 + exp(\\mathbf{x_i'}\\beta)}\\)\n\n(You can also write out the individual variable names.) There is a new R package equatiomatic that can also be used to help write the equations from regression models. It’s not perfect, but should get you there for most basic models.\n## First time, you need to install one of these\n#remotes::install_github(\"datalorax/equatiomatic\")\n#install.packages(\"equatiomatic\")\n\n## Each time after, run library\nlibrary(equatiomatic)\n\n## Will output in latex code, though see package for details on options\nextract_eq(out.logit, wrap = TRUE, terms_per_line = 3)\n\n\\[\n\\begin{aligned}\n\\log\\left[ \\frac { P( \\operatorname{partbinary} = \\operatorname{1} ) }{ 1 - P( \\operatorname{partbinary} = \\operatorname{1} ) } \\right] &= \\alpha + \\beta_{1}(\\operatorname{female}) + \\beta_{2}(\\operatorname{edu})\\ + \\\\\n&\\quad \\beta_{3}(\\operatorname{age}) + \\beta_{4}(\\operatorname{sexism})\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "06-BinaryOutcomes.html#probit-regression",
    "href": "06-BinaryOutcomes.html#probit-regression",
    "title": "6  Binary Dependent Variables",
    "section": "6.3 Probit Regression",
    "text": "6.3 Probit Regression\nProbit regression is very similar to logit except we use a different link function to map the linear predictor into the outcome. Both the logit and probit links are suitable for binary outcomes with a Bernoulli distribution. If we apply a probit transformation, this also restricts our estimates to between 0 and 1.\n\n\\(\\pi_i = Pr(Y_i = 1| X_i) = \\Phi(\\mathbf{x}_i'\\beta)\\)\n\\(\\eta_i = \\Phi^{-1}(\\pi_i) = \\mathbf{x}_i'\\beta\\)\n\nHere, our coefficients \\(\\hat \\beta\\) represent changes in “probits” or changes “z-score” units. We use the Normal CDF (\\(\\Phi()\\)) aka pnorm() in R to transform them back into probabilities, specifically, the probability that \\(Y_i\\) is 1.\nLet’s fit our binary model with probit. We just need to change the link function.\nWe can then fit using glm where family = binomial(link=\"probit\")\n\nout.probit &lt;- glm(partbinary ~ female + edu + age + sexism, data=anes,\n                 family = binomial(link=\"probit\"))\n\nLet’s apply the equation tool to this:\n## Each time after, run library\nlibrary(equatiomatic)\n\n## Will output in latex code, though see package for details on options\nextract_eq(out.probit, wrap = TRUE, terms_per_line = 3)\n\n\\[\n\\begin{aligned}\nP( \\operatorname{partbinary} = \\operatorname{1} ) &= \\Phi[\\alpha + \\beta_{1}(\\operatorname{female}) + \\beta_{2}(\\operatorname{edu})\\ + \\\\\n&\\qquad\\ \\beta_{3}(\\operatorname{age}) + \\beta_{4}(\\operatorname{sexism})]\n\\end{aligned}\n\\]\n\nThe summary output includes the probit coefficients, standard errors, z-scores, and p-values.\n\nsummary(out.probit)\n\n\nCall:\nglm(formula = partbinary ~ female + edu + age + sexism, family = binomial(link = \"probit\"), \n    data = anes)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.6343   0.3188   0.4470   0.6361   1.2477  \n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  0.603661   0.184864   3.265  0.00109 ** \nfemale      -0.202300   0.083407  -2.425  0.01529 *  \nedu          0.179264   0.027611   6.493 8.44e-11 ***\nage          0.005145   0.002257   2.280  0.02261 *  \nsexism      -0.898871   0.186443  -4.821 1.43e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1361.5  on 1584  degrees of freedom\nResidual deviance: 1250.6  on 1580  degrees of freedom\n  (355 observations deleted due to missingness)\nAIC: 1260.6\n\nNumber of Fisher Scoring iterations: 5\n\n\nWe can interepret the sign and significance of the coefficients similarly to OLS. They just aren’t in units of \\(Y\\). In the section next week, we will discuss in detail how to generate quantities of interest from this output."
  },
  {
    "objectID": "06-BinaryOutcomes.html#to-logit-or-to-probit",
    "href": "06-BinaryOutcomes.html#to-logit-or-to-probit",
    "title": "6  Binary Dependent Variables",
    "section": "6.4 To logit or to probit?",
    "text": "6.4 To logit or to probit?\nBoth approaches produce a monotonically increasing S-curve in probability between 0 and 1, which vary according to the linear predictor (\\(\\mathbf{x_i}^T\\beta\\)). In this way, either approach satisfies the need to keep our estimates, when transformed, within the plausible range of \\(Y\\).\n Image from Kosuke Imai.\n\nBoth also start with \\(Y_i\\) as bernoulli\nBoth produce the same function of the log-likelihood BUT define \\(\\pi_i\\) and link function differently\nResults–in terms of sign and significance of coefficients– are very similar\n\nLogit coefficients are roughly 1.6*probit coefficients\n\nResults–in terms of predicted probabilities– are very similar\n\nException– at extreme probabilities– Logit has “thicker tails”, gets to 0 and 1 more slowly\n\nSometimes useful–Logit can also be transformed into “odds ratios”\nBy convention, logit slightly more typically used in political science but easy enough to find examples of either\n\nNote on Odds Ratios in Logistic Regression\nCoefficients are in “logits” or changes in “log-odds” (\\(\\log \\frac{\\pi_i}{1 - \\pi}\\)). Some disciplines like to report “odds ratios”\n\nOdds ratio: \\(\\frac{\\pi_i(x1)/(1 - \\pi(x1))}{\\pi_i(x0)/(1 - \\pi(x0))}\\) (at a value of x1 vs. x0)\n\nIf \\(\\log \\frac{\\pi_i}{1 - \\pi} = logodds\\); \\(\\exp(logodds) = \\frac{\\pi_i}{1 - \\pi}\\)\nTherefore, if we exponentiate our coefficients, this represents an odds ratio: the odds of \\(Y_i = 1\\) increase by a factor of (\\(\\exp(\\hat \\beta_k)\\)) due to 1-unit change in X\n\n\n\n## odds ratio for the 4th coefficient\nexp(coef(out.logit)[4])\n\n    age \n1.00872 \n\n## CI for odds ratios\nexp(confint(out.logit)[4, ])\n\nWaiting for profiling to be done...\n\n\n   2.5 %   97.5 % \n1.000790 1.016801 \n\n\nIn political science, we usually opt to present predicted probabilities instead of odds ratios, but ultimately you should do whatever you think is best."
  },
  {
    "objectID": "06-BinaryOutcomes.html#latent-propensity-representation",
    "href": "06-BinaryOutcomes.html#latent-propensity-representation",
    "title": "6  Binary Dependent Variables",
    "section": "6.5 Latent propensity representation",
    "text": "6.5 Latent propensity representation\nSometimes you will see the binary outcome problem represented as a latent propensity where \\(Y^*_i\\) is a continuous variable that represents an unobserved propensity (e.g., to have a dispute, to be a toxic tweet, to participate), where\n\\[\\begin{gather*}\nY_i = \\begin{cases}1, \\; y^*_i &gt; \\tau \\\\ 0,\\;\ny^*_i \\leq \\tau \\end{cases}\n\\end{gather*}\\]\nand \\(\\tau\\) is some threshold after which a the event (e.g., dispute) occurs.\nThis becomes particularly relevant when the goal is to classify outcome estimates given certain \\(X\\) features. This type of threshold will also be relevant when we move into ordinal outcome variables where we want to estimate the probability an outcome belongs to a specific category."
  },
  {
    "objectID": "06-BinaryOutcomes.html#linear-probability-models",
    "href": "06-BinaryOutcomes.html#linear-probability-models",
    "title": "6  Binary Dependent Variables",
    "section": "6.6 Linear Probability Models",
    "text": "6.6 Linear Probability Models\nPeople (who me? yes, I admit, me) will sometimes still use a linear OLS model when we have dichotomous outcomes. In that case, we interpret the results as a “linear probability model” where a one-unit change in \\(x\\) is associated with a \\(\\hat \\beta\\) change in the probability that \\(Y_i = 1\\).\nThis may sound like a disaster because linear models are generally meant for nice continuous outcomes, and there is no way to prevent extreme values of \\(X\\beta\\) from extending above 1 or below 0. This is not to mention the heteroskedasticity issues that come from binary outcome because the error terms depend on the values of \\(X\\). This website has a good overview of the potential problems with linear regression with binary outcomes.\n\\\nImage from Chelsea Parlett-Pelleriti @ChelseaParlett on Twitter\nHowever, we can address some of these potential issues: 1) we can use robust standard errors to account for non-constant error variance , 2) if you look at the S-curve in the previous section, you will note that a large part of the curve is pretty linear over a wide range of \\(X\\beta\\) values. For many applications, the estimates transformed from a logit or probit into probability will look similar to the estimates from a linear probability model (i.e., OLS). 3) Linear probability models are easier to interpret, and there is no need to transform coefficients.\nLPM vs. logit/probit has spurred a lot of debate throughout the years. Reviewers disagree, twitter users disagree, some people just like to stir the pot, etc. This is just something to be aware of as you choose modeling approaches. Particularly when it comes to experiments and other causal inference approaches, there is a non-trivial push among active scholars to stick with linear probability models when your key independent variable is a discrete treatment indicator variable. See this new article from Robin Gomilla who lays out the considerations for using LPM, particularly in experimental settings, as well as follow up discussion from Andrew Gelman. That said, even if you run with an LPM and cite the Gomilla article, a reviewer may still ask you to do a logit/probit. And there are certainly circumstances where LPM will fall short. So what’s the upshot? Probably try both, and then choose your own adventure."
  },
  {
    "objectID": "06-BinaryOutcomes.html#binary-models-in-r-tutorial",
    "href": "06-BinaryOutcomes.html#binary-models-in-r-tutorial",
    "title": "6  Binary Dependent Variables",
    "section": "6.7 Binary Models in R Tutorial",
    "text": "6.7 Binary Models in R Tutorial\nThis week’s example, we will replicate a portion of “The Effectiveness of a Racialized Counterstrategy” by Antoine Banks and Heather Hicks, published in the American Journal of Political Science in 2018. The replication data are here.\nAbstract: Our article examines whether a politician charging a political candidate’s implicit racial campaign appeal as racist is an effective political strategy. According to the racial priming theory, this racialized counterstrategy should deactivate racism, thereby decreasing racially conservative whites’ support for the candidate engaged in race baiting. We propose an alternative theory in which racial liberals, and not racially conservative whites, are persuaded by this strategy. To test our theory, we focused on the 2016 presidential election. We ran an experiment varying the politician (by party and race) calling an implicit racial appeal by Donald Trump racist. We find that charging Trump’s campaign appeal as racist does not persuade racially conservative whites to decrease support for Trump. Rather, it causes racially liberal whites to evaluate Trump more unfavorably. Our results hold up when attentiveness, old-fashioned racism, and partisanship are taken into account. We also reproduce our findings in two replication studies.\nWe will replicate the analysis in Table 1 of the paper, based on an experiment the authors conducted through SSI. They exposed white survey respondents to either a news story about a Trump ad that includes an “implicit racial cue” or conditions that add to this with “explicitly racial” responses from different partisan actors calling out the ad as racist. Drawing on racial priming theory, racially prejudiced whites should be less supportive of Trump after the racial cues are made explicit. The authors test this hypothesis against their own hypothesis that this effect should be more pronounced among “racially liberal” whites.\n \nWe are going to focus on a secondary outcome related to whether respondents believed the ad to be about race: “We also suspect that whites should provide a justification for either maintaining or decreasing their support for the candidate alleged to be playing the race card. Our results support these expectations. For example, racial liberals who read about a politician calling Trump’s implicit ad racist are more likely than those in the implicit condition to believe Trump’s ad is about race. On the other hand, pointing out the racial nature of the ad does not cause resentful whites to be any more likely to believe the ad is about race. Racially resentful whites deny that Trump’s subtle racial appeal on crime is racially motivated, which provides them with the evidence they need to maintain their support for his presidency” (320).\n\n6.7.1 Loading data and fitting glm\nLet’s load the data.\n\nlibrary(rio)\nstudy &lt;- import(\"https://github.com/ktmccabe/teachingdata/blob/main/ssistudyrecode.dta?raw=true\")\n\nThe data include several key variables\n\nabtrace1: 1= if the respondent thought the ad was about race. 0= otherwise\ncondition2: 1= respondent in the implicit condition. 2= respondent in one of four explicit racism conditions.\nracresent: a 0 to 1 numeric variable measuring racial resentment\noldfash: a 0 to 1 numeric variable measuring “old-fashioned racism”\ntrumvote: 1= respondent has vote preference for Trump 0=otherwise\n\n\nLet’s try to replicate column 5 in Table 1 using probit regression, as the authors do.\n\nWrite down the equation for the regression.\nUse glm to run the regression.\nCompare the output to the table, column 5.\n\n\n\nTry on your own, then expand for the solution.\n\nWe are fitting a probit regression.\n\n## Column 5\nfit.probit5 &lt;- glm(abtrace1 ~ factor(condition2)*racresent + factor(condition2)*oldfash,\n                  data=study, family=binomial(link = \"probit\"))\nsummary(fit.probit5)\n\n\nCall:\nglm(formula = abtrace1 ~ factor(condition2) * racresent + factor(condition2) * \n    oldfash, family = binomial(link = \"probit\"), data = study)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.2659  -0.9371  -0.5194   1.0015   2.0563  \n\nCoefficients:\n                              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                     0.6320     0.2431   2.600 0.009323 ** \nfactor(condition2)2             0.9685     0.2797   3.462 0.000535 ***\nracresent                      -1.9206     0.4174  -4.601  4.2e-06 ***\noldfash                         0.5265     0.3907   1.348 0.177772    \nfactor(condition2)2:racresent  -0.8513     0.4728  -1.801 0.071777 .  \nfactor(condition2)2:oldfash    -0.4197     0.4376  -0.959 0.337476    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1376.9  on 994  degrees of freedom\nResidual deviance: 1148.7  on 989  degrees of freedom\n  (25 observations deleted due to missingness)\nAIC: 1160.7\n\nNumber of Fisher Scoring iterations: 4\n\n\nWe can write the regression as: \\[\\begin{align*}\nPr(Y_i = 1 | X) &= \\\\ \\Phi(\\alpha + \\text{Explicit Politician Condition}_i*\\beta_1 +\\\\ \\text{Racial Resentment}_i *\\beta_2 +\\\\\n\\text{Old Fashioned Racism}_i*\\beta_3 +\\\\ \\text{Explicit Politician Condition}_i*\\text{Racial Resentment}_i*\\beta_4 +\\\\\n\\text{Explicit Politician Condition}_i* \\text{Old Fashioned Racism}_i*\\beta_5)\n\\end{align*}\\]\n\nlibrary(equatiomatic)\nextract_eq(fit.probit5, wrap = TRUE, terms_per_line = 3)\n\n\\[\n\\begin{aligned}\nP( \\operatorname{abtrace1} = \\operatorname{1} ) &= \\Phi[\\alpha + \\beta_{1}(\\operatorname{factor(condition2)}_{\\operatorname{2}}) + \\beta_{2}(\\operatorname{racresent})\\ + \\\\\n&\\qquad\\ \\beta_{3}(\\operatorname{oldfash}) + \\beta_{4}(\\operatorname{factor(condition2)}_{\\operatorname{2}} \\times \\operatorname{racresent}) + \\beta_{5}(\\operatorname{factor(condition2)}_{\\operatorname{2}} \\times \\operatorname{oldfash})]\n\\end{aligned}\n\\]\n\n\n\nA few questions:\n\nHow should we interpret the coefficients?\nHow do the interactions affect this interpretation?\n\n\n\n6.7.2 Numeric Optimization\nLet’s repeat our replication of column 5, but this time, let’s use numeric optimization. We first need to make an X and Y matrix from our data. Because we have already run the models, let’s use a trick below:\n\nX &lt;- model.matrix(fit.probit5)\nY &lt;- as.matrix(fit.probit5$y)\n\nThe next thing we need to do is make a function for the log likelihood. Let’s recall the log likelihood for a Bernoulli random variable from a previous section:\n\\[\\begin{align*}\n\\mathcal L( \\pi | Y_i) &= \\underbrace{\\pi^{y_i}(1 -\\pi)^{(1-y_i)}}_\\text{Likelihood for single observation}\\\\\n\\mathcal L( \\pi | Y) &= \\underbrace{\\prod_{i=1}^n\\pi^{y_i}(1 -\\pi)^{(1-y_i)}}_\\text{Likelihood for all observations}\\\\\n\\ell( \\pi | Y) &= \\underbrace{\\sum_{i=1}^n\\log \\pi^{y_i}(1 -\\pi)^{(1-y_i)}}_\\text{Log likelihood}\n\\end{align*}\\]\nNow we just change the definition of \\(\\pi\\) to be the transformation for a probit, which is \\(\\Phi(X\\beta)\\). We code this up as a function below. Try to make the connection between the log likelihood equation above and the last line of the function. Note: in R, we can use pnorm() to express \\(\\Phi(X\\beta)\\), this is the CDF of the normal distribution.\n\nllik.probit &lt;- function(par, Y, X){\n  beta &lt;- as.matrix(par)\n  link &lt;- pnorm(X %*% beta)\n  like &lt;- sum(Y*log(link) + (1 - Y)*log(1 - link))\n  return(like)\n}\n\nLet’s generate some starting values for the optimization.\n\n## Starting values\nls.lm &lt;- lm(Y ~ X[, -1]) \nstart.par &lt;- ls.lm$coef\n\nFinally, let’s use optim to find our parameter estimates.\n\n## optim()\nout.opt &lt;- optim(start.par, llik.probit, Y=Y,\n                 X=X, control=list(fnscale=-1), method=\"BFGS\",\n                 hessian = TRUE)\n\nWe can compare the estimates recovered from glm and optim.\n\n## Log likelihood\nlogLik(fit.probit5)\nout.opt$value\n\n'log Lik.' -574.3267 (df=6)\n[1] -574.3267\n\n\n\n## Coefficients\ncbind(round(coef(fit.probit5), digits=4),\nround(out.opt$par, digits = 4))\n\n                                 [,1]    [,2]\n(Intercept)                    0.6320  0.6321\nfactor(condition2)2            0.9685  0.9684\nracresent                     -1.9206 -1.9207\noldfash                        0.5265  0.5265\nfactor(condition2)2:racresent -0.8513 -0.8512\nfactor(condition2)2:oldfash   -0.4197 -0.4196\n\n\nHow could we get the standard errors?\n\n\n6.7.3 Predicted Probabilities\nOne thing we could do for our interpretations is to push this further to generate “quantities of interest.”\n\nWe will do one example of this but will talk in more detail about this next week. Let’s generate predicted probabilities for thinking the ad is about race across levels of racial resentment in the sample, for people in the implicit and explicit conditions. For this example, we are going to hold “old-fashioned racism” at its mean value. This is slightly different from what the authors do but should generate similar results. The authors hold old-fashioned racism at its “observed values.”\nWe can rely on the predict function like we did with OLS, but here, we need to set type = response to put the results on the response scale instead of the scale of the linear predictor. What this does is apply our function \\(\\Phi(\\mathbf{x_i}' \\hat \\beta)\\) for our designated values of \\(X\\) and estimates for \\(\\hat \\beta\\).\n\npredvals.imp &lt;- predict(fit.probit5, newdata = data.frame(condition2=1, \n                                                         racresent = seq(0, 1, .0625),\n                                         oldfash = mean(study$oldfash, na.rm=T)),\n                    type=\"response\")\npredvals.exp &lt;- predict(fit.probit5, newdata = data.frame(condition2=2,\n                                                         racresent = seq(0, 1,.0625),\n                                         oldfash = mean(study$oldfash, na.rm=T)),\n                    type=\"response\")\n\n## Plot results\nplot(x=seq(0, 1, .0625), y=predvals.imp, type=\"l\",\n     ylim = c(0, 1), lty=2,\n     ylab = \"Predicted Probability\",\n     xlab = \"Racial Resentment\",\n     main = \"Predicted Probability of Viewing the Ad as about Race\",\n     cex.main = .7)\nlegend(\"bottomleft\", lty= c(2,1), c(\"Implicit\", \"Explicit\"))\npoints(x=seq(0, 1, .0625), y=predvals.exp, type=\"l\")\n\n\n\n\nAdditional Questions\n\nFor extra practice, you can try replicating column 4 in the model.\nYou can also try replicating the results in the figure with a logit model. Are the predicted probabilities similar?"
  },
  {
    "objectID": "07-QuantitiesofInterest.html#using-the-response-functions-to-generate-quantities-of-interest",
    "href": "07-QuantitiesofInterest.html#using-the-response-functions-to-generate-quantities-of-interest",
    "title": "7  Quantities of Interest",
    "section": "7.1 Using the response functions to generate quantities of interest",
    "text": "7.1 Using the response functions to generate quantities of interest\nRecall, in linear regression, to get our estimated values \\(\\hat Y\\) we said \\(\\hat Y = X\\hat\\beta\\).\n\nIn glm’s, we can do the same to get our estimated values on the scale of the linear predictor \\(\\hat \\eta = X\\hat\\beta\\).\nWe then use our \\(Link^{-1}\\) response function to transform these values into the quantity of interest.\n\nE.g., in logistic regression we want \\(\\hat{\\pi} = \\frac{\\exp(X\\hat\\beta)}{1 + \\exp(X\\hat\\beta)}\\).\nE.g., in probit regression we want \\(\\hat{\\pi} = \\Phi(X \\hat \\beta)\\).\nThese represent the predicted probability of \\(Y_i = 1\\) given our coefficient estimates and designated values of the covariates\n\n\nLet’s use a subset of the MIDs mids.txt data available here.\nThis dataset has variables related to whether a dyad of states is engaged in a militarized interstate dispute between the two countries in a given year. The variable that will be our outcome of interest is Conflict which takes the values 0 or 1. We will also look at the relationship between a few independent variables and the propensity for conflict. Data are at Dyad Level.\n\nwhether the pair of countries\n\ninclude a major power (MajorPower, 1=yes, 0=otherwise),\nare contiguous ( Contiguity, 1=yes, 0=otherwise),\nare allies (Allies, 1=yes, 0=otherwise),\nand/or have similar foreign policy portfolios (ForeignPolicy, 1=yes, 0=otherwise)\n\nBalanceofPower: balance of military power\nYearsSince: the number of years since the last dispute\n\n\n## Load data\nmids &lt;- read.table(\"https://raw.githubusercontent.com/ktmccabe/teachingdata/main/midsshort.txt\")\n\ntable(mids$Conflict)\n\n\n    0     1 \n99657   343 \n\n\nAs can be seen from the table above, conflicts (fortunately) are relatively rare in our data. This means are predicted probabilities are likely going to be pretty small in this example.\nWe will run a logistic regression with a few covariates.\n\nout.logit &lt;-glm(Conflict ~  MajorPower + Contiguity + Allies + ForeignPolicy +\n    BalanceOfPower + YearsSince, \n                family = binomial(link = \"logit\"), data = mids)\n\nOur logistic regression equation is:\n\n\\(\\log \\frac{\\pi_i}{1-\\pi_i} = \\mathbf{x_i'}\\beta\\), or alternatively\n\\(Pr(Y_i = 1 | \\mathbf{x}_i) = logit^{-1}(\\mathbf{x}_i'\\beta) = \\frac{exp(\\mathbf{x_i'}\\beta)}{1 + exp(\\mathbf{x_i'}\\beta)}\\)\n\nOur coefficients are in the “logit” aka log-odds scale of the linear predictor, so we use the response function to put them into probability estimates.\nFor logistic regression, we can generate predicted probabilities for each \\(Y_i\\) using the\n\npredict(model, type=\"response\") function, using the\nplogis function with \\(X\\hat \\beta\\), or\n\nQuestion: What would be the equivalent function for probit?\n\nmanually writing down the response function \\(\\frac{exp(\\mathbf{x_i'}\\beta)}{1 + exp(\\mathbf{x_i'}\\beta)}\\).\n\nWe use the predict function exactly the same way as before, but the key argument we need to specify is type. If you have type = link this generates answers that are still on the log-odds linear predictor scale. It is switching this to response that goes into probability in the logit case or probit case.\nExample\nFirst, let’s just generate predicted probabilities for each observation in our data, without specifiying any designated values for \\(X\\)– just keeping the values as they are in the data aka “as observed.”\n\n## Method with predict()\n## When you don't specify newdata, R assumes you want the X data from the model\npp &lt;- predict(out.logit, type = \"response\")\n\n## Manual way #1\nX &lt;- model.matrix(out.logit)\nbh &lt;- coef(out.logit)\npplogis &lt;- plogis(X %*% bh)\n\n## Manual way # 2\nppexp &lt;- exp(X %*% bh)/(1 + exp(X %*% bh))\n\n## Compare the first five rows of each to see they are the same\ncbind(pplogis, ppexp, pp)[1:5,]\n\n                                           pp\n78627  0.0004204501 0.0004204501 0.0004204501\n295818 0.0001035526 0.0001035526 0.0001035526\n251841 0.0006211178 0.0006211178 0.0006211178\n98068  0.0004270812 0.0004270812 0.0004270812\n209797 0.0001005396 0.0001005396 0.0001005396\n\n\nThe code above generates a predicted probability associated with each observation in the data. This is similar to generating a fitted value \\(\\hat y\\) for each observation in OLS."
  },
  {
    "objectID": "07-QuantitiesofInterest.html#qoi-at-designated-values",
    "href": "07-QuantitiesofInterest.html#qoi-at-designated-values",
    "title": "7  Quantities of Interest",
    "section": "7.2 QOI at Designated Values",
    "text": "7.2 QOI at Designated Values\nUsually in social science we have hypotheses about how the predicted probabilities change as one or more of our independent variables change. We will now turn to calculating predicted responses according to specific values of the independent variables.\nRecall, sometimes in linear regression, we wanted to calculate a specific estimated value of \\(\\hat Y_i\\) for when we set \\(X\\) at particular values. (e.g., What value do we estimate for \\(Y\\) when \\(X1 = 2\\) and \\(X2=4\\)?)\n\nIn OLS, this would be \\(\\hat Y_i = \\hat \\alpha + 2*\\hat \\beta_1 + 4*\\hat \\beta_2\\)\n\nHere, we can do the same for GLMs by setting specific values for \\(X\\) when we apply the \\(Link^{-1}\\) response function.\n\nE.g., What is the predicted probability of \\(Y_i = 1\\) when \\(X1 = 2\\) and \\(X2=4\\)?\n\nIn logistic regression, \\(\\hat{\\pi_i} = \\frac{\\exp(\\hat \\alpha + 2*\\hat \\beta_1 + 4*\\hat \\beta_2)}{1 + \\exp(\\hat \\alpha + 2*\\hat \\beta_1 + 4*\\hat \\beta_2)}\\)\n\n\nExample 1\nExample using the Conflict data using different approaches in R.\n\n## Predicted probability when Allies = 1, and all other covariates = 0\nallies1 &lt;- predict(out.logit, newdata = \n                     data.frame( MajorPower = 0,\n                                 Contiguity = 0, \n                                 Allies = 1, \n                                 ForeignPolicy = 0,BalanceOfPower = 0, \n                                 YearsSince = 0),\n                   type = \"response\")\nallies1\n\n          1 \n0.002632504 \n\n## for allies = 1, careful of the order to make same as coefficients\nX &lt;- cbind(1, 0, 0, 1, 0, 0 , 0) \nBh &lt;- coef(out.logit)\n\n## Approach 1\nplogis(X %*% bh)\n\n            [,1]\n[1,] 0.002632504\n\n## Approach 2\nexp(X%*% bh)/(1 + exp(X%*% Bh))\n\n            [,1]\n[1,] 0.002632504\n\n\nExample 2\nSecond example keeping X at observed values. Here the manual approach is easier given the limitations of predict (at least until we learn a new package). Now we are estimating \\(N\\) predicted probabilities, so we take the mean to get the average estimate.\n\n## for allies = 1careful of the order to make same as coefficients\nX &lt;- model.matrix(out.logit)\nX[, \"Allies\"] &lt;- 1 # change Allies to 1, leave everything else as is\nBh &lt;- coef(out.logit)\n\n## Approach 1\nmean(plogis(X %*% bh))\n\n[1] 0.002759902\n\n## Approach 2\nmean(exp(X%*% bh)/(1 + exp(X%*% Bh)))\n\n[1] 0.002759902\n\n\nThis is the average predicted probability of having a dispute when the dyad states are Allies, holding other covariates at their observed values.\nHere is a brief video with a second example of the process above, leading into the discussion of marginal effects below. It uses the anes data from Banda and Cassese in section 6.\n\n\n7.2.1 Marginal Effects\nRecall, in linear regression a one-unit change in \\(X_k\\) is associated with a \\(\\hat \\beta_k\\) change in \\(Y\\) no matter where we are in the domain of \\(X_k\\). (The slope of a line is constant!)\nThe catch for glm’s, again, is that our linear predictor (\\(\\eta\\)) is often not in the units of \\(Y\\) that we want. E.g., In logistic regression, a one-unit change in \\(X_k\\) is associated with a \\(\\hat \\beta_k\\) logits change\n\nRecall, for logit and probit, this takes us into an S-curve for \\(Pr(Y_i = 1)\\) instead of a line\nWell, the slope of an S-curve is not constant. Depending on where we are in \\(X\\), it will influence how much change we have in the predicted probability of \\(Y_i = 1\\).\nTherefore, to understand the marginal effect in glm’s we have to set \\(X\\) to particular values and be careful about the values we select.\n\nBy “careful,” this means choosing sensible, theoretically informed values of interest.\n\n\nYou can generate predictions based on any values. Here are three common approaches for understanding the marginal effect of a particular variable \\(X_k\\).\n\nMarginal effects at the mean\nAverage marginal effects\nMarginal effects at representative values\n\nWait, what do we mean by marginal effects?\n\nFor a discrete (categorical/factor) variable (\\(X_k\\)) this will be the change in predicted probability associated with a one-unit change in (\\(X_k\\)).\nFor continuous variables (\\(X_k\\)), this technically is the instantaneous rate of change (change in probability associated with a very small change in \\(X\\)).\n\nUsually instead of estimating this (what is a very small change anyway?) we will do this by hand instead, and set the specific amount of change). Often, this is called “discrete change” or “first difference” effect.\n\n\n\n\n7.2.2 Marginal effects at the mean\nIn this approach, when we calculate the difference in predicted probability resulting from a one-unit change in \\(X_k\\), we set all other covariates \\(X_j\\) for \\(j \\neq k\\) at their mean values.\n\nThis gives us 1 estimate for the difference in predicted probability\nWhen can this be problematic? (think categorical variables)\n\n\n\n7.2.3 Marginal effects at representative values\nIn this approach, when we calculate the difference in predicted probability resulting from a one-unit change in \\(X_k\\), we set all other covariates \\(X_j\\) for \\(j \\neq k\\) at values that are of theoretical interest. This could be the mean value, modal vale, or some other value that makes sense for our research question.\n\nThis gives us 1 estimate for the difference in predicted probability\nDepending on your research question, there may/may not be a particularly interesting set of representative values on all of your covariates.\n\nThe example above where we held all other covariates at zero would be an example of calculating marginal effects at representative values.\n\n\n7.2.4 Average marginal effects\nIn this approach, when we calculate the difference in predicted probability resulting from a one-unit change in \\(X_{ik}\\), we hold all covariates \\(X_{ij}\\) for \\(j \\neq k\\) at their observed values.\n\nThis gives us \\(N\\) estimates for the difference in predicted probability\nWe report the average of these estimates\n\nHere is an example for average marginal effects. Let’s sat we were interested in the difference in probability of a dispute for Allies vs. non-Allies, when all other covariates are zero. We can do this manually or in predict.\n\n## Extract beta coefficients\nBh &lt;- coef(out.logit)\n\n## Set Allies to 1, hold all other covariates as observed\nX1 &lt;- model.matrix(out.logit)\nX1[, \"Allies\"] &lt;- 1\n\n## Set Allies to 0, hold all other covariates as observed\nX0 &lt;- model.matrix(out.logit)\nX0[, \"Allies\"] &lt;- 0\n\npp1 &lt;- mean(plogis(X1 %*% Bh))\npp0 &lt;- mean(plogis(X0 %*% Bh))\npp1 - pp0\n\n[1] -0.0009506303\n\n\nThis represents the average difference in predicted probability of having a dispute for Dyads that are Allies vs. not Allies.\n\n\n7.2.5 marginaleffects, prediction and margins packages.\nThere are functions that can make this easier so long as you understand what they are doing.\nOne package developed by Dr. Thomas Leeper is prediction. A second is margins. Documentation available here and here. More recently, Vincent Arel-Bundock has developed the marginaleffects package, which has more expansive functionality here.\nIt is always important to understand what’s going on in a package because, for one, it’s possible that the package will stop being updated, and you will have to find an alternative solution.\n\n7.2.5.1 marginaleffects\nWe first look at the marginaleffects package. The predictions function generates specific quantities of interest. An advantage it has over the built-in predict function is that it makes it easier to “hold all other variables at observed values.” In the predictions function, you specify the designated values for particular variables, and then by default, it assumes you want to hold all other variables at observed values. Here is an example of generating predicted probabilities for Allies = 1 and Allies = 0. It will generate the summary means of these two predictions.\n\nlibrary(marginaleffects)\npreout &lt;- predictions(\n    out.logit,\n    type = \"response\",\n    by = \"Allies\",\n    newdata = datagridcf(Allies = 0:1))\n\nThe datagridcf argument stands for a counterfactual dataset, where we are keeping the data as they are observed, but counterfactually changing Allies from 0 to 1. By putting type=\"response\", we make sure our results are in terms of predicted probabilities.\nWe can then conduct comparisons using the comparison or avg_comparisons functions, looking at, for example, the average difference in predicted probability, going from Allies being 0 to 1, keeping all covariates at their observed values.\n\ncomp &lt;- comparisons(out.logit,\n  variables = list(Allies = c(0, 1)))\nmean(comp$estimate)\n\n[1] -0.0009506303\n\ncomp_avg &lt;- avg_comparisons(out.logit,\n  variables = list(Allies = c(0, 1)))\ncomp_avg$estimate\n\n[1] -0.0009506303\n\n\n\n\n7.2.5.2 prediction\nHere we will focus on the prediction package. The prediction function generates specific quantities of interest. An advantage it has over the built-in predict function is that it makes it easier to “hold all other variables at observed values.” In the prediction function, you specify the designated values for particular variables, and then by default, it assumes you want to hold all other variables at observed values. Here is an example of generating predicted probabilities for Allies = 1 and Allies = 0. It will generate the summary means of these two predictions.\n\n## install.packages(\"prediction\")\nlibrary(prediction)\n\n## By default, allows covariates to stay at observed values unless specified\nprediction(out.logit, at = list(Allies = c(0, 1)), \n           type = \"response\")\n\nData frame with 200000 predictions from\n glm(formula = Conflict ~ MajorPower + Contiguity + Allies + ForeignPolicy + \n    BalanceOfPower + YearsSince, family = binomial(link = \"logit\"), \n    data = mids)\nwith average predictions:\n\n\n Allies        x\n      0 0.003711\n      1 0.002760\n\n## compare with the manual calculated values above\npp0\n\n[1] 0.003710532\n\npp1\n\n[1] 0.002759902\n\n\n\n\n\n7.2.6 QOI Practice Problems\n\nConduct the following regression using glm\n\n\\(Pr(Conflict_i = 1 | X) = logit^{-1}(\\alpha + \\beta_1 * Allies_i + \\beta_2 * MajorPower_i + \\beta_3 * ForeignPolicy_i)\\)\n\nWhat is the predicted probability of entering a dispute when the dyad includes a major power, holding all covariates at observed values?\nRepeat the previous exercise, but now use probit. How similar/different are the predicted probability estimates?\n\n\n\nTry on your own, then expand for the solution.\n\n\n## Problem 1\nout.logit2 &lt;- glm(Conflict ~ Allies + MajorPower + ForeignPolicy, data=mids,\n                  family = binomial(link = \"logit\"))\n\n## Problem 2\nlibrary(prediction)\nprediction(out.logit, at = list(MajorPower = 1), \n           type = \"response\")\n\nData frame with 100000 predictions from\n glm(formula = Conflict ~ MajorPower + Contiguity + Allies + ForeignPolicy + \n    BalanceOfPower + YearsSince, family = binomial(link = \"logit\"), \n    data = mids)\nwith average prediction:\n\n\n MajorPower        x\n          1 0.007745\n\n## Problem 3\nout.probit &lt;- glm(Conflict ~ Allies + MajorPower + ForeignPolicy, data=mids,\n                  family = binomial(link = \"probit\"))\nprediction(out.probit, at = list(MajorPower = 1), \n           type = \"response\")\n\nData frame with 100000 predictions from\n glm(formula = Conflict ~ Allies + MajorPower + ForeignPolicy, \n    family = binomial(link = \"probit\"), data = mids)\nwith average prediction:\n\n\n MajorPower       x\n          1 0.01451\n\n## Manual approach\nX &lt;- model.matrix(out.probit)\nX[, \"MajorPower\"] &lt;- 1\nBhat &lt;- coef(out.probit)\n\nmean(pnorm(X %*% Bhat))\n\n[1] 0.01450613"
  },
  {
    "objectID": "07-QuantitiesofInterest.html#uncertainty",
    "href": "07-QuantitiesofInterest.html#uncertainty",
    "title": "7  Quantities of Interest",
    "section": "7.3 Uncertainty",
    "text": "7.3 Uncertainty\nUsually, we want to report a confidence interval around our predicted probabilities, average predicted probabilities, or around the difference in our predicted probabilities or difference in our average predicted probabilities.\nThis is different from the uncertainty of a coefficient, which we already have from our glm output. Here, if we say there is a 0.01 probability of a dispute, that is just an estimate, it is going to vary over repeated samples. We want to generate a confidence interval that represents this variability in \\(\\hat \\pi\\).\nWe have already discussed using the predict function in lm to generate confidence intervals for OLS estimates. In a limited set of cases, we can also use this shortcut for glm by taking advantage of the distribution being approximately normal on the scale of the linear predictor. When we are estimating confidence intervals around 1) one or multiple single quantities of interest (a predicted probability, as opposed to a difference in predicted probability) 2) where the \\(X\\) values are set at specific values (and not at their observed values) then, we can plug this into the predict function in the following way:\n\nGenerate the prediction and standard errors of the prediction on the link linear predictor scale.\n\nOn the scale of the linear predictor, the standard errors of the prediction are calculated as \\(\\sqrt{\\mathbb{x'}_c \\text{vcov(fit)} \\mathbb{x}_c}\\) using the delta method.\n\nCalculate the CI on the linear predictor scale: \\(CI(\\hat \\theta) = \\hat \\theta - z_{crit}*se_{\\hat \\theta}\\) ; \\(\\hat \\theta + z_{crit}*se_{\\hat \\theta}\\)\n\n\\(z_{crit}\\) for the 95% confidence interval is 1.96 (so this is saying +/- about 2 standard errors). We get this by using qnorm().\n\nConvert the prediction and confidence intervals to the response scale.\n\nHere is an example:\n\n## Predicted probability when Allies = 1 and all other covariates = 0\n## Note type = \"link\"\nallies1.link &lt;- predict(out.logit, newdata = \n                     data.frame( MajorPower = 0,\n                                 Contiguity = 0, \n                                 Allies = 1, \n                                 ForeignPolicy = 0,BalanceOfPower = 0, \n                                 YearsSince = 0),\n                   type = \"link\", se = T)\n\nallies1 &lt;-  plogis(allies1.link$fit)\nallies1.lb &lt;- plogis(allies1.link$fit - qnorm(.975)*allies1.link$se.fit)\nallies1.ub &lt;- plogis(allies1.link$fit + qnorm(.975)*allies1.link$se.fit)\n\n## Confidence interval\nc(allies1, allies1.lb, allies1.ub)\n\n          1           1           1 \n0.002632504 0.001302711 0.005312514 \n\n## By hand (using x as a k x 1 vector)\nx.c &lt;- rbind(1, 0, 0, 1, 0, 0, 0)\nse.hand &lt;- sqrt(t(x.c) %*% vcov(out.logit) %*% x.c)\np.hand &lt;- t(x.c) %*% coef(out.logit)\nallies1.hand &lt;- plogis(p.hand)\nallies1.hand.lb &lt;- plogis(p.hand- qnorm(.975)*se.hand)\nallies1.hand.ub &lt;- plogis(p.hand + qnorm(.975)*se.hand)\nc(allies1.hand, allies1.hand.lb, allies1.hand.ub)\n\n[1] 0.002632504 0.001302711 0.005312514\n\n\nBeyond this simple case, there are three general approaches to calculating the uncertainty of the quantities of interest. Here is a video with an overview of these three processes. The course notes contain additional detail below. It continues with the anes data example from Banda and Cassese in section 6, as did the other video in this section.\n\n\nDelta Method (based on calculus, first order Taylor Expansion approximation)\nBootstrapping (very flexible, common, computationally demanding)\nQuasi-bayesian (flexible, less computationally demanding), also described as simulation/Monte carlo simulation.\n\nFor now, we will focus on the second two methods, but some statistical software programs will report uncertainty estimates based on the Delta method. Here is more information on this method and the deltamethod function in R.\n\n7.3.1 Bootstrapping\nBootstrapping simulates the idea of conducting repeated samples to generate a distribution of estimates of your quantity of interests. We “resample” from our existing data to generate thousands of new datasets, and use each dataset to generate a slightly different quantity of interest. This distribution is then used to construct the confidence interval.\nProcess:\n\nSample from the data to generate new data frame\nRun the model: this gives new coefficient estimates and new covariate matrices\nUse new coefficient and covariate estimates to compute quantity of interest\nReplicate the previous process for about 1000 iterations to get 1000 estimates of quantity of interest\nUse the distribution of these estimates to calculate confidence intervals\n\nWhy? How does this work?\n\nIt is simulating the exercise of hypothetical repeated samples\nSimilar to Law of Large Numbers- with sufficient iterations, the empirical “bootstrap” distribution is a good approximation of the true distribution (will get closer and closer to the truth)\n\nIt won’t help us correct a bad estimate– have to work from the data we have.\n\nThe logic is we think the distribution of \\(\\bar x\\) sample estimate is centered on \\(\\mu\\) (the truth), and then we assume the distribution of \\(\\bar x*\\) (the bootstrapped estimate) is centered on \\(\\bar x\\)\n\nThis would be a good place to review the Bootstrap resources at the front of the section:\n\nPezullo, John. The Bootstrap Method for Standard Errors and Confidence Intervals.\nBanks, David. Lecture from Duke University.\n\nHow do we implement this procedure?\nExample\nFind the point estimate and 95% CI for the average predicted probability of conflict when the dyad are allies and all other covariates are held at observed values\n\n## Original regression\nout.logit &lt;-glm(Conflict ~  MajorPower + Contiguity + Allies + ForeignPolicy +\n    BalanceOfPower + YearsSince, \n                family = binomial(link = \"logit\"), data = mids)\n\n## We need to build our bootstrap procedure\n## Let's assume we just want 1 iteration\n\n## Step 1: sample to generate new data\n## this selects N row numbers from mids, with replacement\nwrows &lt;- sample(x =1:nrow(mids), size = nrow(mids), replace = T)\n\n## Create subset of data based on these rows\nsubdata &lt;- mids[wrows, ]\n\n## Step 2: run your regression model with the new data\nboot.logit &lt;-glm(Conflict ~  MajorPower + Contiguity + Allies + ForeignPolicy +\n    BalanceOfPower + YearsSince, \n                family = binomial(link = \"logit\"), data = subdata)\n\n## Step 3: generate average predicted probability\nXboot &lt;- model.matrix(boot.logit)\nXboot[, \"Allies\"] &lt;- 1\nBh &lt;- coef(boot.logit)\np.boot &lt;- mean(plogis(Xboot %*% Bh))\n\n\n\nExpand below for more details on what the sample function does.\n\nLet’s say we have a dataframe of different colors and shapes.\n\nsomedata &lt;- data.frame(colors = c(\"red\", \"blue\", \"yellow\", \"green\", \n                                  \"purple\", \"orange\", \"black\"),\n                       shapes = c(\"circle\", \"square\", \"triangle\", \n                                  \"rectangle\", \"diamond\", \"line\", \"sphere\"))\nsomedata                                  \n\n  colors    shapes\n1    red    circle\n2   blue    square\n3 yellow  triangle\n4  green rectangle\n5 purple   diamond\n6 orange      line\n7  black    sphere\n\n\nI could generate a new “resampled” dataset with the sample function. We tell the function three things: 1) choose from the row numbers in my dataframe (1:nrow(somedata)), 2) pick \\(N\\) row numbers in total (nrow(somedata)), 3) Each time you pick a given row number \\(i\\), put it back in the data, allowing the possibility that you may randomly sample it again (replace = TRUE).\n\nsample(1:nrow(somedata), nrow(somedata), replace = TRUE)\n\n[1] 3 3 4 5 7 7 2\n\nsample(1:nrow(somedata), nrow(somedata), replace = TRUE)\n\n[1] 1 4 2 6 4 5 3\n\nsample(1:nrow(somedata), nrow(somedata), replace = TRUE)\n\n[1] 7 5 3 7 6 1 7\n\n\nWhat happened is the function generated a set of row numbers. Note how it is possible for the same row number to be picked multiple times. Each time we run the sample function, we get slightly different row numbers.\nWe can subset our data based on these row indices.\n\n## store row indices\nwrows &lt;- sample(1:nrow(somedata), nrow(somedata), replace = TRUE)\nwrows\n\n[1] 7 3 6 6 1 5 4\n\n## subset data to include rows sampled\n## note if row indices are in wrows more than once, they will also be in the subset more than once\nsubdata &lt;- somedata[wrows,]\nsubdata\n\n    colors    shapes\n7    black    sphere\n3   yellow  triangle\n6   orange      line\n6.1 orange      line\n1      red    circle\n5   purple   diamond\n4    green rectangle\n\n\nGiven that each time the sample function runs, we get slightly different random samples of the data, that’s how we end up with a distribution of slightly different estimates of our quantities of interest. Each time the regression is run with a slightly different dataset.\n\nThis gives us one estimate of the average predicted probability stored in p.boot. However, the idea of a bootstrap is that we repeat this procedure at least 1000 times to generate a distribution of estimates of the quantity of interest, the average predicted probability in this case.\nWe could literally repeat that code chunk 1000 times…. but, we have better things to do than that much copy/paste. Instead, we will create a function that will do this automatically.\nTo do so, we are going to wrap our procedure above inside the syntax for creating functions in R. In R, to create a function,\n\nWe first name the function. (Let’s call this myboot. You could call yours anything.)\nThe next syntax is always myboot &lt;- function(){}.\nInside the function() part, you tell R what you are going to supply the function each time you want it to run. Sometimes functions only have one input, others like lm have multiple inputs.\n\nFor example, in the function mean(x), we always supply that function with a vector of values.\nFor this bootstrap example, we are going to write the function as one where we will supply the function with a dataframe. Let’s call this df.\n\nThe inside part of the function, between the {} is the procedure from above. All we do is\n\nInstead of writing mids, we keep it generic by writing df.\nWe add a final line that tells R what we want it to return() as the output of the function. Here, we want it to return the average predicted probability.\n\n\n\n## We need to build our bootstrap function\n## Step 4: Let's wrap our current steps into a function that we can replicate\n\n## Note: all we need as an input is our data.frame mids\n## I will label it something generic to show how a function can work\nmyboot &lt;- function(df){\n  wrows &lt;- sample(x =1:nrow(df), size = nrow(df), replace = T)\n  ## Create subset of data based on these rows\n  subdata &lt;- df[wrows, ]\n  ## Step 2: run your regression model with the new data\n  boot.logit &lt;-glm(Conflict ~  MajorPower + Contiguity + Allies + ForeignPolicy +\n    BalanceOfPower + YearsSince, \n                family = binomial(link = \"logit\"), data = subdata)\n  ## Step 3: generate average predicted probability\n  Xboot &lt;- model.matrix(boot.logit)\n  Xboot[, \"Allies\"] &lt;- 1\n  Bh &lt;- coef(boot.logit)\n  p.boot &lt;- mean(plogis(Xboot %*% Bh))\n  return(p.boot)\n}\n\nNote: here, our quantity of interest is the predicted probability of a dispute when the dyad are Allies. Let’s say, instead, we wanted the difference in predicted probability of a dispute between Allies and Non-Allies. Well, we would just adjust our function to calculate the mean probabilities for Allies and Non-Allies and return the difference in these means as the quantity of interest. We would then get 10000 estimates of this difference in probabilties.\nNow that we have the function from above, instead of copying/pasting this 1000 times, we will use the function called replicate which will do this for us. We indicate the number of estimates we want and then indicate which function (and in our case, which dataframe inside the function) we want to replicate.\n\n## This may take a minute to run. \n## We will do just 50, Normally you will want this to be more like 1000\nset.seed(1234) # this helps us get the same results each time, good for reproducibility\nmyestimates &lt;- replicate(50, myboot(mids))\n\nThe bootstrapping approach is very computationally demanding given it has to repeat an operation several (thousand) times. After you hit “run,” just sit back, relax and wait for the water to run dry.\n\n\nFor a troubleshooting tip, expand.\n\nIf you get an error message at the replicate(1000, myboot(mids)) stage, it is best to see if your function runs at all. Try just the below to see if it generates output:\n\nmyboot(mids)\n\n[1] 0.002250429\n\n\nIf you get the error here, then it means there is a bug within the function code, not the replicate code.\n\nEach time we replicate the function, it will generate slightly different results because the sample functions is randoming sampling rows of data each time. We can plot the distribution of estimates to show this.\n\nlibrary(ggplot2)\nggplot(data.frame(x = myestimates), aes(x = myestimates)) + \n  geom_histogram(aes(y=..density..)) + geom_density(color=\"red\")\n\nWarning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead.\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nThe final step after generating the bootstrap distribution of estimates is to use it to construct a confidence interval for the quantity of interest. There are a few ways to do this.\n\nNormal approximation\nPercentile\nBias correction\n\nIn each approach, we take our original “point estimate” from the computation of the quantity of interest from our original data and use the bootstrap estimates for the lower and upper bounds of the confidence interval. Here we will assume we want a 95% confidence interval.\n\n## Find the original point estimate\nBh &lt;- coef(out.logit)\nX1 &lt;- model.matrix(out.logit)\nX1[, \"Allies\"] &lt;- 1\npe1 &lt;- mean(plogis(X1 %*% Bh))\n\n## Normal\nc((pe1 - qnorm(.975)*sqrt(var(myestimates))),(pe1 + qnorm(.975)*sqrt(var(myestimates))))\n\n[1] 0.002174802 0.003345002\n\n## Percentile\nquantile(myestimates, c(0.025, .975))\n\n       2.5%       97.5% \n0.002275356 0.003431285 \n\n## Bias correction\nbc &lt;- 2*pe1 - myestimates\nquantile(bc, c(0.025, .975))\n\n       2.5%       97.5% \n0.002088520 0.003244449 \n\n\nEach of these is pretty commonly used, but they may generate slightly different results.\n\n\n7.3.2 Simulated Confidence Intervals\nQuasi-Bayesian or simulated confidence intervals take advantage of the large sample properties of our estimates \\(\\hat \\beta\\) having a Normal sampling distribution due to the Central Limit Theorem.\nLike the bootstrap, the simulation procedure also generates hypothetical new samples. However, here, we are sampling new \\(\\hat \\beta\\) estimates each time instead of sampling a new underlying dataset each time. This allows use to skip the step of generating a new dataset and running the regression 1000 times. Here, we just run the regression model once. The simulation process takes place after this step.\nProcess\n\nEstimate your model (e.g., with optim or glm)\nSample \\(\\sim\\) 1000 new estimates of the vector of \\(\\hat \\beta\\) by using the vcov of \\(\\hat \\beta\\) to generate the uncertainty\nFor each of these new \\(\\hat \\beta_c\\), calculate \\(\\hat \\theta_c\\), in this case, the predicted probabilities.\nEstimate “fundamental uncertainty” by drawing new y’s based on these parameters\n\nOnly necessary in some cases. Depends on Jensen’s Inequality discussed in the Gary King resources and in the details on the use of rbinom function below.\nWe are going to average over this, which means we are calculating “expected values.”\n\nUse this distribution to compute the CI’s\n\nThis would be a good place to review the resources from Gary King:\n\nOverview of simulation approach for calculating uncertainty from King, Tomz, and Wittenberg 2000.\nLecture video from Gary King on simulating quantities of interest\n\nExample\nThe code for this approach will more simple in a case where we are computing quantities of interest when covariates are held at means or representative values (cases where we get just one predicted probability associated with each set of \\(X\\) values). It will look a little more complex in cases where we want to hold covariates at observed values and calculate the average predicted probability.\nFirst, let’s find the point estimate and 95% CI for the predicted probability of conflict when the dyad are Allies, and all other covariates are held at zero.\n\n## install.packages(\"mvtnorm\")\nlibrary(mvtnorm)\n\n## Step 2: Sample 1000 new Bhs (we will use 50 for this example)\n## This uses the multivariate normal distribution for resampling\nset.seed(1234)\nnumsims &lt;- 50\nqb.beta &lt;- rmvnorm(numsims, coef(out.logit), vcov(out.logit)) \n## This generates numsims X k coefficients matrix\n\n## Step 3: Iterate through the estimates\n## Create an empty vector to store 1000 quantities of interest\nqestimatessimple &lt;- rep(NA, numsims)\n\n## Here, our covariate matrix stays the same each time\n## We have a 1 for intercept and 1 for Allies, everything else at zero\nX1 &lt;- cbind(1, 0, 0, 1, 0, 0 , 0) \n## X1 is a 1 X k matrix\n\n## Use a loop to iterate through each set of betas\nfor(i in 1:numsims){\n  \n  ## for each set of betas, calculate p probs\n  ## for a given set of betas, this gives us nrow(X1) predicted probabilities\n  pestimate &lt;-plogis(X1 %*% qb.beta[i,])\n  \n  ## Fundamental uncertainty\n  ## not required for logit/probit, but we will show how\n  ## rbinom generates 1000 0's or 1's based on the predicted probability\n  ## We use rbinom bc of the bernoulli, other glm's will have other distributions\n  ## then we take the mean to get our estimate of the predicted probability\n  moutcome &lt;- mean(rbinom(numsims, 1, pestimate))\n  \n  qestimatessimple[i] &lt;-moutcome\n  ## repeat for set of simulated betas\n}\n\n## Step 4: Similar to bootstrap distribution, find CI using the percentiles\nquantile(qestimatessimple, c(0.025, 0.975))\n\n 2.5% 97.5% \n 0.00  0.02 \n\n\nFor more information on loops in R, you can follow this tutorial we created for 2020 SICSS-Rutgers.\n\n\nExpand for details on rbinom.\n\nrbinom is the random generation function for the binomial distribution. If we supply it with number of trials (in the Bernoulli, this is 1), and a probability of success, it will generate our desired number of outcomes according to this distribution.\nFor example, let’s say we wanted to generate a random set of 100 coin flips for a coin that is fair– where the probability of success is .5. We will get a sample of 0’s and 1’s. If we take the mean, it will be close to .5, and with enough coin flips, will converge on .5.\n\nrb.res &lt;- rbinom(100, 1, .5)\nrb.res\n\n  [1] 0 1 0 0 0 0 1 0 0 0 1 0 1 1 0 1 0 1 0 0 1 0 1 0 1 1 0 1 0 1 0 1 1 1 1 0 0\n [38] 0 0 0 0 1 1 1 1 0 1 0 1 0 1 1 0 1 1 0 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 1 1\n [75] 1 1 1 1 0 0 1 0 1 0 0 1 0 1 1 0 1 1 1 1 1 1 0 1 0 0\n\nmean(rb.res)\n\n[1] 0.49\n\n\nIn logit/probit, this step is unneccessary because the \\(\\mathbb{E}(y_i) = \\pi_i\\). When we take the mean of our rbinom estimates, we are just going to recover the probability we supplied to it.\nHowever, in other cases, Jensen’s inequality may apply, which states that \\(\\mathbb{E}[g(X)] \\neq g(\\mathbb{E}[X])\\). For example, if we have an outcome that is distributed according to the exponential distribution: Here, the \\(\\theta\\) is \\(\\lambda\\) where \\(\\lambda = \\frac{1}{e^{X\\beta}}\\) but \\(\\mathbb{E}(y)= \\frac{1}{\\lambda}\\). Unfortunately, \\(\\mathbb{E}(\\frac{1}{\\hat \\lambda}) \\neq \\frac{1}{\\mathbb{E}(\\hat \\lambda)} = \\frac{1}{\\mathbb{E}(e^{X\\beta})}\\). For that example, the rexp() step in this case would be essential.\nIf we’re not sure when Jensen’s inequality will apply, we can just keep the fundamental uncertainty step as part of the process.\n\nFind the point estimate and 95% CI for the average predicted probability of conflict when the dyad are allies and all other covariates are held at observed values. Here, the code is more complicated, because every time we generate a predicted probability (for any observed value), we need to go through the fundamental uncertainty step (when applicable).\n\n## install.packages(\"mvtnorm\")\nlibrary(mvtnorm)\n\n## Step 2: Sample 1000 new Bhs (we will use 50 for this example)\n## This uses the multivariate normal distribution for resampling\nset.seed(1234)\nnumsims &lt;- 50\nqb.beta &lt;- rmvnorm(numsims, coef(out.logit), vcov(out.logit)) \n## This generates numsims X k coefficients matrix\n\n## Step 3: Iterate through the estimates\n## Create an empty vector to store 1000 quantities of interest\nqestimates &lt;- rep(NA, numsims)\n\n## Here, our covariate matrix stays the same\nX1 &lt;- model.matrix(out.logit)\nX1[, \"Allies\"] &lt;- 1\n\n## Use a loop to \nfor(i in 1:numsims){\n  \n  ## for each set of betas, calculate p probs\n  ## for a given set of betas, this gives us nrow(X1) predicted probabilities\n  pestimates &lt;-plogis(X1 %*% qb.beta[i,])\n  \n  ## Fundamental uncertainty\n  ## not required for logit/probit, but we will show how\n  \n  ## generate empty vector for outcomes\n  moutcomes &lt;- rep(NA, numsims)\n  \n  ## for each probability estimate, calculate the mean of simulated y's\n  for(j in 1:length(pestimates)){\n    ## rbinom generates 1000 0's or 1's based on the predicted probability\n    ## We use rbinom bc of the bernoulli, other glm's will have other distributions\n    ## then we take the mean to get our estimate of the predicted probability for a given observation\n    moutcomes[j] &lt;- mean(rbinom(numsims, 1, pestimates[j]))\n  }\n  \n  ## take the mean of the predicted probability estimates across all observations\n  qestimates[i] &lt;- mean(moutcomes)\n  ## repeat for set of simulated betas\n}\n\n## Step 4: Similar to bootstrap distribution, find CI using the percentiles\nquantile(qestimates, c(0.025, 0.975))\n\n      2.5%      97.5% \n0.00232977 0.00344796 \n\n\nBecause the shortcut applies where we do not need to calculate fundamental uncertainty in the logit / probit case, we can simplify this to:\n\n## install.packages(\"mvtnorm\")\nlibrary(mvtnorm)\n\n## Step 2: Sample 1000 new Bhs (we will use 50 for this example)\n## This uses the multivariate normal distribution for resampling\nset.seed(1234)\nnumsims &lt;- 50\nqb.beta &lt;- rmvnorm(numsims, coef(out.logit), vcov(out.logit)) \n## This generates numsims X k coefficients matrix\n\n## Step 3: Iterate through the estimates\n## Create an empty vector to store 1000 quantities of interest\nqestimates &lt;- rep(NA, numsims)\n\n## Here, our covariate matrix stays the same\nX1 &lt;- model.matrix(out.logit)\nX1[, \"Allies\"] &lt;- 1\n\nfor(i in 1:numsims){\n  pestimates &lt;-plogis(X1 %*% qb.beta[i,])\n  qestimates[i] &lt;- mean(pestimates)\n}\n\n## Step 4: Similar to bootstrap distribution, find CI using the percentiles\nquantile(qestimates, c(0.025, 0.975))\n\n       2.5%       97.5% \n0.002314344 0.003419644 \n\n\nWe can also plot the distribution of estimates\n\nlibrary(ggplot2)\nggplot(data.frame(x = qestimates), aes(x = qestimates)) + \n  geom_histogram(aes(y=..density..)) + geom_density(color=\"red\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`."
  },
  {
    "objectID": "07-QuantitiesofInterest.html#visualizing-results",
    "href": "07-QuantitiesofInterest.html#visualizing-results",
    "title": "7  Quantities of Interest",
    "section": "7.4 Visualizing Results",
    "text": "7.4 Visualizing Results\nNow that we have our point estimates of our quantities of interest and our confidence intervals, we can kick it up a notch by visualizing these results.\nLet’s plot our predicted probability when Allies = 1 with percentile Quasi-Bayesian CI’s and Bias-Corrected Bootstrap CI’s\n\nplot(x = 1:2, y= c(pe1, pe1),\n     ylim = c(0, .004),\n     xlim = c(0.5, 2.5),\n     pch = 20, cex = 1.6,\n     main = \"Average Predicted Probability of Dispute when Allies\",\n     cex.main = .7, cex.lab = .7, cex.axis = .7,\n     ylab = \"Predicted Probability\",\n     xlab = \"\", \n     xaxt = \"n\") # removes x-axis\n# add lines from c(x1, x2) on the x-axis and from c(y1, y2) on the y-axis\n# note, we don't want any horizontal movement, so we keep x1 and x2 the same c(1,1)\nlines(c(1,1), c(quantile(qestimates, c(0.025, 0.975))), lwd = 1.5)\nlines(c(2,2), c(quantile(bc, c(0.025, .975))), lwd = 1.5)\n# add text a\ntext(c(1, 2), c(0.001, 0.001), c(\"Quasi-Bayesian\", \"BC Bootstrap\"), cex = .7 )\n\n\n\n\nHere is the same but in ggplot form.\n\n## ggplot works best if you create a dataframe of the data you want to plot\nmyg &lt;- data.frame(rbind(c(pe1, quantile(qestimates, c(0.025, 0.975))),\n                       c(pe1, quantile(bc, c(0.025, .975)))))\ncolnames(myg) &lt;- c(\"pp\", \"qb\", \"bc\") \nmyg\n\n           pp          qb          bc\n1 0.002759902 0.002314344 0.003419644\n2 0.002759902 0.002088520 0.003244449\n\n## now provide this dataframe to ggplot\nggplot(myg, aes(x = 1:nrow(myg), y = pp))+ \n  geom_point(stat = \"identity\", size = 3) +\n  geom_errorbar(data=myg, aes(x =1:nrow(myg),  \n                         ymin = qb, ymax = bc), \n                width = 0, size = 1.1) +\n  xlim(.5, 2.5) +\n  ylim(0, .005) +\n  theme(axis.title.x=element_blank(),\n        axis.text.x=element_blank(),\n        axis.ticks.x=element_blank(),\n        plot.title = element_text(hjust = 0.5)) +\n  ylab(\"Predicted Probability\") +\n  ggtitle(\"Average Predicted Probability of Dispute when Allies\") +\n  annotate(\"text\", x = c(1,2), y = .004, label = c(\"Quasi-Bayesian\", \"BC Bootstrap\"))\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead."
  },
  {
    "objectID": "07-QuantitiesofInterest.html#additional-r-shortcuts",
    "href": "07-QuantitiesofInterest.html#additional-r-shortcuts",
    "title": "7  Quantities of Interest",
    "section": "7.5 Additional R shortcuts",
    "text": "7.5 Additional R shortcuts\nThere are a few R packages that help generate these quantities of interest AND estimate uncertainty. If you understand what is going on underneath the packages, then you should feel free to use them to avoid manually coding up each process.\n\n7.5.1 marginaleffects\nThe Marginal Effects Zoo is from Vincent Arel-Bundock. It is a relatively new package marginaleffects that computes a variety of quantities of interest. You can install the package below:\n\ninstall.packages(\"marginaleffects\")\n\nAs discussed above, we can generate specific estimates from designated values of our covariates of interest, holding all other covariates at their observed values.\n\nlibrary(marginaleffects)\npreout &lt;- predictions(\n    out.logit,\n    type = \"response\",\n    by = \"Allies\",\n    newdata = datagridcf(Allies = 0:1))\n\nWe can then conduct comparisons using the avg_comparisons function, looking at, for example, the average difference in predicted probability, going from Allies being 0 to 1, keeping all covariates at their observed values. Note that the avg_comparisons function returns standard errors and p-values. These are calculated through the delta method by default.\n\ncomp_avg &lt;- avg_comparisons(out.logit,\n  variables = list(Allies = c(0, 1)))\ncomp_avg\n\n\n   Term Contrast  Estimate Std. Error     z Pr(&gt;|z|)   S    2.5 %   97.5 %\n Allies    1 - 0 -0.000951   0.000403 -2.36   0.0184 5.8 -0.00174 -0.00016\n\nColumns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response \n\n\n\n\n7.5.2 Prediction\nThis package is useful for generating confidence intervals around single quantities of interest. The package below (margins) is better for constructing confidence intervals around the difference between quantities of interest.\n\nlibrary(prediction)\npreout &lt;- prediction(out.logit, at = list(Allies = c(0, 1)), \n           type = \"response\", calculate_se = T)\nsummary(preout)\n\n at(Allies) Prediction        SE      z         p    lower    upper\n          0   0.003711 0.0002270 16.343 4.878e-60 0.003266 0.004156\n          1   0.002760 0.0003135  8.804 1.322e-18 0.002145 0.003374\n\n\nI believe prediction relies on the delta method for uncertainty. The others below will have options for simulations or bootstrapped standard errors.\n\n\n7.5.3 Margins\nThomas Leeper developed a package called margins which is modeled after the Stata margins command. Here is some documentation for this package.\nWe first fit a regression model like normal.\n\nout &lt;- glm(Conflict ~  MajorPower + Contiguity + Allies + ForeignPolicy +\n    BalanceOfPower + YearsSince, data=mids, family=binomial(link = \"logit\"))\n\nOpen the package and use the margins command. It is similar to prediction but we will specify the uncertainty with vce and tell it which variable we want the marginal effect, and what type of change in that variable we want to calculate the effect.\n\n## install.packages(\"margins\")\nlibrary(margins)\n\n## Difference in Allies 0 vs. 1 holding other covariates at 0\n## Using Delta Method for uncertainty\nmarg1 &lt;- margins(out, vce = \"delta\",\n                 at = list(MajorPower=0, Contiguity=0,\n              ForeignPolicy=0, BalanceOfPower=0, YearsSince=0),\n                 variables = \"Allies\",\n                 change = c(0, 1),\n              type=\"response\") \n\nWarning in check_values(data, at): A 'at' value for 'BalanceOfPower' is outside\nobserved data range (0.000173599997651763,1)!\n\nsummary(marg1)\n\n factor MajorPower Contiguity ForeignPolicy BalanceOfPower YearsSince     AME\n Allies     0.0000     0.0000        0.0000         0.0000     0.0000 -0.0010\n     SE       z      p   lower   upper\n 0.0004 -2.3910 0.0168 -0.0018 -0.0002\n\n\nLet’s try a second example, shifting the uncertainty estimate and the X covariates.\n\n## Difference in Allies 0 vs. 1 holding MajorPower at 1, other covariates at observed values\n## Using simulations for uncertainty\nmarg2 &lt;- margins(out, vce = \"simulation\",\n                 at = list(MajorPower =1),\n                 variables = \"Allies\",\n                 change = c(0, 1),\n              type=\"response\") \nsummary(marg2)\n\n factor MajorPower     AME     SE       z      p   lower  upper\n Allies     1.0000 -0.0020 0.0010 -1.9576 0.0503 -0.0041 0.0000\n\n## Manual equivalent of point estimate\nX.marg1 &lt;- model.matrix(out)\nX.marg1[, \"Allies\"] &lt;-1\nX.marg1[, \"MajorPower\"] &lt;- 1\n\nX.marg0 &lt;- model.matrix(out)\nX.marg0[, \"Allies\"] &lt;-0\nX.marg0[, \"MajorPower\"] &lt;- 1\n\nBH &lt;- coef(out)\name &lt;- mean(plogis(X.marg1 %*% BH) - plogis(X.marg0 %*% BH))\n\n## Compare\name\n\n[1] -0.002039019\n\nsummary(marg2)$AME\n\n      Allies \n-0.002039019 \n\n\n\n\n7.5.4 Using expand.grid\nWhen we use the predict function in R, we specify a “`newdata” dataframe to indicate for which values of \\(X\\) we want to estimate values of the outcome. When we do this, we are actually building a dataframe, like the below:\n\ndf &lt;- data.frame(MajorPower = 0, Allies=1, Contiguity = 0)\ndf\n\n  MajorPower Allies Contiguity\n1          0      1          0\n\n\nIn cases where we want to make predictions for multiple values of a given variable, we can feed a vector into the data.frame:\n\ndf &lt;- data.frame(MajorPower = 0, Allies=c(0,1), Contiguity = 0)\ndf\n\n  MajorPower Allies Contiguity\n1          0      0          0\n2          0      1          0\n\n\nHowever, this becomes more tedious when we want to estimate for combinations of different variables (e.g., MajorPower and Allies as 0 or 1). You have to tell R precisely which rows should include which values. This means indicating four separate values for each variables to get all possible combinations.\n\ndf &lt;- data.frame(MajorPower = c(0, 0, 1,1), Allies=c(0,1, 0, 1), Contiguity = 0)\ndf\n\n  MajorPower Allies Contiguity\n1          0      0          0\n2          0      1          0\n3          1      0          0\n4          1      1          0\n\n\nWhat expand.grid does is let you more quickly indicate that you want to estimate for all possibles combinations of the values of the variables you supply. E.g.:\n\ndf &lt;- expand.grid(MajorPower = c(0, 1), Allies=c(0,1), Contiguity = 0)\ndf\n\n  MajorPower Allies Contiguity\n1          0      0          0\n2          1      0          0\n3          0      1          0\n4          1      1          0\n\n\nThis can be a useful shortcut when you need to look at combinations of variables that have many different values."
  },
  {
    "objectID": "07-QuantitiesofInterest.html#quantities-of-interest-tutorial",
    "href": "07-QuantitiesofInterest.html#quantities-of-interest-tutorial",
    "title": "7  Quantities of Interest",
    "section": "7.6 Quantities of Interest Tutorial",
    "text": "7.6 Quantities of Interest Tutorial\nLet’s return to the example at the end of section 6, and calculate predicted probabilities, now with estimates of uncertainty. Recall the Banks and Hicks data include the following variables below. We add partyid as a variable for this analysis.\n\nabtrace1: 1= if the respondent thought the ad was about race. 0= otherwise\ncondition2: 1= respondent in the implicit condition. 2= respondent in one of four explicit racism conditions.\nracresent: a 0 to 1 numeric variable measuring racial resentment\noldfash: a 0 to 1 numeric variable measuring “old-fashioned racism”\ntrumvote: 1= respondent has vote preference for Trump 0=otherwise\npartyid: A 1 to 7 numeric variables indicating partisanship from strong Democrat to strong Republican. Below 4 is a Democrat/Democratic leaner, above 4 is a Republican/Republican leaner.\n\nLet’s load the data again.\n\n## install.packages(\"rio\")\nlibrary(rio)\nstudy &lt;- import(\"https://github.com/ktmccabe/teachingdata/blob/main/ssistudyrecode.dta?raw=true\")\n\nLet’s set the experiment aside for now and focus on vote choice as the outcome: trumvote. Let’s suppose we were interested in understanding whether partisanship influences vote choice.\n\nWhat would be a simple regression model we could run? (e.g., what would be on the left, what would be on the right?)\nWhat type of model should it be? (e.g., OLS, glm, logit, probit, etc.?)\nRun this model below\nWhat do you conclude about the influence of party on vote choice?\n\n\n\nTry on your own, then expand for one solution.\n\nfit1 &lt;- glm(trumvote ~ partyid, data=study, family=binomial(link=\"probit\"))\n\nlibrary(texreg)\nVersion:  1.38.6\nDate:     2022-04-06\nAuthor:   Philip Leifeld (University of Essex)\n\nConsider submitting praise using the praise or praise_interactive functions.\nPlease cite the JSS article in your publications -- see citation(\"texreg\").\ntexreg::knitreg(fit1)\n\n\n\nStatistical models\n\n\n\n\n \n\n\nModel 1\n\n\n\n\n\n\n(Intercept)\n\n\n-2.06***\n\n\n\n\n \n\n\n(0.13)\n\n\n\n\npartyid\n\n\n0.43***\n\n\n\n\n \n\n\n(0.03)\n\n\n\n\nAIC\n\n\n1056.38\n\n\n\n\nBIC\n\n\n1066.23\n\n\n\n\nLog Likelihood\n\n\n-526.19\n\n\n\n\nDeviance\n\n\n1052.38\n\n\n\n\nNum. obs.\n\n\n1019\n\n\n\n\n\n\n***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05\n\n\n\n\n\nWe took a probit approach. What other approaches could we have taken?\n\nIn a glm model, we have to make a leap from a probit/logit coefficient to vaguely answer our research question. What we want to do instead, is become more focused in our research questions, hypotheses, and estimations to generate precise quantities of interest that speak directly to our theoretical questions.\nLet’s try this again. Let’s be more focused in our question/hypothesis.\n\nAs people shift from strong Democrats to strong Republicans, how does their probability of voting for Trump change?\nHow can we transform our model from before into quantities that speak directly to this question?\n\n\n\nComputing probabilities.\n\nWe only have one covariate, so this becomes an easier problem.\n\nnewdata.party &lt;- data.frame(partyid = 1:7)\n\n## Set type to be response\nresults1 &lt;- predict(fit1, newdata = newdata.party, type = \"response\")\n\n\n\n\n\n\nx\n\n\n\n\n0.0512844\n\n\n0.1147444\n\n\n0.2204047\n\n\n0.3669367\n\n\n0.5362029\n\n\n0.6990679\n\n\n0.8295963\n\n\n\n\n\n\nHow can we best communicate this to our readers?\n\n\nComputing probabilities.\n\n\nplot(x=1:7, y=results1,\n     main = \"Predicted probability by partisanship\",\n     type=\"b\",\n     xlab = \"Partisanship: Strong Dem to Strong Rep\",\n     ylab=\"Predicted Probability\")\n\n\n\nlibrary(ggplot2)\nggres &lt;- data.frame(probs=results1, partisanship=1:7)\nggplot(ggres, aes(x=partisanship, y=probs))+\n  geom_line()+\n  geom_point()+\n  xlab(\"Partisanship: Strong Dem to Strong Rep\")+\n  ylab(\"Predicted Probability\")+\n  ggtitle(\"Predicted probability by partisanship\")\n\n\n\n\n\nHow can we improve this even further?\n\nCalculate uncertainty!\n\nAgain, because we only have one covariate, the process is a little more simple. Let’s use the predict function to calculate the standard errors for us on the link scale with se.fit = T and then construct our own confidence intervals.\n\n\nTry on your own, then expand for the solution.\n\n\nresults1.link &lt;- predict(fit1, newdata=newdata.party, type=\"link\", se.fit=T)\nresults1.link\n\n$fit\n         1          2          3          4          5          6          7 \n-1.6325260 -1.2016765 -0.7708270 -0.3399775  0.0908720  0.5217215  0.9525710 \n\n$se.fit\n         1          2          3          4          5          6          7 \n0.10002223 0.07628414 0.05617531 0.04487352 0.04892006 0.06553079 0.08784628 \n\n$residual.scale\n[1] 1\n\n\nNow for each value, we get the standard error estimate. We now have to convert these to the response scale.\n\nm.results &lt;-  pnorm(results1.link$fit)\nresults1.lb &lt;- pnorm(results1.link$fit - qnorm(.975)*results1.link$se.fit)\nresults1.ub &lt;-pnorm(results1.link$fit + qnorm(.975)*results1.link$se.fit)\n\n## Let's look at the results from the original point estimates and this approach\ncbind(results1, m.results, results1.lb, results1.ub)\n\n    results1  m.results results1.lb results1.ub\n1 0.05128436 0.05128436  0.03373233  0.07543205\n2 0.11474445 0.11474445  0.08831718  0.14636254\n3 0.22040474 0.22040474  0.18917824  0.25439421\n4 0.36693674 0.36693674  0.33435178  0.40051009\n5 0.53620285 0.53620285  0.49800149  0.57407307\n6 0.69906787 0.69906787  0.65294495  0.74220540\n7 0.82959626 0.82959626  0.78242093  0.86965177\n\n\nFinally, let’s add it to our plot.\n\nplot(x=1:7, y=results1,\n     main = \"Predicted probability by partisanship\",\n     type=\"b\",\n     xlab = \"Partisanship: Strong Dem to Strong Rep\",\n     ylab=\"Predicted Probability\")\npoints(x=1:7, y= results1.ub, type=\"l\")\npoints(x=1:7, y= results1.lb, type=\"l\")\n\n\n\nlibrary(ggplot2)\nggres &lt;- data.frame(cbind(m.results, results1.lb, results1.ub), partisanship=1:7)\nggplot(ggres, aes(x=partisanship, y=m.results))+\n  geom_line()+\n  geom_point()+\n  #geom_ribbon(aes(ymin=results1.lb, ymax=results1.ub), alpha=.5)+\n  geom_errorbar(aes(ymin=results1.lb, ymax=results1.ub), width=.02)+\n  xlab(\"Partisanship: Strong Dem to Strong Rep\")+\n  ylab(\"Predicted Probability\")+\n  ggtitle(\"Predicted probability by partisanship\")\n\n\n\n\n\nThe marginaleffects and prediction packages in R will give us a shortcut by calculating the confidence intervals for us. We can repeat the previous process inside these functions.\n\n# install.packages(\"marginaleffects\")\nlibrary(marginaleffects)\n\npred.resultsme &lt;- avg_predictions(fit1,  type = \"response\",by = \"partyid\", newdata = datagridcf(partyid = 1:7))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npartyid\nestimate\nstd.error\nstatistic\np.value\ns.value\nconf.low\nconf.high\n\n\n\n\n1\n0.0512844\n0.0105264\n4.871990\n1.1e-06\n19.78779\n0.0306531\n0.0719157\n\n\n2\n0.1147444\n0.0147835\n7.761643\n0.0e+00\n46.76136\n0.0857693\n0.1437196\n\n\n3\n0.2204047\n0.0166507\n13.236974\n0.0e+00\n130.45307\n0.1877700\n0.2530395\n\n\n4\n0.3669367\n0.0168967\n21.716505\n0.0e+00\n344.96174\n0.3338199\n0.4000536\n\n\n5\n0.5362029\n0.0194359\n27.588317\n0.0e+00\n554.14221\n0.4981093\n0.5742965\n\n\n6\n0.6990679\n0.0228165\n30.638670\n0.0e+00\n682.41374\n0.6543483\n0.7437874\n\n\n7\n0.8295963\n0.0222636\n37.262409\n0.0e+00\n1007.12816\n0.7859604\n0.8732322\n\n\n\n\n\n# install.packages(\"prediction\")\nlibrary(prediction)\n## prediction can take a new dataframe or specific values of covariates\n\npred.results &lt;- prediction(fit1, at=list(partyid = 1:7), type=\"response\", calculate_se = T)\n\n\nsummary(pred.results)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nat(partyid)\nPrediction\nSE\nz\np\nlower\nupper\n\n\n\n\n1\n0.0512844\n0.0105264\n4.871990\n1.1e-06\n0.0306531\n0.0719157\n\n\n2\n0.1147444\n0.0147835\n7.761644\n0.0e+00\n0.0857693\n0.1437196\n\n\n3\n0.2204047\n0.0166507\n13.236974\n0.0e+00\n0.1877700\n0.2530395\n\n\n4\n0.3669367\n0.0168967\n21.716505\n0.0e+00\n0.3338199\n0.4000536\n\n\n5\n0.5362029\n0.0194359\n27.588317\n0.0e+00\n0.4981093\n0.5742965\n\n\n6\n0.6990679\n0.0228165\n30.638670\n0.0e+00\n0.6543483\n0.7437874\n\n\n7\n0.8295963\n0.0222636\n37.262407\n0.0e+00\n0.7859604\n0.8732322\n\n\n\n\nWhen we have covariates, we have a few more decisions to make about how to calculate quantities of interest and how to compute uncertainty. Let’s amend our model to include additional covariates.\n\nfit2 &lt;- glm(trumvote ~ partyid + racresent + oldfash, data=study, family=binomial(link=\"probit\"))\n\nNow, we have the same research question, but we have covariates. We have to decide how we want to calculate the predicted probabilities of voting for Trump at different levels of partisanship.\n\nWhere should we set racresent and oldfash when computing these values?\n\nLet’s suppose we want to hold them at observed values. This means we will calculate the average predicted probability of voting for Trump at each value of partisanship, holding the other covariates at observed values.\nWe can do this in a few ways.\nHere, let’s do this manually:\n\nbh &lt;- coef(fit2)\n\n## party id 1\nX.1 &lt;- model.matrix(fit2)\nX.1[, \"partyid\"] &lt;- 1\np.1 &lt;- pnorm(X.1 %*% bh)\np.1.mean &lt;- mean(p.1)\n\n## party id 2\nX.2 &lt;- model.matrix(fit2)\nX.2[, \"partyid\"] &lt;- 2\np.2 &lt;- pnorm(X.2 %*% bh)\np.2.mean &lt;- mean(p.2)\n\n## More efficient approach 1:\np.means &lt;- rep(NA, 7)\nfor(i in 1:7){\n  X &lt;- model.matrix(fit2)\n  X[, \"partyid\"] &lt;- i\n  p &lt;- pnorm(X %*% bh)\n  p.means[i] &lt;- mean(p)\n}\n\n## More efficient approach 2:\nmyest &lt;- function(value){\n  X &lt;- model.matrix(fit2)\n  X[, \"partyid\"] &lt;- value\n  p &lt;- pnorm(X %*% bh)\n  p.mean &lt;- mean(p)\n  return(p.mean)\n}\np.means &lt;- sapply(1:7, myest)\n\nOr, we can use marginaleffects or prediction again.\nHow would you apply marginaleffects? Below is the code from the prediction package:\n\npred.results2 &lt;- prediction(fit2, at=list(partyid = 1:7), type=\"response\")\n\nLet’s compare the output:\n\ncbind(p.means, summary(pred.results2)$Prediction)\n\n        p.means           \n[1,] 0.08296357 0.08296357\n[2,] 0.14824083 0.14824083\n[3,] 0.24098759 0.24098759\n[4,] 0.35835987 0.35835987\n[5,] 0.49072437 0.49072437\n[6,] 0.62383575 0.62383575\n[7,] 0.74331307 0.74331307\n\n\nWhat if we want uncertainty?\n\nWe can bootstrap or simulate confidence intervals around each quantity of interest, or use a function to do this for us.\n\n\n\nBootstrap Details\n\n\n## Step 1: sample new rows of the data and subset\nwrows &lt;- sample(x =1:nrow(study), size = nrow(study), replace = T)\nsubdata &lt;- study[wrows, ]\n\n## Step 2: run your regression model with the new data\nboot.probit &lt;-glm(trumvote ~ partyid + racresent + oldfash, \n                  data=subdata, family=binomial(link=\"probit\"))\n\n## Step 3: generate average predicted probability\nXboot &lt;- model.matrix(boot.probit)\nXboot[, \"partyid\"] &lt;- 1\nBh &lt;- coef(boot.probit)\np.boot &lt;- mean(pnorm(Xboot %*% Bh))\n\n## Step 4: wrap it in a function, make data generic\nmyboot &lt;- function(df){\n  wrows &lt;- sample(x =1:nrow(df), size = nrow(df), replace = T)\n  subdata &lt;- df[wrows, ]\n  boot.probit &lt;-glm(trumvote ~ partyid + racresent + oldfash, \n                  data=subdata, family=binomial(link=\"probit\"))\n  Xboot &lt;- model.matrix(boot.probit)\n  Xboot[, \"partyid\"] &lt;- 1\n  Bh &lt;- coef(boot.probit)\n  p.boot &lt;- mean(pnorm(Xboot %*% Bh))\n  return(p.boot)\n}\n\n## Step 5: Uncomment and replicate 1000 times\n#bestimates.party1 &lt;- replicate(1000, myboot(study))\n\n## Extract confidence interval\n#quantile(bestimates.party1, c(0.025, .975))\n\nWe would then repeat this for each partyid value. Alternatively, we could use prediction!"
  },
  {
    "objectID": "07-QuantitiesofInterest.html#putting-everything-together",
    "href": "07-QuantitiesofInterest.html#putting-everything-together",
    "title": "7  Quantities of Interest",
    "section": "7.7 Putting everything together",
    "text": "7.7 Putting everything together\nRecall, we can take something that looks like column 5 in the table from Antoine Banks and Heather Hicks example from the previous section and move it into a figure, as the authors did.\n \nLet’s run the model from column 5.\n\nfit.probit5 &lt;- glm(abtrace1 ~ factor(condition2)*racresent \n                   + factor(condition2)*oldfash,\n                  data=study, family=binomial(link = \"probit\"))\n\nLet’s generate predicted probabilities for thinking the ad is about race across levels of racial resentment in the sample, for people in the implicit and explicit conditions, holding all covariates at observed values.\n\nlibrary(prediction)\npr.imp &lt;- prediction(fit.probit5, at= list(racresent = seq(0, 1,.0625),\n                                               condition2=1),\n                         calculate_se = TRUE)\n## Let's store the summary output this time\n## And to make it easier to plot, we'll store as dataframe\npr.imp.df &lt;- summary(pr.imp)\n\npr.exp &lt;- prediction(fit.probit5, at= list(racresent = seq(0, 1,.0625),\n                                               condition2=2),\n                         calculate_se = TRUE)\npr.exp.df &lt;- summary(pr.exp)\n\nYou can peek inside pr.imp.df to see the format of the output.\nLet’s now visualize! We will try to stay true to the authors’ visual choices here.\n\n## Plot results\nplot(x=pr.imp.df$`at(racresent)`, y=pr.imp.df$Prediction, \n     type=\"l\",\n     ylim = c(0, 1), lty=2,\n     ylab = \"Predicted Probability\",\n     xlab = \"Racial Resentment\",\n     main = \"Predicted Probability of Viewing the Ad as about Race\",\n     cex.main = .7)\n## add explicit point values\npoints(x=pr.exp.df$`at(racresent)`, y=pr.exp.df$Prediction, type=\"l\")\n\n## add additional lines for the upper and lower confidence intervals\npoints(x=pr.exp.df$`at(racresent)`, y=pr.exp.df$lower, type=\"l\", col=\"gray\")\npoints(x=pr.exp.df$`at(racresent)`, y=pr.exp.df$upper, type=\"l\", col=\"gray\")\n\npoints(x=pr.imp.df$`at(racresent)`, y=pr.imp.df$lower, type=\"l\", lty=3)\npoints(x=pr.imp.df$`at(racresent)`, y=pr.imp.df$upper, type=\"l\", lty=3)\n\n## Legend\nlegend(\"bottomleft\", lty= c(2,1, 3,1), \n       c(\"Implicit\", \"Explicit\", \n         \"Implicit 95% CI\", \"Explicit 95% CI\"), cex=.7)\n\n\n\n\nLet’s combine the two dataframes.\n\npr.comb &lt;- rbind(pr.imp.df, pr.exp.df)\n\n\nlibrary(ggplot2)\nggplot(pr.comb, aes(x=`at(racresent)`, \n                    y= Prediction, \n                    color=as.factor(`at(condition2)`)))+\n  geom_line()+\n  geom_ribbon(aes(ymin=lower, ymax=upper, fill=as.factor(`at(condition2)`)), alpha=.5)+\n  xlab(\"Racial Resentment\")+\n  theme_bw()+\n  theme(legend.position = \"bottom\") +\n  scale_color_discrete(name=\"Condition\",\n                         breaks=c(1,2),\n                         labels=c(\"Implicit\", \"Explicit\"))+\n  scale_fill_discrete(name=\"Condition\",\n                         breaks=c(1,2),\n                         labels=c(\"Implicit\", \"Explicit\"))\n\n\n\n\nWe could instead show the difference between the conditions across levels of racial resentment.\n\nlibrary(margins)\nmarest &lt;- margins(fit.probit5, at= list(racresent = seq(0, 1,.0625)),\n                  variables=\"condition2\",\n                  change = c(1,2),\n                         vce=\"delta\",\n                  type=\"response\")\n\n## Store summary as dataframe\nmarest.df &lt;- summary(marest)\n\n## plot res\nggplot(marest.df, aes(x=racresent, y=AME))+\n  geom_line()+\n  geom_errorbar(aes(ymin=lower, ymax=upper), alpha=.5, width=0)+\n  theme_bw()+\n  xlab(\"Racial Resentment\")+\n  ggtitle(\"AME: Explicit - Implicit Condition on Pr(Ad About Race)\")+\n  geom_hline(yintercept = 0, color=\"red\")"
  },
  {
    "objectID": "08-OrdinalandOutcomes.html#ordinal-outcome-data",
    "href": "08-OrdinalandOutcomes.html#ordinal-outcome-data",
    "title": "8  Ordinal Outcomes",
    "section": "8.1 Ordinal Outcome Data",
    "text": "8.1 Ordinal Outcome Data\nHere is a motivating example for the use of ordered data from Paluck and Green “Deference, Dissent, and Dispute Resolution: An Experimental Intervention Using Mass Media to Change Norms and Behavior in Rwanda” which was published in the American Political Science Review in 2009. doi:10.1017/S0003055409990128\nAbstract. Deference and dissent strike a delicate balance in any polity. Insufficient deference to authority may incapacitate government, whereas too much may allow leaders to orchestrate mass violence. Although cross-national and cross-temporal variation in deference to authority and willingness to express dissent has long been studied in political science, rarely have scholars studied programs designed to change these aspects of political culture. This study, situated in post-genocide Rwanda, reports a qualitative and quantitative assessment of one such attempt, a radio program aimed at discouraging blind obedience and reliance on direction from authorities and promoting independent thought and collective action in problem solving. Over the course of one year, this radio program or a comparable program dealing with HIV was randomly presented to pairs of communities, including communities of genocide survivors, Twa people, and imprisoned genocidaires … Although the radio program had little effect on many kinds of beliefs and attitudes, it had a substantial impact on listeners’ willingness to express dissent and the ways they resolved communal problems.\nIn a field experiment, the authors have randomly assigned participants in different research sites to listen to a radio program over the course of a year that varied in its message. As the authors note, “Because radios and batteries are relatively expensive for Rwandans, they usually listen to the radio in groups. Thus, we used a group-randomized design in which adults from a community listened together either to the treatment (reconciliation) program or to the control program (another entertainment-education radio soap opera about health and HIV).” The authors have 14 clusters without 40 individuals within each cluster.\n\nTreatment (treat): radio program with one of two messages, where 1=the treatment condition with a reconciliation message and 0=control, listening to a health message.\nOutcome (dissent): Willingness to Display Dissent: An ordered scale with four categories from 1 (“I should stay quiet”) to 4 (“I should dissent”)\n\nLet’s load the data and look at the treatment and outcome.\n\nlibrary(rio)\npg &lt;- import(\"https://github.com/ktmccabe/teachingdata/blob/main/paluckgreen.dta?raw=true\")\n\n## Let's treat the outcome as a factor\npg$dissent &lt;- as.factor(pg$dissent)\n\n## Let's visualize the outcome by group\nlibrary(ggplot2)\nlibrary(tidyverse)\npg %&gt;%\n  filter(is.na(dissent)==F) %&gt;%\n  ggplot(aes(x=dissent, group=factor(treat), \n               fill=factor(treat)))+\n  geom_bar(aes(y=..prop..),stat= \"count\", position=\"dodge\", color=\"black\")+\n  theme_minimal()+\n  theme(legend.position = \"bottom\")+\n  scale_fill_brewer(\"Condition\", labels=c(\"Control\", \"Treatment\"),palette=\"Paired\")+\n  scale_x_discrete(labels= c(\"Should Stay Quiet\", \"2\", \"3\", \"Should Dissent\"))\n\n\n\n\nWe can see variation in the outcome, where some people are at the “stay quiet” end of the scale, while others are at the opposite end. We might have a few questions about the outcome:\n\nWhat is the probability of being in a particular category given a set of \\(\\mathbf{x_i'}\\) values?\nDoes the treatment influence likelihood of expressing dissent?\nDoes the treatment significantly affect the probability of responding in a particular category?\n\nWhat model should they use to help answer these questions?\nOne approach would be to use OLS.\n\nThey could treat 1 to 4 scale as continuous from Should stay quiet to Should dissent\nIf they do this, the interpretation of the regression coefficients would be:\n\nGoing from Control to Treatment (0 to 1), is associated with \\(\\hat \\beta\\) movement on this scale.\n\nWhat could be problematic here?\n\nMight go below or above scale points\nDistance between scale points might not be an equal interval\nDoesn’t answer the “probability” question, just describes movement up and down the scale.\n\n\nA second approach could be to collapse the scale to be dichotomous and use logit/probit or a linear probability model.\n\nFor example, they could treat the outcome to 0 = (lean toward stay quiet/stay quiet) vs. 1 (lean toward dissent/dissent)\nHere, after converting the outcomes in probability, the interpretation would be\n\nGoing from Control to Treatment (0 to 1), is associated with an average difference in predicted probability of dissent (\\(Y_i = 1\\))\n\nWhat could be problematic here?\n\nWe lose information.\n\n\nA third approach–and the focus of this section– would be to use an ordinal logistic or probit regression.\n\nThis is appropriate when our goal, at least in part, is to estimate the probability of being in a specific category, and these categories have a natural ordering to them.\n\n\n8.1.1 Ordinal Model\nWith ordered data, we have an outcome variable \\(Y_i\\) that can fall into different, ordered categories: \\(Y_i \\in \\{C_1, C_2, \\ldots, C_J \\}\\) with some probability.\n\n\n\n\n\nThe image above shows a distribution where the area under the curve sums to 1, with the area divided into 4 categories, separated by three cutpoints. The area represents probability mass. For example, the area to the left of z1 represents the \\(Pr(Y_i^\\ast \\leq z1)\\).\nIn our ordered model, we assume that there is a latent (unobserved) \\(Y^\\ast_i = X_i\\beta + \\epsilon\\)\n\nThis means we can still have a single model \\(X_i \\beta\\), which determines what outcome level is achieved (this requires an assumption).\nwhere \\(\\epsilon\\) is either assumed to be normally distributed (probit) or distributed logistically (logit), and corresponds to the fuzziness of the cutpoints (\\(\\zeta_j\\)), which define in which category an outcome is observed to fall. Instead of fine lines, we estimate probabilistically in which category \\(Y_i\\) is predicted to fall.\n\nWe observe this category in our outcome: \\(Y_i\\).\n\nFor example, we observe if someone said “should stay quiet” or “should dissent” vs. one of the two middle categories.\n\n\\[\\begin{gather*}\nY_i=\\begin{cases}\nC_1 \\textrm{ if } Y^\\ast_i  \\le \\zeta_1\\\\\nC_2 \\textrm{ if } \\zeta_1 &lt; Y^\\ast_i  \\le \\zeta_2\\\\\nC_3 \\textrm{ if } \\zeta_2 &lt;  Y^\\ast_i  \\le \\zeta_3\\\\\n\\ldots\\\\\nC_J\\textrm{ if } \\zeta_{J-1} &lt; Y^\\ast_i \\\\\n\\end{cases}\\\\\n\\end{gather*}\\]\nThe \\(\\zeta_j\\) are called “cutpoints”\n\nNeed cutpoints that are distinct, but the distance between cutpoints does not have to be the same.\n\nThis can be particularly useful when we have scales that have a natural ordering, but the distance between scale points might not have the same meaning or be the same (e.g., “Agree”,“Disagree”, “Revolt”). This is different from an interval variable, where we assume the difference between scale points carries the same meaning (e.g., credit score, cups of flour in a recipe).\n\nNote: There is no intercept in linear prediction model in this case. Instead of the intercept, we have the specific cutpoints.\nRule of thumb: Estimation with more than 3-5 categories unstable\n\n\n\n8.1.2 Interpretation\nHere is an example of what the ordered probit model output can look like from the authors’ paper. You can see coefficients similar to the models we’ve been working with before but instead of an intercept, we have the different cutpoints, in this case, three cutpoints for \\(J-1\\) categories.\n\nWe can interpret the coefficients as a one-unit increase in \\(x\\) has a \\(\\hat \\beta\\) increase/decrease in the linear predictor scale of \\(Y^\\ast\\) (in log-odds for logit or probit z-score standard deviations).\n\nThis gives us an initial sense (based on sign and significance) of how an independent variable positively or negatively affects the position of \\(Y*\\). However, it does not give us any information about specific categories.\nThus, alas, this is unsatisfying for a couple of reasons.\n\n\\(Y_i^\\ast\\) is an unobserved variable (not the categories themselves)\nThe scale is harder to interpret than probability\n\nTherefore, we will generally want to convert our estimates into probabilities (our quantities of interest)\n\nOne wrinkle here is we now have \\(J\\) predicted probabilities to estimate, one for each category.\nA second wrinkle here is any change in the probability of being in the \\(jth\\) category of \\(Y\\) also affects the probabilities of being in the \\(\\neq jth\\) categories because the probabilities of being in each category have to sum together to 1. (E.g., Increasing the probability that someone said “should dissent” affects the probability they said “should stay quiet.)"
  },
  {
    "objectID": "08-OrdinalandOutcomes.html#likelihood-framework",
    "href": "08-OrdinalandOutcomes.html#likelihood-framework",
    "title": "8  Ordinal Outcomes",
    "section": "8.2 Likelihood Framework",
    "text": "8.2 Likelihood Framework\nIn the binary case, we wanted to estimate \\(Pr(Y_i = 1 | X)\\). Our goal in an ordinal model is to estimate the probability that \\(Y_i\\) is in a particular \\(j\\) category \\(C_j\\).\n\\(Pr(Y_i = C_j | X)\\)\nTo do so, we are going to use the cumulative distribution functions to estimate the probability that \\(Y_i\\) is below a particular cutpoint \\(\\zeta_j\\) or between two cutpoints \\(\\zeta_j\\) and \\(\\zeta_{j+1}\\).\nFinding predicted probabilities for a given \\(j\\) category can be written as follows: \\(Pr(Y_i|X_i) = \\mathbf 1(Y_i=C_j) \\{ \\Pr(Y^\\ast \\le \\zeta_{j})- \\Pr(Y^\\ast \\le \\zeta_{j-1})\\}\\)\nWe can spell this out more explicitly for each \\(j\\) category:\n\n\\(Pr(Y_i = C_{J} | X) = 1 - Pr(Y^\\ast\\leq \\zeta_{J - 1} | X_i)\\)\n\\(Pr(Y_i = C_{3} | X) = Pr(Y^\\ast \\leq \\zeta_{3} | X_i) - Pr(Y^\\ast \\leq \\zeta_{2} | X_i)\\)\n\\(Pr(Y_i = C_{2} | X) = Pr(Y^\\ast \\leq \\zeta_{2} | X_i) - Pr(Y^\\ast \\leq \\zeta_{1} | X_i)\\)\n\\(Pr(Y_i = C_{1} | X) = Pr(Y^\\ast \\leq \\zeta_{1} | X_i)\\)\n\nJust as in the binary case, we use our \\(\\Phi()\\) pnorm or \\(\\frac{exp^{X\\beta}}{1 + exp^{X\\beta}}\\) plogis functions to get our probabilities from the linear predictors. However, in this ordered case, we also have to include the estimate for the cutpoint \\(\\zeta_j\\) when performing this operation. You can kind of think of this as having a separate intercept for each category instead of just one intercept in the binary case.\nFor example, in the ordinal logit case, the linear predictor is in the scale of log of the proportional-odds. We can write our regression as:\n\\(\\log \\frac{P(Y \\leq j | X)}{P(Y &gt; j | X)} = (\\zeta_j - \\eta)\\) where \\(\\eta = x_1\\beta_1 + x_2\\beta_2 + ... + x_k\\beta_k\\)\n\nTo get probability we apply the plogis function \\(logit^{-1}(\\zeta_j - \\eta)\\)\nSame for probit, but we use pnorm: \\(probit^{-1}(\\zeta_j - \\eta)\\)\n\n\n8.2.1 Likelihood\nThe likelihood of all observations, assuming independence is:\n\\(\\mathcal L(\\beta, \\zeta | Y) = \\prod_{i=1}^{N} Pr(Y_i = C_j)\\) for a given category. To incorporate all \\(J\\) categories, we can write:\n\\(\\mathcal L(\\beta, \\zeta | Y) = \\prod_{i=1}^{N} \\prod_{j=1}^{J} \\mathbf 1(Y_i=C_j) \\{ \\Pr(Y^\\ast \\le \\zeta_{j}) - \\Pr(Y^\\ast \\le \\zeta_{j-1})\\}\\) where \\(\\mathbf 1(Y_i=C_j)\\) is an indicator for whether or not a given \\(Y_i\\) is observed in the \\(jth\\) category.\nNote that here instead of estimating just \\(\\beta\\), we now also estimate the cutpoints \\(\\zeta\\).\nThe log likelihood then just changes this to the sum:\n\\[\\begin{align*}\n\\mathcal l(\\beta, \\zeta | Y) &= \\sum_{i=1}^{N} \\sum_{j=1}^{J}\\mathbf 1(Y_i=C_j) \\{  \\log( \\Pr(Y^\\ast  \\le \\zeta_{j}) - \\Pr(Y^\\ast  \\le \\zeta_{j-1}))\\}\\\\\n&= \\sum_{i=1}^{N} \\sum_{j=1}^{J} \\mathbf 1(Y_i=C_j) \\{\\log(\\Phi(\\zeta_j - \\mathbf x_i'\\beta) - \\Phi(\\zeta_{j-1} - \\mathbf x_i'\\beta))\\}\n\\end{align*}\\]\nIn addition to assuming independence of our observations, we assume each category has a nonzero probability of being observed and that the cutpoints are monotonically increasing: \\(\\zeta_j &lt; \\zeta_{j+1}\\)."
  },
  {
    "objectID": "08-OrdinalandOutcomes.html#fitting-ordinal-models-in-r",
    "href": "08-OrdinalandOutcomes.html#fitting-ordinal-models-in-r",
    "title": "8  Ordinal Outcomes",
    "section": "8.3 Fitting Ordinal Models in R",
    "text": "8.3 Fitting Ordinal Models in R\nTo fit an ordinal logit or probit model in R, we can use the MASS package and polr function.\nYou may need to install this in your RStudio the first time you use it.\n\ninstall.packages(\"MASS\")\n\nWhen we are ready to run the model, we then open the package and use the function polr. The first inputs are very similar to lm and glm. However, we add an argument specifying the method = which can be “logistic” (the default) or “probit.” We also specify Hess=T to make sure we get the uncertainty estimates with the results.\n\nBy the way, why would that argument be called Hess?\n\n\nlibrary(MASS)\nfit &lt;- polr(as.factor(dissent) ~ treat, data= pg, Hess = T, method = \"probit\")\nsummary(fit)\n\nCall:\npolr(formula = as.factor(dissent) ~ treat, data = pg, Hess = T, \n    method = \"probit\")\n\nCoefficients:\n       Value Std. Error t value\ntreat 0.2645     0.1021   2.589\n\nIntercepts:\n    Value   Std. Error t value\n1|2 -0.5102  0.0761    -6.7017\n2|3 -0.0576  0.0740    -0.7784\n3|4  0.2697  0.0742     3.6344\n\nResidual Deviance: 1253.844 \nAIC: 1261.844 \n(116 observations deleted due to missingness)\n\n\nTo get p-values, we can use the AER package.\n\nlibrary(AER)\ncoeftest(fit)\n\n\nz test of coefficients:\n\n       Estimate Std. Error z value  Pr(&gt;|z|)    \ntreat  0.264455   0.102143  2.5891 0.0096238 ** \n1|2   -0.510237   0.076135 -6.7017  2.06e-11 ***\n2|3   -0.057570   0.073958 -0.7784 0.4363235    \n3|4    0.269665   0.074198  3.6344 0.0002786 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nOr calculate manually\n\nround(2*pnorm(abs(summary(fit)$coefficients[,3]), \n              lower.tail = F), digits=6)\n\n   treat      1|2      2|3      3|4 \n0.009624 0.000000 0.436323 0.000279 \n\n\n\n\nFor details on how to fit this manually in R using optim, expand.\n\n\n# Grab X and Y, listwise deletion\npgsub &lt;- na.omit(cbind(pg$dissent, pg$treat))\nY &lt;- pgsub[,1]\nX &lt;- pgsub[, 2]\n\n## Log likelihood in R for this example\nllikor &lt;- function(par, Y, X){\n  k &lt;- 1 # X columns\n  j &lt;- 4 # categories\n  beta &lt;- par[1:k]\n  zeta &lt;- par[(k+1):length(par)]\n  X &lt;-cbind(X)\n  beta &lt;- cbind(beta)\n  ## linear predictor\n  eta &lt;- X %*% beta\n  ## indicator variables\n  y1 &lt;- ifelse(Y==1, 1, 0)\n  y2 &lt;- ifelse(Y==2, 1, 0)\n  y3 &lt;- ifelse(Y==3, 1, 0)\n  y4 &lt;- ifelse(Y==4, 1, 0)\n  ## likelihood for each category\n  l1 &lt;- y1*log(pnorm(zeta[1] - eta))\n  l2 &lt;- y2*log(pnorm(zeta[2] - eta) - pnorm(zeta[1] - eta)) \n  l3 &lt;- y3*log(pnorm(zeta[3] - eta) - pnorm(zeta[2] - eta)) \n  l4 &lt;- y4*log(1 - pnorm(zeta[3] - eta))\n  ## sum together\n  llik &lt;- sum(c(l1, l2, l3, l4))\n  return(llik)\n}\n\n## Optimize\nopt.out &lt;- optim(par=c(.2, 1, 2, 3), fn=llikor, X=X, Y=Y, method=\"BFGS\",\n      control=list(fnscale=-1), hessian = T)\n\n## Coefficients\nopt.out$par[1]\n\n[1] 0.2644911\n\ncoef(fit)\n\n    treat \n0.2644546 \n\n## Cutpoints\nopt.out$par[2:4]\n\n[1] -0.51019872 -0.05757855  0.26969063\n\nfit$zeta\n\n       1|2        2|3        3|4 \n-0.5102370 -0.0575699  0.2696646 \n\n## Standard errors\nsqrt(diag(solve(- opt.out$hessian)))\n\n[1] 0.10214310 0.07613473 0.07395763 0.07419776\n\nsqrt(diag(vcov(fit)))\n\n     treat        1|2        2|3        3|4 \n0.10214309 0.07613503 0.07395770 0.07419775 \n\n\n\n\n8.3.1 Quantities of Interest\nIn discussing the results of Table 3 from the ordered probit analysis in the paper, the authors note, “A shift of .26 probits implies, for example, that a health group respondent with a 30% chance of strongly agreeing to dissent would move to a 40% chance if assigned to the reconciliation program group. This is a large, but not implausibly large, shift in opinion.”\nSimilar to the binary case, we can convert our estimates to probability, but here we will do it for specific categories. Let’s first conduct an estimate of the difference in probability of being in the top category of dissent for those in the treatment - control conditions.\nWe will follow the formulas outlined above. For the top category (category 4 here) we have:\n\n\\(Pr(Y_i = C_4) = 1 - Pr(Y^* \\leq \\zeta_{4-1} | X_i)\\) where our estimate of \\(Y* = X\\hat \\beta\\).\n\nTo calculate this we take \\(1 -\\) pnorm\\((\\zeta_{4-1} - X\\hat \\beta)\\)\n\nNote that we use pnorm because this is a probit model. If it were a logistic model, we would use plogis instead.\nWe are estimating the probability that our observation is in category 4, when setting \\(X\\) to particular values. In this case, we will estimate the probability when an observation is in the treatment or control group.\n\nLet’s find \\(X\\)\n\nX &lt;- model.matrix(fit)\nhead(X)\n\n  (Intercept) treat\n1           1     0\n2           1     0\n3           1     0\n4           1     0\n5           1     0\n6           1     0\n\n\nLet’s estimate the probability when someone is in the treatment group vs. control group. For this, we will set X to be 1 or 0. Because we only have one coefficient in our model, we actually don’t need the entire matrix. For example, once we set treat to be 1, all observations (rows) are identical to each other:\n\nX[, \"treat\"] &lt;- 1 # in treatment\nhead(X)\n\n  (Intercept) treat\n1           1     1\n2           1     1\n3           1     1\n4           1     1\n5           1     1\n6           1     1\n\n\nSo instead of estimating the outcome for every single row and then take the mean, we could just estimate the outcome for one row given that all rows are identical. In regressions where you have other covariates, those covariates will likely have different values, making the rows distinct. This is a special case where the below code would give identical results.\n\nXt &lt;- cbind(1,1) # A 1 for the intercept and 1 for the treatment\nXc &lt;- cbind(1,0) # A 1 for the intercept and 0 for the treatment (being in the control)\n\nBy default, R will include an Intercept in the model.matrix. We actually do not want this when calculating ordinal regression quantities of interest because the cutpoints will be serving the purpose of the intercept. So we will remove this from \\(X\\), but we will use as.matrix() to make sure R will still treat the object like a matrix.\n\nIf you instead used the full \\(X\\) matrix, the only extra step you would need to do is to take the mean() of the probability estimates below (e.g., the mean of pp1.4). See the practice problem solution in 8.5 for an example.\n\n\nX &lt;- as.matrix(X[, -1])\nhead(X)\n\n  [,1]\n1    1\n2    1\n3    1\n4    1\n5    1\n6    1\n\nXt &lt;- as.matrix(Xt[, -1])\nXt\n\n     [,1]\n[1,]    1\n\nXc &lt;- as.matrix(Xc[, -1])\nXc\n\n     [,1]\n[1,]    0\n\n\n\n## probability of being in top category\ncutpoint3 &lt;- fit$zeta[3] # grab the top cutpoint J - 1, 4-1=3\nb &lt;- coef(fit) # coefficients\n\npp1.4 &lt;- 1 - pnorm(cutpoint3 - Xt%*%b) # prob of top category for treatment\npp1.4\n\n          [,1]\n[1,] 0.4979215\n\npp0.4 &lt;- 1 - pnorm(cutpoint3 - Xc%*%b) # prob of top category for control\npp0.4\n\n          [,1]\n[1,] 0.3937091\n\n## difference\npp1.4 - pp0.4\n\n          [,1]\n[1,] 0.1042124\n\n\nWe recover that there is an 10 percentage point difference in probability of being in the top category of dissent, predicted for those in the treatment vs. control.\nIf we were interested in other categories, we could similarly compare probabilities within those. For example, what about category 3?\n\nHere we just apply a different formula from 8.2: \\(Pr(Y_i = C_3) = Pr(Y^* \\leq \\zeta_{3} | X_i) - Pr(Y^* \\leq \\zeta_{2} | X_i)\\) where our estimate of \\(Y^* = X\\hat \\beta\\).\n\nTo calculate this we take pnorm\\((\\zeta_{3} - X\\hat \\beta)\\) - pnorm\\((\\zeta_{2} - X\\hat \\beta)\\)\n\ncutpoint3 &lt;- fit$zeta[3]\ncutpoint2 &lt;- fit$zeta[2]\n\npp1.3 &lt;- pnorm(cutpoint3 - Xt%*%b) - pnorm(cutpoint2 - Xt%*%b)\npp1.3\n\n          [,1]\n[1,] 0.1283614\n\npp0.3 &lt;- pnorm(cutpoint3 - Xc%*%b) - pnorm(cutpoint2 - Xc%*%b)\npp0.3\n\n          [,1]\n[1,] 0.1292452\n\npp1.3 - pp0.3\n\n             [,1]\n[1,] -0.000883825\n\n\nThese represent the estimated probabilities of being in category 3, or the estimated difference in probability of being in category 3 for the treatment vs. control condition.\nWhat about category 2 or category 1?\n\n\nTry on your own, and then expand for the solution.\n\n\nCategory 2: Here we just apply a different formula from 8.2: \\(Pr(Y_i = C_2) = Pr(Y^* \\leq \\zeta_{2} | X_i) - Pr(Y^* \\leq \\zeta_{1} | X_i)\\).\nCategory 1: Here we just apply a different formula from 8.2: \\(Pr(Y_i = C_1) = Pr(Y^* \\leq \\zeta_{1} | X_i)\\).\n\n\ncutpoint1 &lt;- fit$zeta[1]\ncutpoint2 &lt;- fit$zeta[2]\n\n## category 2\npp1.2 &lt;- pnorm(cutpoint2 - Xt%*%b) - pnorm(cutpoint1 - Xt%*%b)\npp1.2\n\n          [,1]\n[1,] 0.1544561\n\npp0.2 &lt;- pnorm(cutpoint2 - Xc%*%b) - pnorm(cutpoint1 - Xc%*%b)\npp0.2\n\n          [,1]\n[1,] 0.1721029\n\npp1.2 - pp0.2\n\n            [,1]\n[1,] -0.01764679\n\n## category 1 (bottom category)\npp1.1 &lt;- pnorm(cutpoint1 - Xt%*%b)\npp1.1\n\n          [,1]\n[1,] 0.2192609\n\npp0.1 &lt;- pnorm(cutpoint1 - Xc%*%b) \npp0.1\n\n          [,1]\n[1,] 0.3049427\n\npp1.1 - pp0.1\n\n            [,1]\n[1,] -0.08568175\n\n\n\nWe can use our R shortcuts for all categories. Here, to make sure we are in probabilities, we can use type=\"probs\".\n\n## predict\nprs &lt;- predict(fit, newdata = data.frame(treat = c(0,1)), type = \"probs\")\nprs\n\n          1         2         3         4\n1 0.3049427 0.1721029 0.1292452 0.3937091\n2 0.2192609 0.1544561 0.1283614 0.4979215\n\n\nThe first row shows the probability of being in each category (1 through 4) for the first entry for the treat variable (0), implying that the observation was in the control group. The second row does the same thing for when the treat variable is 1, implying that the observation was in the treatment group. These should match up with our manual calculations.\nThe marginaleffects and prediction packages will also work to some extent, which is nice when you have covariates you want to hold at observed values. Note that we put whatever the names of the levels of our variables are.\n\nIt just so happens that in this case, both treat and our dissent outcomes have numbers for the categories. In other cases, you might have text as the category names, in which case you would enter those text values (e.g., “Somewhat Agree” or “Treatment condition”) as the variable or category labels.\n\n\n## marginaleffects\nlibrary(marginaleffects)\n## predicts for all categories\noutme &lt;- predictions(fit, newdata =  datagrid(treat = c(0,1)))\noutme\n\n\n Group treat Estimate Std. Error     z Pr(&gt;|z|)     S  2.5 % 97.5 %\n     1     0    0.305     0.0267 11.44   &lt;0.001  98.2 0.2527  0.357\n     1     1    0.219     0.0245  8.95   &lt;0.001  61.3 0.1713  0.267\n     2     0    0.172     0.0176  9.79   &lt;0.001  72.8 0.1377  0.207\n     2     1    0.154     0.0164  9.41   &lt;0.001  67.4 0.1223  0.187\n     3     0    0.129     0.0151  8.56   &lt;0.001  56.3 0.0997  0.159\n     3     1    0.128     0.0150  8.54   &lt;0.001  56.0 0.0989  0.158\n     4     0    0.394     0.0285 13.79   &lt;0.001 141.4 0.3378  0.450\n     4     1    0.498     0.0313 15.91   &lt;0.001 186.8 0.4366  0.559\n\nColumns: rowid, group, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, dissent, treat \nType:  probs \n\n## to filter for a specific category, like category 4:\nsubset(outme, group==4)\n\n\n Group Estimate Std. Error    z Pr(&gt;|z|)     S CI low CI high\n     4    0.394     0.0285 13.8   &lt;0.001 141.4  0.338   0.450\n     4    0.498     0.0313 15.9   &lt;0.001 186.8  0.437   0.559\n\nColumns: rowid, group, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, dissent, treat \n\n\n\n## Prediction\nlibrary(prediction)\n## can specify which category\noutp &lt;- prediction(fit, at = list(treat = c(0,1)), \n                   category = \"4\")\nsummary(outp)$Prediction\n\n[1] 0.3937091 0.4979215\n\n\nIf we wanted to compare treatment conditions to see the difference in probability, we would move to the avg_comparisons function in marginaleffects.\n\nlibrary(marginaleffects)\nmarg.me &lt;- avg_comparisons(fit, type=\"probs\",\n                           variables = list(treat = c(0,1)))\nsubset(marg.me, group == 3)\n\n\n Group  Term Contrast  Estimate Std. Error      z Pr(&gt;|z|)   S   CI low CI high\n     3 treat    1 - 0 -0.000884    0.00186 -0.475    0.635 0.7 -0.00453 0.00276\n\nColumns: term, group, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n\n\nFinally, the margins package should also work in many cases. You can compare this output with the pp1.4 - pp0.4 calculation from above.\n\nlibrary(margins)\nmarg.obj &lt;- summary(margins(fit, variables = \"treat\", change = c(0, 1), \n                            vce=\"bootstrap\", category = \"4\"))\nmarg.obj\n\n factor    AME     SE      z      p  lower  upper\n  treat 0.1042 0.0417 2.4965 0.0125 0.0224 0.1860\n\n\nHow can we visualize the results?\n\nGiven our independent variable of interest, treat just has two categories, we could make a plot similar to the barplot at the beginning of the section.\nWhen our independent variable of interest has several categories, things can get a bit messier. We will do an example in a weekly tutorial.\n\nHow can we get uncertainty?\n\nLike before, we can use bootstrap or simulation, or ask marginaleffects or margins to do it for us.\n\nHow can we incorporate covariates?\n\nLike before, we would just have a matrix of \\(X\\) covariates instead of a single column for the treatment.\nLike before, we could also calculate probabilities holding those covariates at observed values or setting them to designated values of interest.\n\nHow could we fit a logit version?\n\nWe just switch the method = \"logistic\", and then we should be careful to also use plogis() in place of pnorm for the probability calculation.\nNote: ordered logits also have the benefit of having the odds ratio interpretation when we exp(coef(fit)) exponentiate the coefficients. See this post’s section on “Interpreting the odds ratio” halfway down the page for more information. Again, it is more common in political science to see probabilities than odds ratios, but different disciplines prefer different quantities of interest."
  },
  {
    "objectID": "08-OrdinalandOutcomes.html#assumptions",
    "href": "08-OrdinalandOutcomes.html#assumptions",
    "title": "8  Ordinal Outcomes",
    "section": "8.4 Assumptions",
    "text": "8.4 Assumptions\nA key assumption for the ordinal models is Parallel lines/Proportional Odds: We only have one set of \\(k \\times 1\\) \\(\\hat \\beta\\), not a separate set of coefficients for each ordinal category.\n\nThis means that the relationship between the lowest versus all higher categories of the response variable are assumed to be the same as those that describe the relationship between the next lowest category and all higher categories, etc.\n\nFor each \\(X\\) term included in the model, the coefficient ‘slopes’ are the same regardless of the threshold. If not, we would need a separate set of coefficients describing each pair of outcomes (e.g., Slopes for being in Cat 1 vs. Cat 2; Cat 2 vs. Cat 3, etc.)\n\nEven though we have different cutpoint values across categories, a one-unit change, going from control to treatment, the effects are parallel across response categories.\nFor example, if theoretically, being a woman vs. a man has a positive effect on moving between Categories 3 and 4 in a particular model, but you believe it would have the opposite effect for moving from Category 1 to 2, this would suggest the ordinal model is not appropriate.\n\nWhat to do if assumption is violated? Ignore, Do Binary, Do multinomial (discussed in the next session), use a model that has been developed for relaxing this assumption (e.g., see clm function in R).\nOne test for this that has been developed for the ordered logit case is the Brant test.\n\nfit.l &lt;- polr(as.factor(dissent) ~ treat, data= pg, \n            Hess = T, method = \"probit\")\n\n## One way to test this\n#install.packages(\"brant\")\nlibrary(brant)\nbrant(fit.l,by.var=F)\n\n-------------------------------------------- \nTest for    X2  df  probability \n-------------------------------------------- \nOmnibus     3.93    2   0.14\ntreat       3.93    2   0.14\n-------------------------------------------- \n\nH0: Parallel Regression Assumption holds\n\n## Second way- compare fit of ordinal and multinom models\n#install.packages(\"nnet\")\nlibrary(nnet)\nmlm &lt;- multinom(as.factor(dissent) ~ treat, data=pg)\n\n# weights:  12 (6 variable)\ninitial  value 686.215709 \niter  10 value 625.130759\nfinal  value 625.130585 \nconverged\n\nM1 &lt;- logLik(fit.l)\nM2 &lt;- logLik(mlm)\nG &lt;- -2*(M1[1] - M2[1])\npchisq(G, 6 - 4,lower.tail = FALSE)\n\n[1] 0.1667305\n\n\nIn both cases, our p-value is large enough that we cannot reject the null hypothesis, meaning that we are “okay” sticking with the assumption in this case."
  },
  {
    "objectID": "08-OrdinalandOutcomes.html#ordinal-practice-problems",
    "href": "08-OrdinalandOutcomes.html#ordinal-practice-problems",
    "title": "8  Ordinal Outcomes",
    "section": "8.5 Ordinal Practice Problems",
    "text": "8.5 Ordinal Practice Problems\nHere are a few practice problems for ordered models.\n\nTry to replicate column 2 from Table 3 by adding factor(site_pr) to the regression model. Note that the standard errors will not exactly match. We will discuss why in a follow-up section.\nCalculate the average predicted probability of leaning toward staying quiet (category 2) for the treatment and control group, holding covariates at observed values.\n\nNote: If you do this manually, you need to remove the intercept column that is automatically added to the model.matrix\n\n\n\n\nTry on your own, and then expand for the solution.\n\n\nfit.c2 &lt;- polr(as.factor(dissent) ~ treat + factor(site_pr), \n               data=pg, method=\"probit\", Hess =T)\ncoef(fit.c2)\n\n           treat factor(site_pr)2 factor(site_pr)3 factor(site_pr)4 \n      0.28484306      -0.17480464      -0.25513562      -0.17731799 \nfactor(site_pr)5 factor(site_pr)6 factor(site_pr)7 \n     -0.30547159      -0.11384681       0.04659582 \n\n## Option 1a\nlibrary(marginaleffects)\np1me &lt;- avg_predictions(fit.c2, by = \"treat\",\n                        newdata = datagridcf(treat=c(0,1)))\nsubset(p1me, group==2)\n\n\n Group Estimate Std. Error    z Pr(&gt;|z|)    S CI low CI high\n     2    0.173     0.0176 9.81   &lt;0.001 73.0  0.138   0.207\n     2    0.154     0.0164 9.39   &lt;0.001 67.2  0.122   0.186\n\nColumns: group, treat, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n\n## Option 1b\nlibrary(prediction)\np1 &lt;- prediction(fit.c2, at=list(treat=c(0,1)), category=2)\nsummary(p1)$Prediction\n\n[1] 0.1726136 0.1537839\n\n## Option 2\nX &lt;- model.matrix(fit.c2)[,-1]\nX[, \"treat\"]&lt;-1\nbh &lt;- coef(fit.c2)\neta &lt;- X %*% bh\ncutpoint2 &lt;- fit.c2$zeta[2]\ncutpoint1 &lt;- fit.c2$zeta[1]\nmean(pnorm(cutpoint2 - eta) - pnorm(cutpoint1 - eta))\n\n[1] 0.1537839\n\nX[, \"treat\"] &lt;-0\nbh &lt;- coef(fit.c2)\neta &lt;- X %*% bh\ncutpoint2 &lt;- fit.c2$zeta[2]\ncutpoint1 &lt;- fit.c2$zeta[1]\nmean(pnorm(cutpoint2 - eta) - pnorm(cutpoint1 - eta))\n\n[1] 0.1726136\n\n\n\n\n8.5.1 A note on robust standard errors\nIn writing down our likelihood equations, we assume our observations are independent. However, that is not always the case. For example, in Paluck and Green’s article, the experimental design is based on clustered pairs, and the treatment is randomly assigned to one unit of a pair. Ideally, we want to account for this non-independence in our model.\nIn linear models (e.g., using lm), this is often accomplished post-estimation by adjusting the standard errors. We might adjust them for clustering if we have a case like the authors’ or for suspected heteroskedasticity (maybe the errors are not constant, but instead, are larger for higher values of a given X variable).\nProfessor Jeffrey Wooldridge, an expert on econometrics, is a proponent of robust standard errors, given the stringent assumption of homoskedasticity.\n\nThere are many types of these “robust” standard errors, and we may encounter more later in the course. Generally, what each of these robust standard errors does is alter the nature of the variance covariance matrix from which we extract the standard errors. Recall in OLS, we have an expression for the variance of the coefficients that looks like the below. By assuming constant error variance, we can simplify it.\n\\[\\begin{align*}\n\\mathbf{V}(\\widehat{\\beta})  &= (X^T X)^{-1}X^T \\mathbf{V}( \\epsilon) X (X^T X)^{-1}\\\\\n&= \\underbrace{(X^T X)^{-1}X^T \\sigma^2I_n X (X^T X)^{-1}}_\\text{Assume homoskedasticity}\\\\\n&= \\underbrace{\\sigma^2(X^T X)^{-1} X^T X (X^T X)^{-1}}_\\text{Because it is a constant, we can move it out in front of the matrix multiplication, and then simplify the terms.}  \\\\\n&= \\sigma^2(X^T X)^{-1}\n\\end{align*}\\]\nAfter we plug in \\(\\hat \\sigma^2\\) for \\(\\sigma^2\\), we arrive at our variance-covariance matrix vcov:\\(\\hat \\sigma^2(X^T X)^{-1}=\\frac{1}{N-K} \\mathbf{e'e}(X^T X)^{-1}\\)\nWhen we don’t have constant error variance, we have to pause here:\n\\((X^T X)^{-1}X^T \\mathbf{V}( \\epsilon) X (X^T X)^{-1}\\)\nand assume something different about the structure of the error terms in \\(\\mathbf{V}( \\epsilon)\\) and end up with a different final expression for vcov.\nThe robust estimators differ in how they specify an estimate for \\(\\mathbf{V}( \\epsilon) = \\Sigma\\) and whether they take into account the degrees of freedom in the model (number of observations, number of variables, number of clusters, etc.)\nFor example, let’s fit the authors’ model as a linear model using MacKinnon and White HC1 robust standard errors. Those specify the following inside portion for an estimate of \\(\\mathbf{V}( \\epsilon)\\).This is what Stata uses as a default “robust” standard error:\n\\(\\Sigma = \\frac{n}{n-k} \\text{diag}(\\hat \\epsilon_i^2)\\)\nThis means we have a slightly different term for each observation instead of a constant estimate \\(\\hat \\sigma^2\\) for all observations.\n\nfit.lin &lt;- lm(as.numeric(dissent) ~ treat + factor(site_pr), \n               data=pg)\n\n## Option 1: Adjusting for heteroskedasticity\nlibrary(sandwich)\nnewvcov &lt;- vcovHC(fit.lin, type=\"HC1\")\nsqrt(diag(newvcov))\n\n     (Intercept)            treat factor(site_pr)2 factor(site_pr)3 \n       0.1661596        0.1145570        0.2128812        0.2158906 \nfactor(site_pr)4 factor(site_pr)5 factor(site_pr)6 factor(site_pr)7 \n       0.2164289        0.2176110        0.2175578        0.2085747 \n\n## Option 2: Adjusting for heteroskedasticity\nlibrary(estimatr)\nfit.lin2 &lt;- lm_robust(as.numeric(dissent) ~ treat + factor(site_pr), \n               data=pg, se_type = \"stata\")\nsqrt(diag(vcov(fit.lin2)))\n\n     (Intercept)            treat factor(site_pr)2 factor(site_pr)3 \n       0.1661596        0.1145570        0.2128812        0.2158906 \nfactor(site_pr)4 factor(site_pr)5 factor(site_pr)6 factor(site_pr)7 \n       0.2164289        0.2176110        0.2175578        0.2085747 \n\n\nAnother adjustment takes into account the grouped or “clustered” nature of the data. Here, we have a separate estimate for each group in the data. This can be implemented in R by using vcovCL or the clusters argument in lm_robust.\n\n## Option 1: Adjusting for clustering\nlibrary(sandwich)\nnewvcov &lt;- vcovCL(fit.lin, type=\"HC1\", cadjust=T, cluster = pg$sitcode)\nsqrt(diag(newvcov))\n\n     (Intercept)            treat factor(site_pr)2 factor(site_pr)3 \n      0.07685775       0.07066623       0.12388030       0.09271341 \nfactor(site_pr)4 factor(site_pr)5 factor(site_pr)6 factor(site_pr)7 \n      0.07174477       0.12894245       0.20605985       0.06962861 \n\n## Option 2: Adjusting for clustering\nlibrary(estimatr)\nfit.lin2 &lt;- lm_robust(as.numeric(dissent) ~ treat + factor(site_pr), \n               data=pg, se_type = \"stata\", clusters = sitcode)\nsqrt(diag(vcov(fit.lin2)))\n\n     (Intercept)            treat factor(site_pr)2 factor(site_pr)3 \n      0.07685775       0.07066623       0.12388030       0.09271341 \nfactor(site_pr)4 factor(site_pr)5 factor(site_pr)6 factor(site_pr)7 \n      0.07174477       0.12894245       0.20605985       0.06962861 \n\n\nThis is all great, but the authors don’t have a linear model, they have an ordered probit model. It turns out that people have developed similar robust standard error estimators to adjust the standard errors from these non-linear models. For more details, see slides from Molly Roberts on this point.\nThese take a slightly different form, but the intuition is the same. What we are altering is the structure of the variance-covariance matrix (which is a function of the Hessian in the likelihood case), making adjustments across all observations or clusters of observations.\nWe can also implement these in R using HC0 standard errors.\n\n## Adjusting for clustered standard errors\nfit.c2 &lt;- polr(as.factor(dissent) ~ treat + factor(site_pr), \n               data=pg, method=\"probit\", Hess =T)\nclval &lt;- vcovCL(fit.c2, type=\"HC0\", cluster = pg$sitcode)\n\nYou can compare these standard errors to column 2 from Table 3 in the paper.\n\nsqrt(diag(clval))\n\n           treat factor(site_pr)2 factor(site_pr)3 factor(site_pr)4 \n      0.06212521       0.11636326       0.07559847       0.06387804 \nfactor(site_pr)5 factor(site_pr)6 factor(site_pr)7              1|2 \n      0.10879657       0.16765260       0.06391195       0.07144512 \n             2|3              3|4 \n      0.07494704       0.08409084 \n\n\nThe coeftest function in R, when applied to a new vcov will reproduce the full regression output.\n\nlibrary(lmtest)\ncoeftest(fit.c2, vcov=clval)\n\n\nz test of coefficients:\n\n                  Estimate Std. Error z value  Pr(&gt;|z|)    \ntreat             0.284843   0.062125  4.5850  4.54e-06 ***\nfactor(site_pr)2 -0.174805   0.116363 -1.5022 0.1330371    \nfactor(site_pr)3 -0.255136   0.075598 -3.3749 0.0007385 ***\nfactor(site_pr)4 -0.177318   0.063878 -2.7759 0.0055052 ** \nfactor(site_pr)5 -0.305472   0.108797 -2.8077 0.0049892 ** \nfactor(site_pr)6 -0.113847   0.167653 -0.6791 0.4970975    \nfactor(site_pr)7  0.046596   0.063912  0.7291 0.4659633    \n1|2              -0.646720   0.071445 -9.0520 &lt; 2.2e-16 ***\n2|3              -0.190723   0.074947 -2.5448 0.0109348 *  \n3|4               0.138053   0.084091  1.6417 0.1006487    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nI got 99 problems, and standard errors are just one\nHowever, robust standard errors cannot correct underlying model misspecification. Recall that the initial likelihood equation is what assumes independence– this is the equation we use for everything, not only for computing standard errors but also for estimating the coefficients themselves. If we truly believe we have dependence among our observations, we might consider using an entirely different likelihood function– one that incorporates this dependence, instead of just adjusting the standard errors after the fact.\nNonetheless, we may still think our model is good enough, even if not “correct.” So long as you recognize that the robust standard errors don’t correct for this underlying problem, some are still proponents for using robust standard errors in these cases. For example, Wooldridge recommends reporting robust standard errors, and writes,"
  },
  {
    "objectID": "08-OrdinalandOutcomes.html#ordinal-tutorial",
    "href": "08-OrdinalandOutcomes.html#ordinal-tutorial",
    "title": "8  Ordinal Outcomes",
    "section": "8.6 Ordinal Tutorial",
    "text": "8.6 Ordinal Tutorial\nFor this example, we will replicate a portion of the article”How Empathic Concern Fuels Partisan Polarization” by Elizabeth N. Simas, Scott Clifford, and Justin H. Kirkland.published in 2020 in the American Political Science Review. Replication files are available here\nAbstract. Over the past two decades, there has been a marked increase in partisan social polarization, leaving scholars in search of solutions to partisan conflict. The psychology of intergroup relations identifies empathy as one of the key mechanisms that reduces intergroup conflict, and some have suggested that a lack of empathy has contributed to partisan polarization. Yet, empathy may not always live up to this promise. We argue that, in practice, the experience of empathy is biased toward one’s ingroup and can actually exacerbate political polarization. First, using a large, national sample, we demonstrate that higher levels of dispositional empathic concern are associated with higher levels of affective polarization. Second, using an experimental design, we show that individuals high in empathic concern show greater partisan bias in evaluating contentious political events. Taken together, our results suggest that, contrary to popular views, higher levels of dispositional empathy actually facilitate partisan polarization.\nWe are going to replicate Study 1’s analysis testing Hypotheses 1 and 2. Here, the authors conduct an original survey fielded by YouGov during May 2016 with 1000 respondents.\nLet’s load the data.\n\nHow many observations do we have?\n\n\nlibrary(foreign)\nemp &lt;- read.dta(\"https://github.com/ktmccabe/teachingdata/blob/main/week6.dta?raw=true\")\n\nThe authors’ first two hypotheses are:\n\nEmpathic concern should predict more positive affect for copartisans, relative to outpartisans (H1).\nEmpathic concern should increase negative affect for outpartisans (H2).\n\nOutcome Measure: “To examine this type of partisan favoritism, we utilize responses to two questions asking respondents to rate the Democratic and Republican Parties on a seven-point scale ranging from”very favorable” to “very unfavorable.” We then subtract respondents’ ratings of the opposite party from their ratings of their own party to create an ordinal measure that ranges from six (highest inparty rating, lowest outparty rating) to −6 (lowest inparty rating and highest outparty rating)”\n\naffectpol: an ordinal measure that ranges from six (highest inparty rating, lowest outparty rating) to −6 (lowest inparty rating and highest outparty rating)”\noutfav: rating of opposing party on a seven-point scale ranging from “very favorable” to “very unfavorable.”\n\nLet’s take a look at these variables.\n\nAre they coded as the authors describe?\nWhat class are they currently?\n\n\ntable(emp$affectpol)\n\n\n -6  -4  -3  -2  -1   0   1   2   3   4   5   6 \n  1   1   4   5  11 106  80  80 103 148 137 116 \n\nclass(emp$affectpol)\n\n[1] \"numeric\"\n\n\n\ntable(emp$outfav)\n\n\n  1   2   3   4   5   6   7 \n431 163  73  85  16  16   8 \n\nclass(emp$outfav)\n\n[1] \"numeric\"\n\n\nIndependent Variables.\n\nempconc: Mean of empathetic concern items from the Interpersonal Reactivity Index (IRI)\nAdditional variables to measure other dimensions of empathy:\n\nempdist personal distress, emppers perspective taking, empfant fantasy\n\nAdditional controls for strength of party identification pidext, ideological extremity ideoext, news interest news, dummy variable for party membership dem, and demographics: educ, age, male, white, inc3miss (income)\n\nWhat type of model could they use to test H1 and H2?\nThey choose to run an ordinal logistic regression. Let’s do as they do. To run an ordinal model in R, we need to make sure our outcome is ordinal! (meaning a factor variable)\n\nemp$outfav &lt;- as.factor(emp$outfav)\ntable(emp$outfav)\n\n\n  1   2   3   4   5   6   7 \n431 163  73  85  16  16   8 \n\nclass(emp$outfav)\n\n[1] \"factor\"\n\n\nGo ahead and replicate the first regression in the table with all of the controls, making sure to treat income as a factor variable but education as a numeric variable.\n\n\nNote: your coefficients will not exactly match because the authors weight their data using survey weights.\n\n\nlibrary(MASS)\nfit.emp &lt;- polr(outfav ~ empconc + empdist + emppers + empfant +\n                  + pidext + ideoext + news + dem +\n                  as.numeric(educ) + age + male + white + factor(inc3miss),\n                data=emp, Hess=T, method=\"logistic\")\nsummary(fit.emp)\n\nCall:\npolr(formula = outfav ~ empconc + empdist + emppers + empfant + \n    +pidext + ideoext + news + dem + as.numeric(educ) + age + \n    male + white + factor(inc3miss), data = emp, Hess = T, method = \"logistic\")\n\nCoefficients:\n                      Value Std. Error  t value\nempconc           -0.805771   0.492455 -1.63623\nempdist            0.321857   0.413656  0.77808\nemppers            0.433013   0.528121  0.81991\nempfant           -0.020821   0.408769 -0.05093\npidext            -0.246698   0.095193 -2.59155\nideoext           -0.556725   0.072652 -7.66288\nnews              -0.557133   0.094192 -5.91487\ndem                0.002573   0.156964  0.01639\nas.numeric(educ)   0.071461   0.054630  1.30809\nage               -0.004130   0.004784 -0.86345\nmale              -0.310108   0.160583 -1.93114\nwhite             -0.015491   0.176718 -0.08766\nfactor(inc3miss)2 -0.049340   0.196223 -0.25145\nfactor(inc3miss)3 -0.127770   0.200626 -0.63686\nfactor(inc3miss)4 -0.148389   0.242841 -0.61105\n\nIntercepts:\n    Value   Std. Error t value\n1|2 -3.4355  0.6385    -5.3808\n2|3 -2.3032  0.6308    -3.6513\n3|4 -1.5674  0.6259    -2.5042\n4|5 -0.2119  0.6325    -0.3351\n5|6  0.3713  0.6467     0.5741\n6|7  1.5460  0.7177     2.1541\n\nResidual Deviance: 1828.862 \nAIC: 1870.862 \n(245 observations deleted due to missingness)\n\n\n\nLet’s take a look out how the weights affect the result by using the survey package.\n\nThere are many options in establishing an svydesign. Ours is a relatively simple case where all we have is a vector of weights. In other cases, samples might include information about the sampling units or strata.\nOnce we establish an svydesign object, we now need to use svy commands for our operations, such as svymean or svyglm or svyolr\n\n\n\n## install.packages(\"survey\", dependencies =T)\nlibrary(survey)\nempd &lt;- svydesign(ids=~1, weights = emp$weight_group, data=emp)\nfit.empw2 &lt;- svyolr(outfav ~ empconc + empdist + emppers + empfant +\n                  + pidext + ideoext + news + dem +\n                  as.numeric(educ) + age + \n                    male + white + factor(inc3miss), \n               design=empd, method=\"logistic\")\nsummary(fit.empw2)\n\nCall:\nsvyolr(outfav ~ empconc + empdist + emppers + empfant + +pidext + \n    ideoext + news + dem + as.numeric(educ) + age + male + white + \n    factor(inc3miss), design = empd, method = \"logistic\")\n\nCoefficients:\n                          Value  Std. Error    t value\nempconc           -1.4141333759 0.585070456 -2.4170309\nempdist            0.7759996312 0.561768870  1.3813504\nemppers            1.1700466598 0.672754809  1.7391874\nempfant            0.9733692488 0.475484526  2.0471103\npidext            -0.3281938237 0.124714241 -2.6315665\nideoext           -0.4970353573 0.106530714 -4.6656531\nnews              -0.5759629296 0.130609880 -4.4097960\ndem               -0.0456812052 0.195236242 -0.2339791\nas.numeric(educ)   0.0284167135 0.065624317  0.4330211\nage               -0.0007305433 0.005493873 -0.1329742\nmale              -0.1759866726 0.212493495 -0.8281979\nwhite              0.0366164598 0.215144017  0.1701951\nfactor(inc3miss)2 -0.0470362687 0.226737718 -0.2074479\nfactor(inc3miss)3 -0.0794217660 0.246811912 -0.3217907\nfactor(inc3miss)4 -0.0588030650 0.330225820 -0.1780693\n\nIntercepts:\n    Value   Std. Error t value\n1|2 -2.9118  0.9199    -3.1652\n2|3 -1.7749  0.9029    -1.9658\n3|4 -1.0853  0.9058    -1.1982\n4|5  0.3080  0.8948     0.3442\n5|6  0.9101  0.9006     1.0106\n6|7  2.8740  0.9296     3.0918\n(245 observations deleted due to missingness)\n\n\nThe weights seem to make a difference! Now we are closely matching what the authors report. The use of survey weights represents yet another point of researcher discretion.\nLet’s use the weighted results and proceed to make them easier to interpret. Recall, H2 was: Empathic concern should increase negative affect for outpartisans (H2).\nWe want to show how negative affect toward the outparty changes across levels of empathic concern. How should we visualize this?\n\nCould calculate probabilities of being in each of the outfav seven categories across different levels of empathetic concern.\nCould calculate probabilities of being in theoretically interesting outfav categories across different levels of empathetic concern.\n\nNote: in each case, we need to decide where to set our covariate values and potentially also calculate uncertainty estimates.\nWhat do they do? (focus on the right side for the out-party measure)\n\nLet’s estimate the probability of being in the lowest category for empathy values from 0 to 1 by .2 intervals. Let’s set all covariates at their observed values.\n\nWe could do this in predict or manually. We will do it manually for now.\n\n\n## Set covariates to particular values (here, we hold at observed)\nX &lt;- model.matrix(fit.empw2)\nX &lt;- X[, -1] #remove intercept\nX[,\"empconc\" ] &lt;- 0\n\n# Find Xb and zeta\ncoef(fit.empw2)\n\n          empconc           empdist           emppers           empfant \n    -1.4141333759      0.7759996312      1.1700466598      0.9733692488 \n           pidext           ideoext              news               dem \n    -0.3281938237     -0.4970353573     -0.5759629296     -0.0456812052 \n as.numeric(educ)               age              male             white \n     0.0284167135     -0.0007305433     -0.1759866726      0.0366164598 \nfactor(inc3miss)2 factor(inc3miss)3 factor(inc3miss)4               1|2 \n    -0.0470362687     -0.0794217660     -0.0588030650     -2.9118275702 \n              2|3               3|4               4|5               5|6 \n    -1.7749101884     -1.0852947154      0.3080104278      0.9101173775 \n              6|7 \n     2.8740344326 \n\n# this piece [1:ncol(X)] makes sure we omit the zetas\n# this is only necessary for svyolr. The polr function already omits zetas from coef()\nb &lt;- coef(fit.empw2)[1:ncol(X)]\neta &lt;- X %*% b\nzeta &lt;- fit.empw2$zeta\n\n## Find Pr(lowest category)\nemp0 &lt;- mean(plogis(zeta[1] - eta))\nemp0\n\n[1] 0.3025648\n\n\n\n## Repeat for each value of empconc of interest\nX[,\"empconc\"] &lt;- .2\n# Find Xb and zeta\neta &lt;- X %*% coef(fit.empw2)[1:ncol(X)]\nzeta &lt;- fit.empw2$zeta\n## Find Pr(lowest category)\nemp2 &lt;- mean(plogis(zeta[1] - eta))\nemp2\n\n[1] 0.3555723\n\n\n\n## Or put it in a function to be faster\nfindpr &lt;- function(val){\n  X[,\"empconc\"] &lt;- val\n  # Find Xb and zeta\n  eta &lt;- X %*% coef(fit.empw2)[1:ncol(X)]\n  zeta &lt;- fit.empw2$zeta\n  ## Find Pr(lowest category)\n  pr &lt;- mean(plogis(zeta[1] - eta))\n  return(pr)\n}\n## Does it work? Test\nfindpr(0)\n\n[1] 0.3025648\n\n## Repeat for all values of empathy\nemp.prs &lt;- sapply(seq(0, 1, .2), findpr)\nemp.prs\n\n[1] 0.3025648 0.3555723 0.4116936 0.4696702 0.5281235 0.5856687\n\n\nWe can visualize these estimates similar to the authors.\n\nplot(x=seq(0, 1, .2),\n     y=emp.prs,\n     ylim = c(.1, .7),\n     type=\"l\",\n     xlab = \"Empathic Concern\",\n     ylab = \"\",\n     main = \"Outparty Favorability \\n Pr(Outparty Very Unfavorable)\",\n     cex.main=.8)\n\n\n\n\nWe could add lines for all categories. We’ll just add it to the function for now.\n\nfindprall &lt;- function(val){\n  X[,\"empconc\"] &lt;- val\n  # Find Xb and zeta\n  eta &lt;- X %*% coef(fit.empw2)[1:ncol(X)]\n  zeta &lt;- fit.empw2$zeta\n  ## Find Pr(7th lowest category)\n  pr7 &lt;- mean(1 - plogis(zeta[6] - eta))\n  ## Find Pr(6th lowest category)\n  pr6 &lt;- mean(plogis(zeta[6] - eta) - plogis(zeta[5] - eta))\n  ## Find Pr(5th lowest category)\n  pr5 &lt;- mean(plogis(zeta[5] - eta) - plogis(zeta[4] - eta))\n  ## Find Pr(4th lowest category)\n  pr4 &lt;- mean(plogis(zeta[4] - eta) - plogis(zeta[3] - eta))\n  ## Find Pr(3rd lowest category)\n  pr3 &lt;- mean(plogis(zeta[3] - eta) - plogis(zeta[2] - eta))\n  ## Find Pr(2nd lowest category)\n  pr2 &lt;- mean(plogis(zeta[2] - eta) - plogis(zeta[1] - eta))\n  ## Find Pr(lowest category)\n  pr1 &lt;- mean(plogis(zeta[1] - eta))\n  return(c(pr1, pr2, pr3, pr4, pr5, pr6, pr7))\n}\n## Repeat for all values of empathy\nemp.prsall &lt;- sapply(seq(0, 1, .2), findprall)\n\nA note on the survey weights:\n\nWhen we take the mean() of our estimates for each observation, we are treating each row of our \\(X\\) matrix as if it should have equal weight in this mean. This is normally fine, but with weighted data, we might instead want to weight the mean, according to the survey weights we used in our regression. The function weighted.mean can facilitate this. Either approach is valid, but you may want to remember this distinction and the possibility that you could even incorporate the survey weights at this final stage.\n\nWe can add these lines to the plot. Yikes! A bit messy. You can see why they focus on the first category only.\n\nplot(x=seq(0, 1, .2),\n     y=emp.prsall[1,],\n     ylim = c(0, 1),\n     type=\"l\",\n     xlab = \"Empathic Concern\",\n     ylab = \"\",\n     main = \"Outparty Favorability \\n Pr(Outparty Very Unfavorable)\",\n     cex.main=.8)\npoints(x=seq(0, 1, .2), y=emp.prsall[2,], type=\"l\", lty=2)\npoints(x=seq(0, 1, .2), y=emp.prsall[3,], type=\"l\", lty=3)\npoints(x=seq(0, 1, .2), y=emp.prsall[4,], type=\"l\", lty=4)\npoints(x=seq(0, 1, .2), y=emp.prsall[5,], type=\"l\", lty=5)\npoints(x=seq(0, 1, .2), y=emp.prsall[6,], type=\"l\", lty=6)\npoints(x=seq(0, 1, .2), y=emp.prsall[7,], type=\"l\", lty=7)\nlegend(\"topleft\",  lty=1:7, c(\"Very Unfav\", \"2\", \"3\", \"4\", \"5\", \"6\", \"Very Fav\"), cex=.6)\n\n\n\n\nA last step would be to calculate uncertainty. Just like before, we could use simulation or the bootstrap method. In the bootstrap method, the manual calculations would be the “meat” of the bootstrap function that you used in the previous course notes section.\nA special note: The svyolr function does not appear compatible with the prediction function. As an alternative, we could fit the polr model with an extra argument for weights. These should produce the same coefficient estimates, though the standard errors might be incorrect. You could potentially use this to generate the raw probability estimates. See for example, estimates for being in category 1 below:\n\nfit.emp2 &lt;- polr(outfav ~ empconc + empdist + emppers + empfant +\n                  + pidext + ideoext + news + dem +\n                  as.numeric(educ) + age + male + white + factor(inc3miss),\n                data=emp, Hess=T, method=\"logistic\", weights = emp$weight_group)\n\nWarning in eval(family$initialize): non-integer #successes in a binomial glm!\n\nsummary(fit.emp2)\n\nCall:\npolr(formula = outfav ~ empconc + empdist + emppers + empfant + \n    +pidext + ideoext + news + dem + as.numeric(educ) + age + \n    male + white + factor(inc3miss), data = emp, weights = emp$weight_group, \n    Hess = T, method = \"logistic\")\n\nCoefficients:\n                       Value Std. Error t value\nempconc           -1.4141936   0.479987 -2.9463\nempdist            0.7759968   0.412555  1.8810\nemppers            1.1700975   0.527905  2.2165\nempfant            0.9733691   0.406788  2.3928\npidext            -0.3281915   0.097894 -3.3525\nideoext           -0.4970277   0.078991 -6.2922\nnews              -0.5759557   0.094099 -6.1207\ndem               -0.0456691   0.161136 -0.2834\nas.numeric(educ)   0.0284158   0.056125  0.5063\nage               -0.0007309   0.004667 -0.1566\nmale              -0.1759908   0.164116 -1.0724\nwhite              0.0366286   0.168809  0.2170\nfactor(inc3miss)2 -0.0470201   0.205414 -0.2289\nfactor(inc3miss)3 -0.0794134   0.207121 -0.3834\nfactor(inc3miss)4 -0.0587784   0.233469 -0.2518\n\nIntercepts:\n    Value   Std. Error t value\n1|2 -2.9118  0.6315    -4.6109\n2|3 -1.7749  0.6251    -2.8394\n3|4 -1.0853  0.6211    -1.7473\n4|5  0.3080  0.6257     0.4923\n5|6  0.9101  0.6363     1.4303\n6|7  2.8741  0.7763     3.7022\n\nResidual Deviance: 1862.152 \nAIC: 1904.152 \n(245 observations deleted due to missingness)\n\n## marginaleffects\npreds.me &lt;- avg_predictions(fit.emp2, by = \"empconc\",\n                            newdata = datagridcf(empconc = seq(0,1,.2)))\nsubset(preds.me, group==1)\n\n\n Group Estimate Std. Error     z Pr(&gt;|z|)     S CI low CI high\n     1    0.303     0.0616  4.92   &lt;0.001  20.1  0.182   0.423\n     1    0.356     0.0485  7.34   &lt;0.001  42.0  0.261   0.451\n     1    0.412     0.0333 12.36   &lt;0.001 114.1  0.346   0.477\n     1    0.470     0.0200 23.54   &lt;0.001 404.4  0.431   0.509\n     1    0.528     0.0201 26.33   &lt;0.001 505.1  0.489   0.567\n     1    0.586     0.0333 17.60   &lt;0.001 227.9  0.520   0.651\n\nColumns: group, empconc, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n\n## prediction\npreds &lt;- prediction(fit.emp2, at=list(empconc = seq(0,1,.2)), category=1)\n\nWarning in check_values(data, at): A 'at' value for 'empconc' is outside\nobserved data range (0.0357142873108387,1)!\n\nround(summary(preds), digits=3)\n\n at(empconc) Prediction SE  z  p lower upper\n         0.0      0.303 NA NA NA    NA    NA\n         0.2      0.356 NA NA NA    NA    NA\n         0.4      0.412 NA NA NA    NA    NA\n         0.6      0.470 NA NA NA    NA    NA\n         0.8      0.528 NA NA NA    NA    NA\n         1.0      0.586 NA NA NA    NA    NA\n\n## Compare with the manual approach\nround(emp.prsall[1,], digits=3) # category 1\n\n[1] 0.303 0.356 0.412 0.470 0.528 0.586"
  },
  {
    "objectID": "09-NominalOutcomes.html#overview-of-nominal-data",
    "href": "09-NominalOutcomes.html#overview-of-nominal-data",
    "title": "9  Multinomial Outcomes",
    "section": "9.1 Overview of Nominal Data",
    "text": "9.1 Overview of Nominal Data\nOur goal in these models is generally to predict the probability of being in a particular category \\(C_j\\).\nThe model we will use to estimate this is the multinomial logit.\n\nThis is a generalization of binary and ordered logit.\nCoefficients defined relative to baseline outcome category \\(J\\):\n\n\\(\\log \\frac{\\pi_j}{\\pi_J} = \\eta_j\\) where \\(\\eta_j = \\alpha_j + X_1\\beta_{1j} + X_2\\beta_{2j} + ... X_k\\beta_{kj}\\)\n\nNote that \\(\\pi_i\\) (our probability) is now indexed by \\(j\\) or \\(J\\)\n\nThis means we have more than just one set of estimates for \\(\\hat \\beta\\), depending on the category comparison we make\n(Recall that in ordinal logit we had to assume that one set of coefficients was sufficient. Here, we have \\(J-1\\) sets of coefficients.)\n\n\\(\\beta_J = 0\\) by design for identification (\\(J\\) represents the baseline category);\n\\(\\sum_{j=1}^{J} \\pi_j = 1\\), The probabilities of being in each category, together, must sum to 1.\n\\(Y_i = C_j\\) according to \\(Y_{ij}^* = max(Y_{i1}^*, Y_{i2}^*, ..., Y_{ij}^*)\\). The outcome belongs to the category that has the highest \\(Y_i*\\).\nThe probability of \\(Y_i\\) being in a particular category is:\n\n\\[\\begin{align*}\n\\pi_{ij} = Pr(Y_i = C_j | x_i) &= \\frac{\\exp(\\mathbf x_i^T\\beta_j)}{1 + \\sum_{j=1}^{J-1} \\exp(\\mathbf x_i^T\\beta_j)}\n\\end{align*}\\]\n\n9.1.1 Multinomial Likelihood\nHere, similar to the ordinal log likelihood, we need to sum over all observations and all outcome categories to represent the joint probability:\n\\(\\mathcal L(\\beta, | Y) = \\prod_i^N \\prod_{j=1}^{J} \\mathbf 1(Y_i=C_j){\\pi_{i, j}}\\)\n\\(\\mathcal l(\\beta, | Y) = \\sum_i^N \\sum_{j=1}^{J} \\mathbf 1(Y_i=C_j){\\log \\pi_{i, j}}\\)\nLike we have done previously in likelihood, where \\(\\pi\\) is a function of \\(X\\) and \\(\\beta\\)."
  },
  {
    "objectID": "09-NominalOutcomes.html#motivating-example",
    "href": "09-NominalOutcomes.html#motivating-example",
    "title": "9  Multinomial Outcomes",
    "section": "9.2 Motivating Example",
    "text": "9.2 Motivating Example\nTo work through the model we will use data from the article “Empowering Women? Gender Quotas and Women’s Political Careers” by Yann Kerevel, in The Journal of Politics here\nWomen’s representation in executive office continues to lag behind that of their female peers in legislatures, and several studies find women face an executive glass ceiling. One way to shatter this executive glass ceiling is through the adoption of legislative gender quotas. However, scholars know little about how legislative quotas affect women’s access to executive office. Previous research has been unable to determine whether once elected through quotas, women advance to executive office at similar rates to men. Using data on the future career paths of nearly 2,000 Mexican legislators, I find women face a glass ceiling in winning nomination to executive office. Using career data before and after quota implementation, and exploiting lax enforcement in the district component of the mixed electoral system, I find quotas have done little to increase the advancement of women into executive office, although they have increased opportunities for women in other legislative positions.\nOn pg. 1169, Kerevel writes, “If a glass ceiling exists, women may find it harder to advance to executive office compared to men. Female legislators may still be able to develop successful political careers in similar roles, such as seeking reelection, winning nomination to other legislative offices, or receiving some type of political appointment. However, women may be less likely than men to secure executive nominations to elected positions or appointments to important cabinet posts. The introduction of gender quotas is unlikely to shatter this glass ceiling given the numerous ways women are marginalized once elected and the general lack of executive quotas.”\nThe first hypothesis following this claim is:\n\nH1. Glass ceiling- Women legislators will be nominated to future executive office at lower rates than men.\n\nWe will focus on this first hypothesis, but if you are interested, the author also discusses specific hypotheses related to the potential negative and positive effects of gender quotas in the paper.\nData\nThe data include information on the future career moves of nearly 2000 Mexican federal deputies who served between 1997-2009.\n\nOutcome: genderimmedambcat3, first future career move\n\nballot access for state legislature/city council, for Senate, for mayor/governor; appointment to cabinet or party leadership; bureaucratic appointment; other office\n\nExplanatory variables: female, party_cat, leg_exp_dum, exec_exp_dum, leadership\n\nLet’s load the data and take a look at the outcome variable.\n\nlibrary(foreign)\nker &lt;- read.dta(\"https://github.com/ktmccabe/teachingdata/blob/main/kerevel.dta?raw=true\")\n\ntable(ker$genderimmedambcat3)\n\n\n                       other deputy/regidor ballot access \n                         680                          241 \n        senate ballot access      mayor/gov ballot access \n                         130                          243 \n        cabinet/party leader                     bur appt \n                         137                          329 \n\n\nThe outcome categories focus on nominations to these offices (i.e., ballot access), as the author finds that gender has less to do with electoral success.\n\n9.2.1 Assumption and Considerations\nLet’s focus on the author’s research question. How could we evaluate this relationship? The UCLA site has a good discussion of these tradeoffs.\n\nOne option would be to collapse this to a more simple problem of multiple separate binary logit models comparing two outcome categories at a time.\n\nOne possible issue is there is no constraint such that multiple pairwise logits won’t generate probabilities that when summed together exceed 1. Another is that each pairwise comparison might involve a slightly different sample. In contrast, the multinomial logit uses information from all \\(N\\) observations when jointly estimating the likelihood.\n\nA second option would be to collapse the outcome categories down to two levels only (E.g., 1=Mayor/governor or Bureaucrat vs. 0=Otherwise)\n\nJust like we said collapsing a scale in ordinal models loses information, the same thing would happen here. This only makes sense to do in a case where a collapsed scale is theoretically interesting.\n\nAssess the scale– maybe the categories actually are ordered!\n\nThink about whether the categories actually could be considered ordered or even on an interval scale. If that is the case, then you might be able to return to a linear or ordinal model.\n\n\nInstead, if we want to keep the nominal categories as they are, and we believe they are not ordered, we can move forward with the multinomial model.\n\nIf we do so, we will want to make sure we have enough data in each outcome category to feel comfortable estimating the predicted probability of being in that category. We also want to avoid situations where there is no variation in our independent variables for a given outcome category (e.g., maybe only men were nominated for mayor/governor)\n\nIf we run into those situations, we might need more data or think about alternative modelling strategies.\n\n\n\n\n9.2.2 Key Assumption: Independence of Irrelevant Alternatives\nThe IIA assumption requires that our probability of choosing a particular outcome over another (A over B) does not depend on the choice set, in particular, the presence or absence of some choice C.\nClassic case: bus example\n\nSuppose you have the transportation choice set below, where the numbers are the probabilities of choosing a particular form of transportation.\nChoice set: {train, red bus, blue bus} = {.5, .25, .25}\n\nChoice set: {train, red bus} = {.5, .5}–Violates IIA\nChoice set: {train, red bus} = {2/3, 1/3}–Does not violate IIA\n\n\nThe is because we assume the independence of the error terms \\(\\epsilon_i\\) across choices. In a multinomial probit model, this can be relaxed. However, multinomial probit is very computationally intensive, often not used."
  },
  {
    "objectID": "09-NominalOutcomes.html#running-multinomial-logit-in-r",
    "href": "09-NominalOutcomes.html#running-multinomial-logit-in-r",
    "title": "9  Multinomial Outcomes",
    "section": "9.3 Running multinomial logit in R",
    "text": "9.3 Running multinomial logit in R\nThe multinomial specification will estimate coefficients that are relative to a baseline level of the outcomes. We should be thoughtful about what we choose as the baseline so that the coefficients are useful to us. Here, the author chooses to use “other” as the baseline.\nIn R, we can easily change the baseline level of a factor variable. So first, we should check the class of the outcome variable and convert it into a factor variable if needed.\n\nclass(ker$genderimmedambcat3)\n\n[1] \"factor\"\n\n\nWe can then adjust the baseline level using the relevel command as has the ref argument for specifying a reference category.\n\n## Sets a level as the base category for the factor\nker$genderimmedambcat3 &lt;- relevel(ker$genderimmedambcat3, ref = \"other\")\n\nLet’s run a simple regression model with just female as a covariate to see how the regression output differs from other models we have used thus far.\n\n## install.packages(\"nnet\")\nlibrary(nnet)\nfit0 &lt;- multinom(genderimmedambcat3 ~ factor(female), data = ker)\n\n# weights:  18 (10 variable)\ninitial  value 3153.496666 \niter  10 value 2849.378831\nfinal  value 2837.879460 \nconverged\n\n\nYou can see that there is now a set of coefficients– Intercept and for female, for each outcome category-baseline comparison. We also do not automatically have the p-values for the output.\nWe can find the p-values the same way we have done before by calculating z-scores through the division of the coefficients over the standard errors. Once we have the z-scores we can use pnorm the same way we did in the ordinal section.\n\nz &lt;- summary(fit0)$coefficients/summary(fit0)$standard.errors\np &lt;- as.matrix(2*(pnorm(abs(z), lower.tail = F)))\np\n\n                              (Intercept) factor(female)1\ndeputy/regidor ballot access 8.040554e-39      0.10711432\nsenate ballot access         1.096025e-57      0.03803764\nmayor/gov ballot access      1.119624e-33      0.06783023\ncabinet/party leader         2.110040e-54      0.65008684\nbur appt                     2.383587e-26      0.01908118\n\n\nAs you can see, the output here for the coefficients is already messy, and we only have one covariate!! Just imagine how messy it can get with several covariates. Often, because of this, researchers move to present results visually instead. The practice problems will include a chance to replicate one of the author’s visuals from the JOP paper.\n\n9.3.1 Multinomial Quantities of Interest\nLike the previous models, we can calculate predicted probabilities at specific values of our \\(X\\) covariates. What differs here is just the specific form of this function.\nPredicted probabilities of the outcome being in a particular outcome category \\(C_j\\):\n\n\\(Pr(Y_i = C_j | X) = \\frac{\\exp(X\\hat \\beta_j)}{1 + \\sum_{j=1}^{J-1} \\exp(X \\hat \\beta_j)}\\)\n\nThe numerator is very similar to the numerator for when we have a binary logistic regression\nThe denominator sums up \\(\\exp(X \\hat \\beta_j)\\) where \\(\\hat \\beta_j\\) represents the set of coefficients for each outcome category except for the baseline (i.e., \\(J- 1\\) means 1 less than the total number of outcome categories).\nRecall that in the baseline category \\(J\\), \\(\\hat \\beta_J\\) is forced to 0 to help with the estimation or “identification” of the other coefficients relative to that category. Well why bring that up now? Well, \\(exp(X \\hat \\beta_J) = exp(0) = 1\\). When we estimate probabilities in the baseline category, the numerator will just be 1.\n\n\nLet’s take an example of finding the predicted probability of \\(C_j\\) = Senate ballot access when \\(x_i\\) is set to be female = 1.\n\n## Create the model matrix\nXfemale &lt;- cbind(1, 1) # 1 for intercept, 1 for female\n\n## Extract coefficients and give them informative labels\nBsenate &lt;- coef(fit0)[2,] # 2nd row of coefficients\nBdep &lt;- coef(fit0)[1,]\nBmayor &lt;- coef(fit0)[3,]\nBcabinet &lt;- coef(fit0)[4,]\nBbur &lt;- coef(fit0)[5,]\n\n## Probability of senate ballot access for female\nexp(Xfemale %*% Bsenate)/ (1 + exp(Xfemale %*% Bsenate) + exp(Xfemale %*% Bdep) +\n                             exp(Xfemale %*% Bmayor) + exp(Xfemale %*% Bcabinet) +\n                             exp(Xfemale %*% Bbur))\n\n           [,1]\n[1,] 0.09846309\n\n\nLet’s do the same for female = 0. We just need to change X.\n\nXnfemale &lt;- cbind(1, 0)\nexp(Xnfemale %*% Bsenate)/ (1 + exp(Xnfemale %*% Bsenate) + exp(Xnfemale %*% Bdep) +\n                             exp(Xnfemale %*% Bmayor) + exp(Xnfemale %*% Bcabinet) +\n                             exp(Xnfemale %*% Bbur))\n\n           [,1]\n[1,] 0.06829273\n\n\nWe might also be interested in the probability of \\(C_j\\) = other. The syntax here will look slightly different because “other” was the baseline category. The denominator stays the same, but the numerator is just 1.\n\n## Manual- probability of other for female (the base category)\n1/ (1 + exp(Xfemale %*% Bsenate) + exp(Xfemale %*% Bdep) +\n                             exp(Xfemale %*% Bmayor) + exp(Xfemale %*% Bcabinet) +\n                             exp(Xfemale %*% Bbur))\n\n          [,1]\n[1,] 0.3538449\n\n\nJust like in the other models, we can rely on outside packages, too. For some models, these packages are not going to have full capabilities– they might not be able to calculate standard errors, for example. These are “living packages” so you can always check the documentation and update the packages to see if new capabilities have been added.\n\nlibrary(marginaleffects)\np.allgroups &lt;- avg_predictions(fit0, \n                             by=\"female\",\n                             newdata = datagridcf(female = 1))\np.allgroups\n\n\n                        Group female Estimate Std. Error     z Pr(&gt;|z|)     S\n other                             1   0.3538     0.0265 13.34   &lt;0.001 132.5\n deputy/regidor ballot access      1   0.1600     0.0203  7.87   &lt;0.001  48.0\n senate ballot access              1   0.0985     0.0165  5.96   &lt;0.001  28.5\n mayor/gov ballot access           1   0.0892     0.0158  5.64   &lt;0.001  25.8\n cabinet/party leader              1   0.0646     0.0136  4.74   &lt;0.001  18.8\n bur appt                          1   0.2338     0.0235  9.96   &lt;0.001  75.2\n  2.5 % 97.5 %\n 0.3019 0.4058\n 0.1201 0.1999\n 0.0661 0.1309\n 0.0582 0.1202\n 0.0379 0.0913\n 0.1878 0.2799\n\nColumns: group, female, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  probs \n\n\nWe can also calculate the difference in predicted probabilities, for example, between female=1 and female=0.\n\nlibrary(marginaleffects)\nme.allgroups &lt;- avg_comparisons(fit0, variables= list(female = c(0,1)))\n\nWarning: The `female` variable is treated as a categorical (factor) variable, but\n  the original data is of class integer. It is safer and faster to convert\n  such variables to factor before fitting the model and calling a\n  `marginaleffects` function.\n  \n  This warning appears once per session.\n\nme.allgroups\n\n\n                        Group   Term Contrast Estimate Std. Error     z\n bur appt                     female    1 - 0   0.0575     0.0255  2.25\n cabinet/party leader         female    1 - 0  -0.0162     0.0154 -1.05\n deputy/regidor ballot access female    1 - 0   0.0283     0.0222  1.27\n mayor/gov ballot access      female    1 - 0  -0.0599     0.0184 -3.26\n other                        female    1 - 0  -0.0399     0.0295 -1.35\n senate ballot access         female    1 - 0   0.0302     0.0178  1.69\n Pr(&gt;|z|)   S    2.5 %  97.5 %\n  0.02429 5.4  0.00747  0.1076\n  0.29282 1.8 -0.04644  0.0140\n  0.20269 2.3 -0.01524  0.0718\n  0.00113 9.8 -0.09596 -0.0238\n  0.17629 2.5 -0.09769  0.0179\n  0.09040 3.5 -0.00475  0.0651\n\nColumns: term, group, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  probs \n\n\nBelow are examples from the prediction and margins packages from Thomas Leeper.\n\nlibrary(prediction)\np.senate &lt;- prediction(fit0, at  = list(female = 1))\nsummary(p.senate)\n\n at(female) Prediction SE  z  p lower upper\n          1     0.3538 NA NA NA    NA    NA\n\nmean(p.senate$`Pr(senate ballot access)`)\n\n[1] 0.09846309\n\nmean(p.senate$`Pr(other)`)\n\n[1] 0.3538449\n\n\nWe can use margins to calculate the difference in predicted probabilities, for example, between female=1 and female=0. We should specify the category for which we want this comparison. It appears we need to set vce = booststrap for this to work.\n\nlibrary(margins)\nm.senate &lt;- margins(fit0, variables = \"female\", category = \"senate ballot access\", \n                    vce=\"bootstrap\", change=c(0,1))\nsummary(m.senate)\n\nOf course, we could also do this manually, have a quantity of interest party, and run the bootstrap ourselves."
  },
  {
    "objectID": "09-NominalOutcomes.html#practice-problems-for-multinomial",
    "href": "09-NominalOutcomes.html#practice-problems-for-multinomial",
    "title": "9  Multinomial Outcomes",
    "section": "9.4 Practice Problems for Multinomial",
    "text": "9.4 Practice Problems for Multinomial\nWe will try to replicate a portion of the analysis from the paper. Note that different multinomial functions in R and in Stata (which the author used) might rely on slightly different optimization and estimation algorithms, which in smaller samples, might lead to slightly different results. This is one place where your results might not exactly match the authors’, but they should be close.\n\nLet’s try to replicate Figure 1a in the paper (which corresponds to Table 1a in the appendix), which shows the average marginal effect of being female vs. male on the first future career move, for each outcome category in the data.\n\nIn addition to the female covariate, the author includes covariates for party_cat (party identification), leg_exp_dum and exec_exp_dum (legislative and executive experience), and leadership (chamber leadership experience) which are each treated as factor variables in the regression.\nWe should set party_cat to have the baseline of PRI to match the author.\n\n\n\nker$party_cat &lt;- relevel(as.factor(ker$party_cat), ref=\"PRI\")\n\n \nThe author sets covariates to observed levels when estimating the marginal effects.\n\nBased on the marginal effects, how would you evaluate the author’s hypothesis on the effect of gender on future career moves to executive office?\n\n\n\nTry on your own and then expand for the solution.\n\n\nfit2 &lt;- multinom(genderimmedambcat3 ~ factor(female) + party_cat + factor(leg_exp_dum) + \n                   factor(exec_exp_dum) + factor(leadership), data=ker)\n\nlibrary(marginaleffects)\nme.fit2.all &lt;- avg_comparisons(fit2, variables = list(female= c(0, 1)))\n\nLet’s make a visual close to the authors using ggplot We can now use geom_point and geom_errorbar to plot the AME point estimates and bootstrap confidence intervals.\n\nlibrary(ggplot2)\nggplot(me.fit2.all, aes(x=group, y=estimate))+\n  geom_point()+\n  geom_errorbar(aes(ymin=conf.low, ymax=conf.high), width=.05)+\n  theme_bw()+\n  ylim(-.4, .4)+\n  geom_hline(yintercept=0)+\n  ylab(\"Change in probability when Female vs. Male\")+\n  xlab(\"\")\nggsave(\"images/kerplot1.png\", device=\"png\", width=6, height=4)\n\n\n\nlibrary(margins)\nmarg.effect.execnom &lt;- margins(fit2, variables=\"female\", change=c(0, 1), vce= \"bootstrap\", category=\"mayor/gov ballot access\")\nmarg.effect.deputy &lt;- margins(fit2, variables=\"female\", change=c(0, 1), vce= \"bootstrap\", category=\"deputy/regidor ballot access\")\nmarg.effect.senator &lt;- margins(fit2, variables=\"female\", change=c(0, 1), vce= \"bootstrap\", category=\"senate ballot access\")\nmarg.effect.execappt &lt;- margins(fit2, variables=\"female\", change=c(0, 1), vce= \"bootstrap\", category=\"cabinet/party leader\")\nmarg.effect.bureau &lt;- margins(fit2, variables=\"female\", change=c(0, 1), vce= \"bootstrap\", category=\"bur appt\")\nmarg.effect.other &lt;- margins(fit2, variables=\"female\", change=c(0, 1), vce= \"bootstrap\", category=\"other\")\n\nLet’s make a visual close to the authors using ggplot. Recall, ggplot is easiest to work with when the data you want to plot are in a data.frame. So we are going to bind together the summary output and specify which row corresponds to which outcome.\n\nmcomb &lt;- data.frame(rbind(summary(marg.effect.execnom), \n                          summary(marg.effect.deputy), \n                          summary(marg.effect.senator), \n                          summary(marg.effect.execappt),\n                          summary(marg.effect.bureau), \n                          summary(marg.effect.other)))\nmcomb$outcome &lt;- c(\"executive nom\", \"deputy\", \"senator\", \"executive appt\", \"bureaucracy\", \"other\")\nmcomb$outcome &lt;- factor(mcomb$outcome, levels = c(\"deputy\", \"senator\", \"executive nom\", \"executive appt\", \"bureaucracy\", \"other\"))\n\nWe can now use geom_point and geom_errorbar to plot the AME point estimates and bootstrap confidence intervals.\n\nlibrary(ggplot2)\nggplot(mcomb, aes(x=outcome, y=AME))+\n  geom_point()+\n  geom_errorbar(aes(ymin=lower, ymax=upper), width=.05)+\n  theme_bw()+\n  ylim(-.4, .4)+\n  geom_hline(yintercept=0)+\n  ylab(\"Change in probability\")+\n  xlab(\"\")\nggsave(\"images/kerplot.png\", device=\"png\", width=6, height=4)\n\n\nBased on this analysis, women have a significantly lower probability of being nominated for a future mayoral or gubernatorial position, which aligns with the author’s hypothesis."
  },
  {
    "objectID": "09-NominalOutcomes.html#multinomial-model-tutorial",
    "href": "09-NominalOutcomes.html#multinomial-model-tutorial",
    "title": "9  Multinomial Outcomes",
    "section": "9.5 Multinomial Model Tutorial",
    "text": "9.5 Multinomial Model Tutorial\nFor this exercise, we will use data from Amy Lerman, Meredith Sadin, and Samuel Trachtman’s 2017 article in the American Political Science Review, “Policy Uptake as Political Behavior: Evidence from the Affordable Care Act.”\nAbstract. Partisanship is a primary predictor of attitudes toward public policy. However, we do not yet know whether party similarly plays a role in shaping public policy behavior, such as whether to apply for government benefits or take advantage of public services. While existing research has identified numerous factors that increase policy uptake, the role of politics has been almost entirely overlooked. In this paper, we examine the case of the Affordable Care Act to assess whether policy uptake is not only about information and incentives; but also about politics. Using longitudinal data, we find that Republicans have been less likely than Democrats to enroll in an insurance plan through state or federal exchanges, all else equal. Employing a large-scale field experiment, we then show that de-emphasizing the role of government (and highlighting the market’s role) can close this partisan gap.\nIn a portion of their analysis, they use survey data to assess the relationship between partisanship and insurance uptake through the ACA marketplaces. The researchers’ hypothesis is:\nFirst, we expect that partisanship will be a strong predictor of policy behavior. In the case of the ACA, we anticipate that Republicans—who on average are much less supportive of the health insurance reform and are generally more resistant to government intervention in the private market—will be less likely than Democrats to take advantage of health insurance options provided by the ACA.\nKey variables include\n\nins: insurance status, 1=“uninsured”, 3= “marketplace”, 4= “private”\nrepublican: 1= Republican, 0= Democrat\nage2: numeric variable for age\ned: education level of respondent\nracethn: race/ethnicity of respondent\nincome2: categorical income of respondent\nsex: sex of respondent\nstate : respondent’s state of residence (coded as a FIPs code)\nempl2: respondent’s employment status\ndate: date of the survey poll\n\nLet’s load the data and look at the outcome ins.\n\nlibrary(foreign)\nlibrary(nnet) # install this package\nlst &lt;- read.dta(\"https://github.com/ktmccabe/teachingdata/blob/main/lst.dta?raw=true\")\n\n\ntable(lst$ins)\n\n\n   1    3    4 \n2100  724  904 \n\n\nLet’s assess the research question.\n\nOur goal is to understand the relationship between party identification and uptake of ACA marketplace insurance. What are the possible ways we could model this relationship given our outcome data?\nLet’s suppose we decided to go with the multinomial logistic regression.\n\nWhat class() should our outcome variable be?\nWhat would make sense to use as a baseline category?\n\n\n\n\nGo ahead and recode the variable as necessary.\n\nEven if your variable is a factor, I would recommend giving it informative labels instead of numbers so that it is easier to interpret the regression outcome.\n\nclass(lst$ins)\n\n[1] \"numeric\"\n\nlst$ins &lt;- as.factor(ifelse(lst$ins == 1, \"uninsured\",\n                  ifelse(lst$ins == 3, \"marketplace\",\n                         ifelse(lst$ins == 4, \"private\", NA))))\nlst$ins &lt;- relevel(lst$ins, ref = \"marketplace\")\n\n\nLet’s conduct a multinomial logistic regression of the following form:\n\\[\\begin{align*}\n\\log \\frac{ \\pi_{j}}{ \\pi_{J}} &= \\alpha_{j} + Republican_i\\beta_{1j} + \\\\ &\nage2_i \\beta_{2j} + ed_i\\boldsymbol{ \\beta_{kj}} \\\\ & + racethn_i \\boldsymbol{ \\beta_{kj}} + income2_i\\boldsymbol{ \\beta_{kj}} \\\\ & + sex_i\\boldsymbol{ \\beta_{kj}}  + empl2_i\\boldsymbol{ \\beta_{kj}}\n\\end{align*}\\]\nwhere all covariates are treated as factor variables. Note that the \\(k\\)’s are just a placeholder for the coeffcient numbers. For factor variables with several categories, you will end up with several \\(k\\) coefficients for each variable (e.g., one for racethn = Asian, one for racethn = Black, etc.)\n\n\nRun the model.\n\n\nfit &lt;- multinom(as.factor(ins) ~ republican + age2 + factor(ed)\n                + factor(racethn) + factor(income2)\n                + as.factor(sex) \n                + as.factor(empl2), data = lst)\n\n# weights:  60 (38 variable)\ninitial  value 4095.626612 \niter  10 value 3475.264179\niter  20 value 3122.816006\niter  30 value 3067.172420\niter  40 value 3059.543787\nfinal  value 3059.470322 \nconverged\n\n\n\nLet’s consider our assumptions.\n\nWhat is a key assumption of the multinomial logistic regression model?\n\nHow could it potentially be violated in the authors’ case?\n\n\n\n\nExpand for one example.\n\nThe key assumption is the IIA assumption. The authors consider potential violations to this assumption in footnote 4. “Consistent estimation using the multinomial logistic model relies on the Independence of Irrelevant Alternatives (IIA) assumption, which requires that the choice of one of the available options does not depend on whether some alternative option is present. While this assumption is hard to test (Allison 2012), there is some evidence that it could be violated in this case, with the presence of the”uninsured” option affecting the distribution of choices across private and marketplace insurance. Thus, as a robustness check, we also estimate a model in which we first analyze the decision to insure (for the study population), and second, conditional on having insurance, analyze the private versus marketplace choice.”\n\nLet’s assume we are okay with our assumptions. Let’s now try to use the model to evaluate the research hypothesis.\n\nFirst, we should get comfortable extracting coefficients from multinomial output.\n\nExtract the coefficients on the Republican covariate and their standard errors\nCalculate z-scores and p-values\nAssess the statistical significance\nProvide an initial evaluation of the authors’ research hypothesis\n\n\nNote that coef(fit) is now a matrix of output. We want to extract the republican column.\n\nrepcoef &lt;- coef(fit)[, \"republican\"]\nrepse &lt;- summary(fit)$standard.errors[, \"republican\"]\n\n## Calculate z-scores\nrep.zs &lt;- repcoef/repse\nrep.ps &lt;- 2*(pnorm(abs(rep.zs), lower.tail = F))\n\nround(cbind(repcoef, rep.zs, rep.ps), digits=3)\n\n          repcoef rep.zs rep.ps\nprivate     0.942  8.368      0\nuninsured   0.966  9.044      0\n\n\nLet’s now transform these into quantities of interest that closely evaluate the research question.\n\nWhat quantity of interest should we estimate?\n\nCompute the quantity of interest\nCompute uncertainty\nConsider ways to visualize the results\n\n\n\n\nQuantity of Interest calculation.\n\nLet’s calculate the average difference in predicted probability of signing up for marketplace insurance for Republicans vs. Democrats, holding covariates at observed values.\nRecall from above that the probability of \\(Y_i\\) being in a particular category is:\n\\[\\begin{align*}\nPr(Y_i = C_j |X) &= \\frac{\\exp(\\mathbf x_i^T\\beta_j)}{1 + \\sum_{j=1}^{J-1} \\exp(\\mathbf x_i^T\\beta_j)}\n\\end{align*}\\]\nRecall that marketplace insurance is the baseline category. So this means our formula is\n\\[\\begin{align*}\nPr(Y_i = Marketplace | X) &= \\frac{1}{1 + \\sum_{j=1}^{J-1} \\exp(\\mathbf x_i^T\\beta_j)}\n\\end{align*}\\]\n\n## Point estimates\nXrep &lt;- model.matrix(fit)\nXrep[, \"republican\"] &lt;- 1\n\nXdem &lt;- model.matrix(fit)\nXdem[, \"republican\"] &lt;- 0\n\n## Extract all coefficients\nB &lt;- t(coef(fit))\nBprivate &lt;- coef(fit)[1, ]\nBuninsured &lt;- coef(fit)[2, ]\n\n\n## Approach one\nrepmarket.p &lt;- mean(1 / (1 + exp(Xrep %*% Bprivate) + exp(Xrep %*% Buninsured)))\ndemmarket.p &lt;- mean(1 / (1 + exp(Xdem %*% Bprivate) + exp(Xdem %*% Buninsured)))\ndiffmarket.p &lt;- repmarket.p- demmarket.p\n\n## Approach two (easier when you have a lot of outcome categories)\nrepmarket.p &lt;- mean(1 / (1 + rowSums(exp(Xrep %*% B))))\ndemmarket.p &lt;- mean(1 / (1 + rowSums(exp(Xdem %*% B))))\ndiffmarket.p &lt;- repmarket.p - demmarket.p\ndiffmarket.p \n\n[1] -0.1313263\n\n\n\n## Approach three\nlibrary(marginaleffects)\nmarg.effect.all &lt;- avg_comparisons(fit,\n                                   variables=list(republican = c(0,1)))\nsubset(marg.effect.all, group == \"marketplace\")\n\n\n## Approach four\nlibrary(margins)\nmarg.effect.market &lt;- margins(fit, variables=\"republican\", change=c(0, 1), \n                              vce= \"bootstrap\", category=\"marketplace\")\nsummary(marg.effect.market)\n\n\n\n\nUncertainty calculation.\n\nIf we wanted to calculate uncertainty on our own, we could use the simulation or bootstrap approach like before to calculate uncertainty. We will use the bootstrap because the syntax for the simulation approach is more complicated because our coefficients are in a matrix. The package Zelig has this capability. Simulation can be more faster given the multinomial model takes a moment to run.\n\n## Bootstrap\nmyboot &lt;- function(df){\n  \n  wrows &lt;- sample(1:nrow(df), size=nrow(df), replace = T)\n  subdata &lt;- df[wrows, ]\n  \n  fit.boot &lt;- multinom(as.factor(ins) ~ republican + age2 + factor(ed)\n                + factor(racethn) + factor(income2)\n                + as.factor(sex) \n                + as.factor(empl2), data = subdata)\n  \n  ## Point estimates\n  Xrep &lt;- model.matrix(fit.boot)\n  Xrep[, \"republican\"] &lt;- 1\n\n  Xdem &lt;- model.matrix(fit.boot)\n  Xdem[, \"republican\"] &lt;- 0\n\n  ## Extract all coefficients\n  B &lt;- t(coef(fit.boot))\n  Bprivate &lt;- coef(fit.boot)[1, ]\n  Buninsured &lt;- coef(fit.boot)[2, ]\n\n  ## Approach one\n  repmarket.p &lt;- mean(1 / (1 + exp(Xrep %*% Bprivate) + exp(Xrep %*% Buninsured)))\n  demmarket.p &lt;- mean(1 / (1 + exp(Xdem %*% Bprivate) + exp(Xdem %*% Buninsured)))\n  diffmarket.p.boot &lt;- repmarket.p- demmarket.p\n\n  return(cbind(repmarket.p, demmarket.p, diffmarket.p.boot))\n}\n\nmyboot.ests &lt;- do.call(\"rbind\", replicate(1000, myboot(lst), simplify = F))\n\n\n## Confidence intervals around the difference\ncis.diff &lt;- quantile(myboot.ests[, \"diffmarket.p.boot\"], c(0.025, 0.975))\ncis.diff\n\n## Visualize the two distributions\npng(\"images/boostrappid.png\", res=300, width=5, height=4, units=\"in\")\nplot(density(myboot.ests[, \"repmarket.p\"]), col=\"red\",\n     main = \"Bootstrap Distribution for Republican and Democrats \\n Marketplace Uptake Probability\",\n     cex.main = .7,\n     xlim = c(0.05, .3),\n     xlab=\"Estimated Probability of Uptake through Marketplace\")\npoints(density(myboot.ests[, \"demmarket.p\"]), col=\"blue\", type=\"l\")\ndev.off()"
  },
  {
    "objectID": "10-CountData.html#overview-of-count-data",
    "href": "10-CountData.html#overview-of-count-data",
    "title": "10  Count data",
    "section": "10.1 Overview of Count Data",
    "text": "10.1 Overview of Count Data\nMany of our dependent variables in social science may be considered counts:\n\nThe number of arrests or traffic stops\nThe number of bills passed\nThe number of terrorist attacks\nThe number of tweets\nThe number of judges a president nominates per year\n\nEach of these variables shares the features that they are discrete and range from 0 to some positive number.\nExample: Your outcome data might look like this, where each \\(i\\) observation represents a count of some kind:\n\nY &lt;- rpois(n=30, lambda = 2)\nY\n\n [1] 2 1 3 3 3 2 1 2 3 1 4 2 1 1 2 2 3 1 3 1 0 3 5 4 2 2 2 5 0 0\n\n\nA common way one might approach modeling data of this type is to use OLS to estimate a linear model. After all, the data seem quasi-continuous! Sometimes, this might be just fine, but let’s think about some situations where this could go awry.\n\nOftentimes count data are heavily right-skewed and very sparse.\n\nFor example, suppose we were interested in the number of times a social media user makes a comment on a discussion forum. It is very common for a large number of people to make close to zero posts, while a small share of users might make a larger number of posts.\nParticularly in small samples, OLS can struggle with heavily skewed data because the error variance is likely not going to be homogenous, the distribution of errors is not going to be normal1, and the linearity assumption could very well be suspect (violations of the usual assumptions).\nWhen continuous data are heavily right-skewed (e.g., sometimes income is), it is often recommended to \\(\\log\\) transform the \\(y\\) variable before fitting the regression with a linear model. With count data, we can pursue other options. Moreover, if you have a lot of counts that are 0, this transformation is problematic anyway because \\(\\log(0) = -Inf\\). The transformation won’t really work, as standard statistical software will often treat those values as missing.\n\n\nBelow is an example of this type of skew, sparsity, and clustering toward 0.\n\n\n\n\n\n\nNonsensical values?\n\nWith OLS, there is also no guarantee that smaller estimated \\(\\hat y\\) values from the regression line will stay non-negative even though we know that the actual count outcomes are always going to be non-negative.\nThere is also no guarantee that larger \\(\\hat y\\) values will stay within the possible upper-range of \\(y\\) values.\nWhen data are heavily skewed, the regression line, which represents the “conditional mean” \\(\\mathbf E(Y |X=x)\\) given some values of \\(X\\) might be a poor estimate given that generally we know that means can be poor estimates of highly skewed data (e.g., picture how estimates of income given a certain level of education would change if Bill Gates and Mark Zuckerberg are in your sample vs. if they are not).\n\n\nFor more information on dealing with skewed data and non-normal errors in linear regression, see Chapter 12.1 posted on Canvas from the Fox textbook."
  },
  {
    "objectID": "10-CountData.html#poisson-model",
    "href": "10-CountData.html#poisson-model",
    "title": "10  Count data",
    "section": "10.2 Poisson Model",
    "text": "10.2 Poisson Model\nSo we are left unsatisfied with a linear model for our count data. What do we do? Fortunately, there are multiple probability distributions that may be appropriate for the data generating process that generates counts, which we can use maximum likelihood to estimate. Since we have been in Bernoulli world for a while, let’s refresh on the steps we consider when approaching a potential maximum likelihood problem.\n\nWhat is the data generating process? Based on this, describe the probability distribution for \\(Y_i\\).\nDefine the likelihood for a single observation\nDefine the likelihood for all observations\nFind the log-likelihood\nMaximize the function with respect to (wrt) \\(\\theta\\)\n\nThe first probability distribution we will consider is the Poisson. This is a discrete distribution (so we are in pmf instead of pdf territory). Let’s go step-by-step.\n\nWhat is the data generating process? Based on this, describe the probability distribution for \\(Y_i\\).\n\n\\(Y_i \\sim Pois(\\lambda)\\)\n\\(\\Pr(Y=Y_i|\\lambda)= \\frac{exp(-\\lambda) \\lambda^{Y_i}}{Y_i!}\\), which describes the probability that the outcome takes a particular value, given \\(\\lambda\\).\n\n\nWe assume no two events occur at the same time. We also assume the probability of an event occurring at a particular time is not a function of the previous events. Events happen independently. Sometimes the occurrence of an event makes the occurrence of a subsequent event less or more likely. This would be a violation and suggestive of overdispersion, which we will return to later.\n\nGary King uses the example of text messaging. Receiving a text message now probably makes it more likely you will receive additional text messages (i.e., group chats)\n\nRecall that in a normal distribution, we needed two parameters \\(\\mu\\) and \\(\\sigma^2\\), to describe the shape of a normally distributed variable– the mean and variance of a normally distributed outcome.\nIn the Poisson case, the shape of our distribution is defined by the single parameter \\(\\lambda\\). A special feature of the Poisson probability distribution is that \\(\\lambda\\) is both the parameter for the mean \\(\\mathbf E(Y) = \\lambda\\) and the variance \\(\\mathbf V(Y) = \\lambda\\).\n\nTry to remember this detail, because it will come up later when we assess if a Poisson process is actually a good approximation of the data generating process. It’s often not the case that a mean is the same as the variance. It can actually be a quite bad approximation of the variance.\n\nLet’s look at an example of Poisson data to prove to ourselves that if data are Poisson the mean and variance are equivalent:\n\n## We use rpois() to generate some count data according to the Poisson distribution. \n## Let's specify lambda = 4\n## This means the mean of the distribution will be 4 and the variance will be 4\n## In any given sample, it might be slightly off from this\nY &lt;- rpois(n=100000, lambda = 4)\nmean(Y)\n\n[1] 4.0099\n\nvar(Y)\n\n[1] 4.035322\n\n\nThe pmf above describes the probability that Y takes a particular outcome.\n\n## For example given a Y ~ Pois(lambda = 4), let's look at the probability of getting particular counts using dpois, the pmf function for poisson\n\ncounts &lt;- 0:16\nprobs &lt;- dpois(counts, lambda = 4)\nnames(probs) &lt;- counts\nround(probs, digits=3)\n\n    0     1     2     3     4     5     6     7     8     9    10    11    12 \n0.018 0.073 0.147 0.195 0.195 0.156 0.104 0.060 0.030 0.013 0.005 0.002 0.001 \n   13    14    15    16 \n0.000 0.000 0.000 0.000 \n\n## The probability is higher the closer we are to 4, the mean of the distribution\n## Let's check our formula from above for the probability Y = 2\n\nlambda &lt;- 4\nyi &lt;- 2\ndpois(yi, lambda=4)\n\n[1] 0.1465251\n\n## formula from above\nexp(-lambda)*lambda^yi/factorial(yi)\n\n[1] 0.1465251\n\n\nThe expected count here is 4, the mean of the distribution. The probability of any given count is specified according to the pmf above.\nAdding covariates\nIn regression, we will consider \\(\\lambda\\) (the expected count) to be a function of \\(\\mathbf x_i'\\beta\\), and we will try to estimate our outcome \\(\\mathbf E(Y_i | \\mathbf x_i)\\) given values of \\(\\mathbf x_i\\).\nHowever, just like in the logit/probit case, our parameter \\(\\lambda \\neq \\mathbf x_i'\\beta\\). Instead, it is a nonlinear function of \\(\\mathbf x_i'\\beta\\). Here, we just have a different link function. Specifically,\n\n\\(\\lambda_i = exp(\\mathbf x_i^T\\beta)\\)\n\\(\\log \\lambda_i = \\eta_i = \\mathbf x_i^T\\beta\\)\n\\(\\lambda_i = \\mathbf E[Y_i | \\mathbf x_i] = \\exp(\\beta_0 + \\beta_1x_{i1} + ... + \\beta_{k}x_{ik})\\) is the expected number of events per a unit of time or space\n\nAnalogy: Recall, in the Bernoulli case, we had just the parameter \\(\\pi\\), which described the expected probability of success given there is just one trial. Recall, in logistic regression \\(\\pi_i = \\frac{exp(\\mathbf x_i^T\\beta)}{1 + exp(\\mathbf x_i^T\\beta)}\\). Here, the transformation is just a different link function.\nOK, if we are using the existing functions in R, we can essentially stop here and proceed to glm (yes, we get to use our glm friend again). But, let’s look at the likelihood to finish out the process.\n\nDefine the likelihood for a single observation\n\nThis is just that pmf from above. For now, we will just write \\(\\lambda\\), but we know eventually we will need to substitute it with our expression \\(\\lambda_i = exp(\\mathbf x_i^T\\beta)\\).\n\\[\\begin{align*}\n\\mathcal L(\\beta |Y_i)=\\Pr(Y=Y_i|\\lambda)\n\\end{align*}\\]\n\nDefine the likelihood for all observations\n\nHere, we need to multiply across all observations. To do this, we are assuming independence.\n\\[\\begin{align*}\n\\mathcal L(\\beta |Y)&=\\mathcal L(\\beta|Y_1)\\times\\mathcal  L(\\beta|Y_2)\\times \\ldots \\times \\mathcal L(\\beta|Y_{n})\\\\\n\\mathcal L(\\beta|Y)&=\\prod_{i=1}^N\\mathcal L(\\beta|Y_i)\\\\\n&= \\prod_{i = 1}^{N}\\frac{1}{Y_i!}\\lambda_i^{Y_i}\\exp(-\\lambda_i)\n\\end{align*}\\]\n\nFind the log-likelihood\n\nWe’ve seen this party trick before. Taking the \\(\\log\\) gives us the sums:\n\\[\\begin{align*}\n\\mathcal l(\\beta|Y)&=\\sum_{i=1}^N\\mathcal \\log(\\mathcal L(\\beta|Y_i))\\\\\n&= \\sum_{i = 1}^{n}\\log(\\frac{1}{Y_i!}\\lambda_i^{Y_i}\\exp(-\\lambda_i))\\\\\n&= \\sum_{i = 1}^{n}\\log (\\frac{1}{Y_i!})  + Y_i\\log(\\lambda_i) - \\lambda_i\\\\\n&=  \\sum_{i = 1}^{n}Y_i\\mathbf x_i^\\top\\beta - \\exp(\\mathbf x_i^\\top\\beta) - \\log(Y_i!)\n\\end{align*}\\]\n\nMaximize the function with respect to (wrt) \\(\\theta\\)\n\nOof, this is where we take the derivative to find the \\(S(\\theta)\\).\n\nFortunately, the last term does not have a \\(\\beta\\), so it falls out\nRecall, the derivative of \\(e^{z}= e^{z}\\)\n\n\\[\\begin{align*}\n\\frac{\\delta}{\\delta \\beta} \\ell(\\beta | Y, X) &= \\frac{\\delta}{\\delta \\beta} \\sum_{i = 1}^{n}Y_i\\mathbf x_i^\\top\\beta - \\exp(\\mathbf x_i^\\top\\beta) - \\log(Y_i!)\\\\\n&= \\sum_{i = 1}^{n} (Y_i - \\exp(\\mathbf x_i^\\top\\beta))\\mathbf x_i^\\top\\\\\n&= X^TY - X^T\\exp(X\\beta) \\text{ which is a $k \\times 1$}\n\\end{align*}\\]\nThis will not yield a closed form solution for \\(\\hat \\beta\\) when setting it to zero. Instead, we have to use numerical methods to estimate the parameters (e.g., think optim).\nThe good thing is that now that we have taken the first derivative, we can take the second derivative to find the Hessian, which will allow us to estimate uncertainty.\n\\[\\begin{align*}\n  &= - \\sum_{i = 1}^{n} \\mathbf x_i\\mathbf x_i'\\exp(\\mathbf x_i^\\top\\beta)\\\\\n  &= - X^TVX \\text{ where } V = n \\times n \\text{ diagonal matrix of }  \\exp(X\\beta)\n\\end{align*}\\]\nNote this is the \\(k \\times k\\) matrix!\n\nFor the variance estimates of our coefficients, we want the \\((\\mathbf E(-H))^{-1} = (X^TVX)^{-1}\\)\n\nThat is our vcov(fit) in the Poisson case\n\n\nOK, let’s start translating this math into R."
  },
  {
    "objectID": "10-CountData.html#motivating-example-for-count-data",
    "href": "10-CountData.html#motivating-example-for-count-data",
    "title": "10  Count data",
    "section": "10.3 Motivating Example for Count Data",
    "text": "10.3 Motivating Example for Count Data\nWe will use the following article for our motivating example: “Legislative Capacity and Executive Unilateralism” by Alex Bolton and Sharece Thrower, which was published in the American Journal of Political Science in 2015.\nAbstract. This article develops a theory of presidential unilateralism in which both ideological divergence with Congress and legislative capacity influence the president’s use of executive orders. We argue that when Congress is less capable of constraining the executive, the president will issue more executive orders during periods of divided government. Conversely, in periods of high legislative capacity, the president is less likely to issue executive orders when faced with an opposed Congress. Based on an examination of institutional changes, we identify years prior to the mid‐1940s as characterized by low congressional capacity and the subsequent period as characterized by high capacity. Testing the theory between 1905 and 2013, we find strong support for these predictions and demonstrate that legislative capacity conditions the role of ideological disagreement in shaping presidential action. Overall, this article deepens our current understanding of the dynamics of separation‐of‐powers politics and the limits of executive power.\nThe primary research question: Is the president constrained by an ideologically opposed Congress? The authors explore how the number of executive orders made per year varies acording to whether the government is divided or unified.\n\nOutcome: allnoncerm_eo, all non-ceremonial executive orders in a year\nKey Explanatory variable: divided, whether or not there was divided government, where the president and the majority party in either the House or Senate are different parties\nOther explanatory variables include dummy variables for the president, an indicator if it is war time, measures related to the economy, and whether it is close to the beginning or end of an administration\n\nLet’s load the data and look at the outcome variable.\n\nlibrary(foreign)\nbolton &lt;- read.dta(\"https://github.com/ktmccabe/teachingdata/blob/main/bt.dta?raw=true\")\n\ntable(bolton$allnoncerm_eo)\n\n\n 20  26  30  31  34  35  37  38  39  40  41  42  43  45  48  49  50  52  53  54 \n  1   2   1   2   2   2   3   1   1   1   3   2   1   4   2   3   1   2   2   2 \n 55  56  57  61  63  64  65  66  68  69  70  71  72  75  76  78  85  92  96  97 \n  1   1   3   1   1   2   1   2   2   1   2   1   1   1   1   1   1   1   1   1 \n 98 112 116 117 120 146 164 172 188 200 212 219 225 232 241 247 250 253 265 267 \n  2   1   1   1   1   1   2   1   1   1   1   1   2   1   1   3   1   1   1   1 \n273 286 287 303 305 307 309 315 319 328 338 339 345 358 382 393 438 471 473 501 \n  1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1 \n\n\nOften for count variables, it can be useful to visualize them in a histogram. Here is a ggplot version.\n\nlibrary(ggplot2)\nggplot(bolton, aes(allnoncerm_eo))+\n  geom_histogram(binwidth = 5)+\n  theme_minimal()\n\n\n\n\nThe authors distinguish time pre- and post-1945 based on different levels of Congressional capacity. We can look at how the outcome changed over time and note how there were far more executive orders in the earlier period.\n\nplot(x=bolton$year, y=bolton$allnoncerm_eo, pch =20, \n     main = \"Executive Orders by Year\", \n     cex.main = .8, \n     ylim = c(0, 500))\nabline(v=1945, lty =2) # vertical line at 1945\n\n\n\n\n\n10.3.1 Fitting Poisson in R\nWe will investigate the relationship between divided government and executive orders in the first time period.\nThe authors’ hypothesize, “During periods of low legislative capacity (prior to the mid-1940s), the president issues more executive orders under divided government.\nTo fit a Poisson model in R, we use the glm function. However, now we have a different family= \"poisson\" and link = \"log\". We don’t actually have to explicitly write the link because R will use this link by default.\nLet’s fit a regression of our outcome on the key explanatory variables, along with other controls the authors use. Note that because I want the early period, I have to subset the data. I can do this outside, prior to the regression. Or, I can subset in the data argument, as is done in the below code:\n\nfit &lt;- glm(allnoncerm_eo ~ divided + inflation + \n             spending_percent_gdp + war + lame_duck +\n                 administration_change + trend +\n           + tr+ taft + wilson + harding \n           + coolidge + hoover, \n           family = \"poisson\", \n           data = subset(bolton, year &lt; 1945))\n\n\n\n10.3.2 Interpreting regression output\nThe summary output for Poisson is much nicer than the multinomial output we were working with previously. Let’s extract just the divided coefficient output from the summary.\n\nsummary(fit)$coefficients[2,]\n\n    Estimate   Std. Error      z value     Pr(&gt;|z|) \n4.435893e-01 4.195090e-02 1.057401e+01 3.933025e-26 \n\n\nHow should we interpret this coefficient?\n\nRecall \\(\\log \\hat \\lambda = \\mathbf x_i'\\hat \\beta\\)\n\nFor every one-unit change in \\(x\\), we estimate an average \\(\\hat \\beta\\) change in the \\(\\log\\) of the expected executive orders, holding the other covariates constant.\n\nNote that usually counts are measured given a particular time or space interval. For this reason sometimes these are considered “rates” (i.e., the number of executive orders per year)."
  },
  {
    "objectID": "10-CountData.html#poisson-quantities-of-interest",
    "href": "10-CountData.html#poisson-quantities-of-interest",
    "title": "10  Count data",
    "section": "10.4 Poisson Quantities of Interest",
    "text": "10.4 Poisson Quantities of Interest\n\n10.4.1 Expected Counts\nOur primary quantity of interest is the expected count, in this case the expected number of executive orders, given certain values of the covariates\n\\(\\mathbf E(Y | X) = \\lambda = exp(\\mathbf x_i' \\beta)\\)\n\nThis means to get our quantities of interest, we exponentiate \\(exp(\\mathbf x_i' \\hat \\beta)\\) after setting specific values for \\(X\\), such as at the means of the covariates or at the observed values.\n\nLike other glm models, we can also use predict to do this for us by setting type = response or prediction.\n\nFor example, let’s find the averaged executive orders expected for unified vs. divided government, holding other covariates at their observed values. We will do this manually and using avg_predictions or prediction.\n\n\nX &lt;- model.matrix(fit)\nX[, \"divided\"] &lt;- 0\nB &lt;- coef(fit)\neta &lt;- X %*% B\nexpcount &lt;- exp(eta)\navg.exp.count.0 &lt;- mean(expcount)\navg.exp.count.0 \n\n[1] 265.7753\n\nX &lt;- model.matrix(fit)\nX[, \"divided\"] &lt;- 1\nB &lt;- coef(fit)\neta &lt;- X %*% B\nexpcount &lt;- exp(eta)\navg.exp.count.1 &lt;- mean(expcount)\navg.exp.count.1\n\n[1] 414.1552\n\n\nFrom marginaleffects\n\nlibrary(marginaleffects)\navg.exp.counts.me &lt;- avg_predictions(fit,by=\"divided\",\n                                     newdata= datagridcf(divided=c(0,1)), type=\"response\")\navg.exp.counts.me\n\n\n divided Estimate Std. Error    z Pr(&gt;|z|)     S 2.5 % 97.5 %\n       0      266       2.86 92.9   &lt;0.001   Inf   260    271\n       1      414      15.76 26.3   &lt;0.001 503.2   383    445\n\nColumns: divided, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response \n\n\nFrom Thomas Leeper\n\nlibrary(prediction)\navg.exp.counts &lt;- prediction(fit, at=list(divided=c(0, 1)), type = \"response\")\nsummary(avg.exp.counts)\n\n at(divided) Prediction     SE     z          p lower upper\n           0      265.8  2.862 92.85  0.000e+00 260.2 271.4\n           1      414.2 15.759 26.28 3.224e-152 383.3 445.0\n\n\nWe can also find the differences in expected counts by subtracting the above estimates from each other, or computing this directly through avg_comparisons or margins:\n\navg.exp.count.1 - avg.exp.count.0\n\n[1] 148.3798\n\nlibrary(marginaleffects)\navg.diff.me &lt;- avg_comparisons(fit, variables=list(divided=c(0,1)), type=\"response\")\navg.diff.me\n\n\n    Term Contrast Estimate Std. Error   z Pr(&gt;|z|)    S 2.5 % 97.5 %\n divided    1 - 0      148       16.7 8.9   &lt;0.001 60.6   116    181\n\nColumns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response \n\nlibrary(margins)\navg.diff &lt;- margins(fit, variables=\"divided\", change = c(0, 1), vce = \"delta\")\nsummary(avg.diff)\n\n  factor      AME      SE      z      p    lower    upper\n divided 148.3798 16.6768 8.8974 0.0000 115.6938 181.0658\n\n\nJust like in logit and probit, we have the same options for calculating uncertainty: Delta Method, Simulation, and Bootstrap.\n\n\n10.4.2 Sidenote: Multiplicative coefficient interpretation\nFor Poisson, changes in \\(x\\) will have a multiplicative change in \\(y\\):\nRecall a math rule for exponents to follow the below: \\(z^{a+b} = z^a * z^b\\)\n\\[\\begin{align*}\n\\mathbf E(Y | X) &= e^{\\alpha + x_1 \\beta_1 + x_2 \\beta_2}\\\\\n&= e^{\\alpha} * e^{x_1 \\beta_1} *e^{x_2 \\beta_2}\n\\end{align*}\\]\nFor example, compare \\(x_1\\) to \\(x_1 + 1\\)\n\\[\\begin{align*}\n\\mathbf E(Y | X) &= e^{\\alpha + (x_1 + 1) \\beta_1 + x_2 \\beta_2}\\\\\n&= e^{\\alpha} * e^{x_1 \\beta_1}* e^{\\beta_1} *e^{x_2 \\beta_2}\n\\end{align*}\\]\nWe’ve now multiplied the outcome by \\(e^{\\beta_1}\\).\nHere’s an example using a bivariate model\n\nfit.biv &lt;- glm(allnoncerm_eo ~ divided ,\n           family = \"poisson\", \n           data = subset(bolton, year &lt; 1945))\n\n## Let's calculate yhat using predict for divided = 0 or 1\nyhats &lt;- predict(fit.biv, data.frame(divided = c(0, 1)), type=\"response\")\nyhats\n\n       1        2 \n280.6471 282.8333 \n\n## Manual\nyhatx0 &lt;- exp(coef(fit.biv)[1] + coef(fit.biv)[2]*0)\nnames(yhatx0) &lt;- \"when divided=0\"\nyhatx1 &lt;- exp(coef(fit.biv)[1] + coef(fit.biv)[2]*1)\nnames(yhatx1) &lt;- \"when divided=1\"\nc(yhatx0, yhatx1)\n\nwhen divided=0 when divided=1 \n      280.6471       282.8333 \n\n## Multiplicative interpretation \nyhatx0*(exp(coef(fit.biv)[\"divided\"]))\n\nwhen divided=0 \n      282.8333 \n\n\n\n\n10.4.3 Incidence Rate Ratios\nRelatedly, similar to logistic regression where we could exponentiate the coefficients to generate estimated odds ratios, in poisson, we can exponentiate the coefficients to get “incidence rate ratios.”\nWhen we say a one-unit change in the independent variable \\(x\\), this is like saying\n\n\\(\\hat\\beta = \\log \\hat \\lambda_{x + 1} - \\log \\hat \\lambda_{x} = \\log \\frac{\\hat \\lambda_{x + 1}}{\\hat \\lambda_{x}}\\)\n\nIf we exponentiate, the \\(\\log\\) cancels to 1:\n\n\\(exp(\\hat \\beta) = exp(\\log \\frac{\\hat \\lambda_{x + 1}}{\\hat \\lambda_{x}})\\)\n\n\\(exp(\\hat \\beta) = \\frac{\\hat \\lambda_{x + 1}}{\\hat \\lambda_{x}}\\)\nThis quantity represents a ratio of the expected counts or “rates”\nFor a one-unit change in \\(x\\), the expected count is estimated to change by a factor of \\(exp(\\hat \\beta)\\)\n\nThis can be converted to a percent change interpretion by taking \\((IRR - 1) \\times 100\\)\n\n\n\nFor example, the incidence rate ratio for executive orders in a year going from unified to divided government is:\n\nexp(coef(fit)[\"divided\"])\n\ndivided \n1.55829 \n\n\nWe can see how this works out using the quantities calculated above. Multiplying the expected count when divided = 0 by this ratio gives us the expected count when divided = 1.\n\nexp(coef(fit)[\"divided\"])*avg.exp.count.0\n\n divided \n414.1552 \n\navg.exp.count.1\n\n[1] 414.1552\n\n\nNote how this is a percent change interpretation where\n\n((avg.exp.count.1 - avg.exp.count.0)/ avg.exp.count.0)* 100\n\n[1] 55.82903\n\n(exp(coef(fit)[\"divided\"])-1)*100\n\n divided \n55.82903 \n\n\nFor a one-unit change going from unified to divided government, we see a 55.8% increase in executive orders during this period.\n\n\n10.4.4 Where Poisson is poisonous:\nRecall the detail when we specified the pmf, that the mean and variance at the same. When we have covariates \\(X\\), this means we assume the conditional mean and variance are the same:\n\n\\(\\lambda = \\mathbf E[Y_i |x_i] = Var(Y_i | x_i)\\); the mean and variance is \\(\\lambda\\) in the distribution.\nOften, our data are “overdispersed” or “underdispersed”, violating this assumption\n\nWe can investigate this in our example. First, let’s look at the raw mean and variance of the outcome.\n\n## whoa! very different, \n## but the good news is we care about the *conditional* mean and variance\n## not these raw values\nmean(subset(bolton$allnoncerm_eo, bolton$year &lt; 1945))\n\n[1] 280.975\n\nvar(subset(bolton$allnoncerm_eo, bolton$year &lt; 1945))\n\n[1] 9011.256\n\n\nWe can conduct a test of overdispersion in our model using dispersiontest. If we have a significant result and a dispersion constant \\(&gt;\\) 0, this would suggest overdispersion.\n\nlibrary(AER)\n\nLoading required package: car\n\n\nLoading required package: carData\n\n\nLoading required package: lmtest\n\n\nLoading required package: zoo\n\n\n\nAttaching package: 'zoo'\n\n\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric\n\n\nLoading required package: sandwich\n\n\nLoading required package: survival\n\ndispersiontest(fit)\n\n\n    Overdispersion test\n\ndata:  fit\nz = 3.6675, p-value = 0.0001225\nalternative hypothesis: true dispersion is greater than 1\nsample estimates:\ndispersion \n  11.02527 \n\n\nTo check for overdisperson, we can also look at the standardized residuals of the model.\n\n## By hand\nprvals &lt;- predict(fit, type = \"response\") # predicted values\nres &lt;- subset(bolton$allnoncerm_eo, bolton$year &lt; 1945) - prvals # residual y - predicted values\nsres &lt;- res/sqrt(prvals) # standardized residual\n\n## automatic in R\nsres &lt;-  residuals(fit,type=\"pearson\") # automatic\n\nWe can graphically look at the standardized residuals by levels of the predicted values from our regression. Here, we don’t want residuals that exceed +/- 2.\n\n## don't want values above 2 or below -2\nplot(prvals, sres,\n     ylab = \"standardized residuals\", xlab = \"predicted values\")\nabline(h = c(-2, 0, 2), lty = c(2, 1, 2))\n\n\n\n\nWe are in danger!! This model suffers from overdispersion. We have two options\n\nCheck our model. Do we think it is misspecified in terms of the covariates? Is it suffering from omitted variable bias? We can change the specification and re-run the test. Again, the overdispersion is about the conditional mean and variance, so changing the model may change the diagnostics.\nUse a different distribution. Because this assumption is often violated, we tend to use one of the following for count data\n\nOverdispersed poisson (quasipoisson)\nNegative binomial regression\n\n\nMore on this in the next section"
  },
  {
    "objectID": "10-CountData.html#quasipoisson-and-negative-binomial-models",
    "href": "10-CountData.html#quasipoisson-and-negative-binomial-models",
    "title": "10  Count data",
    "section": "10.5 Quasipoisson and Negative Binomial Models",
    "text": "10.5 Quasipoisson and Negative Binomial Models\nThe quaipoisson model relaxes the assumption that the mean and variance have to be equivalent. It is the same as the Poisson but multiplies the standard errors by \\(\\sqrt{d}\\), where \\(d\\) is the dispersion parameter.\nThese models are fit in R almost exactly the same way as poisson. We just switch family = \"quasipoisson\"\n\nNote in the summary output, it lists the dispersion parameter.\n\n\nfitq &lt;- glm(allnoncerm_eo ~ divided + inflation + spending_percent_gdp \n            + war + lame_duck +\n                 administration_change + trend +\n            + tr+ taft + wilson + harding \n           + coolidge + hoover, \n           family = \"quasipoisson\", \n           data = subset(bolton, year &lt; 1945))\nsummary(fitq)\n\n\nCall:\nglm(formula = allnoncerm_eo ~ divided + inflation + spending_percent_gdp + \n    war + lame_duck + administration_change + trend + +tr + taft + \n    wilson + harding + coolidge + hoover, family = \"quasipoisson\", \n    data = subset(bolton, year &lt; 1945))\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-8.2027  -2.4224  -0.1515   2.1532   8.9032  \n\nCoefficients:\n                        Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)            8.0360573  0.9848054   8.160 1.22e-08 ***\ndivided                0.4435893  0.1727719   2.567  0.01634 *  \ninflation              0.0005599  0.0110836   0.051  0.96010    \nspending_percent_gdp  -0.0286140  0.0082874  -3.453  0.00191 ** \nwar                    0.4745189  0.1936737   2.450  0.02133 *  \nlame_duck              0.3692336  0.2769252   1.333  0.19399    \nadministration_change -0.0321975  0.1844677  -0.175  0.86279    \ntrend                 -0.0619311  0.0316454  -1.957  0.06116 .  \ntr                    -2.5190839  0.9562064  -2.634  0.01401 *  \ntaft                  -2.4104113  0.8805222  -2.737  0.01102 *  \nwilson                -1.4750129  0.6866621  -2.148  0.04120 *  \nharding               -1.4205088  0.4700492  -3.022  0.00558 ** \ncoolidge              -1.1070363  0.3792082  -2.919  0.00715 ** \nhoover                -0.9985352  0.3099077  -3.222  0.00341 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for quasipoisson family taken to be 16.96148)\n\n    Null deviance: 1281.92  on 39  degrees of freedom\nResidual deviance:  439.87  on 26  degrees of freedom\nAIC: NA\n\nNumber of Fisher Scoring iterations: 4\n\n\nThe dispersion parameter is estimated using those standardized residuals from the Poisson model.\n\nsres &lt;- residuals(fit, type=\"pearson\")\nchisq &lt;- sum(sres^2)\nd &lt;- chisq/fit$df.residual\nd\n\n[1] 16.96146\n\n\nLet’s compare the Poisson and Quasipoisson model coefficients and standard errors.\n\nround(summary(fit)$coefficients[2,], digits=4)\n\n  Estimate Std. Error    z value   Pr(&gt;|z|) \n    0.4436     0.0420    10.5740     0.0000 \n\nround(summary(fitq)$coefficients[2,], digits=4)\n\n  Estimate Std. Error    t value   Pr(&gt;|t|) \n    0.4436     0.1728     2.5675     0.0163 \n\n\nWe can retrieve the standard error from the quasipoisson by multiplication.\n\n## Multiply the Poisson standard error by sqrt(d)\nround(summary(fit)$coefficients[2,2] * sqrt(d), digits=4)\n\n[1] 0.1728\n\n\nNote that the standard error among the Quasipoisson model is much bigger, accounting for the larger variance from overdispersion.\n\n10.5.1 Negative Binomial Models\nAnother model commonly used for count data is the negative binomial model. This is the model Bolton and Thrower use. This has a more complicated likelihood function, but, like the quasipoisson, it has a larger (generally, more correct) variance term. Analogous to the normal distribution, the negative binomial has both a mean and variance term parameter.\n\\(\\Pr(Y=y) = \\frac{\\Gamma(r+y)}{\\Gamma(r) \\Gamma(y+1)} (\\frac{r}{r+\\lambda})^r (\\frac{\\lambda}{r+\\lambda})^y\\)\n\nModels dispersion through term \\(r\\)\n\\(\\mathbf E(Y_i|X) = \\lambda_i = e^{\\mathbf x_i^\\top \\beta}\\)\n\\(\\mathbf Var(Y_i|X) = \\lambda_i + \\lambda_i^2/r\\) (note, this is no longer just \\(\\lambda_i\\))\n\nNote that while the probability distribution looks much uglier, the mapping of \\(\\mathbf x_i' \\beta\\) is the same. We will still have a \\(\\log\\) link and exponeniate to get our quantities of interest. The mechanics work essentially the same as the poisson.\nIn R, we use the glm.nb function from the MASS package to fit negative binomial models. Here, we do not need to specify a family, but we do specify the link = \"log\".\n\nlibrary(MASS)\nfitn &lt;- glm.nb(allnoncerm_eo ~ divided + inflation + \n                 spending_percent_gdp + war + lame_duck +\n                 administration_change + trend +\n            + tr+ taft + wilson + harding \n           + coolidge + hoover, \n           link=\"log\", \n           data = subset(bolton, year &lt; 1945))\nsummary(fitn)$coefficients[2,]\n\n   Estimate  Std. Error     z value    Pr(&gt;|z|) \n0.442135921 0.141831942 3.117322607 0.001825017 \n\n\nCompare this to column 1 in the model below.\n\nWe have now replicated the coefficients in column 1 of Table 1 in the authors’ paper. Our standard errors do not match exactly because the authors use clustered standard errors by president. Moreover, given a relatively small sample the two programs R (and Stata, which the authors use) might generate slightly different estimates."
  },
  {
    "objectID": "10-CountData.html#count-data-practice-problems",
    "href": "10-CountData.html#count-data-practice-problems",
    "title": "10  Count data",
    "section": "10.6 Count data practice problems",
    "text": "10.6 Count data practice problems\nLet’s reproduce column 2 from Table 1 in the article and related estimates.\n\nFit the following negative binomial model for year \\(&gt;\\) 1944.\n\nNote: the numbers won’t exactly match the authors but should be close\n\n\n\n\n\\[\n\\begin{aligned}\n\\log ({ E( \\operatorname{allnoncerm_eo} ) })  &= \\alpha + \\beta_{1}(\\operatorname{divided})\\ + \\\\\n&\\quad \\beta_{2}(\\operatorname{inflation}) + \\beta_{3}(\\operatorname{spending\\_percent\\_gdp})\\ + \\\\\n&\\quad \\beta_{4}(\\operatorname{war}) + \\beta_{5}(\\operatorname{lame\\_duck})\\ + \\\\\n&\\quad \\beta_{6}(\\operatorname{administration\\_change}) + \\beta_{7}(\\operatorname{trend})\\ + \\\\\n&\\quad \\beta_{8}(\\operatorname{truman}) + \\beta_{9}(\\operatorname{ike})\\ + \\\\\n&\\quad \\beta_{10}(\\operatorname{jfk}) + \\beta_{11}(\\operatorname{lbj})\\ + \\\\\n&\\quad \\beta_{12}(\\operatorname{nixon}) + \\beta_{13}(\\operatorname{ford})\\ + \\\\\n&\\quad \\beta_{14}(\\operatorname{carter}) + \\beta_{15}(\\operatorname{reagan})\\ + \\\\\n&\\quad \\beta_{16}(\\operatorname{bush41}) + \\beta_{17}(\\operatorname{clinton})\\ + \\\\\n&\\quad \\beta_{18}(\\operatorname{bush43})\n\\end{aligned}\n\\]\n\n\n\nConduct a linear model using OLS and a Quasipoisson for comparison\n\nFor each, calculate the average number of executive orders expected for divided government. How do these compare across models?\n\n\n\n\nTry on your own, then expand for the solution.\n\n\nfit.lm &lt;- lm(allnoncerm_eo ~ divided + inflation + spending_percent_gdp + \n               war + lame_duck + administration_change + trend + \n               truman + ike + jfk + lbj + \n               nixon + ford +\n               carter + reagan + bush41 + clinton + bush43,\n             data = subset(bolton, year &gt; 1944))\n\nfit.nb &lt;- glm.nb(allnoncerm_eo ~ divided + inflation + spending_percent_gdp + \n               war + lame_duck + administration_change + trend + \n               truman + ike + jfk + lbj + \n               nixon + ford +\n               carter + reagan + bush41 + clinton + bush43,\n               link=\"log\",\n             data = subset(bolton, year &gt; 1944))\n\nfit.qp &lt;- glm(allnoncerm_eo ~ divided + inflation + spending_percent_gdp + \n               war + lame_duck + administration_change + trend + \n               truman + ike + jfk + lbj + \n               nixon + ford +\n               carter + reagan + bush41 + clinton + bush43,\n              family=\"quasipoisson\",\n             data = subset(bolton, year &gt; 1944))\n\n## Manual\nX &lt;- model.matrix(fit.lm)\nX[, \"divided\"] &lt;- 1\nB &lt;- coef(fit.lm)\nexp.eo.lm &lt;- mean(X %*% B)\n\nX &lt;- model.matrix(fit.nb)\nX[, \"divided\"] &lt;- 1\nB &lt;- coef(fit.nb)\nexp.eo.nb &lt;- mean(exp(X %*% B))\n\nX &lt;- model.matrix(fit.qp)\nX[, \"divided\"] &lt;- 1\nB &lt;- coef(fit.qp)\nexp.eo.qp &lt;- mean(exp(X %*% B))\n\nexp.eo.lm\n\n[1] 56.86582\n\nexp.eo.nb\n\n[1] 55.52576\n\nexp.eo.qp\n\n[1] 55.41505\n\n## marginal effects predictions\nexp.eo.lm.me &lt;- avg_predictions(fit.lm,by=\"divided\",\n                                     newdata= datagridcf(divided=c(0,1)), type=\"response\")\nexp.eo.nb.me &lt;- avg_predictions(fit.nb, by=\"divided\",\n                                     newdata= datagridcf(divided=c(0,1)), type=\"response\") \nexp.eo.qp.me &lt;- avg_predictions(fit.qp, by=\"divided\",\n                                     newdata= datagridcf(divided=c(0,1)), type=\"response\")\nexp.eo.lm.me\n\n\n divided Estimate Std. Error    z Pr(&gt;|z|)     S 2.5 % 97.5 %\n       0     61.2       2.56 23.9   &lt;0.001 417.4  56.2   66.2\n       1     56.9       1.94 29.3   &lt;0.001 625.6  53.1   60.7\n\nColumns: divided, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response \n\nexp.eo.nb.me\n\n\n divided Estimate Std. Error    z Pr(&gt;|z|)     S 2.5 % 97.5 %\n       0     62.2       2.17 28.6   &lt;0.001 594.7  57.9   66.4\n       1     55.5       1.80 30.9   &lt;0.001 692.3  52.0   59.1\n\nColumns: divided, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response \n\nexp.eo.qp.me\n\n\n divided Estimate Std. Error    z Pr(&gt;|z|)     S 2.5 % 97.5 %\n       0     62.3       2.57 24.3   &lt;0.001 429.7  57.3   67.3\n       1     55.4       2.12 26.2   &lt;0.001 499.5  51.3   59.6\n\nColumns: divided, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response \n\n## Prediction\nexp.eo.lm &lt;- prediction(fit.lm, at = list(divided=1))\nexp.eo.nb &lt;- prediction(fit.nb, at = list(divided=1), type=\"response\")\nexp.eo.qp &lt;- prediction(fit.qp, at = list(divided=1), type = \"response\")\nsummary(exp.eo.lm)\n\n at(divided) Prediction    SE     z         p lower upper\n           1      56.87 1.939 29.33 4.77e-189 53.07 60.67\n\nsummary(exp.eo.nb)\n\n at(divided) Prediction    SE     z          p lower upper\n           1      55.53 1.799 30.86 4.027e-209    52 59.05\n\nsummary(exp.eo.qp)\n\n at(divided) Prediction    SE     z          p lower upper\n           1      55.42 2.117 26.18 4.395e-151 51.27 59.56\n\n\n\n\nWe are going to build further on this example to try to reproduce Figure 2 in the authors’ paper. This is a plot that shows the percentage change in executive orders from divided to unified government with 95% confidence intervals.\n\nThese estimates are generated using the negative binomial regression models presented in Table 1. The good news is if we have followed the course notes and practice problems to this point, we have already fit both of these models.\nTo find the percent change, we calculate the incidence rate ratios, which represent: For a one-unit change in \\(x\\), the expected count changes by a factor of \\(exp(\\hat \\beta_j)\\).\n\nThis can be converted to percent change by \\((IRR - 1) \\times 100\\). For example, if the incidence rate ratio was 1.4, then for a one-unit change in \\(x\\), we would see a \\(40\\%\\) change (increase) in the expected count.\nTo find the confidence intervals, we can use confint(fit), exponentiate these, and then follow the same formula for the lower bound and upper bound. Our confidence intervals will be bigger than the authors because we used a different type of standard error.\n\n\n\n\n\n\nTry on your own, then expand for the solution.\n\n\nfitn &lt;- glm.nb(allnoncerm_eo ~ divided + inflation + \n                 spending_percent_gdp + war + lame_duck +\n                 administration_change + trend +\n            + tr+ taft + wilson + harding \n           + coolidge + hoover, \n           link=\"log\", \n           data = subset(bolton, year &lt; 1945))\nfit.nb &lt;- glm.nb(allnoncerm_eo ~ divided + inflation + spending_percent_gdp + \n               war + lame_duck + administration_change + trend + \n               truman + ike + jfk + lbj + \n               nixon + ford +\n               carter + reagan + bush41 + clinton + bush43,\n               link=\"log\",\n             data = subset(bolton, year &gt; 1944))\n\n## Incidence Rate Ratios amd\nirr1 &lt;- exp(coef(fitn)[\"divided\"])\nirr2 &lt;- exp(coef(fit.nb)[\"divided\"])\nci.irr1 &lt;- exp(confint(fitn)[\"divided\",])\n\nWaiting for profiling to be done...\n\nci.irr2 &lt;- exp(confint(fit.nb)[\"divided\",])\n\nWaiting for profiling to be done...\n\n## Percent change\npc1 &lt;- (irr1-1)* 100\nci.pc1 &lt;- (ci.irr1 - 1)*100\npc2 &lt;- (irr2-1)* 100\nci.pc2 &lt;- (ci.irr2 - 1)*100\n\n\n## Prepare data for plotting\ndf &lt;-data.frame(pc = c(pc1, pc2), \n                lower=c(ci.pc1[1], ci.pc2[1]), \n                upper = c(ci.pc1[2], ci.pc2[2]))\ndf$period &lt;- c(\"Regime 1 \\n 1905-1944\", \"Regime 2 \\n 1945-2013\")\n\nggplot(df, aes(y=pc,x=period))+\n  geom_bar(stat=\"identity\")+\n  geom_errorbar(aes(ymin=lower, ymax=upper), width=.05)+\n  ggtitle(\"Effect of Divided Government on EO Usage\")+\n  ylab(\"Percentage Change Divided Relative to Unified\")+\n  xlab(\"\")+\n  theme_minimal()"
  },
  {
    "objectID": "10-CountData.html#count-model-tutorial",
    "href": "10-CountData.html#count-model-tutorial",
    "title": "10  Count data",
    "section": "10.7 Count Model Tutorial",
    "text": "10.7 Count Model Tutorial\nFor this week’s tutorial, we will use data from the article, “Less than you think: Prevalence and predictors of fake news dissemination on Facebook” published in Science Advances by Andrew Guess, Jonathan Nagler, and Joshua Tucker available here.\nAbstract.So-called “fake news” has renewed concerns about the prevalence and effects of misinformation in political campaigns. Given the potential for widespread dissemination of this material, we examine the individual-level characteristics associated with sharing false articles during the 2016 U.S. presidential campaign. To do so, we uniquely link an original survey with respondents’ sharing activity as recorded in Facebook profile data. First and foremost, we find that sharing this content was a relatively rare activity. Conservatives were more likely to share articles from fake news domains, which in 2016 were largely pro-Trump in orientation, than liberals or moderates. We also find a strong age effect, which persists after controlling for partisanship and ideology: On average, users over 65 shared nearly seven times as many articles from fake news domains as the youngest age group.\nThe authors look at the demographics predictors of disseminating fake news using survey data and behavioral data on respondents’ Facebook sharing history.\nThe key outcome variable is num_fake_shares, representing the number of articles an individual shared from a list of fake news domains created by Buzzfeed.\n\nKey independent variables include party,ideology (factor), age (factor), female, black, educ, faminc, num_posts\n\nLet’s load the data and look at the outcome.\n\nlibrary(rio)\nfake &lt;- import(\"https://github.com/ktmccabe/teachingdata/blob/main/sciencerep.RData?raw=true\")\n\ntable(fake$num_fake_shares)\n\n\n   0    1    2    3    4    5    6    7    8    9   10   12   13   25   50 \n1090   63   12    8    5    1    1    2    1    2    2    1    1    1    1 \n\n\nIt looks like our data are a count. Let’s try to visualize this distribution in a histogram. How would you describe this distribution?\n\nlibrary(ggplot2)\nggplot(fake, aes(num_fake_shares))+\n  geom_histogram(bins=50)+\n  theme_minimal()+ \n  ggtitle(\"Distribution of Fake News Shares\") + \n  ylab(\"Number of respondents\") + \n  xlab(\"Number of fake news stories shared\")\n\n\n\n\n\n\nTo filter the histogram by party like Fig. 1 in the paper, expand.\n\n\nlibrary(ggplot2)\nfakesub &lt;- subset(fake, party %in% c(\"Democrat\", \"Republican\", \"Independent\"))\nggplot(fakesub, aes(num_fake_shares, fill=party))+\n  geom_histogram(bins=50)+\n  theme_minimal()+ \n  ggtitle(\"Distribution of Fake News Shares\") + \n  ylab(\"Number of respondents\") + \n  scale_fill_manual(\"\", values = c(\"blue\", \"red\", \"darkgray\")) +\n  xlab(\"Number of fake news stories shared\") + \n  theme(legend.position = \"bottom\", legend.title = element_blank()) \n\nWarning: Removed 1499 rows containing non-finite values (`stat_bin()`).\n\n\n\n\n\n\nLet’s say we were interested in exploring the relationship between age and the number of fake news posts. It can always be useful to conduct more descriptive or exploratory analysis before hitting the heavy machinery. For example, the authors look at the average number of fake news posts shared by age group in Figure 2. We can do the same. Ours will look a little different because it’s unweighted:\n\n## Base r version\nmeansbyage &lt;- tapply(fake$num_fake_shares, fake$age, mean, na.rm=T)\nbarplot(meansbyage, \n        main = \"Age and Fake News Shares\",\n        ylab= \"Mean number of fake news stories shared\",\n        xlab=\"Age group\",\n        ylim=c(0,.6),\n        col=\"black\")\n\n\n\n## ggplot version\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──\n\n\n✔ tibble  3.2.1     ✔ dplyr   1.1.2\n✔ tidyr   1.3.0     ✔ stringr 1.5.0\n✔ readr   2.1.2     ✔ forcats 1.0.0\n✔ purrr   1.0.2     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n✖ dplyr::recode() masks car::recode()\n✖ dplyr::select() masks MASS::select()\n✖ purrr::some()   masks car::some()\n\nfake %&gt;% \n  filter(!is.na(age)) %&gt;% \n  group_by(age) %&gt;% \n  summarise(mean_fakes = mean(num_fake_shares, na.rm = TRUE)) %&gt;%\n  ggplot(aes(x=age, y=mean_fakes)) + \n  geom_bar(stat = \"identity\") +\n  ggtitle(\"Age and Fake News Shares\") + \n  ylab(\"Mean number of fake news stories shared\") + \n  xlab(\"Age group\") + \n  ylim(0, .6)+\n  theme_minimal()\n\n\n\n\n\nLet’s move to a regression approach to study this relationship. How could we model this relationship? What are the pros and cons of each approach?\n\nWould OLS work?\nWould a binary logit or probit work?\nWhat other approaches could work?\n\nLet’s replicate the first column of Table 2 in the authors’ analysis.\n\nWhat type of model do the authors use? Why might this be an appropriate model in their case?\nHow should the coefficients be interpreted?\nWhat would be a quantity of interest?\n\n\n\nRecall that for our primary count models, the regression equation is of the form:\n\n\\(\\log \\lambda_i = \\eta_i = \\mathbf x_i^T\\beta\\)\n\\(\\lambda_i = exp(\\mathbf x_i^T\\beta)\\)\n\\(\\lambda_i = \\mathbf E[Y_i | \\mathbf x_i] = \\exp(\\beta_0 + \\beta_1x_{i1} + ... + \\beta_{k}x_{ik})\\) is the expected number of events per a unit of time or space\n\n\nLet’s fit the model using glm. Be careful to specify the right family.\n\nWhat is the dispersion parameter? What conceptual quantity does this represent?\n\n\n\n\nExpand for details.\n\nWe will fit a quasipoisson following the authors. Note: they apply survey weights through the weights function. You could also use the survey package for this. For now, we will follow the approach of the authors.\n\nmodel1.qp &lt;- glm(num_fake_shares ~ ideology + age + female + black + educ + faminc, \n                  weights = weight_svypop_w3, \n                 data = fake, \n                 family = \"quasipoisson\")\n\nsummary(model1.qp)\n\n\nCall:\nglm(formula = num_fake_shares ~ ideology + age + female + black + \n    educ + faminc, family = \"quasipoisson\", data = fake, weights = weight_svypop_w3)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-3.4510  -0.5414  -0.3406  -0.1929  12.8243  \n\nCoefficients:\n                      Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)          -3.416028   1.379091  -2.477  0.01341 * \nideologyVery liberal  0.486833   1.238165   0.393  0.69426   \nideologyLiberal      -1.127456   1.438798  -0.784  0.43345   \nideologyModerate      0.332692   1.186454   0.280  0.77922   \nideologyConservative  2.186850   1.154932   1.893  0.05857 . \nideologyV. conserv.   2.366213   1.158278   2.043  0.04132 * \nage30-44              0.771712   0.811327   0.951  0.34174   \nage45-65              1.136384   0.764972   1.486  0.13771   \nageOver 65            2.052391   0.766352   2.678  0.00752 **\nfemale               -0.113780   0.216606  -0.525  0.59950   \nblack                -0.879538   0.753713  -1.167  0.24351   \neduc                 -0.084881   0.081317  -1.044  0.29681   \nfaminc               -0.007145   0.008419  -0.849  0.39627   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for quasipoisson family taken to be 3.304861)\n\n    Null deviance: 1676.7  on 1040  degrees of freedom\nResidual deviance: 1172.0  on 1028  degrees of freedom\n  (2459 observations deleted due to missingness)\nAIC: NA\n\nNumber of Fisher Scoring iterations: 7\n\n\nWhat was the form of the regression model?\n\nlibrary(equatiomatic)\nequatiomatic::extract_eq(model1.qp, wrap=T, terms_per_line = 2)\n\n\\[\n\\begin{aligned}\n\\log ({ E( \\operatorname{num_fake_shares} ) })  &= \\alpha + \\beta_{1}(\\operatorname{ideo\\logy}_{\\operatorname{Very\\ liberal}})\\ + \\\\\n&\\quad \\beta_{2}(\\operatorname{ideo\\logy}_{\\operatorname{Liberal}}) + \\beta_{3}(\\operatorname{ideo\\logy}_{\\operatorname{Moderate}})\\ + \\\\\n&\\quad \\beta_{4}(\\operatorname{ideo\\logy}_{\\operatorname{Conservative}}) + \\beta_{5}(\\operatorname{ideo\\logy}_{\\operatorname{V.\\ conserv.}})\\ + \\\\\n&\\quad \\beta_{6}(\\operatorname{age}_{\\operatorname{30-44}}) + \\beta_{7}(\\operatorname{age}_{\\operatorname{45-65}})\\ + \\\\\n&\\quad \\beta_{8}(\\operatorname{age}_{\\operatorname{Over\\ 65}}) + \\beta_{9}(\\operatorname{female})\\ + \\\\\n&\\quad \\beta_{10}(\\operatorname{black}) + \\beta_{11}(\\operatorname{educ})\\ + \\\\\n&\\quad \\beta_{12}(\\operatorname{faminc})\n\\end{aligned}\n\\]\n\n\nWe could have fit a regular poisson model. In that model, the dispersion paramter is taken to be 1, such that the conditional mean and variance are equivalent. In contrast, the quasipoisson accounts for the possibility that the variance is greater.\nLet’s see how much of an issue this is.\n\nmodel1.p &lt;- glm(num_fake_shares ~ ideology + age + female + black + educ + faminc, \n                  weights = weight_svypop_w3, \n                 data = fake, \n                 family = \"poisson\")\n\nWe can conduct a test of overdispersion in our model using dispersiontest. If we have a significant result and a dispersion constant \\(&gt;\\) 0, this would suggest overdispersion. What should we conclude? What are the implications?\n\nlibrary(AER)\ndispersiontest(model1.p)\n\n\n    Overdispersion test\n\ndata:  model1.p\nz = 2.9334, p-value = 0.001676\nalternative hypothesis: true dispersion is greater than 1\nsample estimates:\ndispersion \n  3.027398 \n\n\nNote: The authors address different modeling choices in the paper. “We aggregated all shares to the individual respondent level so that our dependent variables are counts (i.e., number of fake news stories shared). To account for this feature of the data, as well as the highly skewed distribution of the counts, we primarily used Poisson or quasi-Poisson regressions to model the determinants of Facebook sharing behavior. We conducted dispersion tests on the count data and used quasi-Poisson models if the null hypothesis of no dispersion is rejected. Below, we included negative binomial and Ordinary Least Squares (OLS) regressions to show that our results are generally not sensitive to model choice. All models applied weights from YouGov to adjust for selection into the sample. We specifically used sample-matching weights produced for the third wave of the survey, which was closest to the Facebook encouragement sent to respondents (27). (Results also do not appear to be sensitive to the use of weights.)”\n\n\nLet’s generate some quantities of interest.\n\nWhat is the incidence rate ratio for sharing fake news of being Age 65+ vs. the reference category 18-29?\nWhat is the average number of fake news posts expected to be shared by age group?\n\n\n\n\nExpand for details.\n\nIncidence rate ratios\n\n## Incidence rate ratios are exp(Bj)\nexp(coef(model1.qp)[\"ageOver 65\"])\n\nageOver 65 \n  7.786494 \n\n\nHow should we interpret this?\nAverage Fake News Posts Shared by Age Group\n\nlibrary(marginaleffects)\n## Note: levels(fake$age) is a shortcut for putting in the vector of all possible age levels. Only works for factor variables.\ncount.byage.me &lt;- avg_predictions(model1.qp, by=\"age\", newdata=datagridcf(age = levels(fake$age)), type=\"response\")\ncount.byage.me\n\n\n     age Estimate Std. Error    z Pr(&gt;|z|)    S   2.5 % 97.5 %\n 18-29     0.0657     0.0489 1.34  0.17910  2.5 -0.0302  0.162\n 30-44     0.1422     0.0464 3.06  0.00219  8.8  0.0512  0.233\n 45-65     0.2048     0.0347 5.91  &lt; 0.001 28.1  0.1369  0.273\n Over 65   0.5120     0.0865 5.92  &lt; 0.001 28.2  0.3424  0.682\n\nColumns: age, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response \n\n\n\nlibrary(prediction)\n## Note: levels(fake$age) is a shortcut for putting in the vector of all possible age levels. Only works for factor variables.\ncount.byage &lt;- prediction(model1.qp, at=list(age = levels(fake$age)), type=\"response\")\nsummary(count.byage)\n\n at(age) Prediction      SE     z         p    lower  upper\n   18-29    0.06575 0.04894 1.344 1.791e-01 -0.03017 0.1617\n   30-44    0.14225 0.04644 3.063 2.193e-03  0.05122 0.2333\n   45-65    0.20484 0.03468 5.907 3.493e-09  0.13687 0.2728\n Over 65    0.51196 0.08653 5.917 3.285e-09  0.34237 0.6816\n\n\n\n## 65+\nX &lt;- model.matrix(model1.qp)\nX[, \"age30-44\"] &lt;- 0\nX[, \"age45-65\"] &lt;- 0\nX[, \"ageOver 65\"] &lt;- 1\nB &lt;- coef(model1.qp)\neta &lt;- X %*% B\navg.fake.65 &lt;- mean(exp(eta))\n\n## 45-65\nX &lt;- model.matrix(model1.qp)\nX[, \"age30-44\"] &lt;- 0\nX[, \"age45-65\"] &lt;- 1\nX[, \"ageOver 65\"] &lt;- 0\nB &lt;- coef(model1.qp)\neta &lt;- X %*% B\navg.fake.45 &lt;- mean(exp(eta))\n\n## 30-44\nX &lt;- model.matrix(model1.qp)\nX[, \"age30-44\"] &lt;- 1\nX[, \"age45-65\"] &lt;- 0\nX[, \"ageOver 65\"] &lt;- 0\nB &lt;- coef(model1.qp)\neta &lt;- X %*% B\navg.fake.30 &lt;- mean(exp(eta))\n\n## 18-29\nX &lt;- model.matrix(model1.qp)\nX[, \"age30-44\"] &lt;- 0\nX[, \"age45-65\"] &lt;- 0\nX[, \"ageOver 65\"] &lt;- 0\nB &lt;- coef(model1.qp)\neta &lt;- X %*% B\navg.fake.18 &lt;- mean(exp(eta))\n\n## Gather results\nc(avg.fake.18, avg.fake.30, avg.fake.45, avg.fake.65)\n\n[1] 0.06574956 0.14224696 0.20484149 0.51195856"
  },
  {
    "objectID": "10-CountData.html#additional-considerations",
    "href": "10-CountData.html#additional-considerations",
    "title": "10  Count data",
    "section": "10.8 Additional Considerations",
    "text": "10.8 Additional Considerations\n\n10.8.1 Offset\nWe said that counts are often considered rates per interval. Sometimes when we have a count, we know the greatest possible value that an observation could take or we think that will be influenced by a particular variable, which varies by observation. This is called the exposure. For example maybe we are looking at the number of traffic accidents per year per population size.\nIn contrast, in the example from before, counts of executive orders were just measured as a span of one year.\nThis blog post has a set of questions to ask if you should include an offset.”(1) What is the relevant interval in time or space upon which our counts are based? (2) Is this interval different across our observations of counts?”\nWe are going to use an example with an exposure from Gelman and Hill and the study of stop-and-frisk as a policy. This is example is based on the tutorial by Clay Ford here.\nThe data include an outcome desribing the number of stops in a given area. The regression model looks at the relationship between race/ethnicity factor(eth) and the number of stops made in a precinct.\nRun the few lines below to load and prepare the data according to Gelman and Hill’s instructions:\n\n## Stop and frisk. Does race/ethnicity influence number of stops?\n## Prepare noisy data\n\ndat &lt;- read.table( \"http://www.stat.columbia.edu/~gelman/arm/examples/police/frisk_with_noise.dat\", header=TRUE, skip=6)\nstops &lt;- aggregate(cbind(stops, past.arrests) ~ eth + precinct, data=dat, sum)\n\nThe unit of analysis after we run this is the number of stops in a given precinct for a particular racial/ethnicity group.\n\nhead(stops)\n\n  eth precinct stops past.arrests\n1   1        1   202          980\n2   2        1   102          295\n3   3        1    81          381\n4   1        2   132          753\n5   2        2   144          557\n6   3        2    71          431\n\n\nIt is possible that the count of the number of stops may be influenced by the number of past arrests of a particular unit of analysis. We might want to measure a count per number of past arrests for an ethnicity group in a precinct instead of just a count per ethnicity group in a precinct. This will be our exposure and the \\(\\log\\) past.arrests will be our offset.\nWhen we have an offset, our model changes to:\n\n\\(\\frac{\\mathbf E(Y_i | X)}{N_i} = \\exp(\\mathbf x_i'\\beta)\\) or alternatively\n\\(\\mathbf E(Y_i | X) = \\exp(\\mathbf x_i'\\beta + \\log N_i)\\).\n\nThe exposure enters the right side of the equation as what is referred to as an “offset”– the \\(\\log\\) of the exposure.\nWe will not get a regression coeffcient for the offset because it is fixed to 1. However, we will still need to incorporate the offset when we estimate our outcomes.\n\n\nLet’s use a quasipoisson model this time. We enter the offset through an explicit argument offset. (Otherwise it will be treated like any regression variable, and its coefficient won’t be fixed to 1.)\n\n## No natural limit for stops BUT might be influenced by past arrests\nsf.1 &lt;- glm(stops ~ factor(eth), \n            data = stops, \n            family=\"quasipoisson\", \n            offset =  log(past.arrests))\nsummary(sf.1)\n\n\nCall:\nglm(formula = stops ~ factor(eth), family = \"quasipoisson\", data = stops, \n    offset = log(past.arrests))\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-47.327   -7.740   -0.182   10.241   39.140  \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -0.58809    0.05646 -10.416   &lt;2e-16 ***\nfactor(eth)2  0.07021    0.09042   0.777    0.438    \nfactor(eth)3 -0.16158    0.12767  -1.266    0.207    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for quasipoisson family taken to be 222.5586)\n\n    Null deviance: 46120  on 224  degrees of freedom\nResidual deviance: 45437  on 222  degrees of freedom\nAIC: NA\n\nNumber of Fisher Scoring iterations: 5\n\n\nWhen we calculate our quantities of interest, predict will automatically incorporate the offset. When we calculate them manually, we need to explicitly enter the offset.\n\n## Predicted counts with offsets exp(XB + log(N))\nsf.count &lt;- predict(sf.1, type = \"response\")\n\nX &lt;- model.matrix(sf.1)\nB &lt;- coef(sf.1)\nsf.count.man &lt;- exp(X %*% B + log(stops$past.arrests))\n\nNote that \\(e^{X\\hat\\beta + \\log N} = e^{X\\hat\\beta} * e^ {\\log N} = N*e^{X\\hat\\beta}\\). This means we could equivalently write:\n\nsf.count.man2 &lt;- exp(X %*% B)*stops$past.arrests\ncbind(sf.count, sf.count.man, sf.count.man2)[1:6,]\n\n  sf.count                  \n1 544.2816 544.2816 544.2816\n2 175.7562 175.7562 175.7562\n3 180.0316 180.0316 180.0316\n4 418.2082 418.2082 418.2082\n5 331.8515 331.8515 331.8515\n6 203.6578 203.6578 203.6578"
  },
  {
    "objectID": "10-CountData.html#how-to-think-about-zero-counts",
    "href": "10-CountData.html#how-to-think-about-zero-counts",
    "title": "10  Count data",
    "section": "10.9 How to think about Zero Counts",
    "text": "10.9 How to think about Zero Counts\nSometimes our outcome data have an excessive number of zeroes. For example, perhaps there are a lot of people that never post on social media at all, and then there are a smaller number of those that do, and they may post in any positive number of times.\nFor these, we might think there are two decisions\n\nPeople that post vs. do not post. This sounds like a binary model.\nHow many times to post? Ok this sounds more like Poisson or Negative Binomial\n\nA large number of zeroes is not necessarily something that the Poisson and Negative binomial distributions would predict with high probability. For that reason, we might want to use a modeling strategy that accounts for zero excess. We will discuss two: Hurdle models and Zero-inflated poisson/negative binomial.\nThis video provides the overview.\n\n\n10.9.1 Hurdle Models\nHurdle models may be useful when there are possibly sequential steps in achieving a positive count. The above example could motivate a hurdle model. First, someone decides if they want to post, and then if they want to post, they may post any positive \\(&gt;0\\) number of times.\n\nThere is a probability that governs the likelihood of not posting (\\(Pr(Y_i = 0) = \\pi\\))\nAnd then there is a count model restricted to \\(&gt;0\\) (“zero truncated”) describing the number of posts.\n\nThis post from the University of Virginia explains how to fit hurdle models in R.\n\n\n10.9.2 Zero Inflated Poisson/Negative binomial\nWhen you have excess zeroes, the intuitively named zero-inflated poisson or negative binomial model may also be appropriate. These are “mixture models” because there is a mixture of two distributions: the Bernoulli and Poisson/Negative Binomial. Here we think that there are two types of zeroes in the data.\n\nThis is only appropriate to the extent that there are some observations that are truly ineligible from having a positive count– that have zero probability of having a count \\(&gt;\\) 0.\n\nFor example, in the UCLA R tutorial linked to below, they study the number of fish a particular camping group caught at the park. Well some people might not have gone fishing! This would be a case where some of the zeroes may reflect a separate process (the decision to fish)\n\nHowever, even among those that decide to go fishing, some people may still catch zero fish. Just like in a typical Poisson or Negative Binomial process, it is still possible to have a 0 count.\n\nHere, we just think there may be two processes explaining the zeroes, and only using a standard count model does not help explain that first process.\n\nWe fit two models – a logistic regression model and a count model.\n\nThese tutorials from UCLA here and here describe how one would fit these models in R.\nI also recommend reading this blog post from Paul Allison in what to consider when choosing between count models. He argues often it may make just as or even more sense to stick with the overdispersed poisson or negative binomial unless you have a good reason to believe that there are people with zero-probability of having a positive count.\n\nFor those in IR, you might also be interested in this article about the use of zero-inflated models for forecasting civil conflict.\nThis article also provides an overview of different options for modeling count data."
  },
  {
    "objectID": "10-CountData.html#footnotes",
    "href": "10-CountData.html#footnotes",
    "title": "10  Count data",
    "section": "",
    "text": "Count data will always prevent the errors from being normally distributed, which can be problematic for estimates in small samples. In large samples, the uncertainty estimates will still approximate the correct values.↩︎"
  },
  {
    "objectID": "11-SampleSelection.html#sample-selection",
    "href": "11-SampleSelection.html#sample-selection",
    "title": "11  Sample Selection Models",
    "section": "11.1 Sample Selection",
    "text": "11.1 Sample Selection\nHere are a few thought exercises to underscore the potential issues with common sources of data.\nGraduate School Admissions Suppose we observe that college grades are uncorrelated with success in graduate school. Can we infer that college grades are irrelevant?\n\nNo. applicants admitted with low grades may not be representative of the population with low grades. Unmeasured variables (e.g. motivation) used in the admissions process might explain why those who enter graduate school with low grades do as well as those who enter graduate school with high grades.\n\nSelection into graduate school is not random\n\nImplication: may be unmeasured factors that bias our inferences from the sample we do have complete data about (graduate school students)\nSolution: May want to use a sample selection model to account for non-random sample\n\nWhat leads rivals to wage war?\nLemke and Reed 2001 argue that if we only focus on rivals, this may lead to biased inference.\n\nNeed first DV: whether members of “great power” dyads are rivals\nIn addition to second: whether “great power rivals” wage war\n\n“We discover that what makes great powers more likely to be rivals is statistically related to their propensity to experience war.” Results suggest that any analysis of the onset of war between rivals that fails to control for the prior influence of variables on the existence of rivalry almost surely produces inaccurate estimate.\n\n11.1.1 How do we go about estimating this?\nThe technical details, followed by implementation.\n\nSelection equation- into grad school sample?\n\n\\(\\zeta_i = z_i^T\\gamma + \\delta_i\\)\n\n\\(\\zeta_i\\) DV of selection equation\n\\(z_i^T\\) vector of covariates for selection equation\n\\(\\gamma\\) vector of coefficients for selection equation\n\\(\\delta_i\\) random disturbances\n\n\nOutcome equation\n\n\\(\\xi_i = x_i^T\\beta + \\epsilon_i\\)\n\n\\(\\xi_i\\) DV of outcome equation (success in grad school)\n\\(x_i^T\\) vector of covariates for outcome equation\n\\(\\beta\\) vector of coefficients for outcome equation\n\\(\\epsilon_i\\) random disturbances\n\n\nThe Problem\nWe actually want estimates of \\(Y_i\\) not \\(\\xi_i\\).\n\n\\(Y_i =\\)\n\n\\[\\begin{cases} \\text{missing }, \\; \\text{for } \\zeta_i \\leq 0\\\\ \\xi_i,\\;\n\\text{for } \\zeta_i &gt; 0\\end{cases}\\]\nTwo-step Estimation\n\nDefine a dichotomous outcome to indicate if in the sample or not \\(W_i =\\)\n\n\\[\\begin{cases} 1, \\; \\text{if } Y_i \\text{ is observed } \\\\ 0, \\; \\text{ if } Y_i \\text{ is missing } \\end{cases}\\]\n\nFit a probit regression with \\(W_i\\) as the outcome with the linear predictor: \\(\\hat \\psi_i = z_i^T\\hat \\gamma\\)\n\nCalculate the “inverse Mills ratio” of \\(\\hat \\eta_i = \\frac{\\phi(\\hat \\psi_i)}{\\Phi(\\hat \\psi_i)}\\)\nNote this is dnorm()/pnorm(), ratio of the probability density function over the cumulative distribution function for each i\n\nUse \\(\\hat \\eta_i\\) as an auxiliary regressor of \\(Y_i\\) on the \\(x_i^T\\) for those where \\(Y_i\\) is observed.\n\nNote the SEs have to be adjusted (not just the standard OLS errors).\n\n\n\n\n11.1.2 Sample Selection Model Assumptions\nBig assumptions\n\nExclusion restriction – selection equation should contain at least one variable that predicts selection but not the outcome\nErrors in the probit equation are homoskedastic\nError terms for selection equation and outcome are correlated (\\(\\rho_{\\epsilon \\delta}\\)).\n\n\\(\\epsilon_i\\) and \\(\\delta_i\\) should be distributed as bivariate normal if using the MLE approach discussed below.\n\n\\(\\epsilon_i\\) and \\(\\delta_i\\) should be independent of the regressors in their equations\nResults can be sensitive to how you specify the selection equation\n\nA useful discussion in IR about these issues is Simmons and Hopkins (2005). An extension of this model has also been developed for models where the outcome is dichotomous (see: bivariate probit models)"
  },
  {
    "objectID": "11-SampleSelection.html#fitting-sample-selection-in-r",
    "href": "11-SampleSelection.html#fitting-sample-selection-in-r",
    "title": "11  Sample Selection Models",
    "section": "11.2 Fitting Sample Selection in R",
    "text": "11.2 Fitting Sample Selection in R\nWe can use functions in R to do this calculation for us. We will follow the example from the R Pubs resource, which uses data from\n\nMroz, T. A. (1987) “The sensitivity of an empirical model of married women’s hours of work to economic and statistical assumptions.” Econometrica 55, 765–799.\n\nReview the summary at the link above for information about the setup, where our outcome is married women’s wages, and the selection is labor force participation.\n\n## install.packages(\"sampleSelection\")\nlibrary(sampleSelection)\n\ndata(\"Mroz87\")\n\nThe selection variable is lfp, a 0 or 1 variable indicating labor force participation.\n\ntable(Mroz87$lfp)\n\n\n  0   1 \n325 428 \n\n\nThe sample selection issue is we do not observe wages for those out of the labor force. Thus, we can first estimate a model predicting labor force participation, and then a model predicting wages. We do so in the below code.\n\nNote, we need at least one variable in the selection equation that predicts selection but not wages. Here, we use kids for this.\n\n\nMroz87$kids &lt;- (Mroz87$kids5 + Mroz87$kids618)\n\nWe fit the model by providing two regression formulas in the selection function.\n\n## 2-step estimator\nselection1 &lt;- selection(selection = lfp ~ age  +\n                          faminc + kids + educ,\n                        outcome = wage ~ exper + age + educ + city, \n                        data = Mroz87,\n                        method = \"2step\")\nsummary(selection1)\n\n--------------------------------------------\nTobit 2 model (sample selection model)\n2-step Heckman / heckit estimation\n753 observations (325 censored and 428 observed)\n13 free parameters (df = 741)\nProbit selection equation:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.377e-01  4.479e-01   0.307 0.758629    \nage         -2.253e-02  6.898e-03  -3.266 0.001140 ** \nfaminc       5.168e-06  4.150e-06   1.245 0.213428    \nkids        -1.319e-01  3.768e-02  -3.500 0.000493 ***\neduc         8.889e-02  2.285e-02   3.890 0.000109 ***\nOutcome equation:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -1.45192    2.18010  -0.666 0.505627    \nexper        0.01747    0.02167   0.806 0.420270    \nage          0.01570    0.02524   0.622 0.534085    \neduc         0.41456    0.11376   3.644 0.000287 ***\ncity         0.41415    0.31925   1.297 0.194947    \nMultiple R-Squared:0.1261,  Adjusted R-Squared:0.1158\n   Error terms:\n              Estimate Std. Error t value Pr(&gt;|t|)\ninvMillsRatio  -1.1666     1.5530  -0.751    0.453\nsigma           3.2149         NA      NA       NA\nrho            -0.3629         NA      NA       NA\n--------------------------------------------\n\n\nIf the coefficient estimate on the inverse mills ratio is non-zero, that suggests that the selection probability does influence wages. We do not necessarily have strong evidence here given the high p-value. This may vary by how to specify the model. Note also that the estimate for this coefficient is the multiplication of sigma*rho, where rho is the correlation of the errors between equations and sigma is the standard error of the residuals from the regression equation.\nMaximum likelihood also allows us to estimate the equations simultaneously. We just specify ml as the method.\n\n## alternative using maximum likelihood\nselection2 &lt;- selection(selection = lfp ~ age +\n                          faminc + kids + educ,\n                        outcome = wage ~ exper + age + educ + city, \n                        data = Mroz87,\n                        method = \"ml\")\n\nWe can see what is going on under the hood by fitting the 2-step process manually. The only difference is our manual standard errors would be wrong.\n\n## Selection equation\nseleqn1 &lt;- glm(lfp ~ age  + faminc + kids + educ,\n               family=binomial(link=\"probit\"), data=Mroz87)\n\n## Calculate inverse Mills ratio by hand ##\nMroz87$IMR &lt;- dnorm(seleqn1$linear.predictors)/pnorm(seleqn1$linear.predictors)\n\n## Outcome equation correcting for selection \nouteqn1 &lt;- lm(wage ~ exper + exper + age + educ + city+IMR , data=Mroz87,\n              subset=(lfp==1))\n\n## Compare with the selection package results\ncoef(outeqn1)\n\n(Intercept)       exper         age        educ        city         IMR \n-1.45197781  0.01747395  0.01569873  0.41456568  0.41415262 -1.16657579 \n\n\nHow should we interpret these results?\n\nIf variables are only in the outcome equation, like city and experience, we can interpret them like OLS coefficients.\nIf variables appear in both equations, then we can also make an adjustment to have an estimate for the full average marginal effect, that also accounts for selection instead of just specifying the effect of the variable for those “that are selected”.\n\n\n## Example\nselection3 &lt;- selection(selection = lfp ~ age +\n                          faminc + kids + educ,\n                        outcome = wage ~ \n                          exper + age + educ + city, data = Mroz87,\n                        method = \"2step\")\n## average marginal effect:\nbeta.educ.sel &lt;- selection3$coefficients[5]\nbeta.educ.out &lt;- selection3$coefficients[9]\nbeta.IMR &lt;- selection3$coefficients[11]\ndelta &lt;- selection3$imrDelta\n\nmarginal.effect &lt;- beta.educ.out - (beta.educ.sel * beta.IMR * delta)\n## average marginal effect\nmean(marginal.effect)\n\n[1] 0.4757589"
  },
  {
    "objectID": "11-SampleSelection.html#heckman-example-using-survey-data",
    "href": "11-SampleSelection.html#heckman-example-using-survey-data",
    "title": "11  Sample Selection Models",
    "section": "11.3 Heckman Example Using Survey Data",
    "text": "11.3 Heckman Example Using Survey Data\nThis example is from “Poverty and Divine Rewards: The Electoral Advantage of Islamist Political Parties” published in the American Journal of Political Science in 2019.\nAbstract Political life in many Muslim-majority countries has been marked by the electoral dominance of Islamist parties. Recent attempts to explain why have highlighted their material and organizational factors, such as the provision of social services. In this article, we revive an older literature that emphasizes the appeal of these parties’ religious nature to voters experiencing economic hardship. Individuals suffering economic strain may vote for Islamists because they believe this to be an intrinsically virtuous act that will be met with divine rewards in the afterlife. We explore this hypothesis through a series of laboratory experiments in Tunisia. Individuals assigned to treatment conditions instilling feelings of economic strain exhibit greater support for Islamist parties, and this support is causally mediated by an expectation of divine compensation in the hereafter. The evidence suggests that the religious nature of Islamist parties may thus be an important factor in their electoral success.\nWe are going to replicate a small part of their analysis of an experiment:\n\nExperiment 2 induced economic strain by exposing participants (n = 201) to four hypothetical financial scenarios\n\nHalf were randomly assigned to a “hard” condition, in which the four scenarios involved financial costs that were relatively high, whereas\nHalf were assigned to an “easy” condition that involved substantially lower costs.\n\nOne of the secondary dependent variables was: In Experiment 2, why they chose to vote for the party they did, giving them six options, including “Allah will be more pleased if I vote for this party than other parties.”\n\nFor each answer option, we asked respondents for their level of agreement with the statement and\nsubsequently asked them to rank each statement they agreed with in importance.\n\n\nLet’s load the data and explore the variables.\n\nexp2 &lt;- read.csv(\"https://github.com/ktmccabe/teachingdata/raw/main/exp2.csv\")\n\nThe authors are looking to verify that pleasing Allah had something to do with Ennahda vote choice, particularly among poor voters. Let’s look at the variable votenahda, which is a 0 or 1 outcome.\n\n1 if plan to vote for Ennahda if elections held tomorrow, 0 if not\n\n\ntable(exp2$votenahda)\n\n\n  0   1 \n335  66 \n\n\nWhether a voter is poor is determined by if they fall below 7 on the variable inc. Let’s subset our data to only examine poor voters.\n\nsubdata &lt;- subset(exp2, inc &lt; 7)\n\nThey want to understand if pleasing Allah was a top reason for voting for the party. This information is only available for those that agreed or strongly agreed with the statement, “Allah will be more pleased if I vote for this party than other parties.”\nThis information is in the variable voteAllah2\n\nvoteAllah2: 1=strongly agree or agree, 0=otherwise\n\n\ntable(exp2$voteAllah2)\n\n\n  0   1 \n118 169 \n\n\nThe ranking information is available in the variable, voteAllahrank3:\n\nvoteAllahrank3: 1 if voteAllahrank \\(&gt;\\) 4 (top two reasons); NA if voteAllahrank = 0; 0 otherwise.\n\n\ntable(exp2$voteAllahrank3)\n\n\n  0   1 \n104  64 \n\n\nLet’s identify the sample selection issue.\n\nvoteAllahrank3 is only observed for those who strongly agreed or agreed with the statement\n\n\ntable(RankedTopTwo=exp2$voteAllahrank3, Agreed=exp2$voteAllah2)\n\n            Agreed\nRankedTopTwo   0   1\n           0   0 104\n           1   0  64\n\n\nWhat makes this a candidate for a Heckman sample selection model?\n\n\nTry on your own, then expand.\n\n\nWe are interested in estimating whether Ennahda voters are more likely than others to rank pleasing Allah among the top two reasons. Our desired outcome is \\(Y_i =\\) voteAllahrank3\n\n\\(Y_i\\) only observed for those who met some criteria set by another random variable (in this case voteAllah2 = 1).\nHowever, we still have information on the independent variables for all respondents, regardless of whether they are a 0 or 1 on voteAllah2\n\n\n\nEstimate the two-step process, following the authors in terms of what variables to include in each stage according to columns 2 and 3 in the table.\n\n\nPrior to fitting the model, subset the data to include only respondents who voted for votenahda, votenidaa, votejabha, voteirada, indicated by respondents being coded as a 1 on these variables. These correspond to the party variables in columns 2 and 3.\n\nother relevant variables are treat and quran\n\n\n\n\nTry on your own, then expand for the solution.\n\nAs the authors note, “The ranking is a two-step process, as respondents only get to rank factors that they agree with. To model their rankings, we therefore employ a Heckman selection model, analyzing first who agreed that pleasing Allah is important in their vote choice, and then analyzing second who ranked pleasing Allah as one of their top two factors.”\n\nsubdata2 &lt;- subset(subdata, votenahda==1 |\n                     votenidaa==1 | votejabha==1 |\n                     voteirada==1)\ntwo &lt;- selection(selection = voteAllah2~votenahda+votejabha+treat+quran, \n                  outcome = voteAllahrank3~votenahda+votenidaa, \n                  data=subdata2, \n                 method=\"2step\")\n\nsummary(two)\n\n--------------------------------------------\nTobit 2 model (sample selection model)\n2-step Heckman / heckit estimation\n90 observations (31 censored and 59 observed)\n11 free parameters (df = 80)\nProbit selection equation:\n            Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)   0.3418     0.2897   1.180   0.2415  \nvotenahda     0.2442     0.3383   0.722   0.4725  \nvotejabha    -0.1369     0.3646  -0.375   0.7083  \ntreat        -0.4057     0.2906  -1.396   0.1665  \nquran         0.6495     0.2927   2.219   0.0293 *\nOutcome equation:\n             Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept) -0.007066   0.248585  -0.028   0.9774  \nvotenahda    0.312768   0.185545   1.686   0.0958 .\nvotenidaa    0.226522   0.161635   1.401   0.1650  \nMultiple R-Squared:0.0558,  Adjusted R-Squared:0.0043\n   Error terms:\n              Estimate Std. Error t value Pr(&gt;|t|)\ninvMillsRatio   0.3248     0.3356   0.968    0.336\nsigma           0.5187         NA      NA       NA\nrho             0.6263         NA      NA       NA\n--------------------------------------------\n\n\n\nHow should we interpret the results in light of the researchers’ hypothesis that Ennahda voters would be more likely to rank pleasing Allah as a top reason?\n\n\nTry on your own, then expand for the solution.\n\nAs the authors note describing the outcome equation, “results suggest that poor Ennahda voters were about 31% more likely to rank pleasing Allah among their top two factors (p = .096; see the SI, p. 22) than poor supporters of secular parties.”\nWe have to be careful here, that this is from the outcome equation and does not represent the marginal effect based on both selection and outcome processes."
  },
  {
    "objectID": "11-SampleSelection.html#tobit-model-aka-censored-regression-model",
    "href": "11-SampleSelection.html#tobit-model-aka-censored-regression-model",
    "title": "11  Sample Selection Models",
    "section": "11.4 Tobit Model aka Censored regression model",
    "text": "11.4 Tobit Model aka Censored regression model\nDealing with outcomes that are “top-coded” or “bottom-coded” at a threshold value\nExample: if income is top-coded as “above $250k”\n\\(Y_i\\) =\n\\[\\begin{cases} Y_i^*, \\; Y_i &lt; 250 \\\\ \\text{ above 250 } \\; Y_i \\geq 250 \\end{cases}\\]\nWe are interested in \\(Y_i^*\\): actual income, not censored income. Problem- it’s unobserved for part of the sample\n\nExample: want to use SAT as measure of aptitude, but scores capped between 200 and 800\nExample: want to measure support for candidate but legal maximum for campaign donations is $5000\nExample: want to measure like-dislike of candy bars, but candy bars consumed bottom-coded at 0\nNote: in classic tobit model, censoring happens at zero\n\nFor elaboration in R, see this UCLA resource and tobit() in the AER package\n\n11.4.1 Tobit Model Assumptions\n\nAssume homoskedastic and normally distributed errors\nWhen data are censored at zero (clumping at zero), assume same underlying stochastic process to determine\n\nwhether the response is zero or positive\nas well as the value of a positive response\nAny variable which increases the probability of a non-zero value must also increase the mean of positive values.\n\nShould generally be used in cases where the dependent variable could take on negative values\n\nAn alternative model discussed in the count data section: “two-part” and hurdle model–appropriate when the 0 is a “true zero”"
  },
  {
    "objectID": "11-SampleSelection.html#truncated-models",
    "href": "11-SampleSelection.html#truncated-models",
    "title": "11  Sample Selection Models",
    "section": "11.5 Truncated Models",
    "text": "11.5 Truncated Models\nSample selection is determined by values of the \\(Y\\) variable. Do not observe x or y for truncated observations\n\nExample: interested in effect of education on income, but only sample people below a certain income.\nExample: studies of electoral success of newly formed political parties. Problem: likely only observe new parties whn their chance of success is likely high\nExample: studies using newspaper reports on social movements to study predictors of violence in these movements. Problem: newspapers select which movements to report on- likely those with chance of violence.\n\nSolution? Requires information about the mechanism that leads to the incomplete (or truncated) data set\n\nSee more on R implementation here"
  },
  {
    "objectID": "12-PanelData.html#motivating-data-groupiness",
    "href": "12-PanelData.html#motivating-data-groupiness",
    "title": "12  Panel and Hierarchical Data",
    "section": "12.1 Motivating Data Groupiness",
    "text": "12.1 Motivating Data Groupiness\nIn this section, we are going to use data from the CDC on national and state-level COVID hospitalizations and positive case increases between July 2020- end September 2020.\nLet’s load the national and state data.\n\nlibrary(rio)\nnational &lt;- import(\"https://github.com/ktmccabe/teachingdata/blob/main/national.RData?raw=true\")\nstates &lt;- import(\"https://github.com/ktmccabe/teachingdata/blob/main/states.RData?raw=true\")\n\nhead(national)\n\n          date hospitalizedCurrently positiveIncrease\n159 2020-09-30                 31021            44909\n160 2020-09-29                 30601            36766\n161 2020-09-28                 29696            35376\n162 2020-09-27                 29579            34990\n163 2020-09-26                 29670            47268\n164 2020-09-25                 29888            55237\n\nhead(states)\n\n           date hospitalizedCurrently positiveIncrease state\n8849 2020-09-30                    53              104    AK\n8850 2020-09-30                   776             1147    AL\n8851 2020-09-30                   484              942    AR\n8853 2020-09-30                   560              323    AZ\n8854 2020-09-30                  3267             3200    CA\n8855 2020-09-30                   264              511    CO\n\n\nDuring this time period, the national level of hospitalizations was declining.\n\nlibrary(ggplot2)\nggplot(national, aes(x=date, y=hospitalizedCurrently))+\n  geom_line()\n\n\n\n\nHowever, the national data represents aggregated state-level data. It is possible that these trends might look different if we looked within each state.\n\nggplot(states, aes(x=date, y=hospitalizedCurrently))+\n  geom_line()+\n  facet_wrap(~state, scales = \"free\")\n\n\n\n\nWhen we have data that are grouped in some way, it is important to consider how group-specific factors may influence the results.\nLet’s look at the relationship between positive increases in cases and hospitalizations using the state-level data. Note, we are not epidemiologists here, so this is in no way exactly how you would want to model this in practice. We will leave that to the experts. Nonetheless, it will give us some visualizations. The regression line below is from the following regression, which pools across states and dates:\n\\(hospitalizedCurrently_{it} = \\alpha + positiveIncrease_{it} + \\epsilon_{it}\\)\n\nggplot(states, aes(x=positiveIncrease, y=hospitalizedCurrently, color=state))+\n  geom_point()+\n  geom_smooth(aes(y=hospitalizedCurrently),method=\"lm\", se=F, color=\"black\")+\n  ylim(0, 15000)\n\n\n\n\nWhat issues do you have with that type of analysis?\n\nHow could we improve it?\n\nLet’s look at the variation by state.\n\nggplot(states, aes(x=positiveIncrease, y=hospitalizedCurrently, color=state))+\n  geom_point()+\n  geom_smooth(method=\"lm\", se=F)+\n  facet_wrap(~state, scales = \"free\")\n\n\n\n\nOne thing we could do to account for the variation in cases, is to add controls for state. This is called adding “fixed effects.” It allows the intercept for each state to be different in regression, but the slopes are considered the same.\n\nfit &lt;- lm(hospitalizedCurrently ~ positiveIncrease + as.factor(state), data=states)\nfit.pred &lt;- predict(fit)\n\nggplot(states, aes(x=positiveIncrease, y=hospitalizedCurrently, color=state))+\n  geom_point()+\n  geom_smooth(aes(y=hospitalizedCurrently),method=\"lm\", se=F, color=\"black\")+\n  geom_line(aes(y=fit.pred))+\n  ylim(0, 15000)\n\n\n\nggplot(states, aes(x=positiveIncrease, y=hospitalizedCurrently, color=state))+\n  geom_point()+\n  geom_line(aes(y=fit.pred))+\n  facet_wrap(~state, scales = \"free\")\n\n\n\n\nWe could also add interactions with state, which will allow the slopes to vary.\n\nfit &lt;- lm(hospitalizedCurrently ~ positiveIncrease*as.factor(state), data=states)\nfit.pred &lt;- predict(fit)\n\n\nggplot(states, aes(x=positiveIncrease, y=hospitalizedCurrently, color=state))+\n  geom_point()+\n  geom_line(aes(y=fit.pred))+\n  facet_wrap(~state, scales = \"free\")\n\n\n\n\nAny downsides to adding interactions?"
  },
  {
    "objectID": "12-PanelData.html#panel-and-hierarchical-data",
    "href": "12-PanelData.html#panel-and-hierarchical-data",
    "title": "12  Panel and Hierarchical Data",
    "section": "12.2 Panel and Hierarchical Data",
    "text": "12.2 Panel and Hierarchical Data\n\\(Y_i, X_i\\) now become \\(Y_{it}, X_{it}\\) where \\(i\\) is a unique indicator for the unit and \\(t\\) is a unique indicator for time (or question, task, etc., other repeated measurement)\n\nFor example, perhaps we observe a set of \\(N\\) European countries, each indexed by \\(i\\) over \\(10=T\\) years, each indexed by \\(t\\).\nThe dataset can be represented in “wide” or “long/stacked” format (see section 2 of the course notes)\nCan get more complicated. Perhaps we have a Member of Parliament in a year in a country: \\(Y_{itj}, X_{itj}\\). Now we have three indices for our variables.\nWe also might not have variation over time, but instead, multiple levels of units– perhaps students (\\(i\\)) nested in schools (\\(j\\)).\n\nWhy do we like this type of data in social science?\n\nWe think some \\(Z\\) variable is related to some \\(Y\\) outcome.\nSo we test this with data by comparing data that vary on \\(Z\\) to see if they vary systematically on \\(Y\\).\nOur goal is to isolate \\(Z\\). We want to “control” on everything that differs between our units that could affect \\(Y\\) except for \\(Z\\), which varies.\nProblem: this can feel almost impossible in cross-sectional data\n\nExample: Bob and Suzy probably differ in a million ways, but we can only measure and account for so many covariates\nExample: France and Germany probably differ in a million ways, but we can only measure and account for so many covariates\nThis makes comparing Bob vs. Suzy and France vs. Germany for our variation is a somewhat suspect way to show how \\(Z\\) relates to \\(Y\\) if we cannot control on all possible relevant factors.\n\n\nPossible solution: enter repeated observations\n\nIdea: Bob in wave 1 is probably pretty similar to Bob at wave 2. This might be a more sensible comparison than Bob vs. Suzy.\nIdea: France in 1968 vs. France 1970 is probably a more sensible comparison than France vs. Germany\n\nSo, perhaps we incorporate Bob vs. Bob and Suzy vs. Suzy; France vs. France and Germany vs. Germany comparisons instead of just making between-unit comparisons.\n\nIssue: need to learn new methods to account for our grouped/repeated data structure.\n\nWhy?\nRecall OLS\n\\[\\begin{align*}\nY_i =  \\beta_0 + \\beta_1 x_i + \\epsilon_i\n\\end{align*}\\]\nAssumptions:\n\n\\(\\mathbf E(\\epsilon_i) = 0\\);\nErrors independent of regressors: Cov\\((\\epsilon_i, X_i) = 0\\);\nErrors of different observations not correlated: Cov\\((\\epsilon_i, \\epsilon_j) = 0\\)\nConstant error variance \\(V(\\epsilon_i | X) = \\sigma^2\\).\n\nWhen we have grouped/longitudinal data, many of these assumptions are likely violated. For elaboration, see the video on the previous page that discusses how unobserved factors related to particular geographical areas might influence the explanatory variable crime rate video.\nLet’s take a look at why.\nWhat if we have \\[\\begin{align*}\nY_{it} =  \\beta_0 + \\beta_1 x_{it} + \\epsilon_{it}\n\\end{align*}\\]\nWe could still treat this as OLS if we believe the assumptions hold. But often, we are concerned that our model actually looks like this:\n\\[\\begin{align*}\nY_{it} =  \\beta_0 + \\beta_1x_{it}+ (c_i + \\epsilon_{it}).\n\\end{align*}\\]\nWe think there may be \\(c_i\\) unobserved characteristics about our units \\(i\\) that are related to our explanators. If unaccounted for and left as part of the error term, this would cause bias in our coefficients.\n\nNote: in OLS, we assume Cov\\((c_i + \\epsilon_{it}, x_{it}) = 0\\). Here, we believe there may be unmeasured factors that induce covariance between \\(c_i\\) and \\(x_{it}\\).\n\nOne Solution: fixed effects: removes time-constant unobserved characteristics about our units."
  },
  {
    "objectID": "12-PanelData.html#fixed-effects",
    "href": "12-PanelData.html#fixed-effects",
    "title": "12  Panel and Hierarchical Data",
    "section": "12.3 Fixed Effects",
    "text": "12.3 Fixed Effects\nIn fixed effects, we allow the intercept to be different across our \\(N\\) groups (e.g., countries), where these terms are going to account for time-invariant (time constant) characteristics of these areas.\n\\[\\begin{align*}\nY_{it} &=  \\alpha_i + \\beta_1x_{it} + \\epsilon_{it}\n\\end{align*}\\]\nwhere \\(\\alpha_{i}\\) absorbs all unobserved time-invariant characteristics for unit \\(i\\) (e.g., features of geography, culture, etc.)\n\nThis is huge! It’s going to help us get around the assumption we just talked about. The idea is that when we are comparing observations within a particular unit (e.g., within France), all of those time-invariant characteristics specific to France are held constant.\n\nBelow we discuss how to estimate fixed effects models. It is not the solution to everything. Keep these in mind.\n\nStill assume homoskedastic errors \\(\\rightarrow\\) may want to adjust the SE’s.\nCannot model “level-2” factors. Problematic if that’s what you’re interested in! (E.g., if you have time-invariant country-level covariates). Can only do this through interactions.\n\nFor example, suppose we have a measure of a country’s culture (\\(culture_{i}\\)), that does not vary over time. We could not include that in our regression model because the model removes all time-invariant characteristics. In contrast, if something like country GDP changes a lot over time (\\(GDP_{it}\\)), it could still and should be accounted for.\n\nOnly works if you have a decent number of observations\nLeverages within-group variation on your outcomes. Need to have this variation for it to make sense!\n\nThere are two general approaches for fitting fixed effects estimators: the within-estimator and LSDV.\n\n12.3.1 Within-Estimator\nTime Demeaning. First calculate averages of the \\(it\\) terms.\n\nNote, the mean of \\(\\alpha_i\\) is just \\(\\alpha_i\\) because it does not vary over time. \\(\\alpha_i\\) because: \\(\\frac{1}{T} \\sum_{t=1}^T \\alpha_i = \\frac{1}{T}*T*\\alpha_i = \\alpha_i\\).\n\n\\[\\begin{align*}\n\\bar y_i &= \\alpha_i +  \\beta\\bar x_i  + \\bar \\epsilon_i\\\\\n\\end{align*}\\]\nSubtracting we get: \\[\\begin{align*}\nY_{it} - \\bar y_i &= (\\alpha_i -\\alpha_i) +  \\beta(x_{it}- \\bar x_i) + (\\epsilon_{it}- \\bar \\epsilon_i)\\\\\n\\widetilde{Y_{it}} &= \\beta\\widetilde{x_{it}} + \\widetilde{\\epsilon_{it}}\n\\end{align*}\\] \\(\\beta\\) is estimated just from deviations of the units from their means. Removes anything that is time-constant.\nThis video from Ben Lambert goes through the demeaning process.\n\n\n12.3.2 Least Squares Dummy Variable Regression\nWe can estimate the equivalent model by including dummy variables in our regression. This is sometimes why you have seen authors add dummies for region and call them “fixed effects.”\nOLS with dummy variables for each unit (e.g., countries). Start with “stacked” data where: \\(Y = X\\beta + U\\delta + \\epsilon\\).\n\n\\(Y\\) is \\(NT \\times 1\\) outcome vector where \\(N\\) is number of \\(i\\) units and \\(T\\) is number of time points \\(t\\).\n\\(X\\) is \\(NT \\times k\\) matrix where \\(k\\) is the number of covariates plus general intercept\n\\(U\\) is a set of \\(m\\) dummy variables (e.g., country dummies), leaving one as reference\n\nBy accounting for these groups in our regression, we are now estimating how the covariates in \\(\\mathbf x\\) affect our outcomes, holding constant the group– leveraging within-group variation.\n\n\n12.3.3 Fixed Effects Implementation in R\nThe plm package allows us to indicate that our data are panel in nature and fit the within-estimator.\nRun install.packages(\"plm\") and load Grunfeld. 200 observations at the firm-year level. Here, we can think of firms as indexed by \\(i\\) and years indexed by \\(t\\).\n\nlibrary(plm)\ndata(Grunfeld)\nhead(Grunfeld)\n\n  firm year   inv  value capital\n1    1 1935 317.6 3078.5     2.8\n2    1 1936 391.8 4661.7    52.6\n3    1 1937 410.6 5387.1   156.9\n4    1 1938 257.7 2792.2   209.2\n5    1 1939 330.8 4313.2   203.4\n6    1 1940 461.2 4643.9   207.2\n\n## Unique N- firms\nlength(unique(Grunfeld$firm))\n\n[1] 10\n\n## Unique T- years\nlength(unique(Grunfeld$year))\n\n[1] 20\n\n\nFor more detail onplm, see documentation\nThese data are used to study the effects of capital (stock of plant and equipment) on gross investment inv.\nLet’s first run a standard OLS model, which we will call the “pooled” model because it pools observations across firms and years without explicitly accounting for them in the model. We can use the standard lm model to do this or the plm function from the plm package by indicating model=\"pooling\". Both give equivalent results.\n\n## Pooled model\nfit.pool &lt;- plm(inv~ capital, data = Grunfeld, model = \"pooling\")\ncoef(fit.pool)[\"capital\"]\n\n  capital \n0.4772241 \n\n## Equivalent\nfit.ols  &lt;- lm(inv~ capital, data = Grunfeld)\ncoef(fit.ols)[\"capital\"]\n\n  capital \n0.4772241 \n\n\nWe could visualize this model as follows by plotting the regression line. This provides a good benchmark against which we can see how fixed effects change the results. Note that the line ignores which firm that the data are coming from, using between-firm variation to help fit the regression slope.\n\nlibrary(ggplot2)\nggplot(Grunfeld, aes(x=capital, y=inv, color=as.factor(firm)))+\n  geom_point()+\n  geom_smooth(aes(y=inv), se=F, method=\"lm\", color=\"black\")\n\n\n\n\nLet’s now fit our fixed effects model. Here, we have to add an argument that specifies which variables are indexed by \\(i\\) and \\(t\\) so that plm knows how to incorporate the individual firm units. We will first fit the within model using the plm package. We can then use lm by explicitly adding dummy variables. You will note that the individual dummy variable coefficients are only included in the output of lm and not plm. When you have a large number of units, it might be more computationally efficient and cleaner to use plm. However, both give us equivalent results for our coefficients of interest.\n\n## Within model with firm fixed effects\nfit.within &lt;- plm(inv~capital, \n                  data = Grunfeld, \n                  model = \"within\", \n                  index = c(\"firm\", \"year\"))\ncoef(fit.within)[\"capital\"]\n\n  capital \n0.3707496 \n\n## numerical equivalent\nfit.lsdv  &lt;- lm(inv~  capital + as.factor(firm), \n                data = Grunfeld)\ncoef(fit.lsdv)[\"capital\"]\n\n  capital \n0.3707496 \n\n\nRecall, what fixed effects does is create a different intercept by firm. We can visualize this by generating predicted gross investments for each observation in our data.\nFixed effects leverages within-firm variation to fit the regression model. That is, holding constant the firm, how does capital influence investment, on average?\nHere, we see that when the variation is coming from within-firm, the estimated slope for the capital variable is slightly more shallow than when the pooled model used data across different firms to estimate the slope. Sometimes, the difference between the pooled OLS and fixed effects estimation can be so severe as to reverse the sign of the slopes. This is the idea of Simpson’s paradox discussed here.\n\nfit.lsdvpred &lt;- predict(fit.lsdv)\nlibrary(ggplot2)\nggplot(Grunfeld, aes(x=capital, y=inv, color=factor(firm)))+\n  geom_point()+\n  geom_smooth(aes(y=inv), se=F, method=\"lm\", color=\"black\")+\n  geom_line(aes(y=fit.lsdvpred))\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nGreen et al. (2003) also discuss the dangers of pooling data in certain cases in the article ``Dirty Pool.” Below are three examples. In the first two, pooling vs. fixed effects will not make much of a difference, but it will in the third.\nAs the authors describe, “The plot in Figure 1 depicts a positive relationship between X and Y where N = 2 and T = 50. Because both dyads share a common intercept, pooling creates no estimation problems. One obtains similar regression estimates regardless of whether one controls for fixed effects by introducing a dummy variable for each dyad. A pooled regression is preferable in this instance because it saves a degree of freedom. In Figure 2 we encounter another instance in which dyads have different intercepts, but there is no correlation between the intercepts and X. The average value of the independent variable is the same in each dyad. Again, pooled regression and regression with fixed effects give estimates with the same expected value. Figure 3 illustrates a situation in which pooled regression goes awry. Here, the causal relationship between X and Y is negative; in each dyad higher values of X coincide with lower values of Y. Yet when the dyads are pooled together, we obtain a spurious positive relationship. Because the dyad with higher values of X also has a higher intercept, ignoring fixed effects biases the regression line in the positive direction. Controlling for fixed effects, we correctly ascertain the true negative relationship between X and Y” (443-445).\n \n\n\n12.3.4 Standard errors with fixed effects\nEven if we use fixed effects, we may want to adjust standard errors. Fixed effects assumes homoskedasticity (constant error variance). We might instead think that the random error disturbances are correlated in some way or vary across time or units. For example, we might fear we have serial correlation (autocorrelation), where errors are correlated over time. Below are a few options.\nClustered standard errors. We came across this before in our section on ordinal probit models and the Paluck and Green paper. This blog post discusses why you may still want to cluster standard errors even if you have added fixed effects.\nNote: the functions below are designed to work with the plm package and models. For models fit through other functions, you may need to use different functions to make adjustments. See section 8.5.1. For linear models lm_robust from the estimatr package works well. For other models, see the alternative applications in that section.\n\n## Cluster robust standard errors to match stata (c only matters if G is small). Can omit if do not want to adjust for degrees of freedom.\n## Generally want G to be 30 or 40\nG &lt;- length(unique(Grunfeld$firm))\nc &lt;- G/(G - 1)\nsqrt(diag(c*vcovHC(fit.within,type = \"HC1\", cluster = \"group\")))\n\n   capital \n0.06510863 \n\n\nPanel corrected SE: Beck and Katz have also developed standard errors for panel data. Explained in the documentation, observations may be clustered either by “group” to account for timewise heteroskedasticity and serial correlation or by “time” to account for cross-sectional heteroskedasticity and correlation. This is sensitive to \\(N\\) and \\(T\\).\n\n## Alternative: Beck and Katz panel corrected standard errors\nsqrt(diag(vcovBK(fit.within, cluster = \"group\"))) # also can change type\n\n   capital \n0.04260805 \n\n\nAlternative: Adjusted for heteroskedasticity and/or serial correlation.\n\nsqrt(diag(vcovHC(fit.within, type = \"HC1\", method = \"arellano\")))\n\n   capital \n0.06176747 \n\n\nFor more information, see pg. 24 of this resource"
  },
  {
    "objectID": "12-PanelData.html#additional-considerations-for-fixed-effects",
    "href": "12-PanelData.html#additional-considerations-for-fixed-effects",
    "title": "12  Panel and Hierarchical Data",
    "section": "12.4 Additional considerations for fixed effects",
    "text": "12.4 Additional considerations for fixed effects\n\n12.4.1 Binary dependent variables\nYou should use caution when adding fixed effects to binary models, such as the logit. They can suffer from the “incidental parameters” problem. Instead of using the logit, with a large number of fixed effects (some would even say not that large), you can use a linear probability model or the conditional logit mode.\nFor a resource on fitting the conditional logit in R, see here.\n\nThe xtlogit fe command in Stata fits this type of model\nNote that computing marginal effects and predictions from this model can be more difficult\n\n\n\n12.4.2 Two-way fixed effects\nWe might be concerned not just about unobserved unit effects but also unobserved time effects \\[\\begin{align*}\nY_{it} =  \\beta_0 + \\beta_1x_{it}+ (c_i + v_t + \\epsilon_{it}).\n\\end{align*}\\]\nHere, we may include fixed effects for both units and time. Warning: can be difficult to interpret.\nAlternative, sometimes instead of including dummies for each \\(t\\) period, people will include a “time trend.” This could be a linear time trend, for example, a year variable treated as numeric.\n\\[\\begin{align*}\nY_{it} =  \\alpha_i + \\beta_1  x_{it}+ \\beta_2 year_{t} + \\epsilon_{it}.\n\\end{align*}\\]\nOr this could be a polynomial, such as a cubic time trend\n\\[\\begin{align*}\nY_{it} =  \\alpha_i + \\beta_1 x_{it}+ \\beta_2 year_{t} + \\beta_3 year_{t}^2 + \\beta_4year_{t}^3 + \\epsilon_{it}.\n\\end{align*}\\]\n\n\n12.4.3 First Differences\nThe within- and LSDV estimators are not the only way to incorporate fixed effects into our models. Another type of estimation is first differences.\n\\(\\Delta Y_{it} = \\beta \\Delta x_{it} + \\Delta \\epsilon_{it}\\) where, for example, \\(\\Delta Y_{it} = Y_{it} - Y_{it- 1}\\). Instead of demeaning over all time. We subtract the previous instance.\n\nBoth remove unobserved heterogeneity (i.e., \\(\\alpha_i - \\alpha_i\\))\nRequires variation in time. \\(t\\) vs. \\(t-1\\) or else \\(x_{it} - x_{it-1} = 0\\) and falls out\nWhen \\(T = 2\\), fixed effects = first differences\nWhen \\(T &gt; 2\\), fixed effects \\(\\neq\\) first differences\nUnder assumptions, both unbiased and consistent\nWhen no serial correlation, then \\(SE(\\hat \\beta_{FD}) &gt; SE(\\hat \\beta_{FE})\\)\nIf \\(\\Delta \\epsilon_{it}\\) are uncorrelated, FD preferred.\nIn general, hopefully both produce very similar results\n\nI recommend watching this video by Ben Lambert who provides a summary overview of the differences between fixed effects, first differences, and pooled OLS.\n\n\n\n12.4.4 Additional Models in R\n\n## first differences\nfit.fd &lt;- plm(inv~value+capital, data = Grunfeld, model = \"fd\", \n              index = c(\"firm\", \"year\"),\n                  effect = \"individual\")\ncoef(fit.fd)[\"capital\"]\n\n  capital \n0.2917667 \n\n## firm and year effects\nfit.twoway &lt;- plm(inv~value+capital, data = Grunfeld, model = \"within\",\n                  index = c(\"firm\", \"year\"),\n                  effect = \"twoways\")\n                  \ncoef(fit.twoway)[\"capital\"]\n\n  capital \n0.3579163"
  },
  {
    "objectID": "12-PanelData.html#random-effects",
    "href": "12-PanelData.html#random-effects",
    "title": "12  Panel and Hierarchical Data",
    "section": "12.5 Random Effects",
    "text": "12.5 Random Effects\nWe have now discussed pooled OLS and fixed effects estimation (often called no-pooling). We will now shift to estimation strategies that involve partial-pooling.\nTradeoff between complete pooling and no-pooling (i.e., fixed effects).\n\nComplete pooling ignores variation between groups BUT\nNo-pooling may overfit the data within group and prevents estimation of “level-2” effects.\n\nTo motivate partial-pooling, we can take a look at an example.\nExample: What will Mookie Betts’s batting average be in 2018?\n\n\nIn March 2018, Mookie went 2 for 11. If we fit his average just based on his data \\(n_{mookie}=11\\), we get an average of .182.\nOver the entire season, Mookie ended up with an average of .346. Is there any way we could have adjusted our initial estimate to end up with a more accurate estimate of what his average would be?\nPossible solution: fit the average based on a combination of Mookie’s data (\\(\\bar y_{mookie}\\)) and data from all players (\\(\\bar y_{all}\\)). It will move Mookie’s estimate closer to the “grand” average across all players by partially pooling his data with the others.\n\n\n12.5.1 Random effects models\nIn random effects estimation, for the \\(ith\\) observation, we allow the intercept \\(\\alpha\\) to vary by some unit \\(j\\). Where \\(j[i]\\) refers to the group-level coding for the \\(ith\\) observation. Perhaps \\(j=3\\) for \\(j[4]\\), the 4th observation.\n\\[\\begin{align*}\nY_{ij} &=  \\alpha_{j[i]} + \\beta x_{ij} + \\epsilon_{ij}\n\\end{align*}\\]\nThe group index is a factor with \\(J\\) levels.\nApproximation of multilevel estimates of the group average in case of no predictors:\n\\(\\hat \\alpha_j = \\frac{\\frac{n_j}{\\sigma^2_y}\\bar y_j + \\frac{1}{\\sigma^2_\\alpha}\\bar y_{all}}{\\frac{n_j}{\\sigma^2_y} + \\frac{1}{\\sigma^2_\\alpha}}\\)\n\nAssume we have a random sample of \\(n\\) units within each \\(j\\), \\(n_j\\)\nThen, our estimate for a given group \\(j\\) is a weighted average of observations within \\(j\\) (\\(\\bar y_j\\)) and the mean overall \\(j\\)’s (\\(\\bar y_{all}\\)).\nThese are weighted according to the variance in \\(y\\) within \\(j\\) (\\(\\sigma^2_y\\)) and variance among the averages of \\(y\\) across \\(j\\) (\\(\\sigma^2_\\alpha\\))\n\nThis video by Ben Lambert shows how this idea of a “weighted average” extends to regression with covariates. Random effects provides a balance between fixed effects and OLS, no-pooling and pooling, which is why it is considered partial pooling.\n\nWhen \\(\\sigma^2_\\alpha\\) is very small, random effects will be close to OLS.\nIf \\(\\sigma^2_\\alpha\\) is very big, then the weighted balance will be closer to fixed effects.\n\nDifferences with pooling and no pooling\n\nPooling- intercepts all fixed to \\(\\alpha\\) (\\(\\sigma^2_{\\alpha} = 0\\))\nNo pooling- \\(\\alpha_j\\)’s correspond to models fit within each \\(j\\). Do not come from a common distribution.\n\nA downside: cannot include time-invariant group characteristics.\n\nPartial pooling (shrinkage)- \\(\\alpha_j\\)’s have a probabilitiy distribution \\(\\alpha_j \\sim N(u_{\\alpha}, \\sigma^2_{\\alpha})\\). Has the effect of pulling estimates of \\(\\alpha_j\\) toward the mean of all groups.\n\n\\(\\alpha_j = u_{\\alpha} + \\eta_j\\) where \\(\\eta_j\\) is a group-level error term (model without group-level predictors)\nMust assume \\(\\mathbf E(x_{ij}\\alpha_{j[i]}) = 0\\). Unmodeled, unmeasured characteristics about your group-level effects (e.g., countries) that affect the outcome are not correlated with the regressors in your model. \nGood news: Can include group-level predictors: Now \\(\\alpha_j\\) coeffcients have a distribution \\(\\alpha_j \\sim N(U_j\\gamma, \\sigma^2_\\alpha)\\)\n\n\nScholars can have strong feelings about these choices, which often vary by subfield conventions along with the specifics of any research question. For example, the Green et al. piece advocating the use of fixed effects called pooling a ``Dirty Pool.” In response, Beck and Katz made the analogy that “Green, Kim, and Yoon’s fixed-effects”cure” for column 3 is akin to curing a cold with chemotherapy” (492).\n\n\n12.5.2 Random Effects Implementation in R\nIf comparing with fixed effects, can useplm.\n\n## Firm random effects\ndata(\"Grunfeld\")\nfit.re &lt;- plm(inv~value+capital, \n              data = Grunfeld, \n              model = \"random\", \n                index = c(\"firm\"), \n              effect = \"individual\")\ncoef(fit.re)[\"capital\"]\n\n capital \n0.308113 \n\n\nOne approach to deciding whether to use fixed vs. random effects is the Hausman test, which assesses the correlation assumption.\n\n## Compares beta hats for FE and RE and covariance\n## Under null hypothesis of no correlation estimates should be similar\n## Under null, RE preferred due to efficiency gains\nfit.within &lt;- plm(inv~capital+value, \n                  data = Grunfeld, \n                  model = \"within\", \n                  index = c(\"firm\", \"year\"))\nphtest(fit.within, fit.re) \n\n\n    Hausman Test\n\ndata:  inv ~ capital + value\nchisq = 2.3304, df = 2, p-value = 0.3119\nalternative hypothesis: one model is inconsistent\n\n\nWith a small p-value, we reject the null, suggesting fixed effects is preferred for this model. With a large p-value, we fail to reject the null, suggesting random effects is preferred. Note: while the test provides one way to adjudicate between fixed and random effects, it cannot tell you if the model you have specified is “correct.”\n\n\n12.5.3 Using lme4 for random effects\nMore flexible package for random effects: lme4. Uses “REML” by default- a variation on MLE. Ben Bolker provides a great overview of the package here. Here is a video walking through the syntax and output.\n\n\n\n\n\n\n\n\n\n\nlibrary(lme4)\nfit.re2 &lt;- lmer(inv~ value + capital + (1 | firm), data = Grunfeld)\n\n## extract model coefficients\nfixef(fit.re2)[\"capital\"]\n\n  capital \n0.3081881 \n\n\nWHERE ARE MY P VALUES?!?!?!?!\n\nsummary(fit.re2)$coefficients\n\n               Estimate  Std. Error   t value\n(Intercept) -57.8644245 29.37775852 -1.969668\nvalue         0.1097897  0.01052676 10.429581\ncapital       0.3081881  0.01717127 17.947893\n\n\nNot without controversy. Several ways to compute p-values in these models. This post describes three and provides the code.\nFor example, if you load the package lmerTest prior to running the model, it will include these results using the Satterthwaite method by default. Clicking on the pdf through this link will give you information on what this package does.\n\nlibrary(lme4)\nlibrary(lmerTest)\nfit.re2 &lt;- lmer(inv~ value + capital + (1 | firm), data = Grunfeld)\n\nsummary(fit.re2)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: inv ~ value + capital + (1 | firm)\n   Data: Grunfeld\n\nREML criterion at convergence: 2195.9\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.4319 -0.3498  0.0210  0.3592  4.8145 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n firm     (Intercept) 7367     85.83   \n Residual             2781     52.74   \nNumber of obs: 200, groups:  firm, 10\n\nFixed effects:\n             Estimate Std. Error        df t value Pr(&gt;|t|)    \n(Intercept) -57.86442   29.37776  11.10912   -1.97   0.0743 .  \nvalue         0.10979    0.01053 120.76227   10.43   &lt;2e-16 ***\ncapital       0.30819    0.01717 193.71234   17.95   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n        (Intr) value \nvalue   -0.328       \ncapital -0.019 -0.368\nfit warnings:\nSome predictor variables are on very different scales: consider rescaling\n\n\nCan use output to interpret the nature of the variance. Note that the summary output includes a table for the Random Effects, the variance, and Std. Dev. of each. We can interpret the intercept row as the variability of the intercept across firms (firm-to-firm) variability. The residual is the left-over or within-firm variability. One use of this is to compute the intraclass correlation coefficient (ICC).\nThe ICC is the proportion of the variance explained by the grouping structure in the population. The Adjusted ICC indexes how strongly measurements in the same group resemble each other. This index goes from 0, if the grouping conveys no information, to 1, if all observations in a group are identical (Gelman & Hill, 2007, p. 258). If the value were really small, you might not need to include random effects for the grouping factor.\n\n# install.packages(\"performance\")\n7367/(7367+2781)\n\n[1] 0.7259559\n\nperformance::icc(fit.re2)\n\n# Intraclass Correlation Coefficient\n\n    Adjusted ICC: 0.726\n  Unadjusted ICC: 0.140\n\n\n\n\n12.5.4 Random Effects Extensions\nCan have multiple levels. E.g., students (\\(i\\)) nested in classrooms (\\(j\\)) nested in schools (\\(s\\)). Each level may or may not have group-level predictors. Example:\n\n\\(Performance_{ijs} = \\beta_{0js[i]} + \\beta_1 Income_{ijs} + \\beta_2 Gender_{ijs} + \\epsilon_{ijs}\\) where\n\\(\\beta_{0js} = \\gamma_{0s} + \\gamma_{3}Teacher_{js} + r_{js}\\) and\n\\(\\gamma_{0s} = z + v_{s}\\)\nCombined: \\(Performance_{ijs} = m + m_{1}Teacher_{js} + m_2 Income_{ijs} + m_3 Gender_{ijs} + v_{s} + r_{js} + \\epsilon_{ijs}\\)\n\nLevels can be nested or non-nested. Students are nested in schools, but for a different example, survey questions may not be nested in a single poll. Perhaps some items are repeated across different polls.\nCan allow slopes (regression coefficients to vary). Perhaps the effect of gender on student success varies by classroom. Then \\(\\beta_{2}\\) becomes \\(\\beta_{j, 2} = \\gamma_{1s} + r_{1js}\\).\nLet’s return to the Grunfeld data and fit the following model:\n\n## firm and year random intercepts\nfit.retwoways &lt;- lmer(inv~ value + capital + \n                        (1 | firm) + \n                        (1 | year), \n                      data = Grunfeld)\nfixef(fit.retwoways)[\"capital\"]\n\n  capital \n0.3106319 \n\n\nSleep deprivation study: how is a subject’s reaction time associated with sleep deprivation?\n\ndata(sleepstudy)\n## Random intercepts for subjects\nfit.re3 &lt;- lmer(Reaction ~ Days + (1 | Subject), data = sleepstudy)\nfixef(fit.re3)[\"Days\"]\n\n    Days \n10.46729 \n\n## Random intercepts for subject, varying slope for days\nfit.re4 &lt;- lmer(Reaction ~ Days + (Days | Subject), data = sleepstudy)\nfixef(fit.re4)[\"Days\"]\n\n    Days \n10.46729 \n\n\nRecall, the big assumption when adding random effects is that the unobserved group-level random disturbances caught through the intercept terms are not correlated with the regressors.\nOne suggestion is if concerned about correlation between predictors \\(x_i\\) and group effects \\(\\alpha_j\\), add group means of regressors as predictors in the model.\n\nSee here and here.\nDisentangles individual vs. contextual effect of regressors on outcome. First proposed by Mundlak (1978).\n\n\n\n12.5.5 Extracting lmer output\nCaution: coef(), ranef(), and fixef() all different.\n\ncoef() produces the set of regression coefficients associated with each group\n\n\n## Example from random intercepts model\nhead(coef(fit.re3))\n\n$Subject\n    (Intercept)     Days\n308    292.1888 10.46729\n309    173.5556 10.46729\n310    188.2965 10.46729\n330    255.8115 10.46729\n331    261.6213 10.46729\n332    259.6263 10.46729\n333    267.9056 10.46729\n334    248.4081 10.46729\n335    206.1230 10.46729\n337    323.5878 10.46729\n349    230.2089 10.46729\n350    265.5165 10.46729\n351    243.5429 10.46729\n352    287.7835 10.46729\n369    258.4415 10.46729\n370    245.0424 10.46729\n371    248.1108 10.46729\n372    269.5209 10.46729\n\n\n\nfixef() produces the set of fixed effects regression coefficients– similar to OLS– with global intercept.\n\n\n## Example from random intercepts model\nfixef(fit.re3)\n\n(Intercept)        Days \n  251.40510    10.46729 \n\n\n\nranef() produces the set of random effects as deviations from the global estimates\n\n\n## Example from random intercepts model\nhead(ranef(fit.re3)$Subject)\n\n    (Intercept)\n308   40.783710\n309  -77.849554\n310  -63.108567\n330    4.406442\n331   10.216189\n332    8.221238\n\n## Note the relationship between the three for the first [1] subject\nfixef(fit.re3)[1] + ranef(fit.re3)$Subject[1,]\n\n(Intercept) \n   292.1888 \n\ncoef(fit.re3)$Subject[1,1]\n\n[1] 292.1888\n\n\n\n\n12.5.6 Generating predicted values for each unit\n\n## Manual\nallsubjects &lt;- unique(sleepstudy$Subject)\nfor(i in 1:nrow(sleepstudy)){\n  n &lt;- as.character(sleepstudy$Subject[i])\n  sleepstudy$subjectRE[i] &lt;- ranef(fit.re3)$Subject[n, 1]\n}\n\n## XB like usual, but need to add random effects\nyhat &lt;-  model.matrix(fit.re3)%*%fixef(fit.re3) + sleepstudy$subjectRE\n\n## Compare to Automatic\ncbind(yhat, fitted(fit.re3), predict(fit.re3, type = \"response\"))[1:5,]\n\n      [,1]     [,2]     [,3]\n1 292.1888 292.1888 292.1888\n2 302.6561 302.6561 302.6561\n3 313.1234 313.1234 313.1234\n4 323.5907 323.5907 323.5907\n5 334.0580 334.0580 334.0580\n\n\nPredicting out of sample. Can “ignore” random effects if desired.\n\nThis is often how people make predictions for hypothetical values of the main covariates– without specifying specific groups.\n\n\npredict(fit.re3, newdata= data.frame(Days = 4), type = \"response\", re.form = NA)\n\n       1 \n293.2742 \n\nfixef(fit.re3)[1] + 4*fixef(fit.re3)[2]\n\n(Intercept) \n   293.2742 \n\n\nCompare predictions if we ignored the random effects:\n\ncbind(yhat, \n      predict(fit.re3, type = \"response\"),\n      predict(fit.re3, type = \"response\", re.form=NA),\n      model.matrix(fit.re3)%*%fixef(fit.re3))[1:5,]\n\n      [,1]     [,2]     [,3]     [,4]\n1 292.1888 292.1888 251.4051 251.4051\n2 302.6561 302.6561 261.8724 261.8724\n3 313.1234 313.1234 272.3397 272.3397\n4 323.5907 323.5907 282.8070 282.8070\n5 334.0580 334.0580 293.2742 293.2742\n\n\n\n\n12.5.7 Visualizing Random Effects\nIn this section, we are going to again use data from the CDC on national and state-level COVID hospitalizations and positive case increases between July 2020- end September 2020 to give some examples of how to visualize random effects.\nLet’s load the state data.\n\nlibrary(rio)\nstates &lt;- import(\"https://github.com/ktmccabe/teachingdata/blob/main/states.RData?raw=true\")\nhead(states)\n\n           date hospitalizedCurrently positiveIncrease state\n8849 2020-09-30                    53              104    AK\n8850 2020-09-30                   776             1147    AL\n8851 2020-09-30                   484              942    AR\n8853 2020-09-30                   560              323    AZ\n8854 2020-09-30                  3267             3200    CA\n8855 2020-09-30                   264              511    CO\n\n\nLet’s fit a random effects model with random intercepts for states.\n\n## Random Effects\nlibrary(lme4)\nlibrary(lmerTest)\nfit.re &lt;- lmer(hospitalizedCurrently ~ positiveIncrease \n               + (1 | state), data=states)\nsummary(fit.re)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: hospitalizedCurrently ~ positiveIncrease + (1 | state)\n   Data: states\n\nREML criterion at convergence: 72042.8\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-21.4649  -0.1578   0.0012   0.1414  12.3407 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n state    (Intercept) 439843   663.2   \n Residual             194044   440.5   \nNumber of obs: 4780, groups:  state, 53\n\nFixed effects:\n                  Estimate Std. Error        df t value Pr(&gt;|t|)    \n(Intercept)      3.273e+02  9.160e+01 5.131e+01   3.573 0.000779 ***\npositiveIncrease 5.317e-01  7.745e-03 4.774e+03  68.644  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr)\npositvIncrs -0.078\nfit warnings:\nSome predictor variables are on very different scales: consider rescaling\n\nperformance::icc(fit.re)\n\n# Intraclass Correlation Coefficient\n\n    Adjusted ICC: 0.694\n  Unadjusted ICC: 0.325\n\n\nThe function dotplot from the lattice package provides a fast way to visualize the random effects of the model.\n\nlibrary(lattice)\ndotplot(ranef(fit.re), condVar=TRUE)\n\n$state\n\n\n\n\n\nThe merTools package also has many useful functions. For example, the plotREsim function acts similar to the dotplot function and will plot the random effects for each group.\n\n## install.packages(\"merTools\")\nlibrary(merTools)\nplotREsim(REsim(fit.re), labs=TRUE)+\n  coord_flip()\n\n\n\n\nThe package also has a tool for plotting the coefficient results with confidence intervals constructed through simulation. If you had several variables, the plot would display all coefficients.\n\nfeEx &lt;- FEsim(fit.re, 1000)\nhead(feEx)\n\n              term        mean     median           sd\n1      (Intercept) 329.8490921 328.947370 94.191266324\n2 positiveIncrease   0.5318654   0.532214  0.007752293\n\nplotFEsim(feEx) +\n  theme_bw() + \n  labs(title = \"Coefficient Plot\",\n       x = \"Median Effect Estimate\", \n       y = \"Hospitalizations\")\n\n\n\n\nFor more on this tool’s capabilities, particularly as it relates to generating predicted values, see here.\nFinally, let’s try another model using the sleepstudy data to show how the plots can also incorporate varying slopes.\n\nlibrary(lme4)\ndata(\"sleepstudy\")\n## Random intercepts for subject, varying slope for days\nfit.re4 &lt;- lmer(Reaction ~ Days + (Days | Subject), data = sleepstudy)\n\n\nlibrary(merTools)\nplotREsim(REsim(fit.re4), labs=TRUE)+\n  coord_flip()\n\n\n\n\n\n\n12.5.8 Generalized LMER\nGeneralized Linear Mixed Effects Regression: glmer() is to lmer() as glm() is to lm(). Example:\n\n## What if reaction time was dichotomous? \nsleepstudy$rdi &lt;- ifelse(sleepstudy$Reaction &gt;\n                         mean(sleepstudy$Reaction), 1, 0)\n\n## GLMER with logit\nfit.glmer &lt;- glmer(rdi ~ Days + (1 | Subject), \n                   data = sleepstudy, \n                   family = binomial(link = \"logit\"))\n\nGenerating predicted probabilities for each unit\n\n## Manual\nallsubjects &lt;- unique(sleepstudy$Subject)\nfor(i in 1:nrow(sleepstudy)){\n  n &lt;- as.character(sleepstudy$Subject[i])\n  sleepstudy$subjectRE[i] &lt;- ranef(fit.glmer)$Subject[n, 1]\n}\n\n## XB like usual, but need to add random effects\npy &lt;-  plogis(model.matrix(fit.glmer)%*%fixef(fit.glmer) + sleepstudy$subjectRE)\n\n## Compare to Automatic\ncbind(py, fitted(fit.glmer), predict(fit.glmer, type = \"response\"))[1:5,]\n\n       [,1]      [,2]      [,3]\n1 0.1047843 0.1047843 0.1047843\n2 0.1721090 0.1721090 0.1721090\n3 0.2696603 0.2696603 0.2696603\n4 0.3960526 0.3960526 0.3960526\n5 0.5380430 0.5380430 0.5380430"
  },
  {
    "objectID": "12-PanelData.html#summing-up",
    "href": "12-PanelData.html#summing-up",
    "title": "12  Panel and Hierarchical Data",
    "section": "12.6 Summing Up",
    "text": "12.6 Summing Up\nIn your own research you might ask the following questions:\n\nAre my observations independent? Do I have constant error variance?\n\nIf independent, you might be fine with a standard model\nIf worried about the error variance, you could be fine with implementing some type of robust standard errors. See Gary King’s video for guidance and section 8.5.1 of the course notes.\n\nAlternatively, is there some grouping structure to my data?\n\nWhat are the different levels?\n\nAre observations repeated over time?\n\nLarge N, few T, generally considered panel data\nLarge T, fewer N, generally considered time series cross-sectional\n\n\nThen, you might consider the following choices if you have a grouping structure:\n\nConsider clustering standard errors by group in a standard model\n\nSee guidance here, here, and here. For R code, see section 8.5.1.\n\nOr, incorporating the grouping structure into a multilevel random effects model\n\nDo you meet the random effects assumptions?\nDo you need random effects? Perhaps use the ICC to help with this.\nShould you add varying slopes?\n\nOr, incorporating fixed effects for the groups and/or time\n\nHow much data do you have within groups?\nDo you need to model “level-2” factors?\n\n\nWhat we have not yet discussed are the following issues. See links for further study.\n\nUsing fixed effects models for causal inference, specifically\n\nSee Imai and Kim (2019), Imai and Kim (2020)\nAnd R packages PanelMatch and wfe\n\nIncorporating dynamics into longitudinal data: \\(Y_{it} = \\alpha_i + \\rho Y_{i t-1} + \\beta x_{it} + \\epsilon_{it}\\)\n\nOutcome of today is a function of the past outcome modified by new information. Consider including if you think past outcomes influence future outcomes.\n\\(\\rho\\) is an “autocorrelation” term such that \\(|\\rho| &lt; 1\\).\nProblem: Unless \\(\\rho = 0\\), correlation created between regressors and error term \\(\\rightarrow\\) strict exogeneity violated \\(\\rightarrow\\) Nickell Bias. In random effects models, \\(y_{it-1}\\) also correlated with any group-level effects \\(\\alpha_i\\).\nVideo explanation.\nConcern greatest in samples with small \\(T\\).\nA lot of debates on the inclusion of an LDV and “dynamic” models in general. See here\n\nAchen C. H. (2001) Why lagged dependent variables can suppress the explanatory power of other independent variables\nKeele, L. and Kelly N. J. (2005) Dynamic models for dynamic theories: the ins and outs of lagged dependent variables\nArjun Wilkins. (2017). To Lag or Not to Lag?: Re-Evaluating the Use of Lagged Dependent Variables in Regression Analysis\nArellano-Bond and Anderson-Hsiao estimators"
  },
  {
    "objectID": "12-PanelData.html#the-final-problem-set",
    "href": "12-PanelData.html#the-final-problem-set",
    "title": "12  Panel and Hierarchical Data",
    "section": "12.7 The Final Problem Set",
    "text": "12.7 The Final Problem Set\nPlease load donation.dta. This is a data set that includes an original survey of 2815 past campaign donors in the United States.\n\nlibrary(foreign)\ndon &lt;- read.dta(\"https://github.com/ktmccabe/teachingdata/blob/main/donation.dta?raw=true\")\n\nThe researchers randomly sampled survey respondents based on a list from the FEC, which contained the names of anyone who gave $200 or more to a candidate in the two years prior to 2012. The sampling process was designed to focus on donation behavior to the 22 senators who sought reelection in 2013–selecting people who might donate or might be pursued for a donation by one of these senators. The idea was that this sample would represent the senators’ potential “donorate”. Each observation is a senator-donor dyad: with 22 incumbent senators running for reelection and 2815 respondents, there are approximately 61,930 dyads (observations). For each observation, the data contain variables that indicate whether or not a respondent donated to the senator and how much that donation was, as well as the respondent’s total donations. Senators are required to record donations above $200, but some senators voluntarily report donations below this amount. Therefore, the data may include some donation totals between 0 and $200.\nKey Variables include:\n\ndonation: 1=made donation to senator, 0=no donation made\ntotal_donation: Dollar amount of donation made by donor to Senator.\ntotal_donation_all: Dollar amount of all FEC recorded donations made by this donor.\nmaxdon: Dollar amount of the donor’s largest donation.\nnumsens: Number of unique senators a donor gave to.\nnumdonation: Number of unique donations a donor gave.\nperagsen: policy agreement, percent issue agreement between donor and senator\nper2agchal: percent issue agreement between donor and the senator’s challenger\ncook: Cook competitiveness score for the senator’s race. 1 = Solid Dem or Solid Rep; 2 = Likely Dem or Likely Rep; 3 = Leans Dem or Leans Rep; 4 = Toss Up\nsame_state: 1=donor is from senator’s state, 0=otherwise\nsameparty: 1=self-identifies as being in the candidate’s party; 0 otherwise\nmatchcommf: 1=Senator committee matches donor’s profession as reported in FEC file; 0=otherwise\nNetWorth: Donor’s self-estimated net worth. 1=less than 250k, 2=250-500k; 3=500k-1m; 4=1-2.5m; 5=2.5-5m; 6=5-10m; 7=more than 10m\nIncomeLastYear: Donor’s household annual income in 2013. 1=less than 50k; 2=50-100k; 3=100-125k; 4=125-150k; 5=150-250k; 6=250-300k; 7=300-350k; 8=350-400k; 9=400-500k; 10=more than 500k\nidfold2: Donor’s self-described political ideology. 0 = moderate; 1=somewhat conservative or somewhat liberal; 2=conservative or liberal; 3=very liberal or very conservative\nnotwhite: Donor’s self-described ethnicity. 1=not white; 0=white\nmalerespondent: Donor’s self-described gender. 1=male; 0=female\nterms: The number of terms a legislator has been in office\nYearBorn: Donor’s year of birth.\nEdsum: Donor’s self-described educational attainment. 1=less than high school; 2=high school; 3=some college; 4=2-year college degree; 5=4-year college degree; 6=graduate degree\nideologue2: 1=respondent suggested the candidate’s ideological position was “extremely important” or if the respondent attached more importance to this factor than to both whether the “candidate could affect my industry or work” and “I know the candidate personally”, 0=otherwise\nIK2: 1=respondent suggested “I know the candidate personally” was “extremely important” or if the respondent attached more importance to this factor than to both whether the “candidate’s position on the issues is similar to mine” and “the candidate could affect my industry or work”, 0=otherwise\ndonor_id: donor respondent unique id (each respondent is in the data 22 times)\nThe data also include dummy variables for each senator (e.g., sendum1, sendum2)\n\nResearch Question: Does policy agreement motivate people to give to campaigns?\n\nConduct and present an analysis that can help answer the question.\n\n\n\nTry on your own, then expand to see the authors’ approach.\n\nThis dataset comes from “Ideologically Sophisticated Donors: WhichCandidates Do Individual Contributors Finance?” by Michael J. Barber, Brandice Canes-Wrone, and Sharece Thrower. It was published in the American Journal of Political Science in 2017.\nThe authors use a “rare events logit” with clustered standard errors by donor “in order to account for the fact that the decision to give to a candidate may be correlated with decisions regarding other campaigns.” They also use a standard logistic regression as a robustness check in the appendix. They conduct an analysis among the full data, as well as theoretically interesting senator-respondent combinations.\n\nFor information on rare events logits, as a special form of the logit model, see here, a response here, and R code here.\nThey also analyze the amount given by transforming the variable into an incremental count outcome. “Because donations tend to be given in $50 increments, we use a rank-ordered variable that equals 0 for $0, 1 for $1–$49, 2 for $50–$99, and so on. Supplemental Table A9 shows that the results are robust to using the exact dollar amount, a rank based on $100 increments, and one based on $500 increments. Because donations are capped at $5,000, all of these dependent variables reflect this maximum. Moreover, as with the probability of donating, there is an overdispersion of zeros representing cases where a donor did not give to a senator. We accordingly use a zero-inflated negative binomial regression model for analyses of the amount donated. Additionally, for purposes of comparison, we show results for Tobit specifications.”\n\nWe discussed zero-inflated models in section 10.9.2\nWe discussed Tobit models in 11.4\n\nIn our live discussion, we came up with several different possibles approaches, frequently using binary logistic regression with donation as the outcome or a linear model with total_donation as the outcome. Some suggested specifications also included fixed effects for the donors using the plm package and fitting a within model. Finally, we explored using random effects in plm or lme4, though the lme4 approach appeared very stressful for our computers!\nOverall, we came up with consistent support for the positive relationship between policy agreement and giving to campaigns. Here are a few approaches developed:\n\nfit &lt;- glm(donation ~ peragsen + \n             terms + same_state + \n             idfold2, data=don, \n           family=binomial(link = \"logit\"))\n\nfit2 &lt;- glm(donation ~ peragsen + cook +\n              NetWorth + idfold2 + notwhite + malerespondent + terms,\n            data=don, \n            family=binomial(link = \"logit\"))\n\nfit3 &lt;- glm(donation ~ peragsen + cook +\n              NetWorth + idfold2 + notwhite + malerespondent + terms + sameparty,\n            data=don, \n            family=binomial(link = \"logit\"))\n\nlibrary(plm)\n## remove missing data from the fixed effect to get plm to work\ndon2 &lt;- subset(don, is.na(donor_id) ==F)\nfit5 &lt;- plm(total_donation ~ peragsen, data = don2,\n            model = \"within\",\n            index = c(\"donor_id\"))\n\n## random effects\nfit6 &lt;- plm(total_donation ~ peragsen, data = don2,\n            model = \"random\",\n            index = c(\"donor_id\"))\n\nlibrary(texreg)\nhtmlreg(list(fit, fit2, fit3, fit5, fit6),\n        custom.model.names = c(\"Logit\", \"logit\", \"logit\", \"Lin. FE\", \"Lin. RE\"))\n\n\n\nStatistical models\n\n\n\n\n \n\n\nLogit\n\n\nlogit\n\n\nlogit\n\n\nLin. FE\n\n\nLin. RE\n\n\n\n\n\n\n(Intercept)\n\n\n-4.38***\n\n\n-6.38***\n\n\n-6.20***\n\n\n \n\n\n-4.40\n\n\n\n\n \n\n\n(0.08)\n\n\n(0.14)\n\n\n(0.14)\n\n\n \n\n\n(3.39)\n\n\n\n\nperagsen\n\n\n2.02***\n\n\n2.64***\n\n\n1.62***\n\n\n97.09***\n\n\n90.87***\n\n\n\n\n \n\n\n(0.11)\n\n\n(0.11)\n\n\n(0.14)\n\n\n(5.19)\n\n\n(4.99)\n\n\n\n\nterms\n\n\n-0.13***\n\n\n-0.07*\n\n\n-0.05\n\n\n \n\n\n \n\n\n\n\n \n\n\n(0.03)\n\n\n(0.03)\n\n\n(0.03)\n\n\n \n\n\n \n\n\n\n\nsame_state\n\n\n2.65***\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n\n\n \n\n\n(0.05)\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n\n\nidfold2\n\n\n-0.19***\n\n\n-0.17***\n\n\n-0.23***\n\n\n \n\n\n \n\n\n\n\n \n\n\n(0.02)\n\n\n(0.02)\n\n\n(0.02)\n\n\n \n\n\n \n\n\n\n\ncook\n\n\n \n\n\n0.28***\n\n\n0.29***\n\n\n \n\n\n \n\n\n\n\n \n\n\n \n\n\n(0.02)\n\n\n(0.02)\n\n\n \n\n\n \n\n\n\n\nNetWorth\n\n\n \n\n\n0.22***\n\n\n0.21***\n\n\n \n\n\n \n\n\n\n\n \n\n\n \n\n\n(0.01)\n\n\n(0.01)\n\n\n \n\n\n \n\n\n\n\nnotwhite\n\n\n \n\n\n-0.37***\n\n\n-0.38***\n\n\n \n\n\n \n\n\n\n\n \n\n\n \n\n\n(0.11)\n\n\n(0.11)\n\n\n \n\n\n \n\n\n\n\nmalerespondent\n\n\n \n\n\n0.30***\n\n\n0.32***\n\n\n \n\n\n \n\n\n\n\n \n\n\n \n\n\n(0.05)\n\n\n(0.06)\n\n\n \n\n\n \n\n\n\n\nsameparty\n\n\n \n\n\n \n\n\n0.85***\n\n\n \n\n\n \n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.07)\n\n\n \n\n\n \n\n\n\n\nAIC\n\n\n16167.66\n\n\n15151.83\n\n\n14935.63\n\n\n \n\n\n \n\n\n\n\nBIC\n\n\n16212.69\n\n\n15222.81\n\n\n15015.45\n\n\n \n\n\n \n\n\n\n\nLog Likelihood\n\n\n-8078.83\n\n\n-7567.92\n\n\n-7458.81\n\n\n \n\n\n \n\n\n\n\nDeviance\n\n\n16157.66\n\n\n15135.83\n\n\n14917.63\n\n\n \n\n\n \n\n\n\n\nNum. obs.\n\n\n60280\n\n\n52734\n\n\n52536\n\n\n62544\n\n\n62544\n\n\n\n\nR2\n\n\n \n\n\n \n\n\n \n\n\n0.01\n\n\n0.01\n\n\n\n\nAdj. R2\n\n\n \n\n\n \n\n\n \n\n\n-0.04\n\n\n0.01\n\n\n\n\ns_idios\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n316.11\n\n\n\n\ns_id\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n83.38\n\n\n\n\n\n\n***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05\n\n\n\n\n\nWe also discussed that one could use clustered standard errors like the authors after fitting a logistic regression model. Here is an example using fit3.\n\nfit3 &lt;- glm(donation ~ peragsen + cook +\n              NetWorth + idfold2 + notwhite + malerespondent + terms + sameparty,\n            data=don, \n            family=binomial(link = \"logit\"))\n\nlibrary(sandwich)\nclval &lt;- vcovCL(fit3, type=\"HC0\", cluster = don$donor_id)\nlibrary(lmtest)\nnewfit3sum &lt;- coeftest(fit3, vcov=clval)\n\nYou can override the inputs in the texreg functions to incorporate these adjustments. For example:\nhtmlreg(list(fit3),\n        override.se = newfit3sum[, 2],\n        override.pvalues = newfit3sum[, 4])\n\n\n\nStatistical models\n\n\n\n\n \n\n\nModel 1\n\n\n\n\n\n\n(Intercept)\n\n\n-6.20***\n\n\n\n\n \n\n\n(0.17)\n\n\n\n\nperagsen\n\n\n1.62***\n\n\n\n\n \n\n\n(0.16)\n\n\n\n\ncook\n\n\n0.29***\n\n\n\n\n \n\n\n(0.02)\n\n\n\n\nNetWorth\n\n\n0.21***\n\n\n\n\n \n\n\n(0.02)\n\n\n\n\nidfold2\n\n\n-0.23***\n\n\n\n\n \n\n\n(0.03)\n\n\n\n\nnotwhite\n\n\n-0.38**\n\n\n\n\n \n\n\n(0.15)\n\n\n\n\nmalerespondent\n\n\n0.32***\n\n\n\n\n \n\n\n(0.08)\n\n\n\n\nterms\n\n\n-0.05\n\n\n\n\n \n\n\n(0.03)\n\n\n\n\nsameparty\n\n\n0.85***\n\n\n\n\n \n\n\n(0.10)\n\n\n\n\nAIC\n\n\n14935.63\n\n\n\n\nBIC\n\n\n15015.45\n\n\n\n\nLog Likelihood\n\n\n-7458.81\n\n\n\n\nDeviance\n\n\n14917.63\n\n\n\n\nNum. obs.\n\n\n52536\n\n\n\n\n\n\n***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05"
  },
  {
    "objectID": "13-SurveyData.html#whats-the-fuss",
    "href": "13-SurveyData.html#whats-the-fuss",
    "title": "13  Survey Data",
    "section": "13.1 What’s the fuss?",
    "text": "13.1 What’s the fuss?\nLet’s first discuss why we might even want special methods for analyzing survey data in the first place.\nWhen you have survey data, oftentimes as researchers are interested in making inferences from a sample available in the survey to a broader population. However, there may be many ways in which our sample is not representative of the population. The represenativeness of a survey is one component of what is called the “total survey error” framework.\n\nMatt Salganik’s book Bit by Bit describes this in section 3.3\n\nIn the ideal case, we would draw a simple random sample from the population using probability sampling.\n\nSample n units from an N population where each unit has equal probability of being sampled. We can get sample statistics very simply in this case.\n\nFor example, the simple mean of a variable measured in the sample would be equivalent to the expected value in the population.\n\n\n\\[\\begin{align*}\n\\bar y &= \\frac{1}{n}\\sum_{i=1}^n y_i\\\\\n\\mathbf E(\\bar y) &= \\mu\n\\end{align*}\\]\nIn this estimator, we assume that all sample units \\(i\\) represent the same number of units in the population. When could this go wrong? Basically, almost any time we actually try to sample from a population in practice. It is very, very hard to get simple random samples of the population.\nSometimes the way survey samples are collected are through complex designs. Example:\nStratified sample of the United States\n\nConsider each county a “strata”\nConduct a random sample within strata\nWhy? If you conduct a simple random sample at the individual-level, just by chance you might not not sample within each county\n\nClustered random sample within the United States\n\nConsider each county\nSample \\(m\\) number of counties (these are considered to be “clusters”)\nSample within each cluster\n\nWhen you have generated a sample using cluster or stratified sampling, it is best to account for this data generating process in the analysis to get accurate estimates for sample averages and variances. The R package we will use will account for this.\n\n13.1.1 Weighting Surveys\nIn many cases, our survey data are not perfectly representative of the population. In these cases, often a researcher might want to employ weights to adjust the estimates calculated from a sample in order to make them more accurate for the population of interest.\n\nWhat was the problem: Each unit \\(i\\) in the sample no longer represents the same number of units in the cluster, strata, or population. Some types of people might be “overrepresented” in the sample relative to the population, and some might be underrepresented.\nOne Solution: Weights \\(w\\) to reflect/ adjust for the number of units in the population that the sampled unit \\(i\\) represents.\nWhat do we need? Auxiliary information about the target population so we know how to adjust the data\n\nWhere do we get this? Sometimes it is provided by survey firms or described in the codebook of existing surveys. Other times, you could consider constructing your own weights.\n\n\nExample: \\[\\begin{align*}\n\\bar y &= \\sum_{i=1}^n y_i*w_i\\\\\n\\end{align*}\\]\nWhen will weights matter?\n\nWeights will matter particularly when your data are unrepresentative on characteristics that directly influence your outcomes of interest.\n\nFor example, let’s say you had a survey that was representative except for age. You had too many young people in the sample relative to the population. Let’s say you are interested in predicting the proportion of people who voted for Biden in 2020. If age did not matter for vote choice, then it might not matter that your survey was unrepresentative by age. But, if age does matter, then the unweighted estimate from your sample might be biased!\n\n\nWhere can weights go wrong?\n\nLet’s be real here. Weighting is an art as much or more than a science.\n\nIt is not always immediately obvious what the target population is (e.g., if the target population is the set of people who will vote in an election).\nIt is not always immediately obvious how to get accurate data on this target population, even if known (e.g., not every population as up-to-date Census information).\nIt is not always obvious which variables to choose to weight on (e.g., which demographics?)\nYou may also not have all variables of interest available in your sample or at the population level.\nMissing cells. Even if you have all of the above, weighting can still be insufficient if your sample simply does not contain certain subgroups of the population or contains too few members of a certain subgroup of the population. (e.g., suppose your sample only includes 18-25 year olds – it will be hard to infer things about older populations.)\n\n\nFor example, The Upshot gave different polling firms the same survey data. They each came up with different estimates of vote choice in 2016 due to small differences in choices about weighting and identifying the target population.The linked article describes the different choices the pollsters likely made.\n\n\n\n13.1.2 Broad Types of Survey Sampling Techniques\nSurvey sampling techniques are sometimes broken into two types: non-probability and probability sampling.\nIn probability samples, the probability that a respondent is selected for the survey is known, and this helps ensure that the sample will be representative. However, what is becoming increasingly less known, is the probability that a person will respond to a survey. Because this is less well-known, even probability samples will need adjustments for weighting to get accurate estimates.\nNon-probability samples come in many different forms, such as those that use “quota sampling,” convenience samples from online labor markets, or more sophisticated algorithms for choosing respondents that reflect the population. Pew describes different types of non-probability samples here.\nMatt Salganik, as part of the curriculum for the Summer Institutes in Computational Social Science, provides a nice overview of the tradeoffs between probability and non-probability samples and what this means for survey weighting.\n\nPew provides a somewhat more pessimistic take on nonprobability samples in this report."
  },
  {
    "objectID": "13-SurveyData.html#survey-r-package",
    "href": "13-SurveyData.html#survey-r-package",
    "title": "13  Survey Data",
    "section": "13.2 Survey R package",
    "text": "13.2 Survey R package\nLet’s assume for now that we have some survey data, and those data contain some information about the sampling process, as well as survey weights. With this information, we are ready to use the survey package to conduct analysis.\nThe survey package by Thomas Lumley allows you to declare the nature of the survey design.\n\nLoad your dataset as usual into a dataframe in R\nConstruct the variables you will use in analysis\nDeclare to R the survey design by assigning your data frame as a new survey object.\nPerform analyses using functions from the survey package\n\nsvydesign(data = …, weights= …, fpc= …, strata = …, id =…)\n\ndata: data frame\nweights: sampling weights\nfpc: Finite population correction (less common)\nstrata: Formula or vector specifying strata (NULL by default)\nids: Primary sampling unit. Formula or data frame with cluster ids. Use ~1 or ~0 if no clusters.\n\n\n13.2.1 Examples of specifying svydesigns\nThe easiest, but also common, way to specify a survey design occurs when you have survey data and a column of survey weights that the survey firm has provided– no clusters or strata.\nLet’s look at this using data from within the survey package. The apisrs represents a simple random sample with a column pw for survey sampling weights. We can specify the design below:\n\nSee also, a detailed example from Pew analyzing one of their datasets with the survey package.\n\nExample: Sample of students in California.\n\nlibrary(survey)\ndata(api) # loads several dataframes \n\n## Sample with just weights, no clusters/strata\nhead(apisrs)\n\n                cds stype            name                         sname snum\n1039 15739081534155     H  McFarland High                McFarland High 1039\n1124 19642126066716     E Stowers (Cecil  Stowers (Cecil B.) Elementary 1124\n2868 30664493030640     H Brea-Olinda Hig              Brea-Olinda High 2868\n1273 19644516012744     E Alameda Element            Alameda Elementary 1273\n4926 40688096043293     E Sunnyside Eleme          Sunnyside Elementary 4926\n2463 19734456014278     E Los Molinos Ele        Los Molinos Elementary 2463\n                        dname dnum           cname cnum flag pcttest api00\n1039        McFarland Unified  432            Kern   14   NA      98   462\n1124              ABC Unified    1     Los Angeles   18   NA     100   878\n2868      Brea-Olinda Unified   79          Orange   29   NA      98   734\n1273           Downey Unified  187     Los Angeles   18   NA      99   772\n4926 San Luis Coastal Unified  640 San Luis Obispo   39   NA      99   739\n2463  Hacienda la Puente Unif  284     Los Angeles   18   NA      93   835\n     api99 target growth sch.wide comp.imp both awards meals ell yr.rnd\n1039   448     18     14       No      Yes   No     No    44  31   &lt;NA&gt;\n1124   831     NA     47      Yes      Yes  Yes    Yes     8  25   &lt;NA&gt;\n2868   742      3     -8       No       No   No     No    10  10   &lt;NA&gt;\n1273   657      7    115      Yes      Yes  Yes    Yes    70  25   &lt;NA&gt;\n4926   719      4     20      Yes      Yes  Yes    Yes    43  12   &lt;NA&gt;\n2463   822     NA     13      Yes      Yes  Yes     No    16  19   &lt;NA&gt;\n     mobility acs.k3 acs.46 acs.core pct.resp not.hsg hsg some.col col.grad\n1039        6     NA     NA       24       82      44  34       12        7\n1124       15     19     30       NA       97       4  10       23       43\n2868        7     NA     NA       28       95       5   9       21       41\n1273       23     23     NA       NA      100      37  40       14        8\n4926       12     20     29       NA       91       8  21       27       34\n2463       13     19     29       NA       71       1   8       20       38\n     grad.sch avg.ed full emer enroll api.stu    pw  fpc\n1039        3   1.91   71   35    477     429 30.97 6194\n1124       21   3.66   90   10    478     420 30.97 6194\n2868       24   3.71   83   18   1410    1287 30.97 6194\n1273        1   1.96   85   18    342     291 30.97 6194\n4926       10   3.17  100    0    217     189 30.97 6194\n2463       34   3.96   75   20    258     211 30.97 6194\n\n## Specify design\napisrs_design &lt;- svydesign(data = apisrs, \n                             weights = ~pw, \n                             id = ~1)\n\nExample: Sample of students in California. Samples stratified by School type\nHere is an example with a stratified random sample with a known finite population variable. Here, we had the fpc column and strata column stype. We still supply the weights pw.\n\n## Use apistrat\napistrat_design &lt;- svydesign(data = apistrat, \n                             weights = ~pw, \n                             fpc = ~fpc, # population of school types known\n                             id = ~1, # no clusters\n                             strata = ~stype)\n\nExample: Sample of students in California by clustering.\nHere is a more complex example.\n\nSamples clustered by school districts dnum.\nWithin district, five schools were sampled snum.\nFpc for district and school are fpc1 and fpc2\n\n\napiclus_design &lt;- svydesign(data = apiclus2, \n                             weights = ~pw, \n                             fpc = ~fpc1 + fpc2, \n                             id = ~dnum + snum) # start from biggest clu\n\nFor more established surveys, often the codebook might include documentation on how to account for survey weights and sampling design. For example, the American National Election Study will generally include this guidance in recent codebooks. Here is an example accounting for the sampling design in the 2016 ANES.\nYou can use foreign, rio, or what we will use here, haven to load the .dta data.\n\nlibrary(haven)\nan &lt;- read_dta(\"https://github.com/ktmccabe/teachingdata/blob/main/anes_timeseries_2016_Stata12.dta?raw=true\")\n\nOne benefit of loading data through haven and some of the newer packages is it will help retain value labels from the original data that R might not otherwise load. (E.g., labels that would show up in SPSS or Stata but seem lost in R.)\nLet’s check this for an example: “How well does feminist describe you?\n\nlibrary(tidyverse)\nan$V161346 %&gt;% attr(\"labels\")\n\n                                   -9. Refused \n                                            -9 \n-5. Interview breakoff (sufficient partial IW) \n                                            -5 \n                             1. Extremely well \n                                             1 \n                                  2. Very well \n                                             2 \n                              3. Somewhat well \n                                             3 \n                              4. Not very well \n                                             4 \n                                 5. Not at all \n                                             5 \n\n\nOK enough about that. Let’s specify the survey design.\n\nanes_design &lt;-\n    svydesign( \n        ids = ~V160202 , \n        strata = ~V160201 , \n        data = an, \n        weights = ~V160102,\n        nest = TRUE)\n\nThe nest argument indicates if cluster ids should be relabeled to align with strata.\n\n\n13.2.2 Working with survey designs\nThe good news is that once you have specified the survey design, the rest of the analysis will flow similarly across types of survey designs. The bad news is that you will need to use special functions to analyze the data instead of the functions we are used to working with. These generally start with svy….. but the good news is, they are not that different from our normal functions!\nHere are a few examples of common functions using the survey design.\n\n## Means and uncertainty of variables\n## Note: we put design where you would normally see data\nsvymean(~mobility, design = apiclus_design)\n\n           mean    SE\nmobility 18.154 1.471\n\nSE(svymean(~mobility, design = apiclus_design))\n\n         mobility\nmobility 1.471006\n\nconfint(svymean(~mobility, design = apiclus_design))\n\n            2.5 %   97.5 %\nmobility 15.27039 21.03662\n\n\n\n## Linear Regression\nfit.lin &lt;- svyglm(mobility ~ meals + awards, family = \"gaussian\",\n       design = apiclus_design)\n\n## Logit Regression\nfit.logit &lt;- svyglm(awards ~ meals, \n              family = binomial(link = \"logit\"),\n       design = apiclus_design)\n\nWarning in eval(family$initialize): non-integer #successes in a binomial glm!\n\n\nFor an example of ordinal logistic regression see section 8.6 of the course notes.\n\n## Can also subset like normal with subset command\napiclus_design2 &lt;- subset(apiclus_design, meals &gt; 1)\n\n\nThe Pew research center has also written a tutorial for using various tidyverse tools with survey data. See here for details.\nIn addition, there has been a package srvyr recently developed that incorporates more tidyverse language with the same set of survey tools. See this tutorial for details and some analysis examples.\nOur friends the marginaleffects, prediction and margins packages have some capabilities for models fit through the survey package.\n\nWith the margins package, you will want to specify the design in order to make sure the average marginal effects reported are based on the survey design object. Example:\n\n\n\nlibrary(margins)\nres &lt;-margins(fit.logit, variables = \"meals\",\n              design=apiclus_design, \n              change=\"iqr\")\nsummary(res)\n\n factor     AME     SE       z      p   lower  upper\n  meals -0.1018 0.0592 -1.7208 0.0853 -0.2178 0.0142\n\n\n\n\n13.2.3 Why do we need the survey package?\nThe benefits of the survey package really come from the tool’s specialty in calculating appropriate variances for the survey design and accounting for very complex stratified or clustered designs. There are other weighting functions in R that will allow you to recover the same point estimates as the survey tools in some cases. However, it won’t always be the case that they will give you accurate measures of uncertainty.\nIf you don’t need measures of uncertainty, then other functions from base R, such as weighted.mean can also work in cases where you just have a column of survey weights.\n\nweighted.mean(apisrs$mobility, w=apisrs$pw)\n\n[1] 17.46\n\nsvymean(~mobility, design = apisrs_design)\n\n          mean     SE\nmobility 17.46 0.6529"
  },
  {
    "objectID": "13-SurveyData.html#constructing-your-own-weights",
    "href": "13-SurveyData.html#constructing-your-own-weights",
    "title": "13  Survey Data",
    "section": "13.3 Constructing your own weights",
    "text": "13.3 Constructing your own weights\nSometimes you might have collected your own survey data, which you know is not representative, but no weights are provided for you.\nIf you think that weighting the data will be important, then you can construct your own weights! We can:\n\nCollect information on our respondents to compare sample to population (including demographic questions on your survey)\nGather statistics on the population (e.g., Census data)\nConstruct weights to indicate how many units in the population each \\(i\\) unit in our sample should represent\n\nOne example of constructing your own weights is through a process called Raking. Here, we find weights so that the weighted distributions of your variables in your sample match the distributions of the variables in the general population very closely. How?\n\nDecide the variables on which you want to weight your data (e.g., gender, education, age)\nFind the proportion that each subgroup within these variables exist in your target population (e.g., maybe .55 women, .45 men)\nUse a raking algorithm to adjust your sample data to, when weighted, reflect these distributions in the population. For example, perhaps your sample had 70% women. After raking, we would hope that when svymean is applied, you would find the weighted proportion in your data is 55%.\n\n\n\nThis R bloggers tutorial goes through an example using the survey package and function rake.\nIn addition, Pew has released an R package with similar weighting capabilities described here.\nRaking is not the only type of weighting that is possible. Pew describes and evaluates other weighting processes here.\nSeveral pollsters interested in measuring vote choice and public opinion have started turning to a process called multilevel regression and post-stratfication (MRP or, fondly, Mr. P).\n\nThis “multilevel” refers to the same multilevel modeling approach we discussed, using lme4. An R introduction to this is here with a companion paper here that has citations to political science examples using MRP.\nThe technique has been around for a long time, but now is starting to infiltrate mainstream polling in addition to political science.\n\nSee Andy Gelman’s take on it here along with a debate with Nate Silver here.\nFor example, CBS used MRP in their 2020 election tracking .\nDoug Rivers provided a short course on MRP at an AAPOR conference. Slides here."
  },
  {
    "objectID": "14-SurvivalData.html#survival-overview",
    "href": "14-SurvivalData.html#survival-overview",
    "title": "14  Survival Data",
    "section": "14.1 Survival Overview",
    "text": "14.1 Survival Overview\nSurvival data is also known as event history or duration data analysis. When we have this type of data, we are generally interested in these types of questions:\n\nHow long does something last?\n\nExamples: length of conflict, length of peace agreement, length of congressional career\n\nWhat is the expected time to an event?\nAnd how does this duration differ across subgroups?\nHow do covariates influence this duration?\n\nThere are two key components to survival data: time (e.g., days, months, years. etc) and the event of interest or “status” (i.e., whether an event has occurred). Canonically this could be an event such as death, but in political science this might be an event like the experience of a conflict, end of conflict, end of regime, etc.)\n\n14.1.1 Survival and hazard functions\nFor example, we might call something “survival” analysis in a context where we were interested in the time to death of someone who has just had a particular medical diagnosis.\nWe will have two primary components: the survival function \\(S(Y)\\):\n\nThis gives the probability that the duration of survival (time to event) is longer than some specified time (\\(Pr(Y_i &gt; y)\\))\n\nTime to failure \\(Y_i\\) as outcome: \\(Y_i \\geq 0\\)\n\n\\(S(y) = Pr(Y_i &gt; y) = 1 - Pr(Y_i \\leq y)\\)\n\nwhere \\(Pr(Y_i \\leq y)\\) is the CDF\n\nPDF \\(f(y) = - \\frac{d}{dy} S(y)\\)\nThis is nondecreasing.\n\n\n## Example of S(y) according to the Weibull distribution\n## Note: It is 1-pweibull()\ny &lt;- seq(0, 100, 1)\nFy &lt;- pweibull(y, shape=2, scale=50)\nplot(x=y, y=(1-Fy), type=\"l\")\n\n\n\n\nMore on the Weibull distribution here.\nAnd the hazard function \\(h(y)\\): Given survival up until \\(y\\), the instantaneous rate at which something fails (that the event occurs).\n\n\\(h(y)=\\frac{f(y)}{S(y)} =- \\frac{d}{dy} \\log S(y)\\)\n\\(S(y) = exp (-\\int_0^y h(t)dt)\\)\n\nNote: hazard rate is not exactly a probability and is difficult to interpret, but higher hazard rates reflect greater likelihood of failure.\n\n\n14.1.2 Censoring\nIn survival data, right-censoring of the data is common\n\nE.g., An observation does not experience an event during the study period, but the study period ends.\n\nExample: How many years does it take to finish grad school? If we cut off our period of study right now, you might be a censored observation. You are going to finish grad school at some point, but we have not observed that during our observation period.\nCan address by assumption: Given covariates, hazard rates of those who are censored \\(C_i\\) do not systematically differ from those who are not: \\(Y_i \\perp C_i | X_i\\). \\(Y_i\\) independent of censoring status, conditional on covariates."
  },
  {
    "objectID": "14-SurvivalData.html#kaplan-meier-survival-function",
    "href": "14-SurvivalData.html#kaplan-meier-survival-function",
    "title": "14  Survival Data",
    "section": "14.2 Kaplan-Meier Survival Function",
    "text": "14.2 Kaplan-Meier Survival Function\nA common way to summarize survival curves for actual data is through Kaplan-Meier curves.\nThis is a non-parametric analysis: Where \\(n_j\\) is the number of units at “risk” and \\(d_j\\) are the number of units failed at \\(t_j\\)\n\nFor \\(j: t_j \\leq y\\): \\(S(\\hat y) = \\prod \\frac{n_j - d_j}{n_j}\\)\nUnits surviving divided by unit at risk. Units that have died, dropped out, or not reached the time yet are not counted as at risk.\n\nExample from Simmons (2000), “International law and state behavior: Commitment and compliance in international monetary affairs” published in the American Political Science Review.\n\nNote: unlike the nice theoretical parametric curve from above, often survival estimates from real data are more like “step functions.”\n\n\n\n14.2.1 Kaplan-Meier in R\nWe will use the lung data from the survival package. For plotting, we will use the package survminer.\n\n## install.packages(\"survival\")\nlibrary(survival)\ndata(\"lung\")\nhead(lung)\n\n  inst time status age sex ph.ecog ph.karno pat.karno meal.cal wt.loss\n1    3  306      2  74   1       1       90       100     1175      NA\n2    3  455      2  68   1       0       90        90     1225      15\n3    3 1010      1  56   1       0       90        90       NA      15\n4    5  210      2  57   1       1       90        60     1150      11\n5    1  883      2  60   1       0      100        90       NA       0\n6   12 1022      1  74   1       1       50        80      513       0\n\n\n\ntime: days of survival.\nstatus: whether observation has failed or is right-censored. 1=censored, 2=dead.\nsex: Male=1; Female =2\n\nNote: the place where we would normally put our outcome variable in a regression formula now takes Surv(time, event)\n\n## Kaplan-Meier\nsfit &lt;- survfit(Surv(time=time, event=status)~sex, data=lung)\n\n## install.packages(\"survminer\")\nlibrary(survminer)\nggsurvplot(sfit, \n           conf.int=TRUE, \n           risk.table=TRUE, \n           pval = TRUE,\n           legend.labs=c(\"Male\", \"Female\"), \n           legend.title=\"Sex\",  \n           title=\"Kaplan-Meier: for Lung Cancer Survival\")"
  },
  {
    "objectID": "14-SurvivalData.html#modeling-approaches",
    "href": "14-SurvivalData.html#modeling-approaches",
    "title": "14  Survival Data",
    "section": "14.3 Modeling Approaches",
    "text": "14.3 Modeling Approaches\nBy using parametric approaches (using an underlying probability distribution), we can build on Kaplan-Meier to accept a broader range of covariates in the independent variables. There are three common distributions used:\n\nExponential: \\(h(y) = \\tau\\)\n\nhazard function is constant (the hazard of exiting is the same regardless of how long it’s been succeeding). no duration dependence.\n\nWeibull: \\(h(y) = \\frac{\\gamma}{\\mu_i^\\gamma}y^{\\gamma-1}\\)\n\nGeneralizes exponential. Adds parameter for duration dependence.\n\nCox proportional hazards: \\(h(y|x) = h_0(y) \\times r(x)\\)\n\nGeneralizes Weibull. Multiplicative effect of the baseline hazard \\(h_0(y)\\) and changes in covariates \\(r(x) = \\mu_i\\) in this case. Non-monotonic.\nMust assume proportional hazards. Hazard ratio of one group is a multiplicative of another, ratio constant over time.\n\n\nOther variations also exist (e.g., recurrent events, multi-state models).\nExample of Cox Proportional Hazards table from Simmons (2000)\n\n\n14.3.1 Survival Models in R\nWe will do an example of these modeling approaches using the same package in R and dataset.\nThe workhorse of survival modeling is the Surv() function, which tells R the nature of your survival data, such as the duration variable and if the origin should be assumed to be 0 or if, instead, it is located under a different variable. The examples we use are very basic. If you are using this for your own research, I recommend investigating the options available in this function. The documentation is here.\n\n\n14.3.2 Weibull Model\nNote: survreg() fits what are called accelerated failure models, not proportional hazards models. The coefficients are logarithms of ratios of survival times, so a positive coefficient means longer survival. This is a different type of modeling specification than what other softwares, such as Stata, will default to when fitting the Weibull model. Both can be valid, but it matters for interpretation (what a positive vs. negative sign means in the model). Sarah Haile describes alternative ways to present these models using supplemental functions here.\n\nwfit &lt;- survreg(Surv(time=time, event=status)~ age + sex, \n                data=lung,\n                dist = \"weibull\")\nsummary(wfit)\n\n\nCall:\nsurvreg(formula = Surv(time = time, event = status) ~ age + sex, \n    data = lung, dist = \"weibull\")\n               Value Std. Error     z       p\n(Intercept)  6.27485    0.48137 13.04 &lt; 2e-16\nage         -0.01226    0.00696 -1.76  0.0781\nsex          0.38209    0.12748  3.00  0.0027\nLog(scale)  -0.28230    0.06188 -4.56 5.1e-06\n\nScale= 0.754 \n\nWeibull distribution\nLoglik(model)= -1147.1   Loglik(intercept only)= -1153.9\n    Chisq= 13.59 on 2 degrees of freedom, p= 0.0011 \nNumber of Newton-Raphson Iterations: 5 \nn= 228 \n\n\nWe can interpret this as women having a higher survival duration than men. Positive coefficients for this model are associated with longer duration times. The hazard decreases and average survival time increases as the \\(x\\) covariate increases.\nWe can get different quantities of interest, such as a point at which 90% of patients survive.\n\n# 90% of patients of these types survive past time point above\npredict(wfit, type = \"quantile\", p =1-0.9,newdata =\n          data.frame(age=65, sex=c(1,2)))\n\n       1        2 \n64.28587 94.20046 \n\n\nA survival curve can then be visualized as follows:\n\npct &lt;- seq(.99, .01, by = -.01)\nwb &lt;- predict(wfit, type = \"quantile\", p =1-pct,newdata =\n          data.frame(age=65, sex=1))\n\nsurvwb &lt;- data.frame(time = wb, surv = pct, \n                      upper = NA, lower = NA, std.err = NA)\nggsurvplot(fit = survwb, surv.geom = geom_line, \n           legend.labs=\"Male, 65\")\n\n\n\n\n\n\n14.3.3 Cox proportional Hazards Model\nThis model will rely on a different R function coxph(). Note the difference in interpretation.\n\nfit &lt;- coxph(Surv(time, status) ~ age  + sex, \n             data = lung) \nsummary(fit)\n\nCall:\ncoxph(formula = Surv(time, status) ~ age + sex, data = lung)\n\n  n= 228, number of events= 165 \n\n         coef exp(coef)  se(coef)      z Pr(&gt;|z|)   \nage  0.017045  1.017191  0.009223  1.848  0.06459 . \nsex -0.513219  0.598566  0.167458 -3.065  0.00218 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n    exp(coef) exp(-coef) lower .95 upper .95\nage    1.0172     0.9831    0.9990    1.0357\nsex    0.5986     1.6707    0.4311    0.8311\n\nConcordance= 0.603  (se = 0.025 )\nLikelihood ratio test= 14.12  on 2 df,   p=9e-04\nWald test            = 13.47  on 2 df,   p=0.001\nScore (logrank) test = 13.72  on 2 df,   p=0.001\n\n\nHere, positive coefficients mean that the hazard (risk of death) is higher.\n\n\\(exp(\\beta_k)\\) is ratio of the hazards between two units where \\(x_k\\) differ by one unit.\nHazard ratio above 1 is positively associated with event probability (death).\n\nVisualizing Results\n\nThe survminer package has some shortcuts for this type of model.\nA simple plot would be the overall survival probabilities from the model\n\n\nlibrary(ggplot2)\nlibrary(survminer)\ncox1 &lt;- survfit(fit)\nggsurvplot(cox1, data = lung)\n\n\n\n\nWe can also visualize the results by using the same types of approaches we do when we use the predict() function and specify new \\(X\\) data in other types of regression. Here, we will use the command survfit.\n\nsex_df &lt;- data.frame(sex = c(1,2), age = c(62, 62) )\ncox1 &lt;- survfit(fit, data = lung, newdata = sex_df)\nggsurvplot(cox1, data = lung, legend.labs=c(\"Sex=1\", \"Sex=2\"))\n\nWarning: `gather_()` was deprecated in tidyr 1.2.0.\nℹ Please use `gather()` instead.\nℹ The deprecated feature was likely used in the survminer package.\n  Please report the issue at &lt;https://github.com/kassambara/survminer/issues&gt;.\n\n\n\n\n\nYou may wish to test the proportional hazards assumption. Here are two resources for this here and here."
  },
  {
    "objectID": "15-PredictionClassification.html#overview-of-prediction-and-classification",
    "href": "15-PredictionClassification.html#overview-of-prediction-and-classification",
    "title": "15  Prediction and Classification",
    "section": "15.1 Overview of Prediction and Classification",
    "text": "15.1 Overview of Prediction and Classification\nWhen we predict something, we are estimating some unknown using information we do know and trying to do so as accurately and precisely as possible.\n\nOften prediction involves classification– predicting a categorical outcome (e.g., prediction of who wins vs. who loses)\n\nSome social science examples of this might include\n\n\nTrying to detect hate speech online\nTrying to flag “fake news” and other misinformation\nTrying to forecast the results of an election\nTrying to classify a large amount of text into subject or topic categories for analysis\n\n\n15.1.1 How to predict or classify\nGoal: Estimate/guess some unknown using information we have – and do so as accurately and precisely as possible.\n\nChoose an approach\n\nUsing an observed (known) measure as a direct proxy to predict an outcome (e.g., a dictionary)\nUsing one or more observed (known) measures in a (often, regression) model to predict an outcome\nUsing a model to automatically select the measures to use for predicting an outcome\n\nAssess accuracy and precision in-sample and out-of-sample using some sample “training” data where you do know the right answer (“ground truth”).\n\nPrediction error: \\(Truth - Prediction\\)\nBias: Average prediction error: \\(\\text{mean}(Truth - Prediction)\\)\n\nA prediction is `unbiased’ if the bias is zero (if the prediction is on average true)\n\nIn regression: R-squared or Root-mean squared error\n\nRMSE is like `absolute’ error– the average magnitude of the prediction error\n\nFor classification: Confusion Matrix\n\nA cross-tab of predictions you got correct vs. predictions you got wrong (misclassified)\nGives you true positives and true negatives vs. false positives and false negatives\n\n\nRepeat steps 1 and 2 until you are confident in your method for predicting or classifying.\nApply to completely unknown data."
  },
  {
    "objectID": "15-PredictionClassification.html#in-sample-vs.-out-of-sample",
    "href": "15-PredictionClassification.html#in-sample-vs.-out-of-sample",
    "title": "15  Prediction and Classification",
    "section": "15.2 In-sample vs. Out-of-Sample",
    "text": "15.2 In-sample vs. Out-of-Sample\nProblem: Models that fit our existing (“in-sample”) data might not be the best for predicting out-of-sample data. Issues with variability and overfitting.\n\nThere may be idiosyncratic features of the data for which we do know the ground truth (e.g., outliers, different levels of variability) that could lead to poor predictions. This can lead to overfitting– predicting your particular dataset reaaaaaallly well, but any other data, more poorly.\nOur training data could be unrepresentative of the population in some way\n\nFor example, many AI tools suffer from gender and/or racial biases due to unrepresentative training data. If most of the data used to train a speech detection or facial recognition tool was trained on white faces/ white voices, it may have poor accuracy for other groups.\n\nSolutions:\n\nChoose your training data wisely!\nSearch for systematic biases along key variables (instead of aggregate accuracy, also look for accuracy measures by subgroups)\nUse out-of-sample predictions/classification tests to help avoid overfitting\n\nSplit data into train vs. test and/or do this repeatedly with\nCross-validation"
  },
  {
    "objectID": "15-PredictionClassification.html#cross-validation",
    "href": "15-PredictionClassification.html#cross-validation",
    "title": "15  Prediction and Classification",
    "section": "15.3 Cross-Validation",
    "text": "15.3 Cross-Validation\nCross-Validation is an approach to address overfitting issues\n\nTake data for which you know the answer – we call this “training data”\nRandomly subset out a portion of the training data. This will become our “test” data.\nDevelop a model based on the training data.\nTest the accuracy of the model on the test data (out-of-sample data that was not used to train the model).\nRepeat process for different portions of the data.\n\nGoal: See how well our model will generalize to new data (data the model hasn’t seen).\n\n15.3.1 k-fold cross-validation\nDivide your data into folds (how many \\(k\\) depends on how much data you have)\n\nFit your model to \\(k-1\\) folds\nSee how well your model predicts the data in the \\(k\\)th fold.\nCan repeat, leaving out a different fold each time\n\n\n\n15.3.2 Leave-one-out cross-validation\nBest for smaller data\n\nFit your model to all but one observation in your data\nSee how well your model predicts the left-out observation\nCan repeat, continuing to leave out one observation each time"
  },
  {
    "objectID": "15-PredictionClassification.html#examples-of-prediction-and-classification",
    "href": "15-PredictionClassification.html#examples-of-prediction-and-classification",
    "title": "15  Prediction and Classification",
    "section": "15.4 Examples of Prediction and Classification",
    "text": "15.4 Examples of Prediction and Classification\nYou may be wondering why we are covering these topics in a maximum likelihood course. Well, one of the most fundamental types of classification relies on logistic regression. We are just going to reframe the tools we have already learned for a different purpose.\nLet’s use an example with the donation data from 12.7\n\nlibrary(rio)\ndon &lt;- import(\"https://github.com/ktmccabe/teachingdata/blob/main/donation.dta?raw=true\")\n\nRecall that a key outcome variable here was donation indicating if a particular member of the donorate donated to a given senator (=1) or not (=0)\n\ntable(don$donation)\n\n\n    0     1 \n61533  2377 \n\ndon$donation &lt;- as.factor(don$donation)\n\nCan we predict whether someone will donate to a Senator?\nOutcome: donation (1 or 0), binary variable = binary classifier\n\nChoose an approach: Build a model (e.g., logistic regression)\nTrain the model\nAssess accuracy\n\nWithin sample and test model using cross-validation out-of-sample\n\nIf we were very satisfied with our model, we could apply the model to data for which we do not know the answer in the future.\n\nIf you were a campaign, could you predict who will donate?\n\n\nLet’s go!\n\n## build a model (choose \"features\" you think will be good for prediction)\n\n## you will want to remove missing data first\ndonsub &lt;- don[, c(\"donation\", \"NetWorth\", \"Edsum\")]\ndonsub &lt;- na.omit(donsub)\n\nfit &lt;- glm(donation ~ NetWorth + Edsum, \n           family = binomial(link = \"logit\"), data = donsub)\n\nIn-Sample Accuracy Assessments\n\n## generate probability of donation for each observation\ndonsub$prob &lt;- predict(fit, type = \"response\")\n\n## set a prediction threshold\ndonsub$pred &lt;- ifelse(donsub$prob &gt; 0.05, 1, 0)\n\n## accuracy- proportion where prediction matches reality\nmean(donsub$pred == donsub$donation)\n\n[1] 0.7374135\n\n## confusion matrix\ntable(truth = donsub$donation, predicted = donsub$pred)\n\n     predicted\ntruth     0     1\n    0 38485 12754\n    1  1203   710\n\n\nThere are different measures for accuracy that focus on particular types of errors:\n\n## where did we miss\ntable(actual = donsub$donation, pred = donsub$pred)\n\n      pred\nactual     0     1\n     0 38485 12754\n     1  1203   710\n\ntruepos &lt;- table(actual = donsub$donation, pred = donsub$pred)[2,2]\nfalsepos &lt;- table(actual = donsub$donation, pred = donsub$pred)[1,2]\ntrueneg &lt;- table(actual = donsub$donation, pred = donsub$pred)[1,1]\nfalseneg &lt;- table(actual = donsub$donation, pred = donsub$pred)[2,1]\n\n## precision\nprecision &lt;- truepos/(truepos + falsepos)\n\n## specificity\nspecificity &lt;- trueneg / (trueneg + falsepos)\n\n## false positive rate\nfalsepos &lt;- falsepos/(trueneg+ falsepos)\n\n## recall aka sensitivity\nrecall &lt;- truepos/(truepos + falseneg)\n\n## f-score, combination of precision/recall\nF1 &lt;- (2 * precision * recall) / (precision + recall)\n\nSee this post for more details and guidance on which one to choose.\nAnother common way to assess accuracy is through an ROC curve\nROC curves plot the true positive rate/precision (y) vs. 1 - false positive (x) rates. Want the curve to be away from the diagonal, increasing the area under the curive (AUC).\n\n# install.packages(\"pROC\")\nlibrary(pROC)\nROC &lt;- roc(response = donsub$donation,\n                  predictor = donsub$pred)\nplot(ROC, print.thres = \"best\")\n\n\n\nauc(ROC)\n\nArea under the curve: 0.5611\n\n\nFor more information, see here."
  },
  {
    "objectID": "15-PredictionClassification.html#continuous-example",
    "href": "15-PredictionClassification.html#continuous-example",
    "title": "15  Prediction and Classification",
    "section": "15.5 Continuous Example",
    "text": "15.5 Continuous Example\nLet’s now try an example with a continuous outcome: How much will someone donate to a Senator? One way to judge accuracy of a linear model is the root mean squared error\nOne basic approach would be to use a linear regression model. Other than that, it’s the same process as before, but we will use a different assessment for accuracy.\n\nfit &lt;- lm(total_donation ~ NetWorth + Edsum, data = don)\n\n## Root mean squared error\nrmse &lt;- sqrt(sum(residuals(fit)^2)/fit$df.residual)\nrmse\n\n[1] 324.081"
  },
  {
    "objectID": "15-PredictionClassification.html#tidymodels-and-cross-validation",
    "href": "15-PredictionClassification.html#tidymodels-and-cross-validation",
    "title": "15  Prediction and Classification",
    "section": "15.6 Tidymodels and Cross-validation",
    "text": "15.6 Tidymodels and Cross-validation\nWe are going to use a package called tidymodels to implement this process.\n\nThis package allows users to employ a variety of different machine learning models with only minor adjustments to the coding syntax.\nIt also integrates additional pre-processing and post-fit model validation tools to make training and testing models easier.\n\nWe will explore tools for cross-validation and downsampling.\n\nSimilar packages exist, such as the caret R package. Many machine learning models also have their own standalone R packages you can explore.\n\n\n15.6.1 Split data into train vs. test\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(textrecipes)\nlibrary(themis)\nlibrary(yardstick)\nlibrary(glmnet)\nlibrary(ranger)\n\n## Split into train vs. test\nset.seed(123)\ndf_split &lt;- donsub %&gt;% initial_split(prop = 0.8)\ndf_train &lt;- training(df_split)\ndf_test &lt;- testing(df_split)\n\n\n\n15.6.2 Construct features and create model recipe\nWe now need to choose features of our data to predict who will or will not donate. - In tidymodels, we can specify these steps as part of a recipe. In a recipe, you write a formula that is similar to the syntax of standard regression models in R, where you have your outcome y ~ features, data. - Following the initial recipe line, we can then add steps to pre-process and mutate the variables that will be include in the model. - The steps we include below are at our discretion. You may iterate the process changing the pre-processing steps to try to improve your model performance. - There are many additional pre-processing arguments you can add to a recipe.\n\n## recipe for pre-processing variables\ndf_rec &lt;-   recipe(donation ~ NetWorth + Edsum, data = df_train) %&gt;%\n  \n  \n  ## Downsample based on outcome\n  themis::step_downsample(donation) \n\nIn the last line, we implement “downsampling” from the themis package. You will not always want to include this line. What downsampling does is adjust the balance of the outcome categories we have in the data by randomly removing rows from training data, so that there is a roughly even balance between those that do and do not donate. Without this type of adjustment, a model may be prone to simply guess whichever class is disproportionately represented if there is an extreme lack of balance between categories.\n\n\n15.6.3 Choose type of model to run\nAt this point, we can specify different types of models we may want to try out as part of the classification process. This represents an advantage of the tidymodels package, as the syntax is roughly the same across model types. In tidymodels, you specify models using three concepts.\n\ntype of models such as logistic regression, decision tree models,random forests, and so forth.\nmode includes options regression and classification; some model types support either of these while some only have one mode. With a dichotomous outcome, we are in classification world.\nengine is the computational tool to fit the model. You will notice glmnet below\n\nSome models require you to install additional R packages for their implementation. This is why we installed glmnet for the logistic regressions.\n\n## Specify model: Three examples\n\n## A penalized logistic regression model\nlogit_mod &lt;- logistic_reg() %&gt;%\n  set_mode(\"classification\") %&gt;%\n  set_engine(\"glm\")\n\n## A penalized logistic model with penalty tuning\nlogit_tune &lt;- logistic_reg(penalty=tune(), mixture = 1) %&gt;%\n  set_mode(\"classification\") %&gt;%\n  set_engine(\"glmnet\")\nlambda_grid &lt;- grid_regular(penalty(), levels = 30)\n\n## Random Forest\nrf_spec &lt;- \n  rand_forest() %&gt;% \n  set_engine(\"ranger\") %&gt;% \n  set_mode(\"classification\")\n\nNote that we could include arguments penalty and mixture in the logistic_reg function. These are specific to this type of model. In this type of penalized logistic regression, we have a penalty that serves to penalize features used for prediction that do not have much predictive power.\n\nHow harsh should this penalty be? Well, this is where the tune() argument comes in during the second approach. We can choose the value of the penalty based on model performance. The grid_regular function tells R that we want to test out 30 different values of this penalty.\n\n\n\n15.6.4 Create a workflow that combines the model and recipe\nNow that we have prepared our data and model formula, we are ready to put things together using a workflow. At this point, we still haven’t actually fit a model.\n\n## workflow penalized logit\ndf_wf &lt;- workflow() %&gt;%\n  add_recipe(df_rec) %&gt;%\n  add_model(logit_mod)\n\n## workflow for the tuning specification\ndf_wft &lt;- workflow() %&gt;%\n  add_recipe(df_rec) %&gt;%\n  add_model(logit_tune)\n\n## workflow for random forest\ndf_wfrf &lt;- workflow() %&gt;%\n  add_recipe(df_rec) %&gt;%\n  add_model(rf_spec)\n\n\n\n15.6.5 Cross-validation\nThus far, we have just broken up our data into two portions, one for training and one for testing. Many times, we might want to perform more rigorous testing of our data. One approach for doing so is called \\(k\\)-fold cross-validation.\nFor this example, we are going to keep our df_test data separate as a “holdout” sample that is never used to train the classifier. However, we are going to further break up our training data df_train into five separate portions (5-fold cross-validation is also common).\n\n\n## cross-validation\nset.seed(123)\ndf_folds &lt;- vfold_cv(df_train, v=5)\n\n\nEach time we run the classifier, we will use all but one fold (four folds) of data and then test the results on the “left out” fold of data.\nWe repeat the process for a total of five times, leaving a different fold out for testing each time.\n\nNote: It is also possible to further repeat this entire process, by repeatedly resampling the data so that observations are shuffled into different folds.\n\nOur performance metrics are then a summary of five tests instead of just one test.\n\n\n\n15.6.6 Fitting the model\nHere are examples of fitting different models. For now, do not run all of them, as they each take a decent amount of computational time.\n\n## Fitting model on training data\n\n## Basic model fit to training data as one sample, no cross-validation\ndf_basic &lt;- df_wf %&gt;%\n  fit(data = df_train)\n\n## Cross-validation on penalized logistic model\nmy_metrics &lt;- yardstick::metric_set(accuracy, ppv, npv)\n\ndf_cv &lt;- fit_resamples(\n  df_wf,\n  df_folds,\n  control = control_resamples(save_pred = TRUE),\n  metrics = my_metrics\n)\n\n## Tuning the penalized logistic model\ndf_rs &lt;- tune_grid(\n  df_wft,\n  df_folds,\n  grid=lambda_grid, # note the use of the grid here\n  control = control_resamples(save_pred = TRUE),\n  metrics = my_metrics\n)\n\n## Random forest with cross-validation\ndf_rf &lt;- fit_resamples(\n  df_wfrf,\n  df_folds,\n  control = control_resamples(save_pred = TRUE),\n  metrics = my_metrics\n)\n\n\n\n15.6.7 Evaluating model performance\nDid we get things right?\n\nThere are many ways to answer this question.\n\nAccuracy– how many did we get right out of the total cases?\nConfusion Matrix– where did we go right vs. wrong?\n\nFalse Positive, True Positive, False Negative, True Negatives\n\nAdditional metrics that focus on particular types of error rates\n\n\nYour choice of metric depends on what you see as best for your use case.\nEvaluating on training data\nFor the models without cross-validation, we can use the predict() function to classify our training data into donate or not.\n\n# Add prediction columns to training data\nresults &lt;- df_train  %&gt;% select(donation) %&gt;% \n  bind_cols(df_basic %&gt;% \n              predict(new_data = df_train))\n\nWe can then evaluate these predictions against the ground truth from the uncvil column in our original training data. Below is a confusion matrix that shows where we were right vs. wrong.\n\nresults %&gt;% \n  conf_mat(truth = donation, estimate = .pred_class)\n\n          Truth\nPrediction     0     1\n         0 24058   578\n         1 16938   947\n\n\nWe can then quantify different types of performance using our metrics.\n\neval_metrics &lt;- metric_set(accuracy, ppv, npv)\neval_metrics(data = results, \n             truth = donation, \n             estimate = .pred_class)\n\n# A tibble: 3 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary        0.588 \n2 ppv      binary        0.977 \n3 npv      binary        0.0529\n\n\nEvaluating with cross-validation\nWhen we have used cross-validation and/or tuning, we can apply the collect_metrics function to our models to evaluate performance across the folds of our data. Here, the results represent performance averaged across the five tests of our data, instead of just a single test.\n\n## cross-validated logistic\nresultscv &lt;- collect_metrics(df_cv)\nresultscv\n\n# A tibble: 3 × 6\n  .metric  .estimator   mean     n std_err .config             \n  &lt;chr&gt;    &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy binary     0.603      5 0.00957 Preprocessor1_Model1\n2 npv      binary     0.0534     5 0.00226 Preprocessor1_Model1\n3 ppv      binary     0.976      5 0.00115 Preprocessor1_Model1\n\n\n\n## cross-validated random forest\nresultsrf &lt;- collect_metrics(df_rf)\nresultsrf\n\n# A tibble: 3 × 6\n  .metric  .estimator   mean     n  std_err .config             \n  &lt;chr&gt;    &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy binary     0.624      5 0.00837  Preprocessor1_Model1\n2 npv      binary     0.0563     5 0.00203  Preprocessor1_Model1\n3 ppv      binary     0.977      5 0.000962 Preprocessor1_Model1\n\n\nWhen you have tuned a model, you can then decide on the value of the “hyperparameter” based on a chosen metric, such as accuracy.\n\nresultstuning &lt;- collect_metrics(df_rs)\n\nchosen_acc &lt;- df_rs %&gt;% select_best(\"accuracy\")\nchosen_acc\n\n# A tibble: 1 × 2\n       penalty .config              \n         &lt;dbl&gt; &lt;chr&gt;                \n1 0.0000000001 Preprocessor1_Model01\n\n\n\n\n15.6.8 Finalize workflow\nOnce we have settled on a model and workflow, we want to finalize it by fitting it to the entire training data.\nBelow are two examples:\nWhen you do not have tuning, you can fit the model to the entire training data.\n\n## Saving the random forest model\nfinal_wfrf &lt;- fit(df_wfrf, df_train)\n\nWhen you have tuning, you want to use finalize_workflow to make sure you save the tuned parameters.\n\n## Saving the tuned workflow-- note we include are chosen lambda\nfinal_wft &lt;- finalize_workflow(df_wft, chosen_acc)  %&gt;%\n  fit(data=df_train)\n\nWe could save this workflow as an R object, which would allow us to store it as a classifier for future use.\n\n## saving decision tree\nsaveRDS(final_wfrf, file = \"donationclassifierRF.RDS\")\n\n## saving tuned workflow\nsaveRDS(final_wft, file = \"donationclassifierT.RDS\")\n\n\n\n15.6.9 Applying model to test data\nAt this point, our model still has not seen our test data. This makes it an especially difficult test for the model. Here is an example using our tuned penalized logistic regression model:\n\ntestres &lt;- df_test %&gt;% select(donation) %&gt;%\n  bind_cols(predict(final_wft, new_data=df_test))\n\ntestres_cf &lt;- testres %&gt;% \n  conf_mat(truth = donation, estimate = .pred_class)\n\ntest_metrics &lt;- metric_set(accuracy, ppv, npv)\ntest_metrics(data = testres, truth = donation, estimate = .pred_class)\n\n# A tibble: 3 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary        0.586 \n2 ppv      binary        0.974 \n3 npv      binary        0.0506\n\n\n\n\n15.6.10 Trying out classifier on completely new data\nFor the classifier to work, the new data must contain the same primary feature columns as the data used to build the model.\nBelow we construct a dataframe with the predictor variables as columns.\n\nnewdf &lt;-  data.frame(NetWorth = c(1, 3,3),\n                     Edsum = c(2, 5, 6))\n\nDoes our classifier identify these individuals as donors?\nWe load our saved classifier and predict donations using our new data.\n\n## cross-validated tree model\ndonationclassRF &lt;- readRDS(\"donationclassifierRF.RDS\")\npredict(donationclassRF, newdf, type = \"class\")\n\n# A tibble: 3 × 1\n  .pred_class\n  &lt;fct&gt;      \n1 0          \n2 0          \n3 0          \n\n\n\n## Tune penalized logistic\ndonationclassT &lt;- readRDS(\"donationclassifierT.RDS\")\npredict(donationclassT, newdf, type = \"class\")\n\n# A tibble: 3 × 1\n  .pred_class\n  &lt;fct&gt;      \n1 0          \n2 0          \n3 0"
  },
  {
    "objectID": "15-PredictionClassification.html#extending-machine-learning",
    "href": "15-PredictionClassification.html#extending-machine-learning",
    "title": "15  Prediction and Classification",
    "section": "15.7 Extending Machine Learning",
    "text": "15.7 Extending Machine Learning\nHow do I know which variables matter? Examples of more complex machine learning methods.\n\nRandom forests\nGradient boosting\nLASSO\nSVM\n\nTradeoffs: Machine Learning as “black box”- see here\nThere are nearly countless ways to use machine learning. See here."
  },
  {
    "objectID": "16-ModelTypesSummary.html#logit",
    "href": "16-ModelTypesSummary.html#logit",
    "title": "16  Choosing Model Types",
    "section": "16.2 Logit",
    "text": "16.2 Logit\nIn binary logistic regression models, our goal is to estimate the probability that our outcome \\(Y_i\\) takes the value 1 (given our covariates). (E.g., The probability someone turns out to vote).\nBecause we want to keep our estimates between 0 and 1 to reflect a probability, we need to apply a transformation. When we use a “logit” link function, our coefficients are now in the units of “logits” or “log odds”– the log of the odds- the probability of an event happening over the probability of the event not happening.\n\nFor example, if we were looking at the effect of age (measured in years) on voter turnout (0 if not turned out, 1 if turned out), we would say, for every year increase in someone’s age, we estimate an average of \\(\\hat \\beta\\) “logits increase” or “increase in the log-odds” they turn out to vote, holding all other variables constant.\n\n\n16.2.1 Ordinal Logit\nWe discussed two variations on the logit model in this course– ordinal and multinomial. In an ordinal model, we have a categorical outcome that is ordered in nature (e.g., a likert survey scale). We are now ultimately estimating the probability that \\(Y_i\\) belongs to a particular category. However, it takes some work to get there.\nOur coefficient values are now in the ordered log-odds scale. We are estimating the log of the odds that \\(Y_i\\) takes a value less than or equal to a certain category \\(j\\).\n\nFor example, if we were looking at the effect of age (measured in years) on support for Ron DeSantis, we would say, for every year increase in someone’s age, we estimate an average of \\(\\hat \\beta\\) increase in the ordered log-odds of supporting DeSantis, holding all other variables constant. If positive, generally, older people are going to be associated with higher probabilities of categories with greater support (e.g., “strongly support”) and lower probabilities of categories with lower support (e.g., “strongly oppose”). To really know the nature of the movement, you woud need to calculate quantities of interest.\n\n\n\n16.2.2 Multinomial Logit\nIn a multinomial model, we have a categorical outcome that is unordered in nature (e.g., religious groups). We are still ultimately going to try to estimate the probability that \\(Y_i\\) belongs to a particular category. However, our coefficients are going to represent contrasts between two specific outcome categories. This is the model where we get multiple sets of coefficients– one for each pairwise comparison between a particular \\(j\\) category of the outcome and a category \\(J\\) that we choose as the baseline.\n\nFor example, if we were looking at the effect of age (measured in years) on belonging to Catholic, Jewish, or Atheist groups, where Atheist was set to be a reference outcome, we would say, for every year increase in someone’s age, we estimate an average of \\(\\hat \\beta_{catholic}\\) increase in the log-odds of being Catholic vs. Atheist, holding all other variables constant and \\(\\hat \\beta_{jewish}\\) increase in the log-odds of being Jewish vs. Atheist, holding all other variables constant."
  },
  {
    "objectID": "16-ModelTypesSummary.html#probit",
    "href": "16-ModelTypesSummary.html#probit",
    "title": "16  Choosing Model Types",
    "section": "16.3 Probit",
    "text": "16.3 Probit\nRecall, in binary logistic regression models, our goal is to estimate the probability that our outcome \\(Y_i\\) takes the value 1 (given our covariates). (E.g., The probability someone turns out to vote).\nBecause we want to keep our estimates between 0 and 1 to reflect a probability, we need to apply a transformation. When we use a “probit” link function, our coefficients are now in the units of “probits” or z-scores of the probability.\n\nFor example, if we were looking at the effect of age (measured in years) on voter turnout (0 if not turned out, 1 if turned out), we would say, for every year increase in someone’s age, we estimate an average of \\(\\hat \\beta\\) “probits increase” that they turn out to vote.\n\n\n16.3.1 Ordered probit\nWe also ran an ordered probit model. Similar to ordered logit, we have a categorical outcome that is ordered in nature (e.g., a likert survey scale). We are now ultimately estimating the probability that \\(Y_i\\) belongs to a particular category.\n\nAs with the binary case, our coefficient values will be in terms of probits.For example, if we were looking at the effect of age (measured in years) on support for Ron DeSantis, we would say, for every year increase in someone’s age, we estimate an average of \\(\\hat \\beta\\) increase in the ordered probits of supporting DeSantis, holding all other variables constant. Generally, if positive, this would mean greater probability in being in the higher categories of support and lower probability of being in the lower categories of support, but you need to calculate quantities of interest to know where the movement is happening."
  },
  {
    "objectID": "16-ModelTypesSummary.html#poisson-quasipoisson-and-negative-binomial",
    "href": "16-ModelTypesSummary.html#poisson-quasipoisson-and-negative-binomial",
    "title": "16  Choosing Model Types",
    "section": "16.4 Poisson, Quasipoisson, and Negative Binomial",
    "text": "16.4 Poisson, Quasipoisson, and Negative Binomial\nWe use poisson models when our outcome is a count variable– a variable that is an integer from 0 to some positive number. Because we do not want to estimate negative counts, we still need to apply a transformation.\nThe poisson model uses a log link function. This means that our coefficients will be in log units.\n\nFor example, if we were looking at the effect of age (measured in years) on the number of social media posts a person makes per year, we would say, for every year increase in someone’s age, we estimate an average of \\(\\hat \\beta\\) increase in the log number of social media posts.\n\nOften, we will exponentiate these coefficient values (\\(\\hat \\beta\\)) to get them back into the units we care about: Increasing age by one year multiplies the rate of social media posts per year by \\(e^{\\hat \\beta}\\).\n\n\nThe quasipoisson and negative binomial are also models used for count data. Both are better able at addressing issues of overdisperson in the conditional variance of our outcome. Fortunately, the coefficient interpretations of these models are similar to the poisson because they rely on the log link."
  },
  {
    "objectID": "16-ModelTypesSummary.html#linear-models",
    "href": "16-ModelTypesSummary.html#linear-models",
    "title": "16  Choosing Model Types",
    "section": "16.1 Linear Models",
    "text": "16.1 Linear Models\nIn linear models, when we get our coefficients \\(\\hat \\beta\\), we interpret them as, for a one-unit increase in \\(x\\), we see a \\(\\hat \\beta\\) increase in the units of \\(Y\\), holding all other variables constant.\n\nFor example, if we were looking at the effect of age (measured in years) on income, measured in dollars, we would say, for every year increase in someone’s age, we estimate an average increase to their income of \\(\\hat \\beta\\) dolllars. (Or decrease if \\(\\hat \\beta\\) is negative)\n\nA special case for linear models is when we apply them in cases where the outcome variable only takes two values– 0 or 1. Here, we call our linear model a “linear probability model.” Now R won’t know our outcome is supposed to be between 0 and 1, but so long as we are not at extremes of our predictions, we can often stay within these bounds. In this special case, usually our interpretation is in probability units:\n\nFor example, if we were looking at the effect of age (measured in years) on voter turnout (0 if not turned out, 1 if turned out), we would say, for every year increase in someone’s age, we estimate an average of \\(\\hat \\beta \\times 100\\) (to turn into a percent) percentage point increase in the probability they turn out to vote, holding all other variables constant.\n\nAnother special case is if we apply a linear model to an ordered categorical outcome (e.g., a survey likert scale, going from 1 to 5, say strongly opposing to strongly supporting Ron DeSantis). Here, we would be estimating changes on this scale.\n\nFor example, if we were looking at the effect of age (measured in years) on support for Ron DeSantis, for every year increase in someone’s age, we estimate an average of \\(\\hat \\beta\\) increase on the 1 to 5 scale of support. (E.g., perhaps an increase of .2 scale points).\n\nOften, people might change their scale to go from 0 to 1 (0, .25, .5, .75, 1), so that the interpretation can be percentage points of the scale. For every year increase in someone’s age, we estimate an average of \\(\\hat \\beta \\time 100\\) percentage point increase on the scale of support.\n\n\nThe nice thing about these linear models is 1) there is no need to transform our coefficients (\\(\\mu_i = X_i^\\prime\\beta = \\eta_i = Y_i\\). The values are already in units of \\(Y\\) (or, in other words, \\(\\mu\\), the expected value of \\(Y_i\\)). This makes interpretations easier. 2) The marginal effect of a variable is constant across values of \\(x\\). In contrast, because the other models are trying to constrain the estimated outcomes to fit particular limits, the marginal effects change slightly depending on where in the domain of \\(x\\) we are estimating them."
  }
]