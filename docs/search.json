[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MLE 2023",
    "section": "",
    "text": "This document will include important links and course notes for fall 2023 Maximum Likelihood Estimation in Political Science.\n\nThis webpage will be updated throughout the semester with new content.\nSprinkled throughout the website are links to additional resources that might provide more in-depth explanations of a given topic. In particular, several of the sections have benefited from previous materials developed by Kosuke Imai and the book Quantitative Social Science, Chris Bail and SICSS, Marc Ratkovic, In Song Kim, Will Lowe, and others.\nThis is a living web resource. If you spot errors or have questions or suggestions, please email me at k.mccabe@rutgers.edu."
  },
  {
    "objectID": "02-ROverview.html#first-time-with-r-and-rstudio",
    "href": "02-ROverview.html#first-time-with-r-and-rstudio",
    "title": "2  R Overview",
    "section": "2.1 First Time with R and RStudio",
    "text": "2.1 First Time with R and RStudio\nThis next section provides a few notes on using R and RStudio now that you have installed it. This is mostly repetitive of the other resources. This includes only the bare essential information for opening an R script and digging into using R as a calculator, which we will do in chapter 3. In this section, we cover the following materials:\n\nUsing R as a calculator and assigning objects using &lt;-\nSetting your working directory and the setwd() function.\nCreating and saving an R script\n\n\n2.1.1 Open RStudio\nRStudio is an open-source and free program that greatly facilitates the use of R, especially for users new to programming. Once you have downloaded and installed R and RStudio, to work in R, all you need to do now is open RStudio (it will open R). It should look like this, though your version number may be different: (Image from Kosuke Imai’s Quantitative Social Science Figure 1.1)\n\nNote: If you only have three windows (e.g., no upper-left window), you may need to open an R script.\n\nTo do this, in RStudio, click on File, New, and then R script. This will open a blank document for text editing in the upper left of the RStudio window. We will return to this window in a moment.\n\n\n\n2.1.2 Using R as a Calculator\nThe bottom left window in your RStudio is the Console. You can type in this window to use R as a calculator or to try out commands. It will show the raw output of any commands you type. For example, we can try to use R as a calculator. Type the following in the Console (the bottom left window) and hit “enter” or “return” on your keyboard:\n\n5 + 3\n\n[1] 8\n\n5 - 3\n\n[1] 2\n\n5^2\n\n[1] 25\n\n5 * 3\n\n[1] 15\n\n5/3\n\n[1] 1.666667\n\n(5 + 3) * 2\n\n[1] 16\n\n\nIn the other RStudio windows, the upper right will show a history of commands that you have sent from the text editor to the R console, along with other items. The lower right will show graphs, help documents and other features. These will be useful later in the course.\n\n\n2.1.3 Working in an R Script\nEarlier, I asked you to open an R script in the upper left window by doing File, then New File, then R Script. Let’s go back to working in that window.\nSet your working directory setwd()\n(Almost) every time you work in RStudio, the first thing you will do is set your working directory. This is a designated folder in your computer where you will save your R scripts and datasets.\nThere are many ways to do this.\n\nAn easy way is to go to Session \\(\\rightarrow\\) Set Working Directory \\(\\rightarrow\\) Choose Directory. I suggest choosing a folder in your computer that you can easily find and that you will routinely use for this class. Go ahead and create/select it.\nNote: when you selected your directory, code came out in the bottom left Console window. This is the setwd() command which can also be used directly to set your working directory in the future.\nIf you aren’t sure where your directory has been set, you can also type getwd() in your Console. Try it now\n\n\n## Example of where my directory was\ngetwd()\n\nIf I want to change the working directory, I can go to the top toolbar of my computer and use Session \\(\\rightarrow\\) Set Working Directory \\(\\rightarrow\\) Choose Directory or just type my file pathway using the setwd() below:\n\n## Example of setting the working directory using setwd().\n## Your computer will have your own file path.\nsetwd(\"/Users/ktmccabe/Dropbox/Rutgers Teaching/\")\n\nSaving the R Script\nLet’s now save our R script to our working directory and give it an informative name. To do so, go to File, then Save As, make sure you are in the same folder on your computer as the folder you chose for your working directory.\nGive the file an informative name, such as: “McCabeWeek1.R”. Note: all of your R scripts will have the .R extension.\n\n\n2.1.4 Preparing your R script\nNow that we have saved our R script, let’s work inside of it. Remember, we are in the top-left RStudio window now.\n\nJust like the beginning of a paper, you will want to title your R script. In R, any line that you start with a # will not be treated as a programming command. You can use this to your advantage to write titles/comments. Below is a screenshot example of a template R script.\nYou can specify your working directory at the top, too. Add your own filepath inside setwd()\n\n\n\nThen you can start answering problems in the rest of the script.\nThink of the R script as where you write the final draft of your paper. In the Console (the bottom-left window), you can mess around and try different things, like you might when you are taking notes or outlining an essay. Then, write the final programming steps that lead you to your answer in the R script. For example, if I wanted to add 5 + 3, I might try different ways of typing it in the Console, and then when I found out 5 + 3 is the right approach, I would type that into my script.\n\n\n\n2.1.5 Executing Commands in your R script\nThe last thing we will note in this initial handout is how to execute commands in your R script.\nTo run / execute a command in your R script (the upper left window), you can\n\nHighlight the code you want to run, and then hold down “command + return” on a Mac or “control + enter” on Windows\nPlace your cursor at the end of the line of code (far right), and hit “command + return” on a Mac or “control + return” on Windows, or\nDo 1 or 2, but instead of using the keyboard to execute the commands, click “Run” in the top right corner of the upper-left window.\n\nTry it: Type 5 + 3 in the R script. Then, try to execute 5 + 3. It should look something like this:\n\nAfter you executed the code, you should see it pop out in your Console:\n\n5 + 3\n\n[1] 8\n\n\n\nNote: The symbol # also allows for annotation behind commands or on a separate line. Everything that follows # will be ignored by R. You can annotate your own code so that you and others can understand what each part of the code is designed to do.\n\n## Example\nsum53 &lt;- 5 + 3 # example of assigning an addition calculation\n\n\n\n2.1.6 Objects\nSometimes we will want to store our calculations as “objects” in R. We use &lt;- to assign objects by placing it to the left of what we want to store. For example, let’s store the calculation 5 + 3 as an object named sum53:\n\nsum53 &lt;- 5 + 3\n\nAfter we execute this code, sum53 now stores the calculation. This means, that if we execute a line of code that just hassum53`, it will output 8. Try it:\n\nsum53\n\n[1] 8\n\n\nNow we no longer have to type 5 + 3, we can just type sum53. For example, let’s say we wanted to subtract 2 from this calculation. We could do:\n\nsum53 - 2\n\n[1] 6\n\n\nLet’s say we wanted to divide two stored calculations:\n\nten &lt;- 5 + 5\ntwo &lt;- 1 + 1\nten / two\n\n[1] 5\n\n\nThe information stored does not have to be numeric. For example, it can be a word, or what we would call a character string, in which case you need to use quotation marks.\n\nmccabe &lt;- \"professor for this course\"\nmccabe\n\n[1] \"professor for this course\"\n\n\nNote: Object names cannot begin with numbers and no spacing is allowed. Avoid using special characters such as % and $, which have specific meanings in R. Finally, use concise and intuitive object names.}\n\nGOOD CODE: practice.calc &lt;- 5 + 3\nBAD CODE: meaningless.and.unnecessarily.long.name &lt;- 5 + 3\n\nWhile these are simple examples, we will use objects all the time for more complicated things to store (e.g., like full datasets!) throughout the course.\nWe can also store an array or “vector” of information using c()\n\nsomenumbers &lt;- c(3, 6, 8, 9)\nsomenumbers\n\n[1] 3 6 8 9\n\n\nImportance of Clean Code\nIdeally, when you are done with your R script, you should be able to highlight the entire script and execute it without generating any error messages. This means your code is clean. Code with typos in it may generate a red error message in the Console upon execution. This can happen when there are typos or commands are misused.\nFor example, R is case sensitive. Let’s say we assigned our object like before:\n\nsum53 &lt;- 5 + 3\n\nHowever, when we went to execute sum53, we accidentally typed Sum53:\n\nSum53\n\nError in eval(expr, envir, enclos): object 'Sum53' not found\n\n\nOnly certain types of objects can be used in mathematical calculations. Let’s say we tried to divide mccabe by 2:\n\nmccabe / 2\n\nError in mccabe/2: non-numeric argument to binary operator\n\n\nA big part of learning to use R will be learning how to troubleshoot and detect typos in your code that generate error messages.\n\n\n\n2.1.7 Practice\nBelow is an exercise that will demonstrate you are able to use R as a calculator and create R scripts.\n\nCreate an R script saved as ``LastnameSetup1.R” (use your last name). Within the R script, follow the example from this handout and title the script.\nSet your working directory, and include the file pathway (within setwd()) at the top of your .R script.\nDo the calculation 4 + 3 - 2 in R. Store it as an object with an informative name.\nDo the calculation 5 \\(\\times\\) 4 in R. Store it as an object with an informative name.\nAdd these two calculations together. In R, try to do this by adding together the objects you created, not the underlying raw calculations."
  },
  {
    "objectID": "02-ROverview.html#data-wrangling",
    "href": "02-ROverview.html#data-wrangling",
    "title": "2  R Overview",
    "section": "2.2 Data Wrangling",
    "text": "2.2 Data Wrangling\nSo you have some data…. AND it’s a mess!!!\nA lot of the data we may encounter in courses has been simplified to allow students to focus on other concepts. We may have data that look like the following:\nnicedata &lt;- data.frame(gender = c(\"Male\", \"Female\", \"Female\", \"Male\"),\n           age = c(16, 20, 66, 44),\n           voterturnout = c(1, 0, 1, 0))\n\n\n  gender age voterturnout\n1   Male  16            1\n2 Female  20            0\n3 Female  66            1\n4   Male  44            0\n\n\nIn the real world, our data may hit us like a ton of bricks, like the below:\nuglydata &lt;- data.frame(VV160002 = c(2, NA, 1, 2),\n           VV1400068 = c(16, 20, 66, 44),\n           VV20000 = c(1, NA, 1, NA))\n\n\n  VV160002 VV1400068 VV20000\n1        2        16       1\n2       NA        20      NA\n3        1        66       1\n4        2        44      NA\n\n\nA lot of common datasets we use in the social sciences are messy, uninformative, sprawling, misshaped, and/or incomplete. What do I mean by this?\n\nThe data might have a lot of missing values. For example, we may have NA values in R, or perhaps a research firm has used some other notation for missing data, such as a 99.\nThe variable names may be uninformative.\n\nFor example, there may be no way to know by looking at the data, which variable represents gender. We have to look at a codebook.\n\nEven if we can tell what a variable is, its categories may not be coded in a way that aligns with how we want to use the data for our research question.\n\nFor example, perhaps you are interested in the effect of a policy on people below vs. 65 and over in age. Well, your age variables might just be a numeric variable. You will have to create a new variable that aligns with your theoretical interest.\n\nDatasets are often sprawling. Some datasets may have more than 1000 variables. It is hard to sort through all of them. Likewise, datasets may have millions of observations. We cannot practically look through all the values of a column to know what is there.\nSometimes we have data shaped into separate columns when we’d rather it be reshaped into different rows.\nMaybe you have encountered a beautiful dataset that provides many measures of your independent variables of interest, but there’s one catch– it has no variable related to your outcome! You have to merge data from multiple sources to answer your research question.\n\nBelow are a few tips and resources. Ultimately, research is a constant debugging process. Loving R means seeing red error messages. The nice thing about R is that a lot of researchers constantly post coding tips and questions online. Google ends up being your friend, but it’s entirely normal to have to devote several hours (days?) to cleaning data.\n\n\n2.2.1 Dealing with Uninformative Variable Names\nHopefully, there is an easy fix for dealing with uninformative variable names. I say “hopefully” because hopefully when you encounter a dataset with uninformative variable names, the place where you downloaded the data will also include a codebook telling you what each variable name means, and how the corresponding values are coded.\nUnfortunately, this may not always be the case. One thing you can do as a researcher is when you create a dataset for your own work, keep a record (in written form, on a word document or in a pdf or code file) of what each variable means (e.g., the survey question it corresponds to or the exact economic measure), as well as how the values of the variables are coded. This good practice will help you in the short-term, as you pause and come back to working on a project over the course of a year, as well as benefit other researchers in the long run after you publish your research.\nFor examples of large codebooks, you can view the 2016 American National Election Study Survey and click on a codebook.\nI recommend that once you locate the definition of a variable of interest, rename the variable in your dataset to be informative. You can do this by creating a new variable or overwriting the name of the existing variable. You might also comment a note for yourself of what the values mean.\n\n## Option 1: create new variable\n## gender 2=Male, 1=Female\nuglydata$gender &lt;- uglydata$VV160002\n\n## Option 2: Overwrite\nnames(uglydata)[1] &lt;- \"gender2\"\n\n\n\n2.2.2 Dealing with Missing Data\nWhen we have a column with missing data, it is best to do a few things:\n\nTry to quantify how much missing data there is and poke at the reason why data are missing.\n\nIs it minor non-response data?\nOr is it indicative of a more systematic issue? For example, maybe data from a whole group of people or countries is missing for certain variables.\n\nIf the data are missing at a very minor rate and/or there is a logical explanation for the missing data that should not affect your research question, you may choose to “ignore” the missing data when performing common analyses, such as taking the mean or running a regression.\n\nIf missing data are a bigger problem, you may consider alternative solutions, such as “imputing” missing data or similarly using some type of auxilliary information to help fill in the missing values.\n\n\nIf we want to figure out how much missing data we have in a variable, we have a couple of approaches:\n\n## Summarize this variable\nsummary(uglydata$gender)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  1.000   1.500   2.000   1.667   2.000   2.000       1 \n\n## What is the length of the subset of the variable where the data are missing\nlength(uglydata$gender[is.na(uglydata$gender) == T])\n\n[1] 1\n\n\nIf we choose to ignore missing data, this can often be easily accomplished in common operations. For example, when taking the mean we just add an argument na.rm = T:\n\nmean(uglydata$VV1400068, na.rm=T)\n\n[1] 36.5\n\n\nIf we do a regression using lm or glm, R will automatically “listwise” delete any observation that has missing data (NA) on any of the variables in our regression model.\nWe should always be careful with missing data to understand how R is treating it in a particular scenario.\nFor example if we were to run table(uglydata$gender), we would have no idea there were missing data unless we knew that the total number of observations nrow(uglydata) was greater than 3. The table() command is omitting the missing values by default.\n\ntable(gender= uglydata$gender)\n\ngender\n1 2 \n1 2 \n\n\n\n\n2.2.3 Dealing with Variable Codings that Aren’t Quite Right\nOften times the ways that variables are coded in datasets we get off-the-shelf are not coded exactly as how we were dreaming up operationalizing our concepts. Instead, we are going to have to wrangle the data to get them into shape.\nThis may involve creating new variables that recode certain values, creating new variables that collapse some values into a smaller number of categories, combining multiple variables into a single variable (e.g., representing the average), or setting some of the variable values to be missing (NA). All of these scenarios may come up when you are dealing with real data.\nChapter 2 of Kosuke Imai’s book Quantitative Social Science walks through some examples of how to summarize your data, subset the data (2.2.3), create new variables using conditional statements (Section 2.2.4, e.g., “If age is below 65, assign the new variable a value of”0”, otherwise, assign it a value of “1”), and creating new factor variables (2.2.5, e.g., coding anyone who is Protestant, Catholic, or Lutheran in the data as “Christian”).\nHere is a short video working through the example from 2.2.4 using conditional statements to construct new variables. It uses the resume dataframe, which can be loaded below.\n\nresume &lt;- read.csv(\"https://raw.githubusercontent.com/ktmccabe/teachingdata/main/resume.csv\")\n\n\nR Studio has its own set of primers on various topics, including summarizing and working with data. See the Work with Data primer, as well as the full list of other topics. These will often rely on tidyverse coding.\n\n\n2.2.4 Dealing with Incomplete Data (Merging!)\nSometimes in order to answer our research questions, we need to combine data from multiple sources. If we have a large amount of data, this may be quite daunting. Fortunately, R has several commands that allow us to merge or append datasets.\nHere is a video working through examples of merging and appending data based on the tutorial below.\n\nHere are a few resources on merging and appending data:\n\nUsing the merge command in R. See Explanation.\n\nIt will look for variable(s) held in common between datasets and join the datasets by the matching values on these variables.\n\nAppending data in R (e.g., Maybe you have one dataset from 2010 and another from 2012, and you want to stack them on top of each other). See Explanation.\n\nSome merging problems are extremely difficult. For example, some researchers need to merge large datasets–like the voter file– with other administrative records. However, how someone’s name is displayed in one dataset might not match at all with the other dataset. For these complex problems, we might need “fuzzy matching.” Here is an R package that helps with this more complex case and related paper.\n\n\n2.2.5 Dealing with Poorly Shaped Data\nData can come in a variety of shapes and sizes. It’s a beautiful disaster.\nSometimes it’s particularly useful to have data in wide formats, where every row relates to a particular unit of data– such as a country or a survey respondent. And perhaps each column represents information about that unit at a particular point in time. For example, perhaps you have a column with information on that subject for the past five years.\ncountrywide &lt;- data.frame(country = c(\"Canada\", \"USA\"),\n                          economy2016 = c(10, 12),\n                          economy2017 = c(11, 11),\n                          economy2018 = c(9, 5),\n                          economy2019 = c(13, 8),\n                          economy2020 = c(12, 6))\n\n\n  country economy2016 economy2017 economy2018 economy2019 economy2020\n1  Canada          10          11           9          13          12\n2     USA          12          11           5           8           6\n\n\nHowever, other times, it would be more useful to you, as you dig into your data analysis, to have this information arranged in “long” format, such that every row is now a unit-year combination. You have a row for Canada in 2020, a row for Canada in 2019, and so on. Countries are now represented in multiple rows of your data.\ncountrylong &lt;- data.frame(country = rep(c(\"Canada\", \"USA\"),5),\n                          year = 2016:2020,\n                          economy= c(10, 12,11, 11,9, 5,13, 8,12, 6))\n\n\n   country year economy\n1   Canada 2016      10\n2      USA 2017      12\n3   Canada 2018      11\n4      USA 2019      11\n5   Canada 2020       9\n6      USA 2016       5\n7   Canada 2017      13\n8      USA 2018       8\n9   Canada 2019      12\n10     USA 2020       6\n\n\nUltimately, different shapes of data are advantageous for different research questions. This means it is best if we have a way to (at least somewhat) easily shift between the two formats.\nHere is a resource on how to “reshape” your data between wide and long from UCLA.\nHere are a few additional resources:\n\nMore on reshape – For the tidyverse fans. Using gather() and spread() in tidyverse from R for Data Science and explained by Chris Bail here.\n\n\n\n2.2.6 Subsetting data by rows and columns\nSometimes we do not want to deal with our entire dataset for an analysis. Instead, we might want to only analyze certain rows (e.g., maybe if we are just studying Democrats, for example). Similarly, we might have a dataframe with 1000 columns, from which we are only using about 20. We might want to remove those extra columns to make it easier to work with our dataframes.\nBelow are a few examples of subsetting data and selecting columns. We will use the resume dataset from the Kosuke Imai QSS book for demonstration. This is a dataset from an experiment describing whether certain applicants, who varied in the gender (sex) and race (race) signaled by their name (firstname), received callbacks (call) for their employment applications.\nHere is a short video working through these examples.\n\nLet’s load the data.\n\nresume &lt;- read.csv(\"https://raw.githubusercontent.com/ktmccabe/teachingdata/main/resume.csv\")\n\nSubset particular rows\nTo do this, put the row numbers you want to keep on the left side of the comma. Putting nothing on the right side means you want to keep all columns.\n\n## numerically\nresume[1,] # first row\n\n  firstname    sex  race call\n1   Allison female white    0\n\nresume[1:4,] # first through 4th rows\n\n  firstname    sex  race call\n1   Allison female white    0\n2   Kristen female white    0\n3   Lakisha female black    0\n4   Latonya female black    0\n\nresume[c(1, 3, 4),] # 1, 3, 4 rows\n\n  firstname    sex  race call\n1   Allison female white    0\n3   Lakisha female black    0\n4   Latonya female black    0\n\n\nUsing the subset command with logical expressions &gt; &gt;= == &lt; &lt;= !=\n\n## by logical expressions\nwomen &lt;- subset(resume, sex == \"female\")\nwomen &lt;- resume[resume$sex == \"female\", ] ## alternative\n\ncalledback &lt;- subset(resume, call == 1)\ncalledback &lt;- subset(resume, call &gt; 0)\n\nAnd or Or statements & or |\n\nblackwomen &lt;- subset(resume, sex == \"female\" & race == \"black\")\nbradbrendan &lt;- subset(resume, firstname == \"Brad\" | \n                        firstname == \"Brendan\")\n\nThe tidyverse also has commands for subsetting. Here is an example using filter.\n\nlibrary(tidyverse)\nblackwomen &lt;- resume %&gt;%\n  filter(sex == \"female\" & race == \"black\")\n\nSelecting particular columns\nNote, now we are working on the right side of the comma.\n\n## numerically\nfirst &lt;- resume[, 1] # first column\nfirstsecond &lt;- resume[, 1:2] # first and second column\nnotfourth &lt;- resume[, -4] # all but the fourth column\n\n\n## by labels\njustthese &lt;- resume[, c(\"firstname\", \"sex\")]\n\nUsing the select command\n\n## install.packages(\"dplyr\")\nlibrary(dplyr)\nsubdata &lt;- resume %&gt;% dplyr::select(firstname, sex) ## just these\nsubdata2 &lt;- resume %&gt;% dplyr::select(-firstname, -sex) ## all but these two\n\n\n\n2.2.7 Visualizing Data\nThere are many commands for plotting your data in R. The most common functions in base R are plot(), barplot() and hist(). You will see many examples throughout the notes with each of these functions.\nTo get you started, the most simple thing to note about the plot() command is that it is based on a coordinate system. You specify the x and y coordinates for which you want to plot a series of points.\nFor example, here is a plot at points 1,40; 3,50; and 4,60.\n\nplot(x = c(1,3,4), y=c(40, 50, 60))\n\n\n\n\nInstead of putting raw numbers as the coordinates, you can provide object names. E.g.,\n\nxcoord &lt;- c(1,3,4)\nycoord &lt;- c(40, 50, 60)\nplot(x = xcoord, y=ycoord)\n\nBeyond that, you can play around with many aesthetics in R, such as the type, pch, lty, as well as labels main, ylab, xlab, font sizes cex.main, cex.axis, cex.lab, and axis limits ylim, xlim. Below is an example. Play around with changing some of the specifications, and see how the plot changes.\n\nxcoord &lt;- c(1,3,4)\nycoord &lt;- c(40, 50, 60)\nplot(x = xcoord, y=ycoord,\n     main = \"Example plot\",\n     ylab= \"Example y axis\",\n     xlab = \"Example x axis\",\n     cex.main = .8,\n     ylim = c(0, 80),\n     xlim = c(1,4),\n     pch = 15,\n     col=\"red\",\n     type = \"b\",\n     lty=2)\n\n\n\n\nThe function barplot takes a single vector of values. This can be a raw vector you have created or a table object or tapply object, for example, displaying the counts of different observations or means.\nYou can add a names.arg argument to specify particular names for each bar. Many of the other aesthetics are the same as plot. You can play around with adding aesthetics.\nExample:\n\nbarplot(ycoord,\n        names.arg = c(\"First\", \"Second\", \"Third\"),\n        col=\"blue\")\n\n\n\n\nFor more on visualizing data, you can see the RStudio primers.\nR also has a package called ggplot2 which includes the function ggplot and many elaborate ways to plot your data. The gg stands for the grammar of graphics. For a video introduction to ggplot I recommend watching Ryan Womack’s video from 27:30 on. It uses the data diamonds which can be loaded in R through the following command. See approximately minute 35 for an example with a bar plot.\n\nlibrary(ggplot2)\ndata(diamonds)\n\n\n\n\n2.2.8 Reproducing your steps\nIt is really important to keep a record of all of the changes you have made to the original data. An R script or R markdown file is a useful way to do this, so long as you add comments that explain what you are doing.\nYou want to get your code to a place when a stranger can open your R file, load your data, and reproduce each step you took to get to the final results— all while never even needing to contact you with questions. That can be difficult, but it’s good to aim for that high bar, even if sometimes, we fall short in how we are documenting each step.\nThis website provides a nice introduction to R Markdown, one tool for embedding R code inside textual documents. See here.\nIf you want to get advanced with reproducibility, you can watch Chris Bail’s on the subject describing the uses of R Markdown and GitHub among other tools for communicating and collaboration. He also links to other resources."
  },
  {
    "objectID": "02-ROverview.html#tools-for-writing-up-results",
    "href": "02-ROverview.html#tools-for-writing-up-results",
    "title": "2  R Overview",
    "section": "2.3 Tools for writing up results",
    "text": "2.3 Tools for writing up results\n\n2.3.1 R Markdown\nR Markdown is a free tool within RStudio that allows you to weave together text and code (along with images and tables) into the same document. It can compile these documents (written inside R Studio) into html, pdf, or Word doc output files. R Markdown can be incredibly helpful for doing things like generating replication files, writing up problem sets, or even writing papers.\nThis site includes an introduction to R Markdown.\n\nSee also here and here\n\nR Markdown has its own syntax, which includes functionality for writing mathematical equations. The pdf output option in R Markdown requires LaTex, described in the next section.\n\n\n2.3.2 LaTex\nLaTex is a typesetting program for drafting documents, much like Microsoft Word. Some advantages of using LaTex in writing empirical research papers is, once you learn the basics of the program, it can become easier to add tables, figures, and equations into a paper with little effort. The downside is that it has its own syntax, which takes a little time to learn. LaTex also has a feature called “beamer” which uses LaTex to generate slides. You can use this for presentations. LaTex “compiles” documents into pdf files.\nHere is one introduction for getting started with LaTex that starts from the installation stage. Here also is a link to a set of slides from Overleaf discussing the basics of LaTex: Slides\nYou can download the Tex distribution for your operating system here. In addition to this, there are many programs available for running LaTex on your computer, which range from free, very basic tools (e.g., TexWorks) to tools with fancy capabilities.\nOverleaf is an online program for drafting LaTex documents. It has a nice feature where it allows you to share a document so that multiple people can work on the document simultaneously. This makes Overleaf a great program for projects where you have co-authors. The basic tools in Overleaf are available for free, but if you want to start sharing documents with a lot of co-authors, it requires a paid account.\nLaTex also has the ability to integrate citations into your documents. The part 2 tutorial from Overleaf goes over this.\nRStudio also has a program built-in called Sweave (.Rnw) documents that works with knitR, which weave together R code and LaTex syntax, allowing you to compile them into pdf documents and slide presentations. This is very similar to how R Markdown works, but with somewhat different syntax. See here for an overview. Your problem sets are generally Sweave/knitR documents.\n\n\n2.3.3 Formatting and Exporting R Results\nR has a number of tools, including the packages texreg, xtable, and stargazer, which can be used to export tables made in R to nicely formatted LaTex or html output.\nHere is a link to the texreg package documentation. Section 5 has examples of the texreg and htmlreg functions within the texreg package. These can be integrated into R Markdown and Sweave documents, and their output can be pasted into LaTex or Microsoft Word.\nYour choice of function will depend on where you ultimately want your results to be compiled. If you are generating results that will be compiled to pdf using LaTex, then texreg works well. If you are exporting results to Word, than you may wish to use the htmlreg function within the texreg package, which will generate output that can be pasted into Word.\nA simple example using R Markdown html output. (Note, if you wanted to export the table to Word, you would add an argument specifying file = \"myfit.doc\" to the function. See the above link for examples:\n\nmydata &lt;- read.csv(\"https://raw.githubusercontent.com/ktmccabe/teachingdata/main/resume.csv\")\nfit &lt;- lm(call ~ race, data=mydata)\n\n\n## First time you use texreg, install it\ninstall.packages(\"texreg\")\n\nlibrary(texreg)\nhtmlreg(list(fit),\n        stars=c(0.001, 0.01, 0.05),\n        caption = \"Regression of Call Backs on Race\")\n\n\n\nRegression of Call Backs on Race\n\n\n\n\n \n\n\nModel 1\n\n\n\n\n\n\n(Intercept)\n\n\n0.06***\n\n\n\n\n \n\n\n(0.01)\n\n\n\n\nracewhite\n\n\n0.03***\n\n\n\n\n \n\n\n(0.01)\n\n\n\n\nR2\n\n\n0.00\n\n\n\n\nAdj. R2\n\n\n0.00\n\n\n\n\nNum. obs.\n\n\n4870\n\n\n\n\n\n\n***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05\n\n\n\n\n\nYou can add more arguments to the function to customize the name of the model and the coefficients. You can also add multiple models inside the list argument, for example, if you wanted to present a table with five regression models at once. Here is an example with two:\nfit2 &lt;- lm(call ~ race + sex, data=mydata)\n\nlibrary(texreg)\nhtmlreg(list(fit, fit2),\n        stars=c(0.001, 0.01, 0.05),\n        caption = \"Regression of Call Backs on Race and Sex\")\n\n\n\nRegression of Call Backs on Race and Sex\n\n\n\n\n \n\n\nModel 1\n\n\nModel 2\n\n\n\n\n\n\n(Intercept)\n\n\n0.06***\n\n\n0.07***\n\n\n\n\n \n\n\n(0.01)\n\n\n(0.01)\n\n\n\n\nracewhite\n\n\n0.03***\n\n\n0.03***\n\n\n\n\n \n\n\n(0.01)\n\n\n(0.01)\n\n\n\n\nsexmale\n\n\n \n\n\n-0.01\n\n\n\n\n \n\n\n \n\n\n(0.01)\n\n\n\n\nR2\n\n\n0.00\n\n\n0.00\n\n\n\n\nAdj. R2\n\n\n0.00\n\n\n0.00\n\n\n\n\nNum. obs.\n\n\n4870\n\n\n4870\n\n\n\n\n\n\n***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05\n\n\n\n\n\n\n\n2.3.4 Additional formatting examples\nHere are some additional examples with different formats. You can run them on your own computer to see what the output looks like.\nThe package texreg has three primary formats\n\ntexreg() for LATEX output;\nhtmlreg() for HTML, Markdown-compatible and Microsoft Word-compatible output;\nscreenreg() for text output to the R console.\n\nIf you are working with a LaTex document, I recommend using texreg(), which will output LaTex syntax in your R console, which you can copy and paste into your article document.\nNote: this function allows you to customize model and coefficient names.\n\nlibrary(texreg)\ntexreg(list(fit, fit2),\n        stars=c(0.001, 0.01, 0.05),\n        caption = \"Regression of Call Backs on Race and Sex\",\n       custom.model.names = c(\"Bivariate\", \"Includes Sex\"),\n       custom.coef.names = c(\"Intercept\",\n                             \"Race- White\",\n                             \"Sex- Male\"))\n\nIf you are working with a Microsoft Word document, I recommend using htmlreg() and specifying a file name for your output. This will export a file to your working directory, which you can copy and paste into your Word article document. Otherwise, the syntax is the same as above.\n\nlibrary(texreg)\nhtmlreg(list(fit, fit2), file = \"models.doc\",\n        stars=c(0.001, 0.01, 0.05),\n        caption = \"Regression of Call Backs on Race and Sex\",\n       custom.model.names = c(\"Bivariate\", \"Includes Sex\"),\n       custom.coef.names = c(\"Intercept\",\n                             \"Race- White\",\n                             \"Sex- Male\"))\n\nIf you are trying to read the output in your R console, that’s when I would use screenreg(). However, for professional manuscript submissions, I would recommend the other formats.\n\nlibrary(texreg)\nscreenreg(list(fit, fit2), \n        stars=c(0.001, 0.01, 0.05),\n        caption = \"Regression of Call Backs on Race and Sex\",\n       custom.model.names = c(\"Bivariate\", \"Includes Sex\"),\n       custom.coef.names = c(\"Intercept\",\n                             \"Race- White\",\n                             \"Sex- Male\"))\n\nThe package stargazer allows similar options. I don’t think there are particular advantages to either package. Whatever comes easiest to you. The default for stargazer will output LaTex code into your R console.\n\nNote that the syntax is similar but has slightly different argument names from the texreg package.\nAlso, the intercept is at the bottom by default for stargazer. Be careful of the covariate ordering when you add labels.\n\n\nlibrary(stargazer)\nstargazer(list(fit, fit2), \n        star.cutoffs=c(0.05,0.01, 0.001),\n        title= \"Regression of Call Backs on Race and Sex\",\n        dep.var.labels.include = F,\n       column.labels = c(\"Call Back\", \"Call Back\"),\n       covariate.labels = c(\"Race- White\",\n                             \"Sex- Male\",\n                             \"Intercept\"))\n\nYou can adjust the type of output in stargazer for other formats, similar to texreg. Here is an example of Microsoft Word output.\n\nlibrary(stargazer)\nstargazer(list(fit, fit2), out = \"modelstar.doc\", type=\"html\",\n        star.cutoffs=c(0.05,0.01, 0.001),\n        dep.var.labels.include = F,\n        title= \"Regression of Call Backs on Race and Sex\",\n       column.labels = c(\"Call Back\", \"Call Back\"),\n       covariate.labels = c(\"Race- White\",\n                             \"Sex- Male\",\n                             \"Intercept\"))\n\n\n\n2.3.5 Additional Table Types\nSometimes you might want to create tables that are not from regression models, such as tables for descriptive statistics. R has other packages for tables of this type.\nFor example xtable can create simple html and latex tables. You just have to supply the function with a table object or matrix.\n\nlibrary(xtable)\ntable1 &lt;- table(race = mydata$race, sex = mydata$sex)\n\n\n## LaTeX\nxtable(table1)\n\n\n## Word\nprint(xtable(table1), type=\"html\", file = \"crosstab.doc\")\n\n## Html\nprint(xtable(table1), type=\"html\")\n\n\n\n\n\n\n\n\n\nfemale\n\n\nmale\n\n\n\n\nblack\n\n\n1886\n\n\n549\n\n\n\n\nwhite\n\n\n1860\n\n\n575"
  },
  {
    "objectID": "02-ROverview.html#exploratory-data-analysis-tools",
    "href": "02-ROverview.html#exploratory-data-analysis-tools",
    "title": "2  R Overview",
    "section": "2.4 Exploratory Data Analysis Tools",
    "text": "2.4 Exploratory Data Analysis Tools\nOne of the first things you may want to do when you have a new dataset is to explore! Get a sense of the variables you have, their class, and how they are coded. There are many functions in base R that help with this, such as summary(), table(), and descriptive statistics like mean or quantile.\nLet’s try this with the built-in mtcars data.\n\ndata(\"mtcars\")\n\nsummary(mtcars$cyl)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  4.000   4.000   6.000   6.188   8.000   8.000 \n\nquantile(mtcars$wt)\n\n     0%     25%     50%     75%    100% \n1.51300 2.58125 3.32500 3.61000 5.42400 \n\nmean(mtcars$mpg, na.rm=T)\n\n[1] 20.09062\n\nsd(mtcars$mpg, na.rm=T)\n\n[1] 6.026948\n\ntable(gear=mtcars$gear, carb=mtcars$carb)\n\n    carb\ngear 1 2 3 4 6 8\n   3 3 4 3 5 0 0\n   4 4 4 0 4 0 0\n   5 0 2 0 1 1 1\n\n\nAs discussed in the visualization, you can also quickly describe univariate data with histograms, barplots, or density plots using base R or ggplot.\n\nhist(mtcars$mpg, breaks=20, main=\"Histogram of MPG\")\n\n\n\nplot(density(mtcars$mpg, na.rm=T),  \n     main=\"Distribution of MPG\")\n\n\n\nbarplot(table(mtcars$gear), main=\"Barplot of Gears\")\n\n\n\n\n\nlibrary(ggplot2)\nggplot(mtcars, aes(mpg))+\n  geom_histogram(bins=20)+\n  ggtitle(\"Histogram of MPG\")\n\n\n\nggplot(mtcars, aes(mpg))+\n  geom_density()+\n  ggtitle(\"Distribution of MPG\")\n\n\n\nggplot(mtcars, aes(gear))+\n  geom_bar(stat=\"count\")+\n  ggtitle(\"Barplot of Gears\")"
  },
  {
    "objectID": "03-TheMATH.html#mathematical-operations",
    "href": "03-TheMATH.html#mathematical-operations",
    "title": "3  The Math",
    "section": "3.1 Mathematical Operations",
    "text": "3.1 Mathematical Operations\nIn this first section, we will review mathematical operations that you have probably encountered before. In many cases, this will be a refresher on the rules and how to read notation.\n\n3.1.1 Order of Operations\nMany of you may have learned the phrase, “Please Excuse My Dear Aunt Sally” which stands for Parentheses (and other grouping symbols), followed by Exponents, followed by Multiplication and Division from left to right, followed by Addition and Subtraction from left to right.\nThere will be many equations in our future, and we must remember these rules.\n\nExample: \\(((1+2)^3)^2 = (3^3)^2 = 27^2 = 729\\)\n\nTo get the answer, we focused on respecting the parentheses first, identifying the inner-most expression \\(1 + 2 = 3\\), we then moved out and conducted the exponents to get to \\((3^3)^2 = 27^2\\).\nNote how this is different from the answer to \\(1 + (2^3)^2 = 1 + 8^2 = 65\\), where the addition is no longer part of the parentheses.\n\n\n3.1.2 Exponents\nHere is a cheat sheet of some basic rules for exponents. These can be hard to remember if you haven’t used them in a long time. Think of \\(a\\) in this case as a number, e.g., 4, and \\(b\\), \\(k\\), and \\(l\\), as other numbers.\n\n\\(a^0 = 1\\)\n\\(a^1 = a\\)\n\\(a^k * a^l = a^{k + l}\\)\n\\((a^k)^l = a^{kl}\\)\n\\((\\frac{a}{b})^k = (\\frac{a^k}{b^k})\\)\n\nThese last two rules can be somewhat tricky. Note that a negative exponent can be re-written as a fraction. Likewise an exponent that is a fraction, the most common of which we will encounter is \\(\\frac{1}{2}\\) can be re-written as a root, in this case the square root (e.g., \\(\\sqrt{a}\\)).\n\n\\(a^{-k} = \\frac{1}{a^k}\\)\n\\(a^{1/2} = \\sqrt{a}\\)\n\n\n\n3.1.3 Summations and Products\nThe symbol \\(\\sum\\) can be read “take the sum of” whatever is to the right of the symbol. This is used to make the written computation of a sum much shorter than it might be otherwise. For example, instead of writing the addition operations separately in the example below, we can simplify it with the \\(\\sum\\) symbol. This is especially helpful if you would need to add together 100 or 1000 or more things. We will see these appear a lot in the course, for better or worse, so getting comfortable with the notation will be useful.\nUsually, there is notation just below and just above the symbol (e.g., \\(\\sum_{i=1}^3\\)). This can be read as “take the sum of the following from \\(i=1\\) to \\(i=3\\). We perform the operation in the expression, each time changing \\(i\\) to a different number, from 1 to 2 to 3. We then add each expression’s output together.\n\nExample: \\(\\sum_{i=1}^3 (i + 1)^2 = (1 + 1)^2 + (2 + 1)^2 + (3+1)^2 = 29\\)\n\nWe will also encounter the product symbol in this course: \\(\\prod\\). This is similar to the summation symbol, but this time we are multiplying instead of adding.\n\nExample: \\(\\prod_{k = 1}^3 k^2 = 1^2 \\times 2^2 \\times 3^2 = 36\\)\n\n\n\n3.1.4 Logarithms\nIn this class, we will generally assume that \\(\\log\\) takes the natural base \\(e\\), which is a mathematical constant equal to 2.718…. In other books, the base might be 10 by default.\n\nIf we have, \\(\\log_{10} x = 2\\), this is like saying 10^2 = 100.\nWith base \\(e\\), we have \\(\\log_e x =2\\), which is \\(e^2 = 7.389\\).\nWe are just going to write \\(\\log_e\\) as \\(\\log\\) but know that the \\(e\\) is there.\n\nA key part of maximum likelihood estimation is writing down the log of the likelihood equation, so this is a must-have for later in the course.\nHere is a cheat sheet of common rules for working with logarithms.\n\n\\(\\log x = 8 \\rightarrow e^8 = x\\)\n\\(e^\\pi = y \\rightarrow \\log y = \\pi\\)\n\\(\\log (a \\times b) = \\log a + \\log b\\)\n\\(\\log a^n = n \\log a\\)\n\\(\\log \\frac{a}{b} = \\log a - \\log b\\)\n\nWhy logarithms? There are many different reasons why social scientists use logs.\n\nSome social phenomena grow exponentially, and logs make it is easier to visualize exponential growth, as is the case in visualizing the growth in COVID cases. See this example.\nRelatedly, taking the log of a distribution that is skewed, will make it look more normal or symmetrical, which has some nice properties.\nSometimes the rules of logarithms are more convenient than non-logarithms. In MLE, we will take particular advantage of this rule: \\(\\log (a \\times b) = \\log a + \\log b\\), which turns a multiplication problem into an addition problem."
  },
  {
    "objectID": "03-TheMATH.html#mathematical-operations-in-r",
    "href": "03-TheMATH.html#mathematical-operations-in-r",
    "title": "3  The Math",
    "section": "3.2 Mathematical Operations in R",
    "text": "3.2 Mathematical Operations in R\nWe will use R for this course, and these operations are all available with R code, allowing R to become a calculator for you. Here are some examples applying the tools above.\n\n3.2.1 PEMDAS\n\n((1+2)^3)^2 \n\n[1] 729\n\n1 + (2^3)^2 \n\n[1] 65\n\n\n\n\n3.2.2 Exponents\nYou can compare the computation below to match with the rules above. Note how the caret ^ symbol is used for exponents, and the asterisk * is used for multiplication. We also have a function sqrt() for taking the square root.\n\n## Let's say a = 4 for our purposes\na &lt;- 4\n## And let's say k= 3 and l=5, b=2\nk &lt;- 3\nl &lt;- 5\nb &lt;- 2\n\n\n\\(a^0 = 1\\)\n\\(a^1 = a\\)\n\n\na^0\na^1\n\n[1] 1\n[1] 4\n\n\nNote how we use parentheses in R to make it clear that the exponent includes not just k but (k + l)\n\n\\(a^k * a^l = a^{k + l}\\)\n\n\na^k * a^l \na^(k + l)\n\n[1] 65536\n[1] 65536\n\n\nNote how we use the asterisk to make it clear we want to multiply k*l\n\n\\((a^k)^l = a^{kl}\\)\n\n\n(a^k)^l\na^(k*l)\n\n[1] 1073741824\n[1] 1073741824\n\n\n\n\\((\\frac{a}{b})^k = (\\frac{a^k}{b^k})\\)\n\n\n(a / b)^k\n(a ^k)/(b^k)\n\n[1] 8\n[1] 8\n\n\n\n\\(a^{-k} = \\frac{1}{a^k}\\)\n\n\na^(-k) \n1 / (a^k)\n\n[1] 0.015625\n[1] 0.015625\n\n\n\n\\(a^{1/2} = \\sqrt{a}\\)\n\n\na^(1/2) \nsqrt(a)\n\n[1] 2\n[1] 2\n\n\n\n\n3.2.3 Summations\nSummations and products are a little more nuanced in R, depending on what you want to accomplish. But here is one example.\nLet’s take a vector (think a list of numbers) that goes from 1 to 4. We will call it ourlist.\n\nourlist &lt;- c(1,2,3,4)\nourlist\n## An alternative is to write this: ourlist &lt;- 1:4\n\n[1] 1 2 3 4\n\n\nNow let’s do \\(\\sum_{i = 1}^4 (i + 1)^2 = (1 +1)^2 + (1+2)^2 + (1 + 3)^2 + (1 + 4)^2 = 54\\)\nIn R, when you add a number to a vector, it will add that number to each entry in the vector. Example\n\nourlist + 1\n\n[1] 2 3 4 5\n\n\nWe can use that now to do the inside part of the summation. Note: the exponent works the same way, squaring each element of the inside expression.\n\n(ourlist + 1)^2\n\n[1]  4  9 16 25\n\n\nNow, we will embed this expression inside a function in R called sum standing for summation. It will add together each of the inside components.\n\nsum((ourlist + 1)^2)\n\n[1] 54\n\n\nIf instead we wanted the product, multiplying each element of the inside together, we could use prod(). We won’t use that function very much in this course.\n\n\n3.2.4 Logarithms\nR also has functions related to logarithms, called log() and exp() for the for the natural base \\(e\\). By default, the natural exponential is the base in the log() R function.\n\n\\(\\log x = 8 \\rightarrow e^8 = x\\)\n\n\nexp(8)\n\n[1] 2980.958\n\nlog(2980.958)\n\n[1] 8\n\n\nNote that R also has a number of built-in constants, like pi.\n\n\\(e^\\pi = y \\rightarrow \\log y = \\pi\\)\n\n\nexp(pi)\n\n[1] 23.14069\n\nlog(23.14069)\n\n[1] 3.141593\n\n\n\n\\(\\log (a \\times b) = \\log a + \\log b\\)\n\n\nlog(a * b)\nlog(a) + log(b)\n\n[1] 2.079442\n[1] 2.079442\n\n\nLet’s treat \\(n=3\\) in this example and enter 3 directly where we see \\(n\\) below. Alternatively you could store 3 as n, as we did with the other letters above.\n\n\\(\\log a^n = n \\log a\\)\n\n\nlog(a ^ 3)\n\n3 * log(a)\n\n[1] 4.158883\n[1] 4.158883\n\n\n\n\\(\\log \\frac{a}{b} = \\log a - \\log b\\)\n\n\nlog(a/b)\nlog(a) - log(b)\n\n[1] 0.6931472\n[1] 0.6931472"
  },
  {
    "objectID": "03-TheMATH.html#derivatives",
    "href": "03-TheMATH.html#derivatives",
    "title": "3  The Math",
    "section": "3.3 Derivatives",
    "text": "3.3 Derivatives\nWe will need to take some derivatives in the course. The reason is because a derivative gets us closer to understanding how to minimize and maximize certain functions, where a function is a relationship that maps elements of a set of inputs into a set of outputs, where each input is related to one output.\nThis is useful in social science, with methods such as linear regression and maximum likelihood estimation because it helps us estimate the values that we think will best describe the relationship between our independent variables and a dependent variable.\nFor example, in ordinary least squares (linear regression), we choose coefficients, which describe the relationship between the independent and dependent variables (for every 1 unit change in x, we estimate \\(\\hat \\beta\\) amount of change in y), based on a method that tries to minimize the squared error between our estimated outcomes and the actual outcomes. In MLE, we will have a different quantity, which we will try to maximize.\nTo understand derivatives, we will briefly define limits.\nLimits\nA limit describes how a function behaves as it approaches (gets very close to) a certain value\n\n\\(\\lim_{x \\rightarrow a} f(x) = L\\)\n\nExample: \\(\\lim_{x \\rightarrow 3} x^2 = 9\\) The limit of this function as \\(x\\) approaches three, is 9. Limits will appear in the expression for calculating derivatives.\n\n3.3.1 Derivatives\nFor intuition on a derivative, watch this video from The Math Sorcerer.\nA derivative is the instantaneous rate at which the function is changing at x: the slope of a function at a particular point.\nThere are different notations for indicating something is a derivative. Below, we use \\(f'(x)\\) because we write our functions as \\(f(x) = x\\). Many times you might see a function equation like \\(y = 3x\\) There, it will be common for the derivative to be written like \\(\\frac{dy}{dx}\\).\nLet’s break down the definition of a derivative by looking at its similarity to the simple definition of a slope, as the rise over the run:\n\nSlope (on average): rise over run: change in \\(f(x)\\) over an interval (\\([c, b]\\) where \\(b-c =h\\)): \\(\\frac{f(b) - f(c)}{b-c}\\)\n\nFor slope at a specific point \\(x\\) (the derivative of f(x) at x), we just make the interval \\(h\\) very small:\n\n\\(f'(x)= \\lim_{h \\rightarrow 0}\\frac{f(a + h) - f(a)}{h}\\)\n\nExample \\(f(x) = 2x + 3\\).\n\n\\(f'(x) = \\lim_{h \\rightarrow 0}\\frac{2(x + h) + 3 - (2x + 3)}{h} = \\lim_{h \\rightarrow 0}\\frac{2x + 2h - 2x}{h} = \\lim_{h \\rightarrow 0}2 = 2\\)\n\nThis twitter thread by the brilliant teacher and statistician Allison Horst, provides a nice cartoon-based example of the derivative. (Note that sometimes the interval \\(h\\) is written as \\(\\Delta x\\), the change in \\(x\\)).\n\n\n3.3.2 Critical Points for Minima or Maxima\nIn both OLS and MLE, we reach points where take what are called “first order conditions.” This means we take the derivative with respect to a parameter of interest and then set the derivative = 0 and solve for the parameter to get an expression for our estimator. (E.g., In OLS, we take the derivative of the sum of squared residuals, set it equal to zero, and solve to get an expression for \\(\\hat \\beta\\)).\nThe reason we are interested in when the derivative is zero, is because this is when the instanaeous rate of change is zero, i.e., the slope at a particular point is zero. When does this happen? At a critical point- maximum or minimum. Think about it– at the top of a mountain, there is no more rise (and no decline). You are completing level on the mountaintop. The slope at that point is zero.\nLet’s take an example. The function \\(f(x) = x^2 + 1\\) has the derivative \\(f'(x) = 2x\\). This is zero when \\(x = 0\\).\nThe question remains: How do we know if it is a maximum or minimum?\nWe need to figure out if our function is concave or convex around this critical value. Convex is a “U” shape, meaning we are at a minimum, while concavity is an upside-down-U, which means we are at a maximum. We do so by taking the second derivative. This just means we take the derivative of the expression we already have for our first derivative. In our case, \\(f''(x) = 2\\). So what? Well the key thing we are looking for is if this result is positive or negative. Here, it is positive, which means our function is convex at this critical value, and therefore, we are at a minimum.\nJust look at the function in R if we plot it.\n\n## Let's define an arbitrary set of values for x\nx &lt;- -3:3\n## Now let's map the elements of x into y using our function\nfx &lt;- x^2 + 1\n\n## Let's plot the results\nplot(x = x, y=fx, \n     xlab = \"x\", type = \"l\",\n     main = \"f(x) = x^2 + 1\")\n\n\n\n\nNotice that when x=0, we are indeed at a minimum, just as the positive value of the second derivative would suggest.\nA different example: \\(f(x) = -2x^2 +1\\). \\(f'(x) = -4x\\) When we set this equal to 0 we find a critical value at \\(x = 0\\). \\(f''(x) = -4\\). Here, the value is negative, and we know it is concave. Sure enough, let’s plot it, and notice how we can draw a horiztonal line at the maximum, representing that zero slope at the critical point:\n\nx &lt;- -3:3\nfx &lt;- -2*x^2 + 1\nplot(x = x, y = fx,\n    ylab = \"f(x)\",\n     xlab = \"x\", type = \"l\",\n     main = \"f(x) = -2x^2 + 1\")\nabline(h=1, col = \"red\", lwd=2)\n\n\n\n\n\n\n3.3.3 Common Derivative Rules\nBelow is a cheat sheet of rules for quickly identifying the derivatives of functions.\nThe derivative of a constant is 0.\n\n\\(f(x) = a; f'(x) = 0\\)\n\nExample: The derivative of 5 is 0.\n\n\nHere is the power rule.\n\n\\(f(x) = ax^n; f'(x) = n\\times a \\times x^{n-1}\\)\n\nExample: The derivative of \\(x^3 = 3x^{(3-1)} = 3x^2\\)\n\n\nWe saw logs in the last section, and, yes, we see logs again here.\n\n\\(f(x) = e^{ax}; f'(x) = ae^{ax}\\)\n\\(f(x) = \\log(x); f'(x) = \\frac{1}{x}\\)\n\nA very convenient rule is that a derivative of a sum = sum of the derivatives.\n\n\\(f(x) = g(x) + h(x); f'(x) = g'(x) + h'(x)\\)\n\nProducts can be more of a headache. In this course, we will turn some product expressions into summation expressions to avoid the difficulties of taking derivatives with products.\n\nProduct Rule: \\(f(x) = g(x)h(x); f'(x) = g'(x)h(x) + h'(x)g(x)\\)\n\nThe chain rule below looks a bit tricky, but it can be very helpful for simplifying the way you take a derivative. See this video from NancyPi for a helpful explainer, as well as a follow-up for more complex applications here.\n\nChain Rule: \\(f(x) = g(h(x)); f'(x) = g'(h(x))h'(x)\\)\n\nExample: What is the derivative of \\(f(x) = \\log 5x\\)?\n\nFirst, we will apply the rule which tells us the derivative of a \\(\\log x\\) is \\(\\frac{1}{x}\\).\nHowever, here, we do not just have \\(x\\), we have \\(5x\\). We are in chain rule territory.\nAfter we apply the derivative to the log, which is \\(\\frac{1}{5x}\\), we then have to take the derivative of \\(5x\\) and multiply the two expressions together.\nThe derivative of \\(5x\\) is \\(5\\).\nSo, putting this together, our full derivative is f′(x) = 5 ∗ \\(\\frac{1}{5x}\\) = \\(\\frac{1}{x}\\)."
  },
  {
    "objectID": "03-TheMATH.html#vectors-and-matrices",
    "href": "03-TheMATH.html#vectors-and-matrices",
    "title": "3  The Math",
    "section": "3.4 Vectors and Matrices",
    "text": "3.4 Vectors and Matrices\nVectors\nFor our purposes, a vector is a list or “array” of numbers. For example, this might be a variable in our data– a list of the ages of all politicians in a country.\nAddition\n\nIf we have two vectors \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\), where\n\n\\(\\mathbf{u} \\ = \\ (u_1, u_2, \\dots u_n)\\) and\n\\(\\mathbf{v} \\ = \\ (v_1, v_2, \\dots v_n)\\),\n\\(\\mathbf{u} + \\mathbf{v} = (u_1 + v_1, u_2 + v_2, \\dots u_n + v_n)\\)\n\nNote: \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) must be of the same dimensionality - number of elements in each must be the same - for addition.\n\nScalar multiplication\n\nIf we have a scalar (i.e., a single number) \\(\\lambda\\) and a vector \\(\\mathbf{u}\\)\n\\(\\lambda \\mathbf{u} = (\\lambda u_1, \\lambda u_2, \\dots \\lambda u_n)\\)\n\nWe can implement vector addition and scalar multiplication in R.\nLet’s create a vector \\(\\mathbf{u}\\), a vector \\(\\mathbf{v}\\), and a number lambda.\n\nu &lt;- c(33, 44, 22, 11)\nv &lt;- c(6, 7, 8, 2)\nlambda &lt;- 3\n\nWhen you add two vectors in R, it adds each component together.\n\nu + v\n\n[1] 39 51 30 13\n\n\nWe can multiply each element of a vector, u by lambda:\n\nlambda * u\n\n[1]  99 132  66  33\n\n\nElement-wise Multiplication\nNote: When you multiply two vectors together in R, it will take each element of one vector and multiply it by each element of the other vector.\n\nu * v \\(= (u_1 * v_1, u_2 * v_2, \\dots u_n * v_n)\\)\n\n\nu * v\n\n[1] 198 308 176  22\n\n\n\n3.4.1 Matrix Basics\nA matrix represents arrays of numbers in a rectangle, with rows and columns.\n\nA matrix with \\(m\\) rows and \\(n\\) columns is defined as (\\(m\\) x \\(n\\)). What is the dimensionality of the matrix A below?\n\n\\(A = \\begin{pmatrix} a_{11} & a_{12} & a_{13}\\\\ a_{21} & a_{22} & a_{23} \\\\ a_{31} & a_{32} & a_{33} \\\\ a_{41} & a_{42} & a_{43} \\end{pmatrix}\\)\nIn R, we can think of a matrix as a set of vectors. For example, we could combine the vectors u and v we created above into a matrix defined as W.\n\n## cbind() binds together vectors as columns\nWcol &lt;- cbind(u, v)\nWcol\n\n      u v\n[1,] 33 6\n[2,] 44 7\n[3,] 22 8\n[4,] 11 2\n\n## rbind() binds together vectors as rows\nWrow &lt;- rbind(u, v)\nWrow\n\n  [,1] [,2] [,3] [,4]\nu   33   44   22   11\nv    6    7    8    2\n\n\nThere are other ways to create matrices in R, but using cbind and rbind() are common.\nWe can find the dimensions of our matrices using dim() or nrow() and ncol() together. For example:\n\ndim(Wcol)\n\n[1] 4 2\n\nnrow(Wcol)\n\n[1] 4\n\nncol(Wcol)\n\n[1] 2\n\n\nNote how the dimensions are different from the version created with rbind():\n\ndim(Wrow)\n\n[1] 2 4\n\nnrow(Wrow)\n\n[1] 2\n\nncol(Wrow)\n\n[1] 4\n\n\nExtracting specific components\nThe element \\(a_{ij}\\) signifies the element is in the \\(i\\)th row and \\(j\\)th column of matrix A. For example, \\(a_{12}\\) is in the first row and second column.\n\nSquare matrices have the same number of rows and columns\nVectors have just one row or one column (e.g., \\(x_1\\) element of \\(\\mathbf{x}\\) vector)\n\nIn R, we can use brackets to extract a specific \\(ij\\) element of a matrix or vector.\n\nWcol\n\n      u v\n[1,] 33 6\n[2,] 44 7\n[3,] 22 8\n[4,] 11 2\n\nWcol[2,1] # element in the second row, first column\n\n u \n44 \n\nWcol[2,] # all elements in the second row\n\n u  v \n44  7 \n\nWcol[, 1] # all elements in the first column\n\n[1] 33 44 22 11\n\n\nFor matrices, to extract a particular entry in R, you have a comma between entries because there are both rows and columns. For vectors, you only have one entry, so no comma is needed.\n\nu\n\n[1] 33 44 22 11\n\nu[2] # second element in the u vector\n\n[1] 44\n\n\n\n\n3.4.2 Matrix Operations\nMatrix Addition\n\nTo be able to add matrix A and matrix B, they must have the same dimensions.\nLike vector addition, to add matrices, you add each of the components together.\n\n\\(A = \\begin{pmatrix} a_{11} & a_{12} & a_{13}\\\\ a_{21} & a_{22} & a_{23} \\\\ a_{31} & a_{32} & a_{33} \\end{pmatrix}\\) and \\(B = \\begin{pmatrix} b_{11} & b_{12} & b_{13}\\\\ b_{21} & b_{22} & b_{23} \\\\ b_{31} & b_{32} & b_{33} \\end{pmatrix}\\)\n\\(A + B = \\begin{pmatrix} a_{11} + b_{11} & a_{12} + b_{12} & a_{13} + b_{13}\\\\ a_{21} + b_{21} & a_{22} + b_{22} & a_{23} + b_{23} \\\\ a_{31} + b_{31} & a_{32} + b_{32} & a_{33} + b_{33} \\end{pmatrix}\\)\n\\(Q = \\begin{pmatrix} 2 & 4 & 1\\\\ 6 & 1 & 5 \\end{pmatrix}\\) \\(+\\) \\(R = \\begin{pmatrix} 9 & 4 & 2\\\\ 11 & 8 & 7 \\end{pmatrix} = Q + R = \\begin{pmatrix} 11 & 8 & 3\\\\ 17 & 9 & 12 \\end{pmatrix}\\)\nScalar Multiplication\nTake a scalar \\(\\nu\\). Just like vectors, we multiply each component of a matrix by the scalar.\n\\(\\nu Q = \\begin{pmatrix} \\nu q_{11} & \\nu q_{12} & \\dots & \\nu q_{1n}\\\\ \\nu q_{21} & \\nu q_{22} & \\dots & \\nu q_{2n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\nu q_{m1} & \\nu q_{m2} & \\dots & \\nu q_{mn} \\end{pmatrix}\\)\nExample: Take \\(c = 2\\) and a matrix A.\n\\(cA = c *\\begin{pmatrix} 4 & 6 & 1\\\\ 3 & 2 & 8 \\end{pmatrix}\\) = \\(\\begin{pmatrix} 8 & 12 & 2\\\\ 6 & 4 & 16 \\end{pmatrix}\\)\nNote the Commutativity/Associativity: For scalar \\(c\\): \\(c(AB) = (cA)B = A(cB) = (AB)c\\).\nMatrix Multiplication\nA matrix A and B must be conformable to multiply AB.\n\nTo be comformable, for \\(m_A\\) x \\(n_A\\) matrix A and \\(m_B\\) x \\(n_B\\) matrix B, the “inside” dimensions must be equal: \\(n_A = m_B\\).\nThe resulting AB has the “outside” dimensions: \\(m_A\\) x \\(n_B\\).\n\nFor each \\(c_{ij}\\) component of \\(C = AB\\), we take the inner product of the \\(i^{th}\\) row of matrix A and the \\(j^{th}\\) column of matrix B.\n\nTheir product C = AB is the \\(m\\) x \\(n\\) matrix where:\n\\(c_{ij} =a_{i1}b_{1j} + a_{i2}b_{2j} + \\dots + a_{ik}b_{kj}\\)\n\nExample: This \\(2 \\times 3\\) matrix is multiplied by a \\(3 \\times 2\\) matrix, resulting in the \\(2 \\times 2\\) matrix.\n\\(\\begin{pmatrix} 4 & 6 & 1\\\\ 3 & 2 & 8 \\end{pmatrix}\\) \\(\\times\\) \\(\\begin{pmatrix} 8 & 12 \\\\ 6 & 4 \\\\ 7 & 10 \\end{pmatrix}\\) = \\(\\begin{pmatrix} (4*8 + 6*6 + 1*7) & (4*12 + 6*4 + 1*10) \\\\ (3*8 + 2*6 + 8*7) & (3*12 + 2*4 + 8*10) \\end{pmatrix}\\)\nFor example, the entry in the first row and second column of the new matrix \\(c_{12} = (a_{11} = 4* b_{11} = 12) + (a_{12} = 6*b_{21} = 4) + (a_{13} = 1*b_{31} = 10)\\)\nWe can also do matrix multiplication in R.\n\n## Create a 3 x 2 matrix A\nA &lt;- cbind(c(3, 4, 6), c(5, 6, 8))\nA\n\n     [,1] [,2]\n[1,]    3    5\n[2,]    4    6\n[3,]    6    8\n\n## Create a 2 x 4 matrix B\nB &lt;- cbind(c(6,8), c(7, 9), c(3, 6), c(1, 11))\nB\n\n     [,1] [,2] [,3] [,4]\n[1,]    6    7    3    1\n[2,]    8    9    6   11\n\n\nNote that the multiplication AB is conformable because the number of columns in A matches the number of rows in B:\n\nncol(A)\nnrow(B)\n\n[1] 2\n[1] 2\n\n\nTo multiply matrices together in R, we need to add symbols around the standard asterisk for multiplication:\n\nA %*% B\n\n     [,1] [,2] [,3] [,4]\n[1,]   58   66   39   58\n[2,]   72   82   48   70\n[3,]  100  114   66   94\n\n\nThat is necessary for multiplying matrices together. It is not necessary for scalar multiplication, where we take a single number (e.g., c = 3) and multiply it with a matrix:\n\nc &lt;- 3\nc*A\n\n     [,1] [,2]\n[1,]    9   15\n[2,]   12   18\n[3,]   18   24\n\n\nNote the equivalence of the below expressions, which combine scalar and matrix multiplication:\n\nc* (A %*% B)\n\n     [,1] [,2] [,3] [,4]\n[1,]  174  198  117  174\n[2,]  216  246  144  210\n[3,]  300  342  198  282\n\n(c* A) %*% B\n\n     [,1] [,2] [,3] [,4]\n[1,]  174  198  117  174\n[2,]  216  246  144  210\n[3,]  300  342  198  282\n\nA %*% (c * B)\n\n     [,1] [,2] [,3] [,4]\n[1,]  174  198  117  174\n[2,]  216  246  144  210\n[3,]  300  342  198  282\n\n\nIn social science, one matrix of interest is often a rectangular dataset that includes column vectors representing independent variables, as well as another vector that includes your dependent variable. These might have 1000 or more rows and a handful of columns you care about."
  },
  {
    "objectID": "03-TheMATH.html#additional-matrix-tidbits-that-will-come-up",
    "href": "03-TheMATH.html#additional-matrix-tidbits-that-will-come-up",
    "title": "3  The Math",
    "section": "3.5 Additional Matrix Tidbits that Will Come Up",
    "text": "3.5 Additional Matrix Tidbits that Will Come Up\nInverse\nAn \\(n\\) x \\(n\\) matrix A is invertible if there exists an \\(n\\) x \\(n\\) inverse matrix \\(A^{-1}\\) such that:\n\n\\(AA^{-1} = A^{-1}A = I_n\\)\nwhere \\(I_n\\) is the identity matrix (\\(n\\) x \\(n\\)), that takes diagonal elements of 1 and off-diagonal elements of 0. Example:\n\n\\(I_n = \\begin{pmatrix} 1_{11} & 0 & \\dots & 0\\\\ 0& 1_{22} & \\dots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & \\dots & 1_{nn} \\end{pmatrix}\\)\n\nMultiplying a matrix by the identity matrix returns in the matrix itself: \\(AI_n = A\\)\n\nIt’s like the matrix version of multiplying a number by one.\n\n\nNote: A matrix must be square \\(n\\) x \\(n\\) to be invertible. (But not all square matrices are invertible.) A matrix is invertible if and only if its columns are linearly independent. This is important for understanding why you cannot have two perfectly colinear variables in a regression model.\nWe will not do much solving for inverses in this course. However, the inverse will be useful in solving for and simplifying expressions.\n\n3.5.1 Transpose\nWhen we transpose a matrix, we flip the \\(i\\) and \\(j\\) components.\n\nExample: Take a 4 X 3 matrix A and find the 3 X 4 matrix \\(A^{T}\\).\nA transpose is usually denoted with as \\(A^{T}\\) or \\(A'\\)\n\n\\(A = \\begin{pmatrix} a_{11} & a_{12} & a_{13}\\\\ a_{21} & a_{22} & a_{23} \\\\ a_{31} & a_{32} & a_{33} \\\\ a_{41} & a_{42} & a_{43} \\end{pmatrix}\\) then \\(A^T = \\begin{pmatrix} a'_{11} & a'_{12} & a'_{13} & a'_{14}\\\\ a'_{21} & a'_{22} & a'_{23} & a'_{24} \\\\ a'_{31} & a'_{32} & a'_{33} & a'_{34} \\end{pmatrix}\\)\nIf \\(A = \\begin{pmatrix} 1 & 4 & 2 \\\\ 3 & 1 & 11 \\\\ 5 & 9 & 4 \\\\ 2 & 11& 4 \\end{pmatrix}\\) then \\(A^T = \\begin{pmatrix} 1 & 3 & 5 & 2\\\\ 4 & 1 & 9 & 11 \\\\ 2& 11 & 4 & 4 \\end{pmatrix}\\)\nCheck for yourself: What was in the first row (\\(i=1\\)), second column (\\(j=2\\)) is now in the second row (\\(i=2\\)), first column (\\(j=1\\)). That is \\(a_{12} =4 = a'_{21}\\).\nWe can transpose matrices in R using t(). For example, take our matrix A:\n\nA\n\n     [,1] [,2]\n[1,]    3    5\n[2,]    4    6\n[3,]    6    8\n\nt(A)\n\n     [,1] [,2] [,3]\n[1,]    3    4    6\n[2,]    5    6    8\n\n\nIn R, you can find the inverse of a square matrix with solve()\n\nsolve(A)\n\nError in solve.default(A): 'a' (3 x 2) must be square\n\n\nNote, while A is not square A’A is square:\n\nAtA &lt;- t(A) %*% A\n\nsolve(AtA)\n\n          [,1]      [,2]\n[1,]  2.232143 -1.553571\n[2,] -1.553571  1.089286\n\n\n\n\n3.5.2 Additional Matrix Properties and Rules\nThese are a few additional properties and rules that will be useful to us at various points in the course:\n\nSymmetric: Matrix A is symmetric if \\(A = A^T\\)\nIdempotent: Matrix A is idempotent if \\(A^2 = A\\)\nTrace: The trace of a matrix is the sum of its diagonal components \\(Tr(A) = a_{11} + a_{22} + \\dots + a_{mn}\\)\n\nExample of symmetric matrix:\n\\(D = \\begin{pmatrix} 1 & 6 & 22 \\\\ 6 & 4 & 7 \\\\ 22 & 7 & 11 \\end{pmatrix}\\)\n\n## Look at the equivalence\nD &lt;- rbind(c(1,6,22), c(6,4,7), c(22,7,11))\nD\n\n     [,1] [,2] [,3]\n[1,]    1    6   22\n[2,]    6    4    7\n[3,]   22    7   11\n\nt(D)\n\n     [,1] [,2] [,3]\n[1,]    1    6   22\n[2,]    6    4    7\n[3,]   22    7   11\n\n\nWhat is the trace of this matrix?\n\n## diag() pulls out the diagonal of a matrix\nsum(diag(D))\n\n[1] 16\n\n\n\n\n3.5.3 Matrix Rules\nDue to conformability and other considerations, matrix operations are somewhat more restrictive, particularly when it comes to commutativity.\n\nAssociative \\((A + B) + C = A + (B + C)\\) and \\((AB) C = A(BC)\\)\nCommutative \\(A + B = B + A\\)\nDistributive \\(A(B + C) = AB + AC\\) and \\((A + B) C = AC + BC\\)\nCommutative law for multiplication does not hold– the order of multiplication matters: $ AB BA$\n\nRules for Inverses and Transposes\nThese rules will be helpful for simplifying expressions. Treat \\(A\\), \\(B\\), and \\(C\\) as matrices below, and \\(s\\) as a scalar.\n\n\\((A + B)^T = A^T + B^T\\)\n\\((s A)^T\\) \\(= s A^T\\)\n\\((AB)^T = B^T A^T\\)\n\\((A^T)^T = A\\) and \\(( A^{-1})^{-1} = A\\)\n\\((A^T)^{-1} = (A^{-1})^T\\)\n\\((AB)^{-1} = B^{-1} A^{-1}\\)\n\\((ABCD)^{-1} = D^{-1} C^{-1} B^{-1} A^{-1}\\)\n\n\n\n3.5.4 Derivatives with Matrices and Vectors\nLet’s say we have a \\(p \\times 1\\) “column” vector \\(\\mathbf{x}\\) and another \\(p \\times 1\\) vector \\(\\mathbf{a}\\).\nTaking the derivative with respect to vector \\(\\mathbf{x}\\).\nLet’s say we have \\(y = \\mathbf{x}'\\mathbf{a}\\). This process is explained here. Taking the derivative of this is called the gradient.\n\n\\(\\frac{\\delta y}{\\delta x} = \\begin{pmatrix}\\frac{\\delta y}{\\delta x_1} \\\\ \\frac{\\delta y}{\\delta x_2} \\\\ \\vdots \\\\ \\frac{dy}{dx_p} \\end{pmatrix}\\)\n\\(y\\) will have dimensions \\(1 \\times 1\\). \\(y\\) is a scalar.\n\nNote: \\(y = a_1x_1 + a_2x_2 + ... + a_px_p\\). From this expression, we can take a set of “partial derivatives”:\n\\(\\frac{\\delta y}{\\delta x_1} = a_1\\)\n\\(\\frac{\\delta y}{\\delta x_2} = a_2\\), and so on\n\\(\\frac{\\delta y}{\\delta x} = \\begin{pmatrix}\\frac{\\delta y}{\\delta x_1} \\\\ \\frac{\\delta y}{\\delta x_2} \\\\ \\vdots \\\\ \\frac{\\delta y}{\\delta x_p} \\end{pmatrix} = \\begin{pmatrix} a_1 \\\\ a_2 \\\\ \\vdots \\\\ a_p \\end{pmatrix}\\)\n\nWell, this is just vector \\(\\mathbf{a}\\)\n\nAnswer: \\(\\frac{\\delta }{\\delta x} \\mathbf{x}^T\\mathbf{a} = \\mathbf{a}\\). We can apply this general rule in other situations.\nExample 2\nLet’s say we want to differentiate the following where vector \\(\\mathbf{y}\\) is \\(n \\times 1\\), \\(X\\) is \\(n \\times k\\), and \\(\\mathbf{b}\\) is \\(k \\times 1\\). Take the derivative with respect to \\(b\\).\n\n\\(\\mathbf{y}'\\mathbf{y} - 2\\mathbf{b}'X'\\mathbf{y}\\)\nNote that the dimensions of the output are \\(1 \\times 1\\), a scalar quantity.\n\nRemember the derivative of a sum is the sum of derivatives. This allows us to focus on particular terms.\n\nThe first term has no \\(\\mathbf{b}\\) in it, so this will contribute 0.\nThe second term is \\(2\\mathbf{b}'X'\\mathbf{y}\\). We can think about this like the previous example\n\n\\(\\frac{\\delta }{\\delta b} 2\\mathbf{b}'X'\\mathbf{y} = \\begin{pmatrix} \\frac{\\delta }{\\delta b_1}\\\\ \\frac{\\delta }{\\delta b_2} \\\\ \\vdots \\\\ \\frac{\\delta }{\\delta b_k} \\end{pmatrix}\\)\nThe output is needs to be \\(k \\times 1\\) like \\(\\mathbf{b}\\), which is what \\(2 * X'\\mathbf{y}\\) is.\n\nThe derivative is \\(-2X'\\mathbf{y}\\)\n\nExample 3\nAnother useful rule when a matrix \\(A\\) is symmetric: \\(\\frac{\\delta}{\\delta \\mathbf{x}} \\mathbf{x}^TA\\mathbf{x} = (A + A^T)\\mathbf{x} = 2A\\mathbf{x}\\).\nDetails on getting to this result. We are treating the vector \\(\\mathbf{x}\\) as \\(n \\times 1\\) and the matrix \\(A\\) as symmetric.\nWhen we take \\(\\frac{\\delta}{\\delta \\mathbf{x}}\\) (the derivative with respect to \\(\\mathbf{x}\\)), we will be looking for a result with the same dimensions \\(\\mathbf{x}\\).\n\\(\\frac{\\delta }{\\delta \\mathbf{x}} = \\begin{pmatrix}\\frac{\\delta }{\\delta x_1} \\\\ \\frac{\\delta }{\\delta x_2} \\\\ \\vdots \\\\ \\frac{d}{dx_n} \\end{pmatrix}\\)\nLet’s inspect the dimensions of \\(\\mathbf{x}^TA\\mathbf{x}\\). They are \\(1 \\times 1\\). If we perform this matrix multiplication, we would be multiplying:\n\\(\\begin{pmatrix} x_1 & x_2 & \\ldots & x_i \\end{pmatrix} \\times \\begin{pmatrix} a_{11} & a_{12} & \\ldots & a_{1j} \\\\ a_{21} & a_{22} & \\ldots & a_{2j} \\\\ \\vdots & \\vdots & \\vdots & \\vdots \\\\ a_{i1} & a_{i2} & \\ldots & a_{ij} \\end{pmatrix} \\times \\begin{pmatrix}x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_j \\end{pmatrix}\\)\nTo simplify things, let’s say we have the following matrices, where \\(A\\) is symmetric:\n\\(\\begin{pmatrix} x_1 & x_2 \\end{pmatrix} \\times \\begin{pmatrix} a_{11} & a_{12} \\\\ a_{21} & a_{22} \\end{pmatrix} \\times \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix}\\)\nWe can perform the matrix multiplication for the first two quantities, which will result in a \\(1 \\times 2\\) vector. Recall in matrix multiplication we take the sum of the element-wise multiplication of the \\(ith\\) row of the first object by the \\(jth\\) column of the second object. This means multiply the first row of \\(\\mathbf{x}\\) by the first column of \\(A\\) for the entry in cell \\(i=1; j=1\\), and so on.\n\\(\\begin{pmatrix} (x_1a_{11} + x_2a_{21}) & (x_1a_{12} + x_2a_{22}) \\end{pmatrix}\\)\nWe can then multiply this quantity by the last quantity\n\\(\\begin{pmatrix} (x_1a_{11} + x_2a_{21}) & (x_1a_{12} + x_2a_{22}) \\end{pmatrix} \\times \\begin{pmatrix}x_1 \\\\ x_2 \\end{pmatrix}\\)\nThis will results in the \\(1 \\times 1\\) quantity: \\((x_1a_{11} + x_2a_{21})x_1 + (x_1a_{12} + x_2a_{22})x_2 = x_1^2a_{11} + x_2a_{21}x_1 + x_1a_{12}x_2 + x_2^2a_{22}\\)\nWe can now take the derivatives with respect to \\(\\mathbf{x}\\). Because \\(\\mathbf{x}\\) is \\(2 \\times 1\\), our derivative will be a vector of the same dimensions with components:\n\\(\\frac{\\delta }{\\delta \\mathbf{x}} = \\begin{pmatrix}\\frac{\\delta }{\\delta x_1} \\\\ \\frac{\\delta }{\\delta x_2} \\end{pmatrix}\\)\nThese represent the partial derivatives of each component within \\(\\mathbf{x}\\)\nLet’s focus on the first: \\(\\frac{\\delta }{\\delta x_1}\\).\n\\[\\begin{align*}\n\\frac{\\delta }{\\delta x_1} x_1^2a_{11} + x_2a_{21}x_1 +   x_1a_{12}x_2 + x_2^2a_{22} &=\\\\\n&= 2x_1a_{11} + x_2a_{21} + a_{12}x_2\\\\\n\\end{align*}\\]\nWe can repeat this for \\(\\frac{\\delta }{\\delta x_2}\\)\n\\[\\begin{align*}\n\\frac{\\delta }{\\delta x_2} x_1^2a_{11} + x_2a_{21}x_1 +   x_1a_{12}x_2 + x_2^2a_{22} &=\\\\\n&= a_{21}x_1  + x_1a_{12} + 2x_2a_{22}\\\\\n\\end{align*}\\]\nNow we can put the result back into our vector format:\n\\(\\begin{pmatrix}\\frac{\\delta }{\\delta x_1} = 2x_1a_{11} + x_2a_{21} + a_{12}x_2\\\\ \\frac{\\delta }{\\delta x_2} = a_{21}x_1 + x_1a_{12} + 2x_2a_{22}\\end{pmatrix}\\)\nNow it’s just about simplifying to show that we have indeed come back to the rule.\nRecall that for a symmetric matrix, the elements in rows and columns \\(ij\\) = the elements in \\(ji\\). This allows us to read \\(a_{21} = a_{12}\\) and combine those terms (e.g., \\(x_2a_{21} + a_{12}x_2 =2a_{12}x_2\\)) :\n\\(\\begin{pmatrix}\\frac{\\delta }{\\delta x_1} = 2x_1a_{11} + 2a_{12}x_2\\\\ \\frac{\\delta }{\\delta x_2} = 2a_{21}x_1 + 2x_2a_{22}\\end{pmatrix}\\)\nSecond, we can now bring the 2 out front.\n\\(\\begin{pmatrix}\\frac{\\delta }{\\delta x_1} \\\\ \\frac{\\delta }{\\delta x_2} \\end{pmatrix} = 2 * \\begin{pmatrix} x_1a_{11} + a_{12}x_2\\\\ a_{21}x_1 + x_2a_{22}\\end{pmatrix}\\)\nFinally, let’s inspect this and show it is equivalent to this multiplication where we have a \\(2 \\times 2\\) \\(A\\) matrix multiplied by a \\(2 \\times 1\\) \\(\\mathbf x\\) vector. Minor note: Because any individual element of a vector is just a single quantity, we can change the order (e.g., \\(a_{11}*x_1\\) vs. \\(x_1*a_{11}\\)). We just can’t do that for full vectors or matrices\n\\(2A\\mathbf{x} = 2* \\begin{pmatrix} a_{11} & a_{12} \\\\ a_{21} & a_{22} \\end{pmatrix} \\times \\begin{pmatrix}x_1 \\\\ x_2 \\end{pmatrix} = 2 * \\begin{pmatrix} a_{11}x_1 + a_{12}x_2 \\\\ a_{21}x_1 + a_{22}x_2 \\end{pmatrix}\\)\nThe last quantity is the same as the previous step. That’s the rule!\nApplying the rules\n\nThis has a nice analogue to the derivative we’ve seen before \\(q^2 = 2*q\\).\nLet’s say we want to take the derivative of \\(\\mathbf{b}'X'X\\mathbf{b}\\) with respect to \\(\\mathbf{b}\\).\nWe can think of \\(X'X\\) as if it is \\(A\\).\n\nThis gives us \\(2X'X\\mathbf{b}\\) as the result.\n\n\nWhy on earth would we care about this? For one, it helps us understand how we get to our estimates for \\(\\hat \\beta\\) in linear regression. When we have multiple variables, we don’t just want the best estimate for one coefficient, but a vector of coefficients. See more here.\nIn MLE, we will find the gradient of the log likelihood function. We will further go into the second derivatives to arrive at what is called the Hessian. More on that later."
  },
  {
    "objectID": "03-TheMATH.html#practice-problems",
    "href": "03-TheMATH.html#practice-problems",
    "title": "3  The Math",
    "section": "3.6 Practice Problems",
    "text": "3.6 Practice Problems\n\nWhat is \\(24/3 + 5^2 - (8 -4)\\)?\nWhat is \\(\\sum_{i = 1}^5 (i*3)\\)?\nTake the derivative of \\(f(x) =v(4x^2 + 6)^2\\) with respect to \\(x\\).\nTake the derivative of \\(f(x) = e^{2x + 3}\\) with respect to \\(x\\).\nTake the derivative of \\(f(x) = log (x + 3)^2\\) with respect to \\(x\\).\n\nGiven \\(X\\) is an \\(n\\) x \\(k\\) matrix,\n\n\\((X^{T}X)^{-1}X^{T}X\\) can be simplified to?\n\\(((X^{T}X)^{-1}X^{T})^{T} =\\) ?\nIf \\(\\nu\\) is a constant, how does \\((X^{T}X)^{-1}X^{T} \\nu X(X^{T}X)^{-1}\\) simplify?\nIf a matrix \\(P\\) is idempotent, \\(PP =\\) ?\n\n\n3.6.1 Practice Problem Solutions\n\nWhat is \\(24/3 + 5^2 - (8 -4)\\)?\n\n\n24/3 + 5^2 - (8 -4)\n\n[1] 29\n\n\n\nWhat is \\(\\sum_{i = 1}^5 (i*3)\\)?\n\nBy hand: \\(1 \\times 3 + 2 \\times 3 + 3 \\times 3 + 4 \\times 3 + 5 \\times 3\\)\n\n\n\n## sol 1\n1*3 + 2*3 + 3*3 + 4*3 + 5*3\n\n[1] 45\n\n## sol 2\ni &lt;- 1:5\nsum(i*3)\n\n[1] 45\n\n\n\nTake the derivative of \\(f(x) =v(4x^2 + 6)^2\\) with respect to \\(x\\).\n\nWe can treat \\(v\\) as a number.\n\n\n\\[\\begin{align*}\nf'(x) &= 2* v(4x^2 + 6) * 8x\\\\\n&= 16vx(4x^2 + 6)\n\\end{align*}\\]\n\nTake the derivative of \\(f(x) = e^{2x + 3}\\) with respect to \\(x\\).\n\n\\[\\begin{align*}\nf'(x) &= 2* e^{2x + 3}\\\\\n&= 2e^{2x + 3}\n\\end{align*}\\]\n\nTake the derivative of \\(f(x) = log (x + 3)^2\\) with respect to \\(x\\).\n\nNote we can re-write this as \\(2 * log (x + 3)\\).\n\n\n\\[\\begin{align*}\nf'(x) &= 2 * \\frac{1}{(x + 3)} * 1\\\\\n&= \\frac{2}{(x + 3)}\n\\end{align*}\\] If we didn’t take that simplifying step, we can still solve:\n\\[\\begin{align*}\n      f'(x) &= \\frac{1}{(x + 3)^2} * 2 * (x + 3) *1\\\\\n      &= \\frac{2}{(x + 3)}\n      \\end{align*}\\]\nGiven \\(X\\) is an \\(n\\) x \\(k\\) matrix,\n\n\\((X^{T}X)^{-1}X^{T}X\\) can be simplified to?\n\n\\(I_k\\) the identity matrix\n\n\\(((X^{T}X)^{-1}X^{T})^{T} =\\) ?\n\nRecall our rule \\((AB)^T = B^TA^T\\)\n\\(X(X^TX)^{-1}\\)\n\nIf \\(\\nu\\) is a constant, how does \\((X^{T}X)^{-1}X^{T} \\nu X(X^{T}X)^{-1}\\) simplify?\n\nWe can pull it out front.\n\n\n\\[\\begin{align*}\n    &= \\nu(X^{T}X)^{-1}X^{T}X(X^{T}X)^{-1} \\\\\n    &= \\nu (X^{T}X)^{-1}\n    \\end{align*}\\]\n\nIf a matrix \\(P\\) is idempotent, \\(PP =\\) ?\n\n\\(P\\) from section 3.5.2"
  },
  {
    "objectID": "04-ReviewofOLS.html#introducing-ols-regression",
    "href": "04-ReviewofOLS.html#introducing-ols-regression",
    "title": "4  Review of OLS",
    "section": "4.1 Introducing OLS Regression",
    "text": "4.1 Introducing OLS Regression\nThe regression method describes how one variable depends on one or more other variables. Ordinary Least Squares regression is a linear model with the matrix representation:\n\\(Y = \\alpha + X\\beta + \\epsilon\\)\nGiven values of variables in \\(X\\), the model predicts the average of an outcome variable \\(Y\\). For example, if \\(Y\\) is a measure of how wealthy a country is, \\(X\\) may contain measures related to the country’s natural resources and/or features of its institutions (things that we think might contribute to how wealthy a country is.) In this equation:\n\n\\(Y\\) is the outcome variable (\\(n \\times 1\\)).1\n\\(\\alpha\\) is a parameter representing the intercept\n\\(\\beta\\) is a parameter representing the slope/marginal effect (\\(k \\times 1\\)), and\n\\(\\epsilon\\) is the error term (\\(n \\times 1\\)).\n\nIn OLS, we estimate a line of best fit to predict \\(\\hat{Y}\\) values for different values of X:\n\n\\(\\hat{Y} = \\hat{\\alpha} + X\\hat{\\beta}\\).\nWhen you see a “\\(\\hat{hat}\\)” on top of a letter, that means it is an estimate of a parameter.\nAs we will see in the next section, in multiple regression, sometimes this equation is represented as just \\(\\hat{Y} = X\\hat{\\beta}\\), where this generally means that \\(X\\) is a matrix that includes several variables and \\(\\hat \\beta\\) is a vector that includes several coefficients, including a coefficient representing the intercept \\(\\hat \\alpha\\)\n\nWe interpret linear regression coefficients as describing how a dependent variable is expected to change when a particular independent variable changes by a certain amount. Specifically:\n\n“Associated with each one unit increase in a variable \\(x_1\\), there is a \\(\\hat{\\beta_1}\\) estimated expected average increase in \\(y\\).”\nIf we have more than one explanatory variable (i.e., a multiple regression), we add the phrase “controlling on/ holding constant other observed factors included in the model.”\n\nWe can think of the interpretation of a coefficient in multiple regression using an analogy to a set of light switches:\n\nWe ask: How much does the light in the room change when we flip one switch, while holding constant the position of all the other switches?\nThis would be a good place to review the Wheelan chapter and Gelman and Hill 3.1 and 3.2 to reinforce what a regression is and how to interpret regression results."
  },
  {
    "objectID": "04-ReviewofOLS.html#diving-deeper-into-ols-matrix-representation",
    "href": "04-ReviewofOLS.html#diving-deeper-into-ols-matrix-representation",
    "title": "4  Review of OLS",
    "section": "4.2 Diving Deeper into OLS Matrix Representation",
    "text": "4.2 Diving Deeper into OLS Matrix Representation\nIn this section, we will review the matrix representation of the OLS regression in more detail and discuss how to derive the estimators for the regression coefficients.2\nOLS in Matrix Form: Let \\(X\\) be an \\(n \\times k\\) matrix where we have observations on k independent variables for n observations. Since our model will usually contain a constant term, one of the columns in the X matrix will contain only ones. This column should be treated exactly the same as any other column in the X matrix.\n\nLet \\(Y\\) be an \\(n \\times 1\\) vector of observations on the dependent variable. Note: because \\(Y\\) is a vector (a matrix with just one column), sometimes it is written in lowercase notation as \\(\\mathbf y\\).\nLet \\(\\epsilon\\) be an \\(n \\times 1\\) vector of disturbances or errors.\nLet \\(\\beta\\) be an \\(k \\times 1\\) vector of unknown population parameters that we want to estimate.\n\n\\(\\begin{pmatrix} y_1 \\\\ y_2 \\\\ y_3 \\\\ y_4 \\\\ ... \\\\ y_n \\end{pmatrix}\\) = \\(\\begin{pmatrix} 1 & x_{11} & x_{12} & x_{13} & ... & x_{1k}\\\\ 1 & x_{21} & x_{22} & x_{23} & ... & x_{2k} \\\\ 1 & x_{31} & x_{32} & x_{33} & ... & x_{3k}\\\\ 1 & x_{41} & x_{42} & x_{43} & ... & x_{4k} \\\\ ... & ... & ... & ... & ... & ... \\\\ 1 & x_{n1} & x_{n2} & x_{n3} & ... & x_{nk}\\end{pmatrix}\\) X \\(\\begin{pmatrix} \\alpha \\\\ \\beta_1 \\\\ \\beta_2 \\\\ \\beta_3 \\\\ ... \\\\ \\beta_k \\end{pmatrix}\\) + \\(\\begin{pmatrix} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\epsilon_3 \\\\ \\epsilon_4 \\\\ ... \\\\ \\epsilon_n \\end{pmatrix}\\)\nOur estimates are then \\(\\mathbf{ \\hat y} = X\\hat \\beta\\). What are the dimensions of this quantity?\nGelman and Hill Section 3.4, pg. 38 provides a nice visual of how this representation maps onto what a typical dataset may look like, where we will try to estimate a set of coefficients that map the relationship between the columns of \\(X\\) and \\(\\mathbf y\\):\n\\\nThis is a good place to review Gelman and Hill 3.4 on different notations for representing the regression model.\n\n4.2.1 Estimating the Coefficients\nModels generally start with some goal. In OLS, our goal is to minimize the sum of squared “residuals.” Here is a video I created to explain why we can represent this as \\(\\mathbf{e'}\\mathbf{e}\\).\n\nNote: at the end of the video it should read \\(X\\hat\\beta\\), not \\(\\hat X \\beta\\)\nWhat is a residual? It’s the difference between y and our estimate of y: \\(y - \\hat y\\). It represents the error in our prediction– how far off our estimate is of the outcome.\nWe can write this in matrix notation in the following way where \\(\\mathbf e\\) is an \\(n \\times 1\\) vector of residuals– a residual for each observation in the data:\n\\[\\begin{align*}\n\\mathbf{e'}\\mathbf{e} &= (Y' - \\hat{\\beta}'X')(Y - X\\hat{\\beta})\\\\\n&=Y'Y - \\hat{\\beta}'X'Y - Y'X\\hat{\\beta} + \\hat{\\beta}'X'X\\hat{\\beta} \\\\\n&= Y'Y - 2\\hat{\\beta}'X'Y + \\hat{\\beta}'X'X\\hat{\\beta}\n\\end{align*}\\]\nRecall we want a line that minimizes this quantity. We minimize the sum of squared residuals by taking the derivative with respect to \\(\\beta\\). (We want to identify the coefficients that help us achieve the goal of minimizing the squared error.) Because we are now deriving an estimate, we will use the hat over \\(\\beta\\):\n\n\\(\\frac{\\delta }{\\delta \\hat \\beta} = -2X'Y + 2X'X\\hat{\\beta}\\)\nSo what is our estimate for \\(\\hat{\\beta}\\)? We take first order conditions\n\n\\[\\begin{align*}\n  0 &=-2X'Y + 2X'X\\hat{\\beta}\\\\\n   \\hat{\\beta} &= (X'X)^{-1}X'Y\n   \\end{align*}\\]\nYou may wonder how we got to these answers. Don’t worry, you will get your chance to solve this! The important thing to note for now, is that we have an analytic solution to our coefficient estimates."
  },
  {
    "objectID": "04-ReviewofOLS.html#ols-regression-in-r",
    "href": "04-ReviewofOLS.html#ols-regression-in-r",
    "title": "4  Review of OLS",
    "section": "4.3 OLS Regression in R",
    "text": "4.3 OLS Regression in R\nTo run a linear regression in R, we use the lm() function.\nThe syntax is lm(y ~ x1, data = mydata) for a regression with y as the name of your dependent variable and there is one explanatory variable x1 where mydata is the name of your data frame.\nlm(y ~ x1 + x2 , data = mydata) is the syntax for a regression with two explanatory variables x1 and x2, where you would add additional variables for larger multivariate regressions. By default, R will include an intercept term in the regression.\n\n4.3.1 Example: Predicting Current Election Votes from Past Election Votes\nIn the American presidential election in 2000, there was an actual controversy in how ballots were cast in the state of Florida. Social scientists used data comparing the election results from 1996 in the state with 2000 as one way to help detect irregularities in the 2000 vote count. For more information on the background of this example, you can watch this video.\nWe will use the data florida.csv available here:\n\n## Load Data\nflorida &lt;- read.csv(\"https://raw.githubusercontent.com/ktmccabe/teachingdata/main/florida.csv\")\n\nThis data set includes several variables described below, where each row represents the voting information for a particular county in Florida.\n\n\n\nName\nDescription\n\n\n\n\ncounty\ncounty name\n\n\nClinton96\nClinton’s votes in 1996\n\n\nDole96\nDole’s votes in 1996\n\n\nPerot96\nPerot’s votes in 1996\n\n\nBush00\nBush’s votes in 2000\n\n\nGore00\nGore’s votes in 2000\n\n\nBuchanan00\nBuchanan’s votes in 2000\n\n\n\nIn 2000, Buchanan was a third party candidate, similar to Perot in 1996. One might think that counties where Perot received a lot of votes in 1996 should also receive a lot in 2000. That is: with a one-vote increase in Perot’s vote, we might expect an average increase in Buchanan’s 2000 vote.\nWe can translate that language into a regression equation:\n\n\\(Buchanan2000 = \\alpha + Perot1996 * \\beta + \\epsilon\\)\n\nIn R, we run this regression the following way. We will save it as an object fit.1. You can name your regression objects anything you want.\n\nfit.1 &lt;- lm(Buchanan00 ~ Perot96, data = florida)\n\n\nsummary(model) provides the summary statistics of the model. In particular, the following statistics are important\n\nEstimate: point estimate of each coefficient\nStd. Error: standard error of each estimate\nt value: indicates the \\(t\\)-statistic of each coefficient under the null hypothesis that it equals zero\nPr(&gt;|t|): indicates the two-sided \\(p\\)-value corresponding to this \\(t\\)-statistic where asterisks indicate the level of statistical significance.\nMultiple R-squared: The coefficient of determination\nAdjusted R-squared: The coefficient of determination adjusting for the degrees of freedom\n\n\nWe will say more to define these quantities in future sections.\n\nsummary(fit.1)\n\n\nCall:\nlm(formula = Buchanan00 ~ Perot96, data = florida)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-612.74  -65.96    1.94   32.88 2301.66 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.34575   49.75931   0.027    0.979    \nPerot96      0.03592    0.00434   8.275 9.47e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 316.4 on 65 degrees of freedom\nMultiple R-squared:  0.513, Adjusted R-squared:  0.5055 \nF-statistic: 68.48 on 1 and 65 DF,  p-value: 9.474e-12\n\n\nR also allows several shortcuts for accessing particular elements of your regression results. Examples:\n\n## Vector of the coefficient estimates only\ncoef(fit.1)\n\n(Intercept)     Perot96 \n 1.34575212  0.03591504 \n\n## Compute confidence intervals for these coefficients\nconfint(fit.1)\n\n                   2.5 %       97.5 %\n(Intercept) -98.03044506 100.72194929\nPerot96       0.02724733   0.04458275\n\n## Table of coefficient results only\nsummary(fit.1)$coefficients\n\n              Estimate   Std. Error    t value     Pr(&gt;|t|)\n(Intercept) 1.34575212 49.759306434 0.02704523 9.785065e-01\nPerot96     0.03591504  0.004340068 8.27522567 9.473505e-12\n\n## Extract standard errors only\nsummary(fit.1)$coefficients[,2]\n\n (Intercept)      Perot96 \n49.759306434  0.004340068 \n\n## Variance-Covariance matrix\nvcov(fit.1)\n\n             (Intercept)       Perot96\n(Intercept) 2475.9885768 -1.360074e-01\nPerot96       -0.1360074  1.883619e-05\n\n## Note that the square root of the diagonal of this matrix provides the standard errors\nsqrt(diag(vcov(fit.1)))\n\n (Intercept)      Perot96 \n49.759306434  0.004340068 \n\n## Degrees of freedom\nfit.1$df.residual\n\n[1] 65\n\n\n\n\n4.3.2 Plotting Regression Results\nWe often don’t want to hide our data under a bushel basket or in complicated regression models. Instead, we might also want to visualize data in R. The function plot() and the function ggplot() from the package ggplot2 are two terrific and flexible functions for visualizing data. We will use the plot() function to visualize the relationship between Perot and Buchanan votes. The example below provides a few arguments you can use within each of these functions, but they are capable of much more.\nAt the core, plotting functions generally work as coordinate systems. You tell R specifically at which x and y coordinates you want your points to be located (e.g., by providing R with a vector of x values and a vector of y values). Then, each function has its own way of allowing you to add bells and whistles to your figure, such as labels (e.g., main, xlab, ylab), point styles (pch), additional lines and points and text (e.g., abline(), lines(), points(), text()), or x and y scales for the dimensions of your axes (e.g., xlim, ylim). You can create a plot without these additional features, but most of the time, you will add them to make your plots look good! and be informative! We will do a lot of plotting this semester.\nNote: feel free to use plot() or ggplot() or both. ggplot has similar capabilities as plot but relies on a different “grammar” of graphics. For example, see the subtle differences in the two plots below.\n\n## Plot\nplot(x = florida$Perot96, # x-values\n     y = florida$Buchanan00, # y-values\n     main = \"Perot and Buchanan Votes\", # label for main title\n     ylab = \"Buchanan Votes\", # y-axis label\n     xlab = \"Perot Votes\", # x-axis label\n     pch = 20) # point type\nabline(fit.1, col = \"red\") # adds a red regression line\n\n\n\n\n\n## ggplot version\nlibrary(ggplot2)\nggplot(data = florida, # which data frame\n       mapping = aes(x = Perot96, y = Buchanan00)) + # x and y coordinates\n  geom_point() +  # tells R we want a scatterplot\n  geom_smooth(method = \"lm\",\n              se = FALSE, colour = \"red\",\n              data = florida, aes(x=Perot96, y=Buchanan00)) + # adds lm regression line\n  ggtitle(\"Perot and Buchanan Votes\") + # main title\n  labs(x = \"Perot Votes\", y = \"Buchanan Votes\") + # x and y labels\n  theme_bw() # changes theme (e.g., color of background)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n## Note: data = florida, aes(x=Perot96, y=Buchanan00) in the geom_smooth line is not necessary if it is the same mapping at the first line. Required if data are different\n\nTip: you might want to save your plots as .pdf or .png after you create it. You can do this straight from your R code. How you do it varies by function. The files will save to your working directory unless you specify a different file path. The code below is the same as above except it has additional lines for saving the plots:\n\n## Plot\npdf(file = \"myfirstmleplot.pdf\", width = 7, height = 5) # play around with the dimensions\nplot(x = florida$Perot96, # x-values\n     y = florida$Buchanan00, # y-values\n     main = \"Perot and Buchanan Votes\", # label for main title\n     ylab = \"Buchanan Votes\", # y-axis label\n     xlab = \"Perot Votes\", # x-axis label\n     pch = 20) # point type\nabline(fit.1, col = \"red\") # adds a red regression line\ndev.off() # this closes your pdf file\n\n## ggplot version\nggplot(data = florida, # which data frame\n       mapping = aes(x = Perot96, y = Buchanan00)) + # x and y coordinates\n  geom_point() +  # tells R we want a scatterplot\n  geom_smooth(method = \"lm\",  \n              se = FALSE, colour = \"red\",\n              data = florida, aes(x=Perot96, y=Buchanan00)) + # adds lm regression line\n  ggtitle(\"Perot and Buchanan Votes\") + # main title\n  labs(x = \"Perot Votes\", y = \"Buchanan Votes\") + # x and y labels\n  theme(plot.title = element_text(hjust = 0.5)) +# centers the title\n  theme_bw() # changes theme (e.g., color of background)\nggsave(\"myfirstmleggplot.png\", device=\"png\", width = 7, height = 5) # saves the last ggplot\n\n\n\n4.3.3 Finding Coefficients without lm\nLet’s put our matrix algebra and R knowledge together. In the previous section, we found that \\(\\hat \\beta = (X'X)^{-1}X'Y\\). If we do that math directly in R, there is no need to use lm() to find those coefficients.\nTo do so, we need \\(X\\) and \\(Y\\).\nRecall \\(Y\\) is an \\(n \\times 1\\) vector representing the outcome of our model. In this case, \\(Y\\) is Buchanan00.\n\nY &lt;- florida$Buchanan00\n\nRecall, \\(X\\) is a \\(n \\times k\\) matrix representing our independent variables and a column of 1’s for the intercept. Let’s build this matrix using cbind which was introduced in section 2.\n\nX &lt;- cbind(1, florida$Perot96)\ndim(X)\n\n[1] 67  2\n\n\nGreat, now we have \\(X\\) and \\(Y\\), so it’s just about a little math. Because \\(Y\\) is a vector, let’s make sure R knows to treat it like an \\(n \\times 1\\) matrix.\n\nY &lt;- cbind(Y)\ndim(Y)\n\n[1] 67  1\n\n\nRecall the solve() and t() functions take the inverse and transpose of matrices.\n\nbetahat &lt;- solve(t(X) %*% X) %*% t(X) %*% Y\n\nFinally, let’s compare the results from our model using lm() with these results.\n\nbetahat\ncoef(fit.1)\n\n              Y\n[1,] 1.34575212\n[2,] 0.03591504\n(Intercept)     Perot96 \n 1.34575212  0.03591504 \n\n\nWe did it! In the problem set, you will get more experience using the analytic solutions to solve for quantities of interest instead of the built-in functions.\n\n\n4.3.4 OLS Practice Problems\nHere are a couple of (ungraded) problems to modify the code above and gain additional practice with data wrangling and visualization in R. As you might have noticed in the example, there is a big outlier in the data. We will see how this observation affects the results.\n\nUsing a linear regression examine the relationship between Perot and Buchanan votes, controlling for Bill Clinton’s 1996 votes.\n\n\nProvide a one sentence summary of the relationship between Perot and Buchanan’s votes.\nIs the relationship significant at the \\(p &lt; 0.05\\) level? What about the relationship between Clinton and Buchanan votes?\nWhat are the confidence intervals for the Perot coefficient results?\nWhat is the residual for the estimate for Palm Beach County– PalmBeach in the county variable?\n\n\nLet’s go back to the bivariate case.\n\n\nSubset the data to remove the county PalmBeach.\nCreate a scatterplot of the relationship between Perot votes and Buchanan votes within this subset. This time make the points blue.\nAdd a regression line based on this subset of data.\nAdd a second regression line in a different color based on the initial bivariate regression we ran in the example, where all data were included.\nDescribe the differences in the regression lines.\n\n\n\n4.3.5 Code for solutions\n\nfit.multiple &lt;- lm(Buchanan00 ~ Perot96 + Clinton96, data = florida)\nsummary(fit.multiple)\n\n\nCall:\nlm(formula = Buchanan00 ~ Perot96 + Clinton96, data = florida)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-705.06  -49.17   -4.71   27.34 2254.89 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept) 14.110353  51.644141   0.273  0.78556   \nPerot96      0.027394   0.010095   2.714  0.00854 **\nClinton96    0.001283   0.001372   0.935  0.35325   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 316.7 on 64 degrees of freedom\nMultiple R-squared:  0.5196,    Adjusted R-squared:  0.5046 \nF-statistic: 34.61 on 2 and 64 DF,  p-value: 6.477e-11\n\nconfint(fit.multiple)[2,]\n\n      2.5 %      97.5 % \n0.007228254 0.047560638 \n\nflorida$res &lt;- residuals(fit.multiple)\nflorida$res[florida$county == \"PalmBeach\"]\n\n[1] 2254.893\n\nflorida.pb &lt;- subset(florida, subset = (county != \"PalmBeach\"))\nfit2 &lt;- lm(Buchanan00 ~ Perot96, data = florida.pb)\n\nggplot(data = florida.pb, # which data frame\n       mapping = aes(x = Perot96, y = Buchanan00)) + # x and y coordinates\n  geom_point(color=\"blue\") +  # tells R we want a scatterplot\n  geom_smooth(method = \"lm\", \n              se = FALSE, colour = \"green\",\n              data = florida.pb, aes(x=Perot96, y=Buchanan00)) + # adds lm regression line\n    geom_smooth(method = \"lm\", \n              se = FALSE, colour = \"red\",\n              data = florida, aes(x=Perot96, y=Buchanan00)) + # adds lm regression line\n  ggtitle(\"Perot and Buchanan Votes\") + # main title\n  labs(x = \"Perot Votes\", y = \"Buchanan Votes\") + # x and y labels\n  theme(plot.title = element_text(hjust = 0.5)) +# centers the title\n  theme_bw() # changes theme (e.g., color of background)\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "04-ReviewofOLS.html#uncertainty-and-regression",
    "href": "04-ReviewofOLS.html#uncertainty-and-regression",
    "title": "4  Review of OLS",
    "section": "4.4 Uncertainty and Regression",
    "text": "4.4 Uncertainty and Regression\nWe have now gone through the process of minimizing the sum of squared errors (\\(\\mathbf{e'e}\\)) and deriving estimates for the OLS coefficients \\(\\hat \\beta = (X'X)^{-1}X'Y\\). In this section, we will discuss how to generate estimates of the uncertainty around these estimates.\nWhere we are going:\n\nIn the last section, we visited an example related to the 2000 election in Florida. We regressed county returns for Buchanan in 2000 (Y) on county returns for Perot in 1996 (X).\n\n\n## Load Data\nflorida &lt;- read.csv(\"https://raw.githubusercontent.com/ktmccabe/teachingdata/main/florida.csv\")\nfit.1 &lt;- lm(Buchanan00 ~ Perot96, data = florida)\nsummary(fit.1)\n\n\nCall:\nlm(formula = Buchanan00 ~ Perot96, data = florida)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-612.74  -65.96    1.94   32.88 2301.66 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.34575   49.75931   0.027    0.979    \nPerot96      0.03592    0.00434   8.275 9.47e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 316.4 on 65 degrees of freedom\nMultiple R-squared:  0.513, Adjusted R-squared:  0.5055 \nF-statistic: 68.48 on 1 and 65 DF,  p-value: 9.474e-12\n\n\nThe summary output of the model shows many different quantities in addition to the coefficient estimates. In particular, in the second column of the summary, we see the standard errors of the coefficients. Like many statistical software programs, the lm() function neatly places these right next to the coefficients. We will now discuss how we get to these values.\n\n4.4.1 Variance of the Coefficients\nThe standard error is the square root of the variance, representing the typical deviation we would expect to see between our estimates \\(\\hat \\beta\\) of the parameter \\(\\beta\\) across repeated samples. So to get to the standard error, we just need to get to an estimate of the variance.\nLet’s take the journey. First the math. As should start becoming familiar, we have our initial regression equation, which describes the relationship between the independent variables and dependent variables.\n\nStart with the model: \\(Y = X\\beta + \\epsilon\\)\n\nWe want to generate uncertainty for our estimate of \\(\\hat \\beta =(X'X)^{-1}X'Y\\)\n\nNote: Conditional on fixed values of \\(X\\) (I say fixed values because this is our data. We know \\(X\\) from our dataset.), the only random component is \\(\\epsilon\\).\n\nWhat does that mean? Essentially, the random error term in our regression equation is what is giving us the uncertainty. If \\(Y\\) was a deterministic result of \\(X\\), we would have no need for it, but it’s not. The relationship is not exact, varies sample to sample, subject to random perturbations, represented by \\(\\epsilon\\).\n\n\nBelow we go through how to arrive at the mathematical quantity representing the variance of \\(\\hat \\beta\\) which we will notate as \\(\\mathbf{V}(\\hat\\beta)\\). The first part of the math below is just substituting terms:\n\\[\\begin{align*}\n\\mathbf{V}(\\widehat{\\beta}) &=\n\\mathbf{V}( (X^T X) ^{-1} X^T Y))  \\\\\n&= \\underbrace{\\mathbf{V}( (X^T X) ^{-1} X^T (X\\beta + \\epsilon))}_\\text{Sub in the expression for Y from above}  \\\\\n&= \\underbrace{\\mathbf{V}((X^T X) ^{-1} X^T X \\beta + (X^T X) ^{-1} X^T \\epsilon)}_\\text{Distribute the term to the items in the parentheses}  \\\\\n&= \\underbrace{\\mathbf{V}(\\beta + (X^T X) ^{-1} X^T \\epsilon)}_\\text{Using the rules of inverses, the two terms next to $\\beta$ canceled each other out}  \n\\end{align*}\\]\nThe next part of the math requires us to use knowledge of the definition of variance and the rules associated. We draw on two in particular:\n\nThe variance of a constant is zero.\nWhen you have a constant multipled by a random variable, e.g., \\(\\mathbf{V}(4d)\\), it can come out of the variance operator, but must be squared: \\(16\\mathbf{V}(d)\\)\nPutting these together: \\(\\mathbf{V}(2 + 4d)= 16\\mathbf{V}(d)\\)\n\nKnowing these rules, we can proceed: \\[\\begin{align*}\n\\mathbf{V}(\\widehat{\\beta}) &= \\mathbf{V}(\\beta + (X^T X) ^{-1} X^T \\epsilon) \\\\\n&=\\underbrace{ \\mathbf{V}((X^T X) ^{-1} X^T \\epsilon)}_\\text{$\\beta$ drops out because in a regression it is an unkown \"parameter\"-- it's constant, which means its variance is zero.}\\\\\n&= \\underbrace{(X^T X)^{-1}X^T \\mathbf{V}( \\epsilon) ((X^T X)^{-1}X^T)^T}_\\text{We can move $(X^T X)^{-1}X^T$ out front because our data are fixed quantities, but in doing so, we have to \"square\" the matrix.}\\\\\n&= (X^T X)^{-1}X^T \\mathbf{V}( \\epsilon) X (X^T X)^{-1}\n\\end{align*}\\]\nThe resulting quantity is our expression for the \\(\\mathbf{V}(\\hat \\beta)\\). However, in OLS, we make an additional assumption that allows us to further simplify the expression. We assume homoscedasticity aka “constant” or “equal error variance” which says that the variance of the errors are the same across observations: \\(\\mathbf{V}(\\epsilon) = \\sigma^2 I_n\\).\n\nIf we assume homoscedastic errors, then Var\\((\\epsilon) = \\sigma^2 I_n\\)\n\n\\[\\begin{align*}\n\\mathbf{V}(\\widehat{\\beta})  &= (X^T X)^{-1}X^T \\mathbf{V}( \\epsilon) X (X^T X)^{-1}\\\\\n&= \\underbrace{(X^T X)^{-1}X^T \\sigma^2I_n X (X^T X)^{-1}}_\\text{Assume homoskedasticity}\\\\\n&= \\underbrace{\\sigma^2(X^T X)^{-1} X^T X (X^T X)^{-1}}_\\text{Because it is a constant, we can move it out in front of the matrix multiplication, and then simplify the terms.}  \\\\\n&= \\sigma^2(X^T X)^{-1}\n\\end{align*}\\]\nAll done! This expression: \\(\\sigma^2(X^T X)^{-1}\\) represents the variance of our coefficient estimates. Note its dimensions: \\(k \\times k\\). It has the same number of rows and columns as the number of our independent variables (plus the intercept).\nThere is one catch, though. How do we know what \\(\\sigma^2\\) is? Well, we don’t. Just like the unknown parameter \\(\\beta\\), we have to estimate it in our regression model.\nJust like with the coefficients, we notate our estimate as \\(\\widehat{\\sigma}^2\\). Our estimate is based on the observed residual errors in the model and is as follows:\n\n\\(\\widehat{\\sigma}^2 = \\frac{1}{N-K}\\sum_{i=1}^N \\widehat{\\epsilon_i^2} = \\frac{1}{N-K} \\mathbf{e'e}\\)\n\nThat means our estimate of the variance of the coefficients is found within: \\(\\hat \\sigma^2(X^T X)^{-1}\\)\nAgain, this is a \\(k \\times k\\) matrix and is often called the variance covariance matrix. We can extract this quantity from our linear models in R using vcov().\n\nvcov(fit.1)\n\n             (Intercept)       Perot96\n(Intercept) 2475.9885768 -1.360074e-01\nPerot96       -0.1360074  1.883619e-05\n\n\nThis is the same that we would get if manually we took the residuals and multiplied it by our \\(X\\) matrix according to the formula above:\n\nX &lt;- cbind(1, florida$Perot96)\ne &lt;- cbind(residuals(fit.1))\nsigmahat &lt;- ((t(e) %*% e) / (nrow(florida) -2)) \n## tell r to stop treating sigmahat as a matrix\nsigmahat &lt;-as.numeric(sigmahat)\nXtX &lt;- solve(t(X) %*%X)\nsigmahat * XtX\n\n             [,1]          [,2]\n[1,] 2475.9885768 -1.360074e-01\n[2,]   -0.1360074  1.883619e-05\n\n\nThe terms on the diagonal represent the variance of a particular coefficient in the model.The standard error of a particular coefficient \\(k\\) is: s.e.(\\(\\hat{\\beta_k}) = \\sqrt{\\widehat{\\sigma}^2 (X'X)^{-1}}_{kk}\\). The off-diagonal components represent the covariances between the coefficients.\nRecall that the standard error is just the square root of the variance. So, to get the nice standard errors we saw in the summary output, we can take the square root of the quantities on the diagonal of this matrix.\n\nsqrt(diag(vcov(fit.1)))\n\n (Intercept)      Perot96 \n49.759306434  0.004340068 \n\nsummary(fit.1)$coefficients[,2]\n\n (Intercept)      Perot96 \n49.759306434  0.004340068 \n\n\nWhy should I care?\n\nWell R actually doesn’t make it that easy to extract standard errors from the summary output. You can see above that the code for extracting the standard errors using what we know about them being the square root of the variance is about as efficient as extracting the second column of the coefficient component of the summary of the model.\nSometimes, we may think that the assumption of equal error variance is not feasible and that we have unequal error variance or “heteroscedasticity.” Researchers have developed alternative expressions to model unequal error variance. Generally, what this means is they can no longer make that simplifying assumption, have to stop at the step with the uglier expression \\((X^T X)^{-1}X^T \\mathbf{V}( \\epsilon) X (X^T X)^{-1}\\) and then assume something different about the structure of the errors in order to estimate the coefficients. These alternative variance estimators are generally what are referred to as “robust standard errors.” There are many different robust estimators, and you will likely come across them in your research.\n\nSome of you may have learned the formal, general definition for variance as defined in terms of expected value: \\(\\mathbb{E}[(\\widehat{m} - \\mathbb{E}(\\widehat{m}))^2 ]\\). We could also start the derivation there. This is not required for the course, but it is below if you find it useful. In particular, it can help show why we wend up needing to square a term when we move it outside the variance operator:\n\\[\\begin{align*}\n\\mathbf{V}(\\widehat{\\beta}) &= \\mathbb{E}[(\\widehat{\\beta} - \\mathbb{E}(\\hat \\beta))^2)] \\\\\n&= \\mathbb{E}[(\\widehat{\\beta} - \\beta)^2]\\\\\n&= \\mathbb{E}[(\\widehat{\\beta} - \\beta)(\\widehat{\\beta} - \\beta)^T ] \\\\ &=\n\\mathbb{E}[(X^T X) ^{-1} X^TY - \\beta)(X^T X) ^{-1} X^TY - \\beta)^T]  \\\\\n&= \\mathbb{E}[(X^T X) ^{-1} X^T(X\\beta + \\epsilon) - \\beta)(X^T X) ^{-1} X^T(X\\beta + \\epsilon) - \\beta)^T]\\\\\n&=  \\mathbb{E}[(X^T X) ^{-1} X^TX\\beta + (X^T X) ^{-1} X^T\\epsilon - \\beta)(X^T X) ^{-1} X^TX\\beta + (X^T X) ^{-1} X^T\\epsilon - \\beta)^T]\\\\\n&=  \\mathbb{E}[(\\beta + (X^T X) ^{-1} X^T\\epsilon - \\beta)(\\beta + (X^T X) ^{-1} X^T\\epsilon - \\beta)^T]\\\\\n&=  \\mathbb{E}[(X^T X) ^{-1} X^T\\epsilon)(X^T X) ^{-1} X^T\\epsilon)^T]\\\\\n&= (X^T X) ^{-1} X^T\\mathbb{E}(\\epsilon\\epsilon^T)X(X^T X) ^{-1}\\\\\n&= \\underbrace{(X^T X) ^{-1} X^T\\sigma^2I_nX(X^T X) ^{-1}}_\\text{Assume homoskedasticity}\\\\\n&= \\sigma^2(X^T X) ^{-1} X^TX(X^T X) ^{-1}\\\\\n&= \\sigma^2(X^T X) ^{-1}\n\\end{align*}\\]\nNote: Along the way, in writing \\(\\mathbb{E}(\\hat \\beta) = \\beta\\), we have implicitly assumed that \\(\\hat \\beta\\) is an “unbiased” estimator of \\(\\beta\\). This is not free. It depends on an assumption that the error term in the regression \\(\\epsilon\\) is independent of our independent variables. This can be violated in some situations, such as when we have omitted variable bias, which is discussed at the end of our OLS section.\n\n\n4.4.2 Hypothesis Testing\nMost of the time in social science, we run a regression because we have some hypothesis about how a change in our independent variable affects the change in our outcome variable.\nIn OLS, we can perform a hypothesis test for each independent variable in our data. The structure of the hypothesis test is:\n\nNull hypothesis: \\(\\beta_k = 0\\)\n\nThis essentially means that we don’t expect a particular \\(x_k\\) independent variable to have a relationship with our outcome variable.\n\nAlternative hypothesis: \\(\\beta_k \\neq 0\\)\n\nWe do expect a positive or negative relationship between a particular \\(x_k\\) and the dependent variable.\n\n\nWe can use our estimates for \\(\\hat \\beta\\) coefficients and their standard errors to come to a conclusion about rejecting or failing to reject the null hypothesis of no relationship by using a t-test.\nIn a t-test, we take our coefficient estimates and divide them by the standard error in order to “standardize” them on a scale that we can use to determine how likely it is we would have observed a value for \\(\\hat \\beta\\) as extreme or more extreme as the one we observed in a world where the true \\(\\beta = 0\\). This is just like a t-test you might have encountered before for a difference in means between groups, except this time our estimate is \\(\\hat \\beta\\).\n\\[\\begin{align*}\nt_{\\hat \\beta_k} &= \\frac{\\hat \\beta_k}{s.e.(\\hat \\beta_k)}\n\\end{align*}\\]\nGenerally speaking, when \\(t\\) is about +/-2 or greater in magnitude, the coefficient will be “significant” at conventional levels (i.e., \\(p &lt;0.05\\)), meaning that we are saying that it is really unlikely we would have observed a value as big as \\(\\hat \\beta_k\\) if the null hypothesis were true. Therefore, we can reject the null hypothesis.\nHowever, to get a specific quantity, we need to calculate the p-value, which depends on the t-statistic and the degrees of freedom in the model. The degrees of freedom in a regression model are \\(N-k\\), the number of observations in the model minus the number of independent variables plus the intercept.\nIn R, we can calculate p-values using the pt() function. By default, most people use two-sided hypothesis tests for regression. So to do that, we are going to find the area on each side of the t values, or alternatively, multiply the area to the right of our positive t-value by 2.\n\n## Let's say t was 2.05 and \n## And there were 32 observations and 3 variables in the regression plus an intercept\nt &lt;- 2.05\ndf.t &lt;- 32 -4\np.value &lt;- 2 * (pt(abs(t), df=df.t, lower.tail = FALSE))\np.value\n\n[1] 0.04983394\n\n\nLet’s do this for the florida example. First, we can find t by dividing our coefficients by the standard errors.\n\nt &lt;- coef(fit.1) / (sqrt(diag(vcov(fit.1))))\nt \n\n(Intercept)     Perot96 \n 0.02704523  8.27522567 \n\n## Compare with output\nsummary(fit.1)$coefficients[, 3]\n\n(Intercept)     Perot96 \n 0.02704523  8.27522567 \n\n\nWe can then find the p-values.\n\nt &lt;- coef(fit.1) / (sqrt(diag(vcov(fit.1))))\ndf.t &lt;- fit.1$df.residual\np.value &lt;- 2 * (pt(abs(t), df=df.t, lower.tail = FALSE))\np.value\n\n (Intercept)      Perot96 \n9.785065e-01 9.473505e-12 \n\nsummary(fit.1)$coefficients[, 4]\n\n (Intercept)      Perot96 \n9.785065e-01 9.473505e-12 \n\n\nWe see that the coefficient for Perot96 is significant. The p-value is tiny. In R, for small numbers, R automatically shifts to scientific notation. The 9.47e-12 means the p-value is essentially zero, with the stars in the summary output indicating the p-value is \\(p &lt; 0.001\\). R will also output a test of the significance of the intercept using the same formula as all other coefficients. This generally does not have much interpretive value, so you are usually safe to ignore it.\nConfidence Intervals\nInstead of representing the significance using p-values, sometimes it is helpful to report confidence intervals around the coefficients. This can be particularly useful when visualizing the coefficients. The 95% confidence interval represents roughly 2 standard errors above and below the coefficient. The key thing to look for is whether it overlaps with zero (not significant) or does not (in which case the coefficient is significant).\nThe precise formula is\n\\(\\widehat{\\beta}_k\\) Confidence intervals: \\(\\widehat{\\beta}_k - t_{crit.value} \\times s.e._{\\widehat{\\beta}_k}, \\widehat{\\beta}_k + t_{crit.value} \\times s.e_{\\widehat{\\beta}_k}\\)\nIn R, we can use qt() to get the specific critical value associated with a 95% confidence interval. This will be around 2, but fluctuates depending on the degrees of freedom in your model (which are function of your sample size and how many variables you have in the model.) R also has a shortcut confint() function to extract the coefficients from the model. Below we do this for the Perot96 coefficient.\n\n## Critical values from t distribution at .95 level\nqt(.975, df = fit.1$df.residual) # n- k degrees of freedom\n\n[1] 1.997138\n\n## Shortcut\nconfint(fit.1)[2,]\n\n     2.5 %     97.5 % \n0.02724733 0.04458275 \n\n## By hand\ncoef(fit.1)[2] - qt(.975, df = fit.1$df.residual)*sqrt(diag(vcov(fit.1)))[2]\n\n   Perot96 \n0.02724733 \n\ncoef(fit.1)[2] + qt(.975, df = fit.1$df.residual)*sqrt(diag(vcov(fit.1)))[2]\n\n   Perot96 \n0.04458275 \n\n\n\n\n4.4.3 Goodness of Fit\nA last noteworthy component to the standard regression output is the goodness of fit statistics. For this class, we can put less attention on these, though there will be some analogues when we get into likelihood.\nThese are measures of how much of the total variation in our outcome measure can be explained by our model, as well as how far off are our estimates from the truth.\nFor the first two measures R-squared and Adjusted R-squared, we draw on three quantities:\n\nTotal Sum of Squares–how much variance in \\(Y_i\\) is there to explain?\n\n\\(TSS: \\sum_{i=1}^N (Y_i -\\overline Y_i)^2\\)\n\nEstimated Sum of Squares–how much of this variance do we explain?\n\n\\(ESS: \\sum_{i=1}^N (\\widehat Y_i -\\overline Y_i)^2\\)\n\nResidual Sum of Squares–how much variance is unexplained?\n\n\\(RSS: \\sum_{i=1}^N ( Y_i -\\widehat Y_i)^2\\)\n\n\\(TSS = ESS + RSS\\)\nMultiple R-squared: \\(\\frac{ESS}{TSS}\\)\n\nThis is a value from 0 to 1, representing the proportion of the variance in the outcome that can be explained by the model. Higher values are generally considered better, but there are many factors that can affect R-squared values. In most social science tasks where the goal is to engage in hypothesis testing of coefficients, this measure is of less value.\n\nAdjusted R-squared: \\(1 - \\frac{\\frac{RSS}{n - k}}{\\frac{TSS}{n - 1}}\\)\n\nThis is essentially a penalized version of R-squared. When you add additional predictors to a model, the R-squared value can never decrease, even if the predictors are useless. The Adjusted R-squared adds a consideration for the degrees of freedom into the equation, creating a penalty for adding more and more predictors.\n\nResidual standard error aka root mean squared error aka square root of the mean squared residual: \\(r.s.e = \\sqrt{\\frac{RSS}{n-k}}\\)\n\nThis represents the typical deviation of an estimate of the outcome from the actual outcome. This quantity is often used to assess the quality of prediction exercises. It is used less often in social science tasks where the goal is hypothesis testing of the relationship between one or more independent variables and the outcome.\n\n\nF-Statistic\nSo far we have conducted hypothesis tests for each individual coefficient. We can also conduct a global hypothesis test, where the null hypothesis is that all coefficients are zero, with the alternative being that at least one coefficient is nonzero. This is the test represented by the F-statistic in the regression output.\nThe F-statistic helps us test the null hypothesis that all of the regression slopes are 0: \\(H_0 = \\beta_1 = \\beta_2 = \\dots = \\beta_k = 0\\)\n\n\\(F_0 = \\frac{ESS/(k - 1)}{RSS/(n - k)}\\)\nThe F-Statistic has two separate degrees of freedom.\n\nThe model sum of squares degrees of freedom (ESS) are \\(k - 1\\).\nThe residual error degrees of freedom (RSS) are \\(n - k\\).\nIn a regression output, the model degrees of freedom are generally the first presented: “F-statistic: 3.595 on \\((k - 1) = 1\\) and \\((n - k) = 48\\) DF.”\n\n\nNote: This test is different from our separate hypothesis tests that a \\(k\\) regression slope is 0. For that, we use the t-tests discussed above."
  },
  {
    "objectID": "04-ReviewofOLS.html#generating-predictions-from-regression-models",
    "href": "04-ReviewofOLS.html#generating-predictions-from-regression-models",
    "title": "4  Review of OLS",
    "section": "4.5 Generating predictions from regression models",
    "text": "4.5 Generating predictions from regression models\nThe regression coefficients tell us how much \\(Y\\) is expected to change for a one-unit change in \\(x_k\\). It does not immediately tell us the values we estimate our outcome (\\(\\hat Y\\)) to take conditional on particular values of \\(x_k\\). While often knowing our independent variables have a significant effect on the outcome and the size of the coefficient is sufficient for testing our hypotheses, it can be helpful for interpretation’s sake, to see the estimated values for the outcome. This is going to be particularly important once we get into models like logistic regression, where the coefficients won’t be immediately interpretable.\nRecall that our equation for estimating values of our outcomes is:\n\n\\(\\hat Y = X\\hat \\beta\\) This can also be written out in long form for any particular observation \\(i\\):\n\\(\\hat y_i = \\hat \\alpha + \\hat \\beta_1*x_1i + \\hat \\beta_2*x_2i + ... \\hat\\beta_k*x_ki\\)\n\nThe estimated values of our regression \\(\\hat Y\\) are often called the “fitted values.” In R, you can identify the estimated values for each observation using the fitted() command.\n\n## Y hat for the first observation in the data\nfitted(fit.1)[1]\n\n      1 \n291.252 \n\n\nAgain, this is just the multiplication of the matrix \\(X\\) and \\(\\hat \\beta\\). If we have already run a regression model in R, one shortcut for getting the \\(X\\) matrix, is to use the model.matrix command. We can get \\(\\hat \\beta\\) using the coef() command.\n\nX &lt;- model.matrix(fit.1)\nhead(X) # head() shows about the first six values of an object\n\n  (Intercept) Perot96\n1           1    8072\n2           1     667\n3           1    5922\n4           1     819\n5           1   25249\n6           1   38964\n\nbetahat &lt;- coef(fit.1)\n\nOur fitted values are then just\n\nyhat &lt;- X %*% betahat\nhead(yhat)\n\n        [,1]\n1  291.25196\n2   25.30108\n3  214.03462\n4   30.76017\n5  908.16461\n6 1400.73939\n\n\nIf I want to generate an estimate for any particular observation, I could also just extract its specific value for Perot96.\n\nflorida$Perot96[1]\n\n[1] 8072\n\n\nLet’s estimate the Buchanan 2000 votes for the first county in the data with Perot 96 votes of 8072. We can write it out as \\(\\hat Buchanan00_1 =\\hat \\alpha + \\hat \\beta*Perot96_1\\)\n\nbuch00hat &lt;- coef(fit.1)[1] + coef(fit.1)[2]*florida$Perot96[1]\nbuch00hat\n\n(Intercept) \n    291.252 \n\n\nWhat is useful about this is that now we have the coefficient estimates, we can apply them to any values of \\(X\\) we wish in order to generate estimates/predictions of the values \\(Y\\) will take given particular values of our independent variables.\nOne function that is useful for this (as a shortcut) is the predict(fit, newdata=newdataframe) function in R. It allows you to enter in “newdata”– meaning values of the \\(X\\) variables for which you want to generate estimates of \\(Y\\) based on the coefficient estimates of your regression model.\nFor example, let’s repeat the calculation from above for Perot96 = 8072.\n\npredict(fit.1, newdata = data.frame(Perot96 = 8072))\n\n      1 \n291.252 \n\n\nWe can also generate confidence intervals around these estimates by adding interval = \"confidence\" in the command.\n\npredict(fit.1, newdata = data.frame(Perot96 = 8072), interval=\"confidence\")\n\n      fit      lwr      upr\n1 291.252 213.7075 368.7964\n\n\nWe can also simultaneously generate multiple predictions by supplying a vector of values in the predict() command. For example, let’s see the estimated Buchanan votes for when the Perot 1996 votes took values of 1000 to 10,000 by intervals of 1,000.\n\npredict(fit.1, newdata = data.frame(Perot96 = c(1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000, 9000, 10000)))\n\n        1         2         3         4         5         6         7         8 \n 37.26079  73.17583 109.09087 145.00591 180.92095 216.83599 252.75104 288.66608 \n        9        10 \n324.58112 360.49616 \n\n\nThe important thing to note about the predict() command is that if you have multiple independent variables, you have to specify the values you want each of them to take when generating the estimated values of y.\n\nfit.2 &lt;- lm(Buchanan00 ~ Perot96 + Clinton96, data = florida)\n\nFor example, let’s build a second model with Clinton96 as an additional predictor. In order to generate the same prediction for different values of Perot 1996 votes, we need to tell R at what values we should “hold constant” Clinton96. I.e., we want to see how hypothetical changes in Perot96 votes influence changes in Buchanan 2000 votes while also leaving the Clinton votes identical. This is that lightswitch metaphor– flipping one switch, while keeping the rest untouched.\nThere are two common approaches to doing this. 1) We can hold constant Clinton96 votes at its mean value in the data 2) We can keep Clinton96 at its observed values in the data. In linear regression, it’s not going to matter which approach you take. In other models we will talk about later, this distinction may matter more substantially because of how our quantities of interest change across different values of \\(X\\hat \\beta\\).\nThe first approach is easily implemented in predict.\n\npredict(fit.2, newdata = data.frame(Perot96 = 8072, Clinton96 = mean(florida$Clinton96)))\n\n      1 \n283.997 \n\n\nFor the second approach, what we will do is generate an estimate for Buchanan’s votes in 2000 when Perot96 takes 8072 votes, and we keep Clinton96’s votes at whatever value it currently is in the data. That is, we will generate \\(n\\) estimates for Buchanan’s votes when Perot takes 8072. Then, we will take the mean of this as our “average estimate” of Buchanan’s votes in 2000 based on Perot’s votes at a level of 8072. We can do this in one of two ways:\n\n## Manipulating the X matrix\nX &lt;- model.matrix(fit.2)\n## Replace Perot96 column with all 8072 values\nX[, \"Perot96\"] &lt;- 8072\nhead(X) #take a peek\n\n  (Intercept) Perot96 Clinton96\n1           1    8072     40144\n2           1    8072      2273\n3           1    8072     17020\n4           1    8072      3356\n5           1    8072     80416\n6           1    8072    320736\n\n## Generate yhat\nyhats &lt;-X %*% coef(fit.2)\n\n## take the mean\nmean(yhats)\n\n[1] 283.997\n\n\n\n## Use predict\nyhats &lt;- predict(fit.2, newdata = data.frame(Perot96=8072, Clinton96=florida$Clinton96))\nmean(yhats)\n\n[1] 283.997\n\n\nNow often, after we generate these predicted values, we want to display them for the whole world to see. You will get a chance to visualize values like this using the plotting functions in the problem sets. We have already seen one example of this in the simple bivariate case, when R plotted the bivariate regression line in section 4.3.2. However, the predict function extends are capabilities to plot very specific values of \\(X\\) and \\(\\hat Y\\) for bivariate or multiple regressions.\n\nThe predict() function is also very relevant when we move to logistic, probit, etc. regressions. This is just the start of a beautiful friendship between you and predict() and associated functions."
  },
  {
    "objectID": "04-ReviewofOLS.html#wrapping-up-ols",
    "href": "04-ReviewofOLS.html#wrapping-up-ols",
    "title": "4  Review of OLS",
    "section": "4.6 Wrapping up OLS",
    "text": "4.6 Wrapping up OLS\nLinear regression is a great way to explain the relationship between one or more independent variables and an outcome variables. However, there is no free lunch. We have already mentioned a couple of assumptions along the way. Below we will summarize these and other assumptions. These are things you should be mindful of when you use linear regression in your own work. Some conditions that generate violations of these assumptions can also motivate why we will seek out alternative methods, such as those that rely on maximum likelihood estimation.\nThis is a good place to review Gelman section 3.6.\n\nExogeneity. This one we haven’t discussed yet, but is an important assumption for letting us interpret our coefficients \\(\\hat \\beta\\) as “unbiased” estimates of the true parameters \\(\\beta\\). We assume that the random error in the regression model \\(\\epsilon\\) is indeed random, and uncorrelated with and independent of our independent variables \\(X\\). Formally:\n\n\\(\\mathbb{E}(\\epsilon| X) = \\mathbb{E}(\\epsilon) = 0\\).\nThis can be violated, for example, when we suffer from Omitted Variable Bias due to having an “endogenous explanatory variable” that is correlated with some unobserved or unaccounted for factor. This bias comes from a situation where there is some variable that we have left out of the model (\\(Z\\)), and is therefore a part of the unobserved error term. Moreover this variable which is correlated with–and a pre-cursor of– our independent variables and is a cause of our dependent variable. A failure to account for omitted variables can create bias in our coefficient estimates. Concerns about omitted variable bias often prompt people to raise their hands in seminars and ask questions like, “Well have you accounted for this? Have you accounted for that? How do you know it is \\(X\\) driving your results and not \\(Z\\)?” If we omit important covariates, we may wrongly attribute an effect to \\(X\\) when it was really the result of our omitted factor \\(Z\\). Messing discusses this here.\nThis is a really tough assumption. The only real way to guarantee the independence of your error term and the independent variables is if you have randomly assigned values to the independent variables (such as what you do when you randomly assign people to different treatment conditions in an experiment). Beyond random assignment, you have to rely on theory to understand what variables you need to account for in the regression model to be able to plausibly claim your estimate of the relationship between a given independent variable and the dependent variable is unbiased. Failing to control for important factors can lead to misleading results, such as what happens in Simpson’s paradox, referenced in the Messing piece.\nDanger Note 1: The danger here, though, is that the motivation for avoiding omitted variable bias might be to keep adding control after control after control into the regression model. However, model building in this way can sometimes be atheoretical and result in arbitrary fluctuations in the size of your coefficients and their significance. At its worse, it can lead to “p-hacking” where researchers keep changing their models until they find the results they like. The Lenz and Sahn article on Canvas talks more about the dangers of arbitrarily adding controls to the model.\nDanger Note 2: We also want to avoid adding “bad controls” to the model. Messing talks about this in the medium article as it relates to collider bias. We want to avoid adding controls to our model, say \\(W\\) that are actually causes of \\(Y\\) and causes of \\(X\\) instead of the other way around.\nModel building is a delicate enterprise that depends a lot on having a solid theory that guides the choice of variables.\n\nHomoscedasticity. We saw this when defining the variance estimator for the OLS coefficients. We assume constant error variance. This can be violated when we think observations at certain values of our independent variables may have different magnitudes of error than observations at other values of our independent variables.\n\nNo correlation in the errors. The error terms are not correlated with each other. This can be violated in time series models (where we might think past, present, and future errors are correlated) or in cases where our observations are nested in some hierarchical structures (e.g., perhaps students in a school) and the errors are correlated.\n\nNo perfect collinearity. The \\(X\\) matrix must be full rank: We cannot have linear dependence between columns in our X matrix. We saw this in the tutorial when we tried to add the dummy variables for all of our racial groups into a regression at once. When there is perfect collinearity between variables, our regression will fail.\n\nWe should also avoid situations where we have severe multicollinearity. This can happen when we include two or more variables in a regression model that are highly correlated (just not perfectly correlated). While the regression will still run in this case, it can inflate the standard errors of the coefficients, making it harder to detect significant effects. This is particularly problematic in smaller samples.\n\nLinearity. The relationship between the independent and dependent variables needs to be linear in the parameters. It should be modeled as the addition of constants or parameters multiplied by the independent variables. If instead the model requires the multiplication of parameters, this is no longer linear (e.g., \\(\\beta^2\\)). Linearity also often refers to the shape of the model. Our coefficients tell us how much change we expect in the outcome for each one-unit change in an independent variable. We might think some relationships are nonlinear– meaning this rate of change varies across values of the independent variables. If that is the case, we need to shift the way our model is specified to account for this or change modeling approaches.\n\nFor example, perhaps as people get older (one-unit changes in age), they become more politically engaged, but at some age level, their political engagement starts to decline. This would mean the slope (that expected change in political engagement for each one-unit change in age) is not constant across all levels of age. There, we might be violating linearity in the curvature of the relationship between the independent and dependent variables. This is sometimes why you might see \\(age^2\\) or other nonlinear terms in regression equations to better model this curvature.\nLikewise, perhaps each additional level of education doesn’t result in the same average increase in \\(y\\). If not, you could consider including categorical dummy variables for different levels of education instead of treating education as a numeric variable.\n\nNormality. We assume that the errors are normally distributed. As Gelman 3.6 notes, this is a less important assumption and is generally not required.\n\nOLS Properties\nWhy we like OLS. When we meet our assumptions, OLS produces the best linear unbiased estimates (BLUE). A discussion of this here. We have linearity in our parameters (e.g., \\(\\beta\\) and not \\(\\beta^2\\) for example). The unbiasedness means that the expected value (aka the average over repeated samples) of our estimates \\(\\mathbb{E}(\\hat \\beta)= \\beta\\) is the true value. Our estimates are also efficient, which has to do with the variance, not only are our estimates true in expectation, but we also have lower variance than an alternative linear unbiased estimator could get us. If our assumptions fail, then we might no longer have BLUE. OLS estimates are also consistent, meaning that as the sample gets larger and larger, the estimates start converging to the truth.\nNow, a final hidden assumption in all of this is that the sample of our data is representative of the population we are trying to make inferences about. If that is not the case, then we may no longer be making unbiased observations to that population level. Further adjustments may be required (e.g., analyses of survey data sometimes use weights to adjust estimates to be more representative).\nWhen we violate these assumptions, OLS may no longer be best, and we may opt for other approaches. More soon!\n\n4.6.1 Practice Problems\n\nLet’s use the florida data. Run a regression according to the following formula:\n\n\\(Buchanan00_i = \\alpha + \\beta_1*Perot96_i + \\beta_2*Dole96 + \\beta_3*Gore00 + \\epsilon\\)\n\nReport the coefficient for Perot96. What do you conclude about the null hypothesis that there is no relationship between 1996 Perot votes and 2000 Buchanan votes?\nWhat is the confidence interval for the Perot96 coefficient estimate?\nWhen Perot 1996 vote is 5500, what is the expected 2000 Buchanan vote?\n\n\n\n4.6.2 Practice Problem Code for Solutions\n\nfit.practice &lt;- lm(Buchanan00 ~ Perot96 + Dole96 + Gore00, data = florida)\n\ncoef(fit.practice)[\"Perot96\"]\n\n   Perot96 \n0.02878927 \n\nconfint(fit.practice)[\"Perot96\", ]\n\n      2.5 %      97.5 % \n0.004316382 0.053262150 \n\nexpbuch &lt;- model.matrix(fit.practice)\nexpbuch[,\"Perot96\"] &lt;- 5500\nmean(expbuch %*% as.matrix(coef(fit.practice)))\n\n[1] 211.1386"
  },
  {
    "objectID": "04-ReviewofOLS.html#week-2-example",
    "href": "04-ReviewofOLS.html#week-2-example",
    "title": "4  Review of OLS",
    "section": "4.7 Week 2 Example",
    "text": "4.7 Week 2 Example\nThis example is based on Dancygier, Rafaela; Egami, Naoki; Jamal, Amaney; Rischke, Ramona, 2020, “Hate Crimes and Gender Imbalances: Fears over Mate Competition and Violence against Refugees”, published in the American Journal of Political Science. Replication data is available here. We will draw on the survey portion of the article and replicate Table 1 in the paper. The pre-print is available here.\nThe abstract is: As the number of refugees rises across the world, anti-refugee violence has become a pressing concern. What explains the incidence and support of such hate crime? We argue that fears among native men that refugees pose a threat in the competition for female partners is a critical but understudied factor driving hate crime. Employing a comprehensive dataset on the incidence of hate crime across Germany, we first demonstrate that hate crime rises where men face disadvantages in local mating markets. Next, we complement this ecological evidence with original survey measures and confirm that individual-level support for hate crime increases when men fear that the inflow of refugees makes it more difficult to find female partners. Mate competition concerns remain a robust predictor even when controlling for antirefugee views, perceived job competition, general frustration, and aggressiveness. We conclude that a more complete understanding of hate crime and immigrant conflict must incorporate marriage markets and mate competition.\nThe authors summarize their hypotheses as, “the notion that male refugees are engaged in romantic relationships with German women has received considerable media attention from a variety of sources, with coverage ranging from the curious to the outright hostile. We argue that the prospect of refugee-native mate competition can trigger or compound resentment against refugees, including support for hate crime” pg. 14\n\nlibrary(foreign)\ndat_use &lt;- read.dta(\"https://github.com/ktmccabe/teachingdata/blob/main/dat_use.dta?raw=true\")\n\nThe data include wave 4 of an online survey fielded in Germany through Respondi from September 2016 to December 2017). Each wave was designed to be nationally representative on age (starting at 18), gender, and state (Bundesland) with a sample of about 3,000 respondents in each wave.\nKey variables include\n\nhate_violence_means representing respondents’ agreement or disagreement to the Only Means question: “When it comes to the refugee problem, violence is sometimes the only means that citizens have to get the attention of German politicians.” from (1) disagree strongly to (4) agree strongly.\nMateComp_cont, Mate Competition. “The inflow of refugees makes it more difficult for native men to find female partners.” from (1) disagree strongly to (4) agree strongly.\nThe data include several other variables related to the demographics of the respondents and measures representing potential alternative explanations, such as JobComp_cont (agreement with “the inflow of young male refugees makes it more difficult for young native men to find apprenticeships and jobs”) and LifeSatis_cont (0-10 scale, ranging from extremely dissatisfied to extremely satisfied).\n\nLet’s pause here to ask a few questions about research design.\n\nWhat is the outcome? What is the independent variable of interest?\n\nHow would we write out the bivariate regression model?\n\nWhy OLS? (e.g., why not experiment?)\nWhat types of alternative explanations might exist?\n\nOk let’s move to replication of the first two regression models in the table:\n\n\n\nTry to code these on your own, then click for the solution\n\n\nlm1 &lt;- lm(hate_violence_means ~ MateComp_cont, data=dat_use)\n\nlm2 &lt;- lm(hate_violence_means ~ MateComp_cont + JobComp_cont + LifeSatis_cont, data=dat_use)\n\n\nNow, let’s compare the summary output of each output.\n\n\nTry on your own, then click for the solution\n\n\nsummary(lm1)\n\n\nCall:\nlm(formula = hate_violence_means ~ MateComp_cont, data = dat_use)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.6804 -0.3694 -0.3694  0.6306  2.6306 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    0.93235    0.03302   28.24   &lt;2e-16 ***\nMateComp_cont  0.43702    0.01635   26.73   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7993 on 3017 degrees of freedom\nMultiple R-squared:  0.1915,    Adjusted R-squared:  0.1912 \nF-statistic: 714.6 on 1 and 3017 DF,  p-value: &lt; 2.2e-16\n\nsummary(lm2)\n\n\nCall:\nlm(formula = hate_violence_means ~ MateComp_cont + JobComp_cont + \n    LifeSatis_cont, data = dat_use)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.8275 -0.4783 -0.1842  0.3171  2.8452 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     0.788623   0.057849   13.63   &lt;2e-16 ***\nMateComp_cont   0.263437   0.020261   13.00   &lt;2e-16 ***\nJobComp_cont    0.249956   0.018672   13.39   &lt;2e-16 ***\nLifeSatis_cont -0.014725   0.006292   -2.34   0.0193 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7751 on 3015 degrees of freedom\nMultiple R-squared:  0.2403,    Adjusted R-squared:  0.2395 \nF-statistic: 317.9 on 3 and 3015 DF,  p-value: &lt; 2.2e-16\n\n\n\nQuestions about the output\n\nHow should we interpret the coefficients?\n\nDo they support the researchers’ hypotheses?\n\nHow would we extract confidence intervals from the coefficients?\nHow should we interpret the goodness of fit statistics at the bottom of the output?\n\nAdditional Models We can also run regressions with even more covariates, as the authors do in models 3-6 in the paper.\n\n\nClick to reveal regression code below.\n\n\nlm3 &lt;- lm(hate_violence_means ~ MateComp_cont + JobComp_cont + LifeSatis_cont + \n            factor(age_group) +     # age group\n            factor(gender) +     # gender \n            factor(state) +     # state  \n            factor(citizenship) +    # german citizen\n            factor(marital) +    # marital status\n            factor(religion) +    # religious affiliation\n            eduyrs +    # education\n            factor(occupation) +    # main activity\n            factor(income) +   # income\n            factor(household_size) +   # household size\n            factor(self_econ),    # subjective social status\n          data=dat_use)\n\nlm4 &lt;- lm(hate_violence_means ~ MateComp_cont + JobComp_cont + LifeSatis_cont + \n            factor(age_group) +   # age group\n            factor(gender) +   # gender \n            factor(state) +   # state  \n            factor(citizenship) +  # german citizen\n            factor(marital) +  # marital status\n            factor(religion) +  # religious affiliation\n            eduyrs +  # education\n            factor(occupation) +  # main activity\n            factor(income) + # income\n            factor(household_size) + # household size\n            factor(self_econ) + # subjective social status\n            factor(ref_integrating) + # Refugee Index (National-level; Q73) 8 in total\n            factor(ref_citizenship) + factor(ref_reduce) + factor(ref_moredone) + factor(ref_cultgiveup) + \n            factor(ref_economy) + factor(ref_crime) + factor(ref_terror),\n          data=dat_use)\n\nlm5 &lt;- lm(hate_violence_means ~ MateComp_cont + JobComp_cont + LifeSatis_cont + \n            factor(age_group) +      # age group\n            factor(gender) +      # gender \n            factor(state) +      # state  \n            factor(citizenship) +     # german citizen\n            factor(marital) +     # marital status\n            factor(religion) +     # religious affiliation\n            eduyrs + # education\n            factor(occupation) +     # main activity\n            factor(income) +    # income\n            factor(household_size) +    # household size\n            factor(self_econ) +    # subjective social status\n            factor(ref_integrating) + # Refugee Index (National-level; Q73) 8 in total\n            factor(ref_citizenship) + factor(ref_reduce) + factor(ref_moredone) + factor(ref_cultgiveup) + \n            factor(ref_economy) + factor(ref_crime) + factor(ref_terror) + \n            factor(ref_loc_services) +    # Refugee Index (Local, Q75)\n            factor(ref_loc_economy) + factor(ref_loc_crime) + factor(ref_loc_culture) + factor(ref_loc_islam) + \n            factor(ref_loc_schools) + factor(ref_loc_housing) + factor(ref_loc_wayoflife), ## end\n          data=dat_use)\n\nformula.5 &lt;- \n  as.character(\"hate_violence_means ~ MateComp_cont + JobComp_cont + \n               LifeSatis_cont +  factor(age_group) + factor(gender) + \n               factor(state) + factor(citizenship) + factor(marital) + \n               factor(religion) + eduyrs + factor(occupation) + \n               factor(income) + factor(household_size) + factor(self_econ) + \n               factor(ref_integrating) + factor(ref_citizenship) + factor(ref_reduce) + \n               factor(ref_moredone) + factor(ref_cultgiveup) + \n               factor(ref_economy) + factor(ref_crime) + factor(ref_terror)  + \n               factor(ref_loc_services) +  factor(ref_loc_economy) + factor(ref_loc_crime) + \n               factor(ref_loc_culture) + factor(ref_loc_islam) + \n               factor(ref_loc_schools) + factor(ref_loc_housing) + factor(ref_loc_wayoflife)\")\n\nformula.6 &lt;- paste(formula.5, \"factor(distance_ref) + factor(settle_ref)\", \n                   \"lrscale + afd + muslim_ind + afd_ind + contact_ind\", \n                   sep=\"+\", collapse=\"+\") \n\nlm6 &lt;- lm(as.formula(formula.6), data=dat_use)\n\n\n\n\nTable 1: Mate Competition Predicts Support for Hate Crime.\n\n\n\n\n \n\n\nModel 1\n\n\nModel 2\n\n\nModel 3\n\n\nModel 4\n\n\nModel 5\n\n\nModel 6\n\n\n\n\n\n\n(Intercept)\n\n\n0.9323***\n\n\n0.7886***\n\n\n1.3982***\n\n\n1.4437***\n\n\n1.4372***\n\n\n1.3878***\n\n\n\n\n \n\n\n(0.0330)\n\n\n(0.0578)\n\n\n(0.2293)\n\n\n(0.2296)\n\n\n(0.2388)\n\n\n(0.2372)\n\n\n\n\nMateComp_cont\n\n\n0.4370***\n\n\n0.2634***\n\n\n0.2361***\n\n\n0.2064***\n\n\n0.1848***\n\n\n0.1550***\n\n\n\n\n \n\n\n(0.0163)\n\n\n(0.0203)\n\n\n(0.0206)\n\n\n(0.0194)\n\n\n(0.0195)\n\n\n(0.0189)\n\n\n\n\nJobComp_cont\n\n\n \n\n\n0.2500***\n\n\n0.2358***\n\n\n0.0772***\n\n\n0.0650***\n\n\n0.0559***\n\n\n\n\n \n\n\n \n\n\n(0.0187)\n\n\n(0.0189)\n\n\n(0.0195)\n\n\n(0.0196)\n\n\n(0.0189)\n\n\n\n\nLifeSatis_cont\n\n\n \n\n\n-0.0147**\n\n\n-0.0136*\n\n\n-0.0034\n\n\n-0.0020\n\n\n-0.0001\n\n\n\n\n \n\n\n \n\n\n(0.0063)\n\n\n(0.0070)\n\n\n(0.0065)\n\n\n(0.0065)\n\n\n(0.0062)\n\n\n\n\nfactor(age_group)30-39\n\n\n \n\n\n \n\n\n-0.1323**\n\n\n-0.1800***\n\n\n-0.1821***\n\n\n-0.1957***\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.0525)\n\n\n(0.0489)\n\n\n(0.0488)\n\n\n(0.0471)\n\n\n\n\nfactor(age_group)40-49\n\n\n \n\n\n \n\n\n-0.2088***\n\n\n-0.2771***\n\n\n-0.2709***\n\n\n-0.2808***\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.0525)\n\n\n(0.0490)\n\n\n(0.0490)\n\n\n(0.0474)\n\n\n\n\nfactor(age_group)50-59\n\n\n \n\n\n \n\n\n-0.2876***\n\n\n-0.3621***\n\n\n-0.3480***\n\n\n-0.3580***\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.0535)\n\n\n(0.0501)\n\n\n(0.0502)\n\n\n(0.0486)\n\n\n\n\nfactor(age_group)60 and older\n\n\n \n\n\n \n\n\n-0.3362***\n\n\n-0.3427***\n\n\n-0.3199***\n\n\n-0.3073***\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.0678)\n\n\n(0.0631)\n\n\n(0.0631)\n\n\n(0.0610)\n\n\n\n\nfactor(gender)Female\n\n\n \n\n\n \n\n\n-0.0247\n\n\n-0.0528*\n\n\n-0.0451\n\n\n-0.0233\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.0299)\n\n\n(0.0281)\n\n\n(0.0282)\n\n\n(0.0272)\n\n\n\n\nfactor(state)Bayern\n\n\n \n\n\n \n\n\n0.0097\n\n\n-0.0168\n\n\n-0.0148\n\n\n-0.0229\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.0531)\n\n\n(0.0494)\n\n\n(0.0491)\n\n\n(0.0474)\n\n\n\n\nfactor(state)Berlin\n\n\n \n\n\n \n\n\n0.0106\n\n\n-0.0023\n\n\n-0.0259\n\n\n0.0037\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.0776)\n\n\n(0.0722)\n\n\n(0.0720)\n\n\n(0.0706)\n\n\n\n\nfactor(state)Brandenburg\n\n\n \n\n\n \n\n\n-0.1572*\n\n\n-0.1023\n\n\n-0.0949\n\n\n-0.1082\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.0896)\n\n\n(0.0833)\n\n\n(0.0834)\n\n\n(0.0805)\n\n\n\n\nfactor(state)Bremen\n\n\n \n\n\n \n\n\n-0.1266\n\n\n-0.1252\n\n\n-0.1750\n\n\n-0.0508\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.1531)\n\n\n(0.1423)\n\n\n(0.1415)\n\n\n(0.1365)\n\n\n\n\nfactor(state)Hamburg\n\n\n \n\n\n \n\n\n-0.0208\n\n\n-0.0140\n\n\n-0.0255\n\n\n-0.0269\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.1016)\n\n\n(0.0946)\n\n\n(0.0941)\n\n\n(0.0914)\n\n\n\n\nfactor(state)Hessen\n\n\n \n\n\n \n\n\n-0.1207*\n\n\n-0.0931\n\n\n-0.0766\n\n\n-0.0853\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.0647)\n\n\n(0.0604)\n\n\n(0.0601)\n\n\n(0.0578)\n\n\n\n\nfactor(state)Mecklenburg-Vorpommern\n\n\n \n\n\n \n\n\n-0.0849\n\n\n-0.1008\n\n\n-0.1015\n\n\n-0.1572*\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.1035)\n\n\n(0.0961)\n\n\n(0.0959)\n\n\n(0.0928)\n\n\n\n\nfactor(state)Niedersachsen\n\n\n \n\n\n \n\n\n-0.0993\n\n\n-0.1052*\n\n\n-0.1055*\n\n\n-0.1190**\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.0607)\n\n\n(0.0564)\n\n\n(0.0561)\n\n\n(0.0543)\n\n\n\n\nfactor(state)Nordrhein-Westfalen\n\n\n \n\n\n \n\n\n-0.0299\n\n\n-0.0277\n\n\n-0.0414\n\n\n-0.0414\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.0501)\n\n\n(0.0465)\n\n\n(0.0465)\n\n\n(0.0450)\n\n\n\n\nfactor(state)Rheinland-Pfalz\n\n\n \n\n\n \n\n\n-0.1178\n\n\n-0.1137\n\n\n-0.1089\n\n\n-0.1407**\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.0750)\n\n\n(0.0700)\n\n\n(0.0697)\n\n\n(0.0675)\n\n\n\n\nfactor(state)Saarland\n\n\n \n\n\n \n\n\n-0.0264\n\n\n0.0227\n\n\n0.0353\n\n\n-0.0250\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.1293)\n\n\n(0.1203)\n\n\n(0.1199)\n\n\n(0.1162)\n\n\n\n\nfactor(state)Sachsen\n\n\n \n\n\n \n\n\n-0.0357\n\n\n-0.0813\n\n\n-0.1118\n\n\n-0.1470**\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.0734)\n\n\n(0.0683)\n\n\n(0.0684)\n\n\n(0.0662)\n\n\n\n\nfactor(state)Sachsen-Anhalt\n\n\n \n\n\n \n\n\n-0.0193\n\n\n-0.0811\n\n\n-0.0765\n\n\n-0.1024\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.0927)\n\n\n(0.0862)\n\n\n(0.0863)\n\n\n(0.0836)\n\n\n\n\nfactor(state)Schleswig-Holstein\n\n\n \n\n\n \n\n\n-0.2402***\n\n\n-0.1693**\n\n\n-0.1725**\n\n\n-0.1839**\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.0862)\n\n\n(0.0806)\n\n\n(0.0802)\n\n\n(0.0773)\n\n\n\n\nfactor(state)Thringen\n\n\n \n\n\n \n\n\n0.0090\n\n\n-0.0076\n\n\n-0.0081\n\n\n-0.0654\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.0957)\n\n\n(0.0889)\n\n\n(0.0887)\n\n\n(0.0858)\n\n\n\n\nfactor(citizenship)1\n\n\n \n\n\n \n\n\n-0.0621\n\n\n-0.0831\n\n\n-0.0739\n\n\n-0.0314\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.1064)\n\n\n(0.0990)\n\n\n(0.0983)\n\n\n(0.0946)\n\n\n\n\nfactor(marital)With partner, not living together\n\n\n \n\n\n \n\n\n0.0825\n\n\n0.0323\n\n\n0.0145\n\n\n0.0099\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.0572)\n\n\n(0.0532)\n\n\n(0.0529)\n\n\n(0.0509)\n\n\n\n\nfactor(marital)With partner, living together\n\n\n \n\n\n \n\n\n0.0968*\n\n\n0.0570\n\n\n0.0582\n\n\n0.0342\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.0562)\n\n\n(0.0524)\n\n\n(0.0521)\n\n\n(0.0501)\n\n\n\n\nfactor(marital)Married\n\n\n \n\n\n \n\n\n0.0884\n\n\n0.0487\n\n\n0.0509\n\n\n0.0165\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.0538)\n\n\n(0.0500)\n\n\n(0.0497)\n\n\n(0.0479)\n\n\n\n\nfactor(marital)Registered partnership\n\n\n \n\n\n \n\n\n0.0982\n\n\n0.1345\n\n\n0.1601\n\n\n0.1976\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.1880)\n\n\n(0.1753)\n\n\n(0.1744)\n\n\n(0.1679)\n\n\n\n\nfactor(marital)Divorced / separated\n\n\n \n\n\n \n\n\n0.1150*\n\n\n0.0938\n\n\n0.0853\n\n\n0.0877\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.0616)\n\n\n(0.0573)\n\n\n(0.0569)\n\n\n(0.0549)\n\n\n\n\nfactor(marital)Widowed\n\n\n \n\n\n \n\n\n0.1612*\n\n\n0.1556*\n\n\n0.1309\n\n\n0.1243\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.0920)\n\n\n(0.0855)\n\n\n(0.0855)\n\n\n(0.0824)\n\n\n\n\nfactor(religion)Roman Catholic\n\n\n \n\n\n \n\n\n-0.0034\n\n\n-0.0300\n\n\n-0.0333\n\n\n-0.0713*\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.0407)\n\n\n(0.0378)\n\n\n(0.0377)\n\n\n(0.0364)\n\n\n\n\nfactor(religion)Protestant\n\n\n \n\n\n \n\n\n-0.0640*\n\n\n-0.0396\n\n\n-0.0249\n\n\n-0.0556*\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.0375)\n\n\n(0.0348)\n\n\n(0.0349)\n\n\n(0.0337)\n\n\n\n\nfactor(religion)Protestant Free Church\n\n\n \n\n\n \n\n\n-0.0225\n\n\n-0.0240\n\n\n-0.0170\n\n\n-0.0780\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.1022)\n\n\n(0.0951)\n\n\n(0.0945)\n\n\n(0.0911)\n\n\n\n\nfactor(religion)Other Protestant\n\n\n \n\n\n \n\n\n0.7822**\n\n\n0.9286***\n\n\n0.9234***\n\n\n0.8768**\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.3855)\n\n\n(0.3587)\n\n\n(0.3565)\n\n\n(0.3441)\n\n\n\n\nfactor(religion)Eastern Orthodox\n\n\n \n\n\n \n\n\n0.2751\n\n\n0.1666\n\n\n0.1350\n\n\n0.1455\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.1869)\n\n\n(0.1744)\n\n\n(0.1735)\n\n\n(0.1672)\n\n\n\n\nfactor(religion)Other Christian\n\n\n \n\n\n \n\n\n-0.0119\n\n\n0.0406\n\n\n0.0645\n\n\n0.0954\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.1633)\n\n\n(0.1518)\n\n\n(0.1514)\n\n\n(0.1458)\n\n\n\n\nfactor(religion)Jewish\n\n\n \n\n\n \n\n\n0.0827\n\n\n-0.1329\n\n\n-0.0855\n\n\n-0.2074\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.3460)\n\n\n(0.3217)\n\n\n(0.3202)\n\n\n(0.3081)\n\n\n\n\nfactor(religion)Muslim\n\n\n \n\n\n \n\n\n-0.0578\n\n\n0.0586\n\n\n0.0046\n\n\n0.0906\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.1667)\n\n\n(0.1554)\n\n\n(0.1549)\n\n\n(0.1509)\n\n\n\n\nfactor(religion)Eastern religion (Buddhism, Hinduism, Sikhism, Shinto, Tao, etc.)\n\n\n \n\n\n \n\n\n-0.0026\n\n\n0.0043\n\n\n0.0289\n\n\n0.0138\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.1215)\n\n\n(0.1130)\n\n\n(0.1129)\n\n\n(0.1086)\n\n\n\n\nfactor(religion)Other non-Christian religion\n\n\n \n\n\n \n\n\n0.3904*\n\n\n0.4759**\n\n\n0.4412**\n\n\n0.2675\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.2333)\n\n\n(0.2175)\n\n\n(0.2168)\n\n\n(0.2089)\n\n\n\n\nfactor(religion)Christian, but not close to a particular religious community\n\n\n \n\n\n \n\n\n-0.0270\n\n\n0.0102\n\n\n0.0083\n\n\n-0.0177\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.0611)\n\n\n(0.0567)\n\n\n(0.0566)\n\n\n(0.0544)\n\n\n\n\nfactor(religion)No answer\n\n\n \n\n\n \n\n\n0.1273\n\n\n0.2257**\n\n\n0.2106**\n\n\n0.1926*\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.1116)\n\n\n(0.1039)\n\n\n(0.1036)\n\n\n(0.0999)\n\n\n\n\neduyrs\n\n\n \n\n\n \n\n\n-0.0179***\n\n\n-0.0139***\n\n\n-0.0121***\n\n\n-0.0088**\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.0042)\n\n\n(0.0039)\n\n\n(0.0039)\n\n\n(0.0038)\n\n\n\n\nfactor(occupation)Parental leave\n\n\n \n\n\n \n\n\n0.1940\n\n\n0.1368\n\n\n0.1319\n\n\n0.1606\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.1589)\n\n\n(0.1478)\n\n\n(0.1473)\n\n\n(0.1417)\n\n\n\n\nfactor(occupation)In schooling / vocational training, student\n\n\n \n\n\n \n\n\n-0.0690\n\n\n-0.1196\n\n\n-0.1279\n\n\n-0.1244\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.0926)\n\n\n(0.0862)\n\n\n(0.0859)\n\n\n(0.0828)\n\n\n\n\nfactor(occupation)Unemployed / seeking work\n\n\n \n\n\n \n\n\n0.0088\n\n\n-0.0339\n\n\n-0.0420\n\n\n-0.0451\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.1080)\n\n\n(0.1004)\n\n\n(0.0999)\n\n\n(0.0961)\n\n\n\n\nfactor(occupation)Retired\n\n\n \n\n\n \n\n\n0.1055\n\n\n0.0493\n\n\n0.0470\n\n\n0.0298\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.0816)\n\n\n(0.0758)\n\n\n(0.0755)\n\n\n(0.0726)\n\n\n\n\nfactor(occupation)Permanently sick or disabled\n\n\n \n\n\n \n\n\n-0.1574\n\n\n-0.1490\n\n\n-0.1458\n\n\n-0.1507\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.1287)\n\n\n(0.1198)\n\n\n(0.1194)\n\n\n(0.1149)\n\n\n\n\nfactor(occupation)Unskilled worker\n\n\n \n\n\n \n\n\n0.1800*\n\n\n0.0893\n\n\n0.0642\n\n\n0.0221\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.0934)\n\n\n(0.0870)\n\n\n(0.0865)\n\n\n(0.0833)\n\n\n\n\nfactor(occupation)Skilled worker\n\n\n \n\n\n \n\n\n0.2026**\n\n\n0.1379*\n\n\n0.1393*\n\n\n0.1073\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.0871)\n\n\n(0.0812)\n\n\n(0.0808)\n\n\n(0.0778)\n\n\n\n\nfactor(occupation)Employee in low / medium position\n\n\n \n\n\n \n\n\n0.1065\n\n\n0.0560\n\n\n0.0524\n\n\n0.0653\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.0740)\n\n\n(0.0688)\n\n\n(0.0685)\n\n\n(0.0659)\n\n\n\n\nfactor(occupation)Employee in high position\n\n\n \n\n\n \n\n\n0.0536\n\n\n-0.0116\n\n\n-0.0165\n\n\n-0.0292\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.0824)\n\n\n(0.0766)\n\n\n(0.0763)\n\n\n(0.0735)\n\n\n\n\nfactor(occupation)Civil servant\n\n\n \n\n\n \n\n\n-0.0009\n\n\n-0.1061\n\n\n-0.1342\n\n\n-0.1687\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.1307)\n\n\n(0.1214)\n\n\n(0.1209)\n\n\n(0.1166)\n\n\n\n\nfactor(occupation)Senior civil servant\n\n\n \n\n\n \n\n\n0.0173\n\n\n0.0085\n\n\n0.0262\n\n\n-0.0256\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.1481)\n\n\n(0.1380)\n\n\n(0.1372)\n\n\n(0.1325)\n\n\n\n\nfactor(occupation)Senior civil servant &lt;96&gt; highest level\n\n\n \n\n\n \n\n\n-0.0083\n\n\n-0.1016\n\n\n-0.0793\n\n\n-0.0611\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.1262)\n\n\n(0.1174)\n\n\n(0.1169)\n\n\n(0.1127)\n\n\n\n\nfactor(occupation)Self-employed / freelancer\n\n\n \n\n\n \n\n\n0.1171\n\n\n0.0323\n\n\n0.0396\n\n\n0.0693\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.0889)\n\n\n(0.0828)\n\n\n(0.0824)\n\n\n(0.0794)\n\n\n\n\nfactor(occupation)Other\n\n\n \n\n\n \n\n\n0.2269\n\n\n0.0467\n\n\n0.0232\n\n\n0.0080\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.1724)\n\n\n(0.1607)\n\n\n(0.1601)\n\n\n(0.1540)\n\n\n\n\nfactor(income)500 to below 1,000 &lt;80&gt;\n\n\n \n\n\n \n\n\n0.0260\n\n\n0.0768\n\n\n0.0750\n\n\n-0.0028\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.1024)\n\n\n(0.0953)\n\n\n(0.0948)\n\n\n(0.0914)\n\n\n\n\nfactor(income)1,000 to below 1,500 &lt;80&gt;\n\n\n \n\n\n \n\n\n0.0677\n\n\n0.0714\n\n\n0.0642\n\n\n-0.0274\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.1011)\n\n\n(0.0943)\n\n\n(0.0937)\n\n\n(0.0904)\n\n\n\n\nfactor(income)1,500 to below 2,000 &lt;80&gt;\n\n\n \n\n\n \n\n\n0.1360\n\n\n0.1289\n\n\n0.1319\n\n\n0.0564\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.1024)\n\n\n(0.0955)\n\n\n(0.0949)\n\n\n(0.0914)\n\n\n\n\nfactor(income)2,000 to below 2,500 &lt;80&gt;\n\n\n \n\n\n \n\n\n0.1320\n\n\n0.1155\n\n\n0.1146\n\n\n0.0028\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.1045)\n\n\n(0.0974)\n\n\n(0.0969)\n\n\n(0.0935)\n\n\n\n\nfactor(income)2,500 to below 3,000 &lt;80&gt;\n\n\n \n\n\n \n\n\n0.0479\n\n\n0.0606\n\n\n0.0615\n\n\n-0.0466\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.1071)\n\n\n(0.0998)\n\n\n(0.0992)\n\n\n(0.0956)\n\n\n\n\nfactor(income)3,000 to below 3,500 &lt;80&gt;\n\n\n \n\n\n \n\n\n0.1659\n\n\n0.1531\n\n\n0.1557\n\n\n0.0384\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.1108)\n\n\n(0.1031)\n\n\n(0.1025)\n\n\n(0.0989)\n\n\n\n\nfactor(income)3,500 to below 4,000 &lt;80&gt;\n\n\n \n\n\n \n\n\n0.2256**\n\n\n0.2133**\n\n\n0.2101**\n\n\n0.0785\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.1138)\n\n\n(0.1059)\n\n\n(0.1054)\n\n\n(0.1017)\n\n\n\n\nfactor(income)4,000 to below 4,500 &lt;80&gt;\n\n\n \n\n\n \n\n\n0.0770\n\n\n0.0396\n\n\n0.0271\n\n\n-0.0996\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.1211)\n\n\n(0.1127)\n\n\n(0.1121)\n\n\n(0.1081)\n\n\n\n\nfactor(income)4,500 to below 5,000 &lt;80&gt;\n\n\n \n\n\n \n\n\n0.2446*\n\n\n0.1782\n\n\n0.1755\n\n\n0.0431\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.1274)\n\n\n(0.1188)\n\n\n(0.1182)\n\n\n(0.1140)\n\n\n\n\nfactor(income)5,000 or more &lt;80&gt;\n\n\n \n\n\n \n\n\n0.2017\n\n\n0.1350\n\n\n0.1128\n\n\n0.0250\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.1227)\n\n\n(0.1143)\n\n\n(0.1136)\n\n\n(0.1095)\n\n\n\n\nfactor(income)No answer\n\n\n \n\n\n \n\n\n0.0325\n\n\n0.0498\n\n\n0.0501\n\n\n-0.0453\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.1062)\n\n\n(0.0990)\n\n\n(0.0984)\n\n\n(0.0949)\n\n\n\n\nfactor(household_size)2\n\n\n \n\n\n \n\n\n0.0362\n\n\n0.0390\n\n\n0.0316\n\n\n0.0617\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.0511)\n\n\n(0.0475)\n\n\n(0.0473)\n\n\n(0.0457)\n\n\n\n\nfactor(household_size)3\n\n\n \n\n\n \n\n\n0.0404\n\n\n0.0374\n\n\n0.0403\n\n\n0.0675\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.0574)\n\n\n(0.0535)\n\n\n(0.0533)\n\n\n(0.0515)\n\n\n\n\nfactor(household_size)4\n\n\n \n\n\n \n\n\n0.0289\n\n\n0.0129\n\n\n0.0114\n\n\n0.0516\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.0647)\n\n\n(0.0602)\n\n\n(0.0599)\n\n\n(0.0578)\n\n\n\n\nfactor(household_size)5\n\n\n \n\n\n \n\n\n0.0161\n\n\n0.0255\n\n\n0.0347\n\n\n0.0345\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.1046)\n\n\n(0.0972)\n\n\n(0.0968)\n\n\n(0.0934)\n\n\n\n\nfactor(household_size)6\n\n\n \n\n\n \n\n\n0.3162*\n\n\n0.3629**\n\n\n0.4030**\n\n\n0.3646**\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.1793)\n\n\n(0.1666)\n\n\n(0.1658)\n\n\n(0.1599)\n\n\n\n\nfactor(household_size)7\n\n\n \n\n\n \n\n\n0.0387\n\n\n0.0311\n\n\n0.0495\n\n\n-0.0145\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.3181)\n\n\n(0.2957)\n\n\n(0.2939)\n\n\n(0.2838)\n\n\n\n\nfactor(household_size)8\n\n\n \n\n\n \n\n\n0.7654**\n\n\n0.9534***\n\n\n0.8352**\n\n\n0.7004**\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.3876)\n\n\n(0.3615)\n\n\n(0.3615)\n\n\n(0.3479)\n\n\n\n\nfactor(household_size)12\n\n\n \n\n\n \n\n\n-0.0289\n\n\n0.0946\n\n\n-0.0435\n\n\n-0.1011\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.7761)\n\n\n(0.7217)\n\n\n(0.7188)\n\n\n(0.6915)\n\n\n\n\nfactor(self_econ)2\n\n\n \n\n\n \n\n\n-0.0271\n\n\n-0.1166\n\n\n-0.1141\n\n\n-0.0368\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.1629)\n\n\n(0.1517)\n\n\n(0.1511)\n\n\n(0.1457)\n\n\n\n\nfactor(self_econ)3\n\n\n \n\n\n \n\n\n-0.2019\n\n\n-0.2069\n\n\n-0.2058\n\n\n-0.1859\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.1513)\n\n\n(0.1408)\n\n\n(0.1405)\n\n\n(0.1353)\n\n\n\n\nfactor(self_econ)4\n\n\n \n\n\n \n\n\n-0.1501\n\n\n-0.1394\n\n\n-0.1358\n\n\n-0.1343\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.1510)\n\n\n(0.1405)\n\n\n(0.1403)\n\n\n(0.1351)\n\n\n\n\nfactor(self_econ)5\n\n\n \n\n\n \n\n\n-0.2279\n\n\n-0.1700\n\n\n-0.1705\n\n\n-0.1569\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.1491)\n\n\n(0.1389)\n\n\n(0.1386)\n\n\n(0.1335)\n\n\n\n\nfactor(self_econ)6\n\n\n \n\n\n \n\n\n-0.2812*\n\n\n-0.2191\n\n\n-0.2186\n\n\n-0.2051\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.1503)\n\n\n(0.1400)\n\n\n(0.1397)\n\n\n(0.1345)\n\n\n\n\nfactor(self_econ)7\n\n\n \n\n\n \n\n\n-0.3444**\n\n\n-0.2527*\n\n\n-0.2484*\n\n\n-0.2383*\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.1518)\n\n\n(0.1415)\n\n\n(0.1414)\n\n\n(0.1360)\n\n\n\n\nfactor(self_econ)8\n\n\n \n\n\n \n\n\n-0.2107\n\n\n-0.1598\n\n\n-0.1765\n\n\n-0.1973\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.1573)\n\n\n(0.1466)\n\n\n(0.1462)\n\n\n(0.1407)\n\n\n\n\nfactor(self_econ)9\n\n\n \n\n\n \n\n\n-0.1747\n\n\n-0.0476\n\n\n-0.0684\n\n\n-0.0658\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.1933)\n\n\n(0.1804)\n\n\n(0.1801)\n\n\n(0.1731)\n\n\n\n\nfactor(self_econ)10 ( TOP )\n\n\n \n\n\n \n\n\n0.3679\n\n\n0.2960\n\n\n0.2701\n\n\n0.2253\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.2349)\n\n\n(0.2192)\n\n\n(0.2183)\n\n\n(0.2100)\n\n\n\n\nfactor(self_econ)0 ( BOTTOM )\n\n\n \n\n\n \n\n\n-0.0023\n\n\n-0.0450\n\n\n-0.0278\n\n\n0.0017\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.2077)\n\n\n(0.1933)\n\n\n(0.1925)\n\n\n(0.1853)\n\n\n\n\nfactor(ref_integrating)2\n\n\n \n\n\n \n\n\n \n\n\n-0.0585\n\n\n-0.0390\n\n\n-0.0304\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0913)\n\n\n(0.0924)\n\n\n(0.0890)\n\n\n\n\nfactor(ref_integrating)3\n\n\n \n\n\n \n\n\n \n\n\n-0.0921\n\n\n-0.0692\n\n\n-0.0772\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0935)\n\n\n(0.0951)\n\n\n(0.0918)\n\n\n\n\nfactor(ref_integrating)4\n\n\n \n\n\n \n\n\n \n\n\n0.0787\n\n\n0.0928\n\n\n0.0585\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0998)\n\n\n(0.1015)\n\n\n(0.0980)\n\n\n\n\nfactor(ref_citizenship)2\n\n\n \n\n\n \n\n\n \n\n\n0.0020\n\n\n-0.0202\n\n\n-0.0245\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0444)\n\n\n(0.0447)\n\n\n(0.0429)\n\n\n\n\nfactor(ref_citizenship)3\n\n\n \n\n\n \n\n\n \n\n\n0.0893*\n\n\n0.0720\n\n\n0.0349\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0493)\n\n\n(0.0497)\n\n\n(0.0480)\n\n\n\n\nfactor(ref_citizenship)4\n\n\n \n\n\n \n\n\n \n\n\n0.1626***\n\n\n0.1425**\n\n\n0.1000*\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0571)\n\n\n(0.0581)\n\n\n(0.0561)\n\n\n\n\nfactor(ref_reduce)2\n\n\n \n\n\n \n\n\n \n\n\n-0.0253\n\n\n-0.0103\n\n\n-0.0041\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0595)\n\n\n(0.0599)\n\n\n(0.0576)\n\n\n\n\nfactor(ref_reduce)3\n\n\n \n\n\n \n\n\n \n\n\n0.0162\n\n\n0.0326\n\n\n-0.0106\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0636)\n\n\n(0.0644)\n\n\n(0.0623)\n\n\n\n\nfactor(ref_reduce)4\n\n\n \n\n\n \n\n\n \n\n\n0.0354\n\n\n0.0465\n\n\n-0.1045\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0744)\n\n\n(0.0754)\n\n\n(0.0736)\n\n\n\n\nfactor(ref_moredone)2\n\n\n \n\n\n \n\n\n \n\n\n0.0930*\n\n\n0.0802*\n\n\n0.0559\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0482)\n\n\n(0.0484)\n\n\n(0.0467)\n\n\n\n\nfactor(ref_moredone)3\n\n\n \n\n\n \n\n\n \n\n\n0.1947***\n\n\n0.1834***\n\n\n0.0920*\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0528)\n\n\n(0.0532)\n\n\n(0.0520)\n\n\n\n\nfactor(ref_moredone)4\n\n\n \n\n\n \n\n\n \n\n\n0.3050***\n\n\n0.2874***\n\n\n0.1561**\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0618)\n\n\n(0.0623)\n\n\n(0.0609)\n\n\n\n\nfactor(ref_cultgiveup)2\n\n\n \n\n\n \n\n\n \n\n\n-0.0125\n\n\n-0.0212\n\n\n-0.0309\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0586)\n\n\n(0.0590)\n\n\n(0.0569)\n\n\n\n\nfactor(ref_cultgiveup)3\n\n\n \n\n\n \n\n\n \n\n\n0.0145\n\n\n-0.0054\n\n\n-0.0505\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0582)\n\n\n(0.0587)\n\n\n(0.0568)\n\n\n\n\nfactor(ref_cultgiveup)4\n\n\n \n\n\n \n\n\n \n\n\n0.1507**\n\n\n0.1281*\n\n\n0.0733\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0652)\n\n\n(0.0656)\n\n\n(0.0636)\n\n\n\n\nfactor(ref_economy)2\n\n\n \n\n\n \n\n\n \n\n\n0.0237\n\n\n0.0456\n\n\n0.0510\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0549)\n\n\n(0.0582)\n\n\n(0.0561)\n\n\n\n\nfactor(ref_economy)3\n\n\n \n\n\n \n\n\n \n\n\n-0.0004\n\n\n0.0343\n\n\n0.0145\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0606)\n\n\n(0.0663)\n\n\n(0.0638)\n\n\n\n\nfactor(ref_economy)4\n\n\n \n\n\n \n\n\n \n\n\n0.1657**\n\n\n0.2379***\n\n\n0.1524**\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0702)\n\n\n(0.0778)\n\n\n(0.0750)\n\n\n\n\nfactor(ref_crime)2\n\n\n \n\n\n \n\n\n \n\n\n0.0183\n\n\n-0.0033\n\n\n-0.0037\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0606)\n\n\n(0.0645)\n\n\n(0.0620)\n\n\n\n\nfactor(ref_crime)3\n\n\n \n\n\n \n\n\n \n\n\n0.0794\n\n\n-0.0061\n\n\n-0.0290\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0661)\n\n\n(0.0715)\n\n\n(0.0688)\n\n\n\n\nfactor(ref_crime)4\n\n\n \n\n\n \n\n\n \n\n\n0.2506***\n\n\n0.1343\n\n\n0.0431\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0774)\n\n\n(0.0835)\n\n\n(0.0806)\n\n\n\n\nfactor(ref_terror)2\n\n\n \n\n\n \n\n\n \n\n\n-0.0689\n\n\n-0.0975*\n\n\n-0.1060*\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0568)\n\n\n(0.0578)\n\n\n(0.0556)\n\n\n\n\nfactor(ref_terror)3\n\n\n \n\n\n \n\n\n \n\n\n-0.0330\n\n\n-0.0818\n\n\n-0.1054*\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0608)\n\n\n(0.0617)\n\n\n(0.0595)\n\n\n\n\nfactor(ref_terror)4\n\n\n \n\n\n \n\n\n \n\n\n-0.0338\n\n\n-0.0865\n\n\n-0.1144*\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0707)\n\n\n(0.0715)\n\n\n(0.0689)\n\n\n\n\nfactor(ref_loc_services)2\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n0.0852\n\n\n0.0888\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0680)\n\n\n(0.0653)\n\n\n\n\nfactor(ref_loc_services)3\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n0.0765\n\n\n0.0788\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0681)\n\n\n(0.0655)\n\n\n\n\nfactor(ref_loc_services)4\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n0.0577\n\n\n0.0699\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0761)\n\n\n(0.0732)\n\n\n\n\nfactor(ref_loc_economy)2\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n-0.1186*\n\n\n-0.1209*\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0718)\n\n\n(0.0690)\n\n\n\n\nfactor(ref_loc_economy)3\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n-0.1420*\n\n\n-0.1562**\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0767)\n\n\n(0.0738)\n\n\n\n\nfactor(ref_loc_economy)4\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n-0.2892***\n\n\n-0.2972***\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0864)\n\n\n(0.0832)\n\n\n\n\nfactor(ref_loc_crime)2\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n0.0813\n\n\n0.0727\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0582)\n\n\n(0.0560)\n\n\n\n\nfactor(ref_loc_crime)3\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n0.2474***\n\n\n0.2050***\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0662)\n\n\n(0.0640)\n\n\n\n\nfactor(ref_loc_crime)4\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n0.3110***\n\n\n0.2766***\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0800)\n\n\n(0.0774)\n\n\n\n\nfactor(ref_loc_culture)2\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n0.0051\n\n\n0.0068\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0524)\n\n\n(0.0507)\n\n\n\n\nfactor(ref_loc_culture)3\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n0.0297\n\n\n0.0078\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0612)\n\n\n(0.0594)\n\n\n\n\nfactor(ref_loc_culture)4\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n0.1232*\n\n\n0.0013\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0737)\n\n\n(0.0718)\n\n\n\n\nfactor(ref_loc_islam)2\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n-0.0128\n\n\n-0.0085\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0531)\n\n\n(0.0511)\n\n\n\n\nfactor(ref_loc_islam)3\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n-0.0085\n\n\n-0.0453\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0550)\n\n\n(0.0531)\n\n\n\n\nfactor(ref_loc_islam)4\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n-0.0281\n\n\n-0.1068*\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0660)\n\n\n(0.0638)\n\n\n\n\nfactor(ref_loc_schools)2\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n0.1254\n\n\n0.1084\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0887)\n\n\n(0.0853)\n\n\n\n\nfactor(ref_loc_schools)3\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n0.0552\n\n\n0.0573\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0832)\n\n\n(0.0801)\n\n\n\n\nfactor(ref_loc_schools)4\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n-0.0806\n\n\n-0.0809\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0845)\n\n\n(0.0814)\n\n\n\n\nfactor(ref_loc_housing)2\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n0.0134\n\n\n0.0095\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0586)\n\n\n(0.0566)\n\n\n\n\nfactor(ref_loc_housing)3\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n0.0068\n\n\n0.0008\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0564)\n\n\n(0.0547)\n\n\n\n\nfactor(ref_loc_housing)4\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n0.0432\n\n\n0.0433\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0608)\n\n\n(0.0590)\n\n\n\n\nfactor(ref_loc_wayoflife)2\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n-0.0586\n\n\n-0.0653\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0621)\n\n\n(0.0597)\n\n\n\n\nfactor(ref_loc_wayoflife)3\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n-0.0341\n\n\n-0.0515\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0609)\n\n\n(0.0586)\n\n\n\n\nfactor(ref_loc_wayoflife)4\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n0.0694\n\n\n0.0550\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0707)\n\n\n(0.0682)\n\n\n\n\nfactor(distance_ref)3-5 kilometers\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n-0.0375\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0362)\n\n\n\n\nfactor(distance_ref)6-10 kilometers\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n0.0256\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0405)\n\n\n\n\nfactor(distance_ref)11-20 kilometers\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n0.0165\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0494)\n\n\n\n\nfactor(distance_ref)21-50 kilometers\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n0.0644\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0534)\n\n\n\n\nfactor(distance_ref)More than 50 kilometer\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n0.0568\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0794)\n\n\n\n\nfactor(distance_ref)Don’t know\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n-0.0389\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0418)\n\n\n\n\nfactor(settle_ref)1 &lt;96&gt; 49\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n-0.0133\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0668)\n\n\n\n\nfactor(settle_ref)50 &lt;96&gt; 249\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n-0.0178\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0661)\n\n\n\n\nfactor(settle_ref)250 &lt;96&gt; 499\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n-0.0455\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0694)\n\n\n\n\nfactor(settle_ref)500 &lt;96&gt; 999\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n-0.0213\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0732)\n\n\n\n\nfactor(settle_ref)1000 and more\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n-0.0536\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0687)\n\n\n\n\nlrscale\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n0.0235***\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0078)\n\n\n\n\nafd\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n0.0044***\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0006)\n\n\n\n\nmuslim_ind\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n0.3152***\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0701)\n\n\n\n\nafd_ind\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n0.3390***\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0489)\n\n\n\n\ncontact_ind\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n0.0741\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.0522)\n\n\n\n\nR2\n\n\n0.1915\n\n\n0.2403\n\n\n0.2883\n\n\n0.3942\n\n\n0.4097\n\n\n0.4592\n\n\n\n\nAdj. R2\n\n\n0.1912\n\n\n0.2395\n\n\n0.2673\n\n\n0.3712\n\n\n0.3821\n\n\n0.4308\n\n\n\n\nNum. obs.\n\n\n3019\n\n\n3019\n\n\n3008\n\n\n3008\n\n\n3008\n\n\n3008\n\n\n\n\n\n\n***p &lt; 0.01; **p &lt; 0.05; *p &lt; 0.1\n\n\n\n\n\n\nFinal Questions\nEven with all these covariates accounted for, the authors still engage in a discussion about possible violations of the OLS assumptions that could bias their results, as well as potential alternative modelling strategies.\n\nIs their survey representative? They replicate using another polling firm.\nAre there even more alternative explanations?\nIs OLS the right choice?\nValidity (discussed in Gelman and Hill). Does the outcome accurately measure the concept? They consider alternative outcomes and visualize the coefficient results in Figure 4.\n\nMessage: Attacks against refugee homes are sometimes necessary to make it clear to politicians that we have a refugee problem.\nJustified : Hostility against refugees is sometimes justified, even when it ends up in violence.\nPrevent : Xenophobic acts of violence are defensible if they result in fewer refugees settling in town.\nCondemn: Politicians should condemn attacks against refugees more forcefully.\n\n\n\nAdditional Practice Questions.\n\nFind the average expected level of “Only Means” agreement at each level of mate competition. Plot the results. Base these results on lm2.\nFit lm2 using the generalized linear model glm approach (with a normal distribution) instead of the lm\nWhat are some of the conceptual differences between ordinary least squares and maximum likelihood estimation?"
  },
  {
    "objectID": "04-ReviewofOLS.html#footnotes",
    "href": "04-ReviewofOLS.html#footnotes",
    "title": "4  Review of OLS",
    "section": "",
    "text": "Recall this notation means rows by columns, \\(Y\\) is a vector of length \\(n\\) (the number of observations), and since there is only 1 outcome measure, it is 1 column.↩︎\nThis video from Ben Lambert provides additional intuition for understanding OLS in a matrix form and how it can be useful.↩︎"
  }
]