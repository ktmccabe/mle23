{
  "hash": "65bd7c722e982279d7d4a6964e022ed2",
  "result": {
    "markdown": "---\ntitle: \"The Math\"\n---\n\n\nThis section will provide an overview of a selection of mathematical skills that will be useful in the course. It will not go into everything, but at least provides a start.\n\nMLE will include a range of concepts and skills from linear algebra, calculus, and probability and statistics. You do not need to be an expert in these to succeed in the course, but there are some fundamentals that will make your life easier.\n\nAgain, the end goal here is not the math. The end goal is to help your become great social scientists. Some popular methods in social science rely on these concepts. The more you can build an intuition for how, when, where, and why these methods work, the more tools you will have to carry out your research and the more credibility and command you will have over the research you conduct.\n\nThink about restaurants you have visited. There is a difference between a server who says, \"Well we have a chicken\" and a server who can tell you exactly how the chicken was cooked, what ingredients were used in the sauce, and maybe even how the chicken was [raised](https://www.youtube.com/watch?v=G__PVLB8Nm4&ab_channel=IFC). Both dishes may be delicious, but you may gain more trust and understanding from the second.\n\nThis will not happen overnight nor by the end of the course, but the goal is to continue to progress in understanding. If you pursue a career in academia, this is only the start of years of continued learning and practice. It is a marathon, not a sprint.\n\n**Extra Resources**\n\nThese are some additional resources that may help supplement the sections to follow. These provide far more detail than you will need in the course, but because of that, are much more comprehensive than the notes that will be outlined:\n\n-   A linear algebra web series. The notes on vector and matrices here will be brief, so this playlist 3Blue1Brown goes into more detail on specific. [topics](https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab)\n-   An alternative way of thinking about vectors and matrices is [here](http://www.coranac.com/documents/geomatrix/) which gives a more geometric interpretation, which some people find helpful.\n-   Will Moore and David Siegel. *A Mathematics Course for Political and Social Research.* You can get the book or visit the [website](http://people.duke.edu/~das76/moosiebook.html), which includes some problem sets and solutions, as well as a video syllabus.\n\n## Mathematical Operations\n\nIn this first section, we will review mathematical operations that you have probably encountered before. In many cases, this will be a refresher on the rules and how to read notation.\n\n### **Order of Operations**\n\nMany of you may have learned the phrase, \"Please Excuse My Dear Aunt Sally\" which stands for Parentheses (and other grouping symbols), followed by Exponents, followed by Multiplication and Division from left to right, followed by Addition and Subtraction from left to right.\n\nThere will be many equations in our future, and we must remember these rules.\n\n-   Example: $((1+2)^3)^2 = (3^3)^2 = 27^2 = 729$\n\nTo get the answer, we focused on respecting the parentheses first, identifying the inner-most expression $1 + 2 = 3$, we then moved out and conducted the exponents to get to $(3^3)^2 = 27^2$.\n\nNote how this is different from the answer to $1 + (2^3)^2 = 1 + 8^2 = 65$, where the addition is no longer part of the parentheses.\n\n### **Exponents**\n\nHere is a cheat sheet of some basic rules for exponents. These can be hard to remember if you haven't used them in a long time. Think of $a$ in this case as a number, e.g., 4, and $b$, $k$, and $l$, as other numbers.\n\n-   $a^0 = 1$\n-   $a^1 = a$\n-   $a^k * a^l = a^{k + l}$\n-   $(a^k)^l = a^{kl}$\n-   $(\\frac{a}{b})^k = (\\frac{a^k}{b^k})$\n\nThese last two rules can be somewhat tricky. Note that a negative exponent can be re-written as a fraction. Likewise an exponent that is a fraction, the most common of which we will encounter is $\\frac{1}{2}$ can be re-written as a root, in this case the square root (e.g., $\\sqrt{a}$).\n\n-   $a^{-k} = \\frac{1}{a^k}$\n-   $a^{1/2} = \\sqrt{a}$\n\n### **Summations and Products**\n\nThe symbol $\\sum$ can be read \"take the sum of\" whatever is to the right of the symbol. This is used to make the written computation of a sum much shorter than it might be otherwise. For example, instead of writing the addition operations separately in the example below, we can simplify it with the $\\sum$ symbol. This is especially helpful if you would need to add together 100 or 1000 or more things. We will see these appear a lot in the course, for better or worse, so getting comfortable with the notation will be useful.\n\nUsually, there is notation just below and just above the symbol (e.g., $\\sum_{i=1}^3$). This can be read as \"take the sum of the following from $i=1$ to $i=3$. We perform the operation in the expression, each time changing $i$ to a different number, from 1 to 2 to 3. We then add each expression's output together.\n\n-   Example: $\\sum_{i=1}^3 (i + 1)^2 = (1 + 1)^2 + (2 + 1)^2 + (3+1)^2 = 29$\n\nWe will also encounter the product symbol in this course: $\\prod$. This is similar to the summation symbol, but this time we are multiplying instead of adding.\n\n-   Example: $\\prod_{k = 1}^3 k^2 = 1^2 \\times 2^2 \\times 3^2 = 36$\n\n### **Logarithms**\n\nIn this class, we will generally assume that $\\log$ takes the natural base $e$, which is a mathematical constant equal to 2.718.... In other books, the base might be 10 by default.\n\n-   If we have, $\\log_{10} x = 2$, this is like saying 10\\^2 = 100.\n-   With base $e$, we have $\\log_e x =2$, which is $e^2 = 7.389$.\n-   We are just going to write $\\log_e$ as $\\log$ but know that the $e$ is there.\n\nA key part of maximum likelihood estimation is writing down the *log* of the likelihood equation, so this is a must-have for later in the course.\n\nHere is a cheat sheet of common rules for working with logarithms.\n\n-   $\\log x = 8 \\rightarrow e^8 = x$\n-   $e^\\pi = y \\rightarrow \\log y = \\pi$\n-   $\\log (a \\times b) = \\log a + \\log b$\n-   $\\log a^n = n \\log a$\n-   $\\log \\frac{a}{b} = \\log a - \\log b$\n\nWhy logarithms? There are many different reasons why social scientists use logs.\n\n-   Some social phenomena grow exponentially, and logs make it is easier to visualize exponential growth, as is the case in visualizing the growth in COVID cases. See this [example](https://www.weforum.org/agenda/2020/04/covid-19-spread-logarithmic-graph/).\n-   Relatedly, taking the log of a distribution that is skewed, will make it look more normal or symmetrical, which has some nice properties.\n-   Sometimes the rules of logarithms are more convenient than non-logarithms. In MLE, we will take particular advantage of this rule: $\\log (a \\times b) = \\log a + \\log b$, which turns a multiplication problem into an addition problem.\n\n## Mathematical Operations in R\n\nWe will use R for this course, and these operations are all available with R code, allowing R to become a calculator for you. Here are some examples applying the tools above.\n\n### PEMDAS\n\n\n::: {.cell}\n\n```{.r .cell-code}\n((1+2)^3)^2 \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 729\n```\n:::\n\n```{.r .cell-code}\n1 + (2^3)^2 \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 65\n```\n:::\n:::\n\n\n### Exponents\n\nYou can compare the computation below to match with the rules above. Note how the caret \\^ symbol is used for exponents, and the asterisk \\* is used for multiplication. We also have a function `sqrt()` for taking the square root.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Let's say a = 4 for our purposes\na <- 4\n## And let's say k= 3 and l=5, b=2\nk <- 3\nl <- 5\nb <- 2\n```\n:::\n\n\n-   $a^0 = 1$\n-   $a^1 = a$\n\n\n::: {.cell}\n\n```{.r .cell-code}\na^0\na^1\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1\n[1] 4\n```\n:::\n:::\n\n\nNote how we use parentheses in R to make it clear that the exponent includes not just `k` but `(k + l)`\n\n-   $a^k * a^l = a^{k + l}$\n\n\n::: {.cell}\n\n```{.r .cell-code}\na^k * a^l \na^(k + l)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 65536\n[1] 65536\n```\n:::\n:::\n\n\nNote how we use the asterisk to make it clear we want to multiply `k*l`\n\n-   $(a^k)^l = a^{kl}$\n\n\n::: {.cell}\n\n```{.r .cell-code}\n(a^k)^l\na^(k*l)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1073741824\n[1] 1073741824\n```\n:::\n:::\n\n\n-   $(\\frac{a}{b})^k = (\\frac{a^k}{b^k})$\n\n\n::: {.cell}\n\n```{.r .cell-code}\n(a / b)^k\n(a ^k)/(b^k)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 8\n[1] 8\n```\n:::\n:::\n\n\n-   $a^{-k} = \\frac{1}{a^k}$\n\n\n::: {.cell}\n\n```{.r .cell-code}\na^(-k) \n1 / (a^k)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.015625\n[1] 0.015625\n```\n:::\n:::\n\n\n-   $a^{1/2} = \\sqrt{a}$\n\n\n::: {.cell}\n\n```{.r .cell-code}\na^(1/2) \nsqrt(a)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2\n[1] 2\n```\n:::\n:::\n\n\n### Summations\n\nSummations and products are a little more nuanced in R, depending on what you want to accomplish. But here is one example.\n\nLet's take a vector (think a list of numbers) that goes from 1 to 4. We will call it `ourlist`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nourlist <- c(1,2,3,4)\nourlist\n## An alternative is to write this: ourlist <- 1:4\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1 2 3 4\n```\n:::\n:::\n\n\nNow let's do $\\sum_{i = 1}^4 (i + 1)^2 = (1 +1)^2 + (1+2)^2 + (1 + 3)^2 + (1 + 4)^2 = 54$\n\nIn R, when you add a number to a vector, it will add that number to each entry in the vector. Example\n\n\n::: {.cell}\n\n```{.r .cell-code}\nourlist + 1\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2 3 4 5\n```\n:::\n:::\n\n\nWe can use that now to do the inside part of the summation. Note: the exponent works the same way, squaring each element of the inside expression.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n(ourlist + 1)^2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1]  4  9 16 25\n```\n:::\n:::\n\n\nNow, we will embed this expression inside a function in R called `sum` standing for summation. It will add together each of the inside components.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsum((ourlist + 1)^2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 54\n```\n:::\n:::\n\n\nIf instead we wanted the product, multiplying each element of the inside together, we could use `prod()`. We won't use that function very much in this course.\n\n### Logarithms\n\nR also has functions related to logarithms, called `log()` and `exp()` for the for the natural base $e$. By default, the natural exponential is the base in the `log()` R function.\n\n-   $\\log x = 8 \\rightarrow e^8 = x$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nexp(8)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2980.958\n```\n:::\n\n```{.r .cell-code}\nlog(2980.958)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 8\n```\n:::\n:::\n\n\nNote that R also has a number of built-in constants, like `pi`.\n\n-   $e^\\pi = y \\rightarrow \\log y = \\pi$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nexp(pi)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 23.14069\n```\n:::\n\n```{.r .cell-code}\nlog(23.14069)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 3.141593\n```\n:::\n:::\n\n\n-   $\\log (a \\times b) = \\log a + \\log b$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlog(a * b)\nlog(a) + log(b)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2.079442\n[1] 2.079442\n```\n:::\n:::\n\n\nLet's treat $n=3$ in this example and enter 3 directly where we see $n$ below. Alternatively you could store 3 as n, as we did with the other letters above.\n\n-   $\\log a^n = n \\log a$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlog(a ^ 3)\n\n3 * log(a)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 4.158883\n[1] 4.158883\n```\n:::\n:::\n\n\n-   $\\log \\frac{a}{b} = \\log a - \\log b$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlog(a/b)\nlog(a) - log(b)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.6931472\n[1] 0.6931472\n```\n:::\n:::\n\n\n## Derivatives\n\nWe will need to take some derivatives in the course. The reason is because a derivative gets us closer to understanding how to minimize and maximize certain functions, where a function is a relationship that maps elements of a set of inputs into a set of outputs, where each input is related to one output.\n\nThis is useful in social science, with methods such as linear regression and maximum likelihood estimation because it helps us estimate the values that we think will best describe the relationship between our independent variables and a dependent variable.\n\nFor example, in ordinary least squares (linear regression), we choose coefficients, which describe the relationship between the independent and dependent variables (*for every 1 unit change in x, we estimate* $\\hat \\beta$ amount of change in y), based on a method that tries to *minimize* the squared error between our estimated outcomes and the actual outcomes. In MLE, we will have a different quantity, which we will try to maximize.\n\nTo understand derivatives, we will briefly define limits.\n\n**Limits**\n\nA limit describes how a function behaves as it approaches (gets very close to) a certain value\n\n-   $\\lim_{x \\rightarrow a} f(x) = L$\n\nExample: $\\lim_{x \\rightarrow 3} x^2 = 9$ The limit of this function as $x$ approaches three, is 9. Limits will appear in the expression for calculating derivatives.\n\n### **Derivatives**\n\nFor intuition on a derivative, watch this [video](https://www.youtube.com/watch?v=i48KWnuAi4Q) from The Math Sorcerer.\n\nA derivative is the instantaneous rate at which the function is changing at x: *the slope of a function at a particular point.*\n\nThere are different notations for indicating something is a derivative. Below, we use $f'(x)$ because we write our functions as $f(x) = x$. Many times you might see a function equation like $y = 3x$ There, it will be common for the derivative to be written like $\\frac{dy}{dx}$.\n\nLet's break down the definition of a derivative by looking at its similarity to the simple definition of a slope, as the rise over the run:\n\n-   Slope (on average): rise over run: change in $f(x)$ over an interval ($[c, b]$ where $b-c =h$): $\\frac{f(b) - f(c)}{b-c}$\n\nFor slope at a specific point $x$ (the derivative of f(x) at x), we just make the interval $h$ very small:\n\n-   $f'(x)= \\lim_{h \\rightarrow 0}\\frac{f(a + h) - f(a)}{h}$\n\nExample $f(x) = 2x + 3$.\n\n-   $f'(x) = \\lim_{h \\rightarrow 0}\\frac{2(x + h) + 3 - (2x + 3)}{h} = \\lim_{h \\rightarrow 0}\\frac{2x + 2h - 2x}{h} = \\lim_{h \\rightarrow 0}2 = 2$\n\nThis [twitter thread](https://twitter.com/allison_horst/status/1307538146121244674) by the brilliant teacher and statistician Allison Horst, provides a nice cartoon-based example of the derivative. (Note that sometimes the interval $h$ is written as $\\Delta x$, the change in $x$).\n\n\n\n### **Critical Points for Minima or Maxima**\n\nIn both OLS and MLE, we reach points where take what are called \"first order conditions.\" This means we take the derivative with respect to a parameter of interest and then set the derivative = 0 and solve for the parameter to get an expression for our estimator. (E.g., In OLS, we take the derivative of the sum of squared residuals, set it equal to zero, and solve to get an expression for $\\hat \\beta$).\n\nThe reason we are interested in when the derivative is zero, is because this is when the instanaeous rate of change is zero, i.e., the slope at a particular point is zero. When does this happen? At a critical point- maximum or minimum. Think about it-- at the top of a mountain, there is no more rise (and no decline). You are completing level on the mountaintop. The slope at that point is zero.\n\nLet's take an example. The function $f(x) = x^2 + 1$ has the derivative $f'(x) = 2x$. This is zero when $x = 0$.\n\nThe question remains: How do we know if it is a maximum or minimum?\n\nWe need to figure out if our function is concave or convex around this critical value. Convex is a \"U\" shape, meaning we are at a minimum, while concavity is an upside-down-U, which means we are at a maximum. We do so by taking the second derivative. This just means we take the derivative of the expression we already have for our first derivative. In our case, $f''(x) = 2$. So what? Well the key thing we are looking for is if this result is positive or negative. Here, it is positive, which means our function is convex at this critical value, and therefore, we are at a minimum.\n\nJust look at the function in R if we plot it.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Let's define an arbitrary set of values for x\nx <- -3:3\n## Now let's map the elements of x into y using our function\nfx <- x^2 + 1\n\n## Let's plot the results\nplot(x = x, y=fx, \n     xlab = \"x\", type = \"l\",\n     main = \"f(x) = x^2 + 1\")\n```\n\n::: {.cell-output-display}\n![](03-TheMATH_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n:::\n\n\nNotice that when x=0, we are indeed at a minimum, just as the positive value of the second derivative would suggest.\n\nA different example: $f(x) = -2x^2 +1$. $f'(x) = -4x$ When we set this equal to 0 we find a critical value at $x = 0$. $f''(x) = -4$. Here, the value is negative, and we know it is concave. Sure enough, let's plot it, and notice how we can draw a horiztonal line at the maximum, representing that zero slope at the critical point:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- -3:3\nfx <- -2*x^2 + 1\nplot(x = x, y = fx,\n    ylab = \"f(x)\",\n     xlab = \"x\", type = \"l\",\n     main = \"f(x) = -2x^2 + 1\")\nabline(h=1, col = \"red\", lwd=2)\n```\n\n::: {.cell-output-display}\n![](03-TheMATH_files/figure-html/unnamed-chunk-19-1.png){width=672}\n:::\n:::\n\n\n### **Common Derivative Rules**\n\nBelow is a cheat sheet of rules for quickly identifying the derivatives of functions.\n\nThe derivative of a constant is 0.\n\n-   $f(x) = a; f'(x) = 0$\n    -   Example: The derivative of 5 is 0.\n\nHere is the power rule.\n\n-   $f(x) = ax^n; f'(x) = n\\times a \\times x^{n-1}$\n    -   Example: The derivative of $x^3 = 3x^{(3-1)} = 3x^2$\n\nWe saw logs in the last section, and, yes, we see logs again here.\n\n-   $f(x) = e^{ax}; f'(x) = ae^{ax}$\n-   $f(x) = \\log(x); f'(x) = \\frac{1}{x}$\n\nA very convenient rule is that a derivative of a sum = sum of the derivatives.\n\n-   $f(x) = g(x) + h(x); f'(x) = g'(x) + h'(x)$\n\nProducts can be more of a headache. In this course, we will turn some product expressions into summation expressions to avoid the difficulties of taking derivatives with products.\n\n-   Product Rule: $f(x) = g(x)h(x); f'(x) = g'(x)h(x) + h'(x)g(x)$\n\nThe chain rule below looks a bit tricky, but it can be very helpful for simplifying the way you take a derivative. See this [video](https://www.youtube.com/watch?v=H-ybCx8gt-8&ab_channel=NancyPi) from NancyPi for a helpful explainer, as well as a follow-up for more complex applications [here](https://www.youtube.com/watch?v=U_qp0isxQYU&ab_channel=NancyPi).\n\n-   Chain Rule: $f(x) = g(h(x)); f'(x) = g'(h(x))h'(x)$\n\nExample: What is the derivative of $f(x) = \\log 5x$?\n\n-   First, we will apply the rule which tells us the derivative of a $\\log x$ is $\\frac{1}{x}$.\n-   However, here, we do not just have $x$, we have $5x$. We are in chain rule territory.\n-   After we apply the derivative to the log, which is $\\frac{1}{5x}$, we then have to take the derivative of $5x$ and multiply the two expressions together.\n-   The derivative of $5x$ is $5$.\n-   So, putting this together, our full derivative is f′(x) = 5 ∗ $\\frac{1}{5x}$ = $\\frac{1}{x}$.\n\n## Vectors and Matrices\n\n**Vectors**\n\nFor our purposes, a vector is a list or \"array\" of numbers. For example, this might be a variable in our data-- a list of the ages of all politicians in a country.\n\n*Addition*\n\n-   If we have two vectors $\\mathbf{u}$ and $\\mathbf{v}$, where\n    -   $\\mathbf{u} \\ = \\ (u_1, u_2, \\dots u_n)$ and\n    -   $\\mathbf{v} \\ = \\ (v_1, v_2, \\dots v_n)$,\n    -   $\\mathbf{u} + \\mathbf{v} = (u_1 + v_1, u_2 + v_2, \\dots u_n + v_n)$\n-   Note: $\\mathbf{u}$ and $\\mathbf{v}$ must be of the same dimensionality - number of elements in each must be the same - for addition.\n\n*Scalar multiplication*\n\n-   If we have a scalar (i.e., a single number) $\\lambda$ and a vector $\\mathbf{u}$\n-   $\\lambda \\mathbf{u} = (\\lambda u_1, \\lambda u_2, \\dots \\lambda u_n)$\n\nWe can implement vector addition and scalar multiplication in R.\n\nLet's create a vector $\\mathbf{u}$, a vector $\\mathbf{v}$, and a number lambda.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nu <- c(33, 44, 22, 11)\nv <- c(6, 7, 8, 2)\nlambda <- 3\n```\n:::\n\n\nWhen you add two vectors in R, it adds each component together.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nu + v\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 39 51 30 13\n```\n:::\n:::\n\n\nWe can multiply each element of a vector, `u` by `lambda`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlambda * u\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1]  99 132  66  33\n```\n:::\n:::\n\n\n*Element-wise Multiplication*\n\nNote: When you multiply two vectors together in R, it will take each element of one vector and multiply it by each element of the other vector.\n\n-   `u * v` $= (u_1 * v_1, u_2 * v_2, \\dots u_n * v_n)$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nu * v\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 198 308 176  22\n```\n:::\n:::\n\n\n### **Matrix Basics**\n\nA matrix represents arrays of numbers in a rectangle, with rows and columns.\n\n-   A matrix with $m$ rows and $n$ columns is defined as ($m$ x $n$). What is the dimensionality of the matrix **A** below?\n\n$A = \\begin{pmatrix} a_{11} & a_{12} & a_{13}\\\\ a_{21} & a_{22} & a_{23} \\\\ a_{31} & a_{32} & a_{33} \\\\ a_{41} & a_{42} & a_{43} \\end{pmatrix}$\n\nIn R, we can think of a matrix as a set of vectors. For example, we could combine the vectors `u` and `v` we created above into a matrix defined as W.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## cbind() binds together vectors as columns\nWcol <- cbind(u, v)\nWcol\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      u v\n[1,] 33 6\n[2,] 44 7\n[3,] 22 8\n[4,] 11 2\n```\n:::\n\n```{.r .cell-code}\n## rbind() binds together vectors as rows\nWrow <- rbind(u, v)\nWrow\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  [,1] [,2] [,3] [,4]\nu   33   44   22   11\nv    6    7    8    2\n```\n:::\n:::\n\n\nThere are other ways to create matrices in R, but using `cbind` and `rbind()` are common.\n\nWe can find the dimensions of our matrices using `dim()` or `nrow()` and `ncol()` together. For example:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndim(Wcol)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 4 2\n```\n:::\n\n```{.r .cell-code}\nnrow(Wcol)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 4\n```\n:::\n\n```{.r .cell-code}\nncol(Wcol)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2\n```\n:::\n:::\n\n\nNote how the dimensions are different from the version created with `rbind()`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndim(Wrow)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2 4\n```\n:::\n\n```{.r .cell-code}\nnrow(Wrow)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2\n```\n:::\n\n```{.r .cell-code}\nncol(Wrow)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 4\n```\n:::\n:::\n\n\n*Extracting specific components*\n\nThe element $a_{ij}$ signifies the element is in the $i$th row and $j$th column of matrix A. For example, $a_{12}$ is in the first row and second column.\n\n-   Square matrices have the same number of rows and columns\n-   Vectors have just one row or one column (e.g., $x_1$ element of $\\mathbf{x}$ vector)\n\nIn R, we can use brackets to extract a specific $ij$ element of a matrix or vector.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nWcol\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      u v\n[1,] 33 6\n[2,] 44 7\n[3,] 22 8\n[4,] 11 2\n```\n:::\n\n```{.r .cell-code}\nWcol[2,1] # element in the second row, first column\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n u \n44 \n```\n:::\n\n```{.r .cell-code}\nWcol[2,] # all elements in the second row\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n u  v \n44  7 \n```\n:::\n\n```{.r .cell-code}\nWcol[, 1] # all elements in the first column\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 33 44 22 11\n```\n:::\n:::\n\n\nFor matrices, to extract a particular entry in R, you have a comma between entries because there are both rows and columns. For vectors, you only have one entry, so no comma is needed.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nu\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 33 44 22 11\n```\n:::\n\n```{.r .cell-code}\nu[2] # second element in the u vector\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 44\n```\n:::\n:::\n\n\n### Matrix Operations\n\n*Matrix Addition*\n\n-   To be able to add matrix **A** and matrix **B**, they must have the same dimensions.\n-   Like vector addition, to add matrices, you add each of the components together.\n\n$A = \\begin{pmatrix} a_{11} & a_{12} & a_{13}\\\\ a_{21} & a_{22} & a_{23} \\\\ a_{31} & a_{32} & a_{33} \\end{pmatrix}$ and $B = \\begin{pmatrix} b_{11} & b_{12} & b_{13}\\\\ b_{21} & b_{22} & b_{23} \\\\ b_{31} & b_{32} & b_{33} \\end{pmatrix}$\n\n$A + B = \\begin{pmatrix} a_{11} + b_{11} & a_{12} + b_{12} & a_{13} + b_{13}\\\\ a_{21} + b_{21} & a_{22} + b_{22} & a_{23} + b_{23} \\\\ a_{31} + b_{31} & a_{32} + b_{32} & a_{33} + b_{33} \\end{pmatrix}$\n\n$Q = \\begin{pmatrix} 2 & 4 & 1\\\\ 6 & 1 & 5 \\end{pmatrix}$ $+$ $R = \\begin{pmatrix} 9 & 4 & 2\\\\ 11 & 8 & 7 \\end{pmatrix} = Q + R = \\begin{pmatrix} 11 & 8 & 3\\\\ 17 & 9 & 12 \\end{pmatrix}$\n\n*Scalar Multiplication*\n\nTake a scalar $\\nu$. Just like vectors, we multiply each component of a matrix by the scalar.\n\n$\\nu Q = \\begin{pmatrix} \\nu q_{11} & \\nu q_{12} & \\dots & \\nu q_{1n}\\\\ \\nu q_{21} & \\nu q_{22} & \\dots & \\nu q_{2n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\nu q_{m1} & \\nu q_{m2} & \\dots & \\nu q_{mn} \\end{pmatrix}$\n\nExample: Take $c = 2$ and a matrix A.\n\n$cA = c *\\begin{pmatrix} 4 & 6 & 1\\\\ 3 & 2 & 8 \\end{pmatrix}$ = $\\begin{pmatrix} 8 & 12 & 2\\\\ 6 & 4 & 16 \\end{pmatrix}$\n\nNote the Commutativity/Associativity: For scalar $c$: $c(AB) = (cA)B = A(cB) = (AB)c$.\n\n*Matrix Multiplication*\n\nA matrix A and B must be conformable to multiply AB.\n\n-   To be comformable, for $m_A$ x $n_A$ matrix A and $m_B$ x $n_B$ matrix B, the \"inside\" dimensions must be equal: $n_A = m_B$.\n-   The resulting AB has the \"outside\" dimensions: $m_A$ x $n_B$.\n\nFor each $c_{ij}$ component of $C = AB$, we take the inner product of the $i^{th}$ row of matrix A and the $j^{th}$ column of matrix B.\n\n-   Their product C = AB is the $m$ x $n$ matrix where:\n-   $c_{ij} =a_{i1}b_{1j} + a_{i2}b_{2j} + \\dots + a_{ik}b_{kj}$\n\n*Example:* This $2 \\times 3$ matrix is multiplied by a $3 \\times 2$ matrix, resulting in the $2 \\times 2$ matrix.\n\n$\\begin{pmatrix} 4 & 6 & 1\\\\ 3 & 2 & 8 \\end{pmatrix}$ $\\times$ $\\begin{pmatrix} 8 & 12 \\\\ 6 & 4 \\\\ 7 & 10 \\end{pmatrix}$ = $\\begin{pmatrix} (4*8 + 6*6 + 1*7) & (4*12 + 6*4 + 1*10) \\\\ (3*8 + 2*6 + 8*7) & (3*12 + 2*4 + 8*10) \\end{pmatrix}$\n\nFor example, the entry in the first row and second column of the new matrix $c_{12} = (a_{11} = 4* b_{11} = 12) + (a_{12} = 6*b_{21} = 4) + (a_{13} = 1*b_{31} = 10)$\n\nWe can also do matrix multiplication in R.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Create a 3 x 2 matrix A\nA <- cbind(c(3, 4, 6), c(5, 6, 8))\nA\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2]\n[1,]    3    5\n[2,]    4    6\n[3,]    6    8\n```\n:::\n\n```{.r .cell-code}\n## Create a 2 x 4 matrix B\nB <- cbind(c(6,8), c(7, 9), c(3, 6), c(1, 11))\nB\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2] [,3] [,4]\n[1,]    6    7    3    1\n[2,]    8    9    6   11\n```\n:::\n:::\n\n\nNote that the multiplication AB is conformable because the number of columns in A matches the number of rows in B:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nncol(A)\nnrow(B)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2\n[1] 2\n```\n:::\n:::\n\n\nTo multiply matrices together in R, we need to add symbols around the standard asterisk for multiplication:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nA %*% B\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2] [,3] [,4]\n[1,]   58   66   39   58\n[2,]   72   82   48   70\n[3,]  100  114   66   94\n```\n:::\n:::\n\n\nThat is necessary for multiplying matrices together. It is not necessary for scalar multiplication, where we take a single number (e.g., c = 3) and multiply it with a matrix:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nc <- 3\nc*A\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2]\n[1,]    9   15\n[2,]   12   18\n[3,]   18   24\n```\n:::\n:::\n\n\nNote the equivalence of the below expressions, which combine scalar and matrix multiplication:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nc* (A %*% B)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2] [,3] [,4]\n[1,]  174  198  117  174\n[2,]  216  246  144  210\n[3,]  300  342  198  282\n```\n:::\n\n```{.r .cell-code}\n(c* A) %*% B\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2] [,3] [,4]\n[1,]  174  198  117  174\n[2,]  216  246  144  210\n[3,]  300  342  198  282\n```\n:::\n\n```{.r .cell-code}\nA %*% (c * B)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2] [,3] [,4]\n[1,]  174  198  117  174\n[2,]  216  246  144  210\n[3,]  300  342  198  282\n```\n:::\n:::\n\n\nIn social science, one matrix of interest is often a rectangular dataset that includes column vectors representing independent variables, as well as another vector that includes your dependent variable. These might have 1000 or more rows and a handful of columns you care about.\n\n## Additional Matrix Tidbits that Will Come Up\n\n**Inverse**\n\nAn $n$ x $n$ matrix A is invertible if there exists an $n$ x $n$ inverse matrix $A^{-1}$ such that:\n\n-   $AA^{-1} = A^{-1}A = I_n$\n-   where $I_n$ is the identity matrix ($n$ x $n$), that takes diagonal elements of 1 and off-diagonal elements of 0. Example:\n\n$I_n = \\begin{pmatrix} 1_{11} & 0 & \\dots & 0\\\\ 0& 1_{22} & \\dots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & \\dots & 1_{nn} \\end{pmatrix}$\n\n-   Multiplying a matrix by the identity matrix returns in the matrix itself: $AI_n = A$\n    -   It's like the matrix version of multiplying a number by one.\n\nNote: A matrix must be square $n$ x $n$ to be invertible. (But not all square matrices are invertible.) A matrix is invertible if and only if its columns are linearly independent. This is important for understanding why you cannot have two perfectly colinear variables in a regression model.\n\nWe will not do much solving for inverses in this course. However, the inverse will be useful in solving for and simplifying expressions.\n\n### **Transpose**\n\nWhen we transpose a matrix, we flip the $i$ and $j$ components.\n\n-   Example: Take a 4 X 3 matrix A and find the 3 X 4 matrix $A^{T}$.\n-   A transpose is usually denoted with as $A^{T}$ or $A'$\n\n$A = \\begin{pmatrix} a_{11} & a_{12} & a_{13}\\\\ a_{21} & a_{22} & a_{23} \\\\ a_{31} & a_{32} & a_{33} \\\\ a_{41} & a_{42} & a_{43} \\end{pmatrix}$ then $A^T = \\begin{pmatrix} a'_{11} & a'_{12} & a'_{13} & a'_{14}\\\\ a'_{21} & a'_{22} & a'_{23} & a'_{24} \\\\ a'_{31} & a'_{32} & a'_{33} & a'_{34} \\end{pmatrix}$\n\nIf $A = \\begin{pmatrix} 1 & 4 & 2 \\\\ 3 & 1 & 11 \\\\ 5 & 9 & 4 \\\\ 2 & 11& 4 \\end{pmatrix}$ then $A^T = \\begin{pmatrix} 1 & 3 & 5 & 2\\\\ 4 & 1 & 9 & 11 \\\\ 2& 11 & 4 & 4 \\end{pmatrix}$\n\nCheck for yourself: What was in the first row ($i=1$), second column ($j=2$) is now in the second row ($i=2$), first column ($j=1$). That is $a_{12} =4 = a'_{21}$.\n\nWe can transpose matrices in R using `t()`. For example, take our matrix `A`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nA\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2]\n[1,]    3    5\n[2,]    4    6\n[3,]    6    8\n```\n:::\n\n```{.r .cell-code}\nt(A)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2] [,3]\n[1,]    3    4    6\n[2,]    5    6    8\n```\n:::\n:::\n\n\nIn R, you can find the inverse of a square matrix with `solve()`\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsolve(A)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in solve.default(A): 'a' (3 x 2) must be square\n```\n:::\n:::\n\n\nNote, while `A` is not square A'A is square:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nAtA <- t(A) %*% A\n\nsolve(AtA)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          [,1]      [,2]\n[1,]  2.232143 -1.553571\n[2,] -1.553571  1.089286\n```\n:::\n:::\n\n\n### **Additional Matrix Properties and Rules**\n\nThese are a few additional properties and rules that will be useful to us at various points in the course:\n\n-   Symmetric: Matrix A is symmetric if $A = A^T$\n-   Idempotent: Matrix A is idempotent if $A^2 = A$\n-   Trace: The trace of a matrix is the sum of its diagonal components $Tr(A) = a_{11} + a_{22} + \\dots + a_{mn}$\n\nExample of symmetric matrix:\n\n$D = \\begin{pmatrix} 1 & 6 & 22 \\\\ 6 & 4 & 7 \\\\ 22 & 7 & 11 \\end{pmatrix}$\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Look at the equivalence\nD <- rbind(c(1,6,22), c(6,4,7), c(22,7,11))\nD\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2] [,3]\n[1,]    1    6   22\n[2,]    6    4    7\n[3,]   22    7   11\n```\n:::\n\n```{.r .cell-code}\nt(D)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2] [,3]\n[1,]    1    6   22\n[2,]    6    4    7\n[3,]   22    7   11\n```\n:::\n:::\n\n\nWhat is the trace of this matrix?\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## diag() pulls out the diagonal of a matrix\nsum(diag(D))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 16\n```\n:::\n:::\n\n\n### **Matrix Rules**\n\nDue to conformability and other considerations, matrix operations are somewhat more restrictive, particularly when it comes to commutativity.\n\n-   Associative $(A + B) + C = A + (B + C)$ and $(AB) C = A(BC)$\n-   Commutative $A + B = B + A$\n-   Distributive $A(B + C) = AB + AC$ and $(A + B) C = AC + BC$\n-   Commutative law for multiplication does not hold-- the order of multiplication matters: \\$ AB \\neq BA\\$\n\n*Rules for Inverses and Transposes*\n\nThese rules will be helpful for simplifying expressions. Treat $A$, $B$, and $C$ as matrices below, and $s$ as a scalar.\n\n-   $(A + B)^T = A^T + B^T$\n-   $(s A)^T$ $= s A^T$\n-   $(AB)^T = B^T A^T$\n-   $(A^T)^T = A$ and $( A^{-1})^{-1} = A$\n-   $(A^T)^{-1} = (A^{-1})^T$\n-   $(AB)^{-1} = B^{-1} A^{-1}$\n-   $(ABCD)^{-1} = D^{-1} C^{-1} B^{-1} A^{-1}$\n\n### **Derivatives with Matrices and Vectors**\n\nLet's say we have a $p \\times 1$ \"column\" vector $\\mathbf{x}$ and another $p \\times 1$ vector $\\mathbf{a}$.\n\nTaking the derivative with respect to vector $\\mathbf{x}$.\n\nLet's say we have $y = \\mathbf{x}'\\mathbf{a}$. This process is explained [here](https://www.youtube.com/watch?v=iWxY7VdcSH8&ab_channel=BenLambert). Taking the derivative of this is called the gradient.\n\n-   $\\frac{\\delta y}{\\delta x} = \\begin{pmatrix}\\frac{\\delta y}{\\delta x_1} \\\\ \\frac{\\delta y}{\\delta x_2} \\\\ \\vdots \\\\ \\frac{dy}{dx_p} \\end{pmatrix}$\n\n-   $y$ will have dimensions $1 \\times 1$. $y$ is a scalar.\n\n    -   Note: $y = a_1x_1 + a_2x_2 + ... + a_px_p$. From this expression, we can take a set of \"partial derivatives\":\n    -   $\\frac{\\delta y}{\\delta x_1} = a_1$\n    -   $\\frac{\\delta y}{\\delta x_2} = a_2$, and so on\n    -   $\\frac{\\delta y}{\\delta x} = \\begin{pmatrix}\\frac{\\delta y}{\\delta x_1} \\\\ \\frac{\\delta y}{\\delta x_2} \\\\ \\vdots \\\\ \\frac{\\delta y}{\\delta x_p} \\end{pmatrix} = \\begin{pmatrix} a_1 \\\\ a_2 \\\\ \\vdots \\\\ a_p \\end{pmatrix}$\n\n-   Well, this is just vector $\\mathbf{a}$\n\nAnswer: $\\frac{\\delta }{\\delta x} \\mathbf{x}^T\\mathbf{a} = \\mathbf{a}$. We can apply this general rule in other situations.\n\n*Example 2*\n\nLet's say we want to differentiate the following where vector $\\mathbf{y}$ is $n \\times 1$, $X$ is $n \\times k$, and $\\mathbf{b}$ is $k \\times 1$. Take the derivative with respect to $b$.\n\n-   $\\mathbf{y}'\\mathbf{y} - 2\\mathbf{b}'X'\\mathbf{y}$\n-   Note that the dimensions of the output are $1 \\times 1$, a scalar quantity.\n\nRemember the derivative of a sum is the sum of derivatives. This allows us to focus on particular terms.\n\n-   The first term has no $\\mathbf{b}$ in it, so this will contribute 0.\n-   The second term is $2\\mathbf{b}'X'\\mathbf{y}$. We can think about this like the previous example\n    -   $\\frac{\\delta }{\\delta b} 2\\mathbf{b}'X'\\mathbf{y} = \\begin{pmatrix} \\frac{\\delta }{\\delta b_1}\\\\ \\frac{\\delta }{\\delta b_2} \\\\ \\vdots \\\\ \\frac{\\delta }{\\delta b_k} \\end{pmatrix}$\n    -   The output is needs to be $k \\times 1$ like $\\mathbf{b}$, which is what $2 * X'\\mathbf{y}$ is.\n-   The derivative is $-2X'\\mathbf{y}$\n\n*Example 3*\n\nAnother useful rule when a matrix $A$ is symmetric: $\\frac{\\delta}{\\delta \\mathbf{x}} \\mathbf{x}^TA\\mathbf{x} = (A + A^T)\\mathbf{x} = 2A\\mathbf{x}$.\n\n**Details on getting to this result.** We are treating the vector $\\mathbf{x}$ as $n \\times 1$ and the matrix $A$ as symmetric.\n\nWhen we take $\\frac{\\delta}{\\delta \\mathbf{x}}$ (the derivative with respect to $\\mathbf{x}$), we will be looking for a result with the same dimensions $\\mathbf{x}$.\n\n$\\frac{\\delta }{\\delta \\mathbf{x}} = \\begin{pmatrix}\\frac{\\delta }{\\delta x_1} \\\\ \\frac{\\delta }{\\delta x_2} \\\\ \\vdots \\\\ \\frac{d}{dx_n} \\end{pmatrix}$\n\nLet's inspect the dimensions of $\\mathbf{x}^TA\\mathbf{x}$. They are $1 \\times 1$. If we perform this matrix multiplication, we would be multiplying:\n\n$\\begin{pmatrix} x_1 & x_2 & \\ldots & x_i \\end{pmatrix} \\times \\begin{pmatrix} a_{11} & a_{12} & \\ldots & a_{1j} \\\\ a_{21} & a_{22} & \\ldots & a_{2j} \\\\ \\vdots & \\vdots & \\vdots & \\vdots \\\\ a_{i1} & a_{i2} & \\ldots & a_{ij} \\end{pmatrix} \\times \\begin{pmatrix}x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_j \\end{pmatrix}$\n\nTo simplify things, let's say we have the following matrices, where $A$ is symmetric:\n\n$\\begin{pmatrix} x_1 & x_2 \\end{pmatrix} \\times \\begin{pmatrix} a_{11} & a_{12} \\\\ a_{21} & a_{22} \\end{pmatrix} \\times \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix}$\n\nWe can perform the matrix multiplication for the first two quantities, which will result in a $1 \\times 2$ vector. Recall in matrix multiplication we take the sum of the element-wise multiplication of the $ith$ row of the first object by the $jth$ column of the second object. This means multiply the first row of $\\mathbf{x}$ by the first column of $A$ for the entry in cell $i=1; j=1$, and so on.\n\n$\\begin{pmatrix} (x_1a_{11} + x_2a_{21}) & (x_1a_{12} + x_2a_{22}) \\end{pmatrix}$\n\nWe can then multiply this quantity by the last quantity\n\n$\\begin{pmatrix} (x_1a_{11} + x_2a_{21}) & (x_1a_{12} + x_2a_{22}) \\end{pmatrix} \\times \\begin{pmatrix}x_1 \\\\ x_2 \\end{pmatrix}$\n\nThis will results in the $1 \\times 1$ quantity: $(x_1a_{11} + x_2a_{21})x_1 + (x_1a_{12} + x_2a_{22})x_2 = x_1^2a_{11} + x_2a_{21}x_1 + x_1a_{12}x_2 + x_2^2a_{22}$\n\nWe can now take the derivatives with respect to $\\mathbf{x}$. Because $\\mathbf{x}$ is $2 \\times 1$, our derivative will be a vector of the same dimensions with components:\n\n$\\frac{\\delta }{\\delta \\mathbf{x}} = \\begin{pmatrix}\\frac{\\delta }{\\delta x_1} \\\\ \\frac{\\delta }{\\delta x_2} \\end{pmatrix}$\n\nThese represent the partial derivatives of each component within $\\mathbf{x}$\n\nLet's focus on the first: $\\frac{\\delta }{\\delta x_1}$.\n\n\n```{=tex}\n\\begin{align*}\n\\frac{\\delta }{\\delta x_1} x_1^2a_{11} + x_2a_{21}x_1 +   x_1a_{12}x_2 + x_2^2a_{22} &=\\\\\n&= 2x_1a_{11} + x_2a_{21} + a_{12}x_2\\\\\n\\end{align*}\n```\n\nWe can repeat this for $\\frac{\\delta }{\\delta x_2}$\n\n\n```{=tex}\n\\begin{align*}\n\\frac{\\delta }{\\delta x_2} x_1^2a_{11} + x_2a_{21}x_1 +   x_1a_{12}x_2 + x_2^2a_{22} &=\\\\\n&= a_{21}x_1  + x_1a_{12} + 2x_2a_{22}\\\\\n\\end{align*}\n```\n\nNow we can put the result back into our vector format:\n\n$\\begin{pmatrix}\\frac{\\delta }{\\delta x_1} = 2x_1a_{11} + x_2a_{21} + a_{12}x_2\\\\ \\frac{\\delta }{\\delta x_2} = a_{21}x_1 + x_1a_{12} + 2x_2a_{22}\\end{pmatrix}$\n\nNow it's just about simplifying to show that we have indeed come back to the rule.\n\nRecall that for a symmetric matrix, the elements in rows and columns $ij$ = the elements in $ji$. This allows us to read $a_{21} = a_{12}$ and combine those terms (e.g., $x_2a_{21} + a_{12}x_2 =2a_{12}x_2$) :\n\n$\\begin{pmatrix}\\frac{\\delta }{\\delta x_1} = 2x_1a_{11} + 2a_{12}x_2\\\\ \\frac{\\delta }{\\delta x_2} = 2a_{21}x_1 + 2x_2a_{22}\\end{pmatrix}$\n\nSecond, we can now bring the 2 out front.\n\n$\\begin{pmatrix}\\frac{\\delta }{\\delta x_1} \\\\ \\frac{\\delta }{\\delta x_2} \\end{pmatrix} = 2 * \\begin{pmatrix} x_1a_{11} + a_{12}x_2\\\\ a_{21}x_1 + x_2a_{22}\\end{pmatrix}$\n\nFinally, let's inspect this and show it is equivalent to this multiplication where we have a $2 \\times 2$ $A$ matrix multiplied by a $2 \\times 1$ $\\mathbf x$ vector. *Minor note: Because any individual element of a vector is just a single quantity, we can change the order (e.g.,* $a_{11}*x_1$ vs. $x_1*a_{11}$). *We just can't do that for full vectors or matrices*\n\n$2A\\mathbf{x} = 2* \\begin{pmatrix} a_{11} & a_{12} \\\\ a_{21} & a_{22} \\end{pmatrix} \\times \\begin{pmatrix}x_1 \\\\ x_2 \\end{pmatrix} = 2 * \\begin{pmatrix} a_{11}x_1 + a_{12}x_2 \\\\ a_{21}x_1 + a_{22}x_2 \\end{pmatrix}$\n\nThe last quantity is the same as the previous step. That's the rule!\n\n**Applying the rules**\n\n-   This has a nice analogue to the derivative we've seen before $q^2 = 2*q$.\n-   Let's say we want to take the derivative of $\\mathbf{b}'X'X\\mathbf{b}$ with respect to $\\mathbf{b}$.\n-   We can think of $X'X$ as if it is $A$.\n    -   This gives us $2X'X\\mathbf{b}$ as the result.\n\nWhy on earth would we care about this? For one, it helps us understand how we get to our estimates for $\\hat \\beta$ in linear regression. When we have multiple variables, we don't just want the best estimate for one coefficient, but a vector of coefficients. See more [here](https://www.youtube.com/watch?v=qeezdYISDlU&ab_channel=BenLambert).\n\nIn MLE, we will find the gradient of the log likelihood function. We will further go into the second derivatives to arrive at what is called the Hessian. More on that later.\n\n## Practice Problems\n\n1.  What is $24/3 + 5^2 - (8 -4)$?\n2.  What is $\\sum_{i = 1}^5 (i*3)$?\n3.  Take the derivative of $f(x) =v(4x^2 + 6)^2$ with respect to $x$.\n4.  Take the derivative of $f(x) = e^{2x + 3}$ with respect to $x$.\n5.  Take the derivative of $f(x) = log (x + 3)^2$ with respect to $x$.\n\nGiven $X$ is an $n$ x $k$ matrix,\n\n6.  $(X^{T}X)^{-1}X^{T}X$ can be simplified to?\n7.  $((X^{T}X)^{-1}X^{T})^{T} =$ ?\n8.  If $\\nu$ is a constant, how does $(X^{T}X)^{-1}X^{T} \\nu X(X^{T}X)^{-1}$ simplify?\n9.  If a matrix $P$ is idempotent, $PP =$ ?\n\n### Practice Problem Solutions\n\n1.  What is $24/3 + 5^2 - (8 -4)$?\n\n\n::: {.cell}\n\n```{.r .cell-code}\n24/3 + 5^2 - (8 -4)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 29\n```\n:::\n:::\n\n\n2.  What is $\\sum_{i = 1}^5 (i*3)$?\n    -   By hand: $1 \\times 3 + 2 \\times 3 + 3 \\times 3 + 4 \\times 3 + 5 \\times 3$\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## sol 1\n1*3 + 2*3 + 3*3 + 4*3 + 5*3\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 45\n```\n:::\n\n```{.r .cell-code}\n## sol 2\ni <- 1:5\nsum(i*3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 45\n```\n:::\n:::\n\n\n3.  Take the derivative of $f(x) =v(4x^2 + 6)^2$ with respect to $x$.\n    -   We can treat $v$ as a number.\n\n\n```{=tex}\n\\begin{align*}\nf'(x) &= 2* v(4x^2 + 6) * 8x\\\\\n&= 16vx(4x^2 + 6)\n\\end{align*}\n```\n\n4.  Take the derivative of $f(x) = e^{2x + 3}$ with respect to $x$.\n\n\n```{=tex}\n\\begin{align*}\nf'(x) &= 2* e^{2x + 3}\\\\\n&= 2e^{2x + 3}\n\\end{align*}\n```\n\n5.  Take the derivative of $f(x) = log (x + 3)^2$ with respect to $x$.\n    -   Note we can re-write this as $2 * log (x + 3)$.\n\n\\begin{align*}\nf'(x) &= 2 * \\frac{1}{(x + 3)} * 1\\\\\n&= \\frac{2}{(x + 3)}\n\\end{align*} If we didn't take that simplifying step, we can still solve:\n\n\n```{=tex}\n\\begin{align*}\n      f'(x) &= \\frac{1}{(x + 3)^2} * 2 * (x + 3) *1\\\\\n      &= \\frac{2}{(x + 3)}\n      \\end{align*}\n```\n\nGiven $X$ is an $n$ x $k$ matrix,\n\n6.  $(X^{T}X)^{-1}X^{T}X$ can be simplified to?\n    -   $I_k$ the identity matrix\n7.  $((X^{T}X)^{-1}X^{T})^{T} =$ ?\n    -   Recall our rule $(AB)^T = B^TA^T$\n    -   $X(X^TX)^{-1}$\n8.  If $\\nu$ is a constant, how does $(X^{T}X)^{-1}X^{T} \\nu X(X^{T}X)^{-1}$ simplify?\n    -   We can pull it out front.\n\n\n```{=tex}\n\\begin{align*}\n    &= \\nu(X^{T}X)^{-1}X^{T}X(X^{T}X)^{-1} \\\\\n    &= \\nu (X^{T}X)^{-1}\n    \\end{align*}\n```\n\n9.  If a matrix $P$ is idempotent, $PP =$ ?\n    -   $P$ from section 3.5.2\n",
    "supporting": [
      "03-TheMATH_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}