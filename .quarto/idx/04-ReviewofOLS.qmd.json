{"title":"Review of OLS","markdown":{"yaml":{"title":"Review of OLS"},"headingText":"Introducing OLS Regression","containsRefs":false,"markdown":"\n<script src=\"https://hypothes.is/embed.js\" async></script>\n\n```{=tex}\n\\newcommand\\bE{\\mathbf{E}}\n\\newcommand\\E{\\mathbb{E}}\n\\newcommand\\V{\\mathbb{V}}\n```\nThis section will provide a review of OLS.\n\nOLS is the workhorse of empirical political science. We will learn a lot beyond OLS, but OLS is often \"good enough\" and sometimes preferable for explaining the relationship between variables. That is to say, MLE will expand your toolkit, but OLS should remain a big part of your toolkit.\n\nI recommend that you review the following readings to familiarize yourself with regression. I will make note within this section where particular readings are most relevant. These readings are available on Canvas in the modules- Week 1 section.\n\n-   Wheelan, Charles. 2012. *Naked Statistics*. W.W. Norton. Chapter 11. This provides an accessible overview of regression and the interpretation of regression results.\n\n-   Gelman, Andrew, and Jennifer Hill. 2006. Data analysis using regression and multilevel/hierarchical models. Cambridge University Press. Chapter 3. This is a slightly more technical overview and includes some R code for running regressions.\n\n-   Building models and breaking models.\n\n    -   (Optional) Fox, John. 2015. Applied Regression Analysis and Generalized Linear Models, 2nd Edition. Sage. Chapter 11. This reading describes diagnostic tests to probe whether the model is a good fit of the data. We won't go into detail about this in this class, but is material classes focused on linear regression will generally cover.\n    -   Messing, Solomon. [\"How to break regression.\"](https://medium.com/pew-research-center-decoded/how-to-break-regression-f48230f0ca68)\n    -   Lenz, G., & Sahn, A. (2020). \"Achieving Statistical Significance with Control Variables and Without Transparency.\" *Political Analysis*, 1-14. doi:10.1017/pan.2020.31. This paper talks about how to build a regression model, and in particular, why adding more and more controls isn't always a good thing.\n\n\nThe regression method describes how one variable depends on one or more other variables. Ordinary Least Squares regression is a linear model with the matrix representation:\n\n$Y = \\alpha + X\\beta + \\epsilon$\n\nGiven values of variables in $X$, the model predicts the average of an outcome variable $Y$. For example, if $Y$ is a measure of how wealthy a country is, $X$ may contain measures related to the country's natural resources and/or features of its institutions (things that we think might contribute to how wealthy a country is.) In this equation:\n\n-   $Y$ is the outcome variable ($n \\times 1$).[^04-reviewofols-1]\n-   $\\alpha$ is a parameter representing the intercept\n-   $\\beta$ is a parameter representing the slope/marginal effect ($k \\times 1$), and\n-   $\\epsilon$ is the error term ($n \\times 1$).\n\n[^04-reviewofols-1]: Recall this notation means rows by columns, $Y$ is a vector of length $n$ (the number of observations), and since there is only 1 outcome measure, it is 1 column.\n\nIn OLS, we estimate a line of best fit to predict $\\hat{Y}$ values for different values of X:\n\n-   $\\hat{Y} = \\hat{\\alpha} + X\\hat{\\beta}$.\n-   When you see a \"$\\hat{hat}$\" on top of a letter, that means it is an estimate of a parameter.\n-   As we will see in the next section, in multiple regression, sometimes this equation is represented as just $\\hat{Y} = X\\hat{\\beta}$, where this generally means that $X$ is a matrix that includes several variables and $\\hat \\beta$ is a vector that includes several coefficients, including a coefficient representing the intercept $\\hat \\alpha$\n\nWe interpret linear regression coefficients as describing how a dependent variable is expected to change when a particular independent variable changes by a certain amount. Specifically:\n\n-   \"Associated with each one unit increase in a variable $x_1$, there is a $\\hat{\\beta_1}$ estimated expected average increase in $y$.\"\n-   If we have more than one explanatory variable (i.e., a multiple regression), we add the phrase \"controlling on/ holding constant other observed factors included in the model.\"\n\nWe can think of the interpretation of a coefficient in multiple regression using an analogy to a set of light switches:\n\n![](images/lightswitch.jpeg){width=\"40%\"}\n\nWe ask: How much does the light in the room change when we flip one switch, while holding constant the position of all the other switches?\n\nThis would be a good place to review the Wheelan chapter and Gelman and Hill 3.1 and 3.2 to reinforce what a regression is and how to interpret regression results.\n\n## Diving Deeper into OLS Matrix Representation\n\nIn this section, we will review the matrix representation of the OLS regression in more detail and discuss how to derive the estimators for the regression coefficients.[^04-reviewofols-2]\n\n[^04-reviewofols-2]: This [video](https://www.youtube.com/watch?v=GMVh02WGhoc&ab_channel=BenLambert) from Ben Lambert provides additional intuition for understanding OLS in a matrix form and how it can be useful.\n\nOLS in Matrix Form: Let $X$ be an $n \\times k$ matrix where we have observations on k independent variables for n observations. Since our model will usually contain a constant term, one of the columns in the X matrix will contain only ones. This column should be treated exactly the same as any other column in the X matrix.\n\n-   Let $Y$ be an $n \\times 1$ vector of observations on the dependent variable. Note: because $Y$ is a vector (a matrix with just one column), sometimes it is written in lowercase notation as $\\mathbf y$.\n-   Let $\\epsilon$ be an $n \\times 1$ vector of disturbances or errors.\n-   Let $\\beta$ be an $k \\times 1$ vector of unknown population parameters that we want to estimate.\n\n$\\begin{pmatrix} y_1 \\\\ y_2 \\\\ y_3 \\\\ y_4 \\\\ ... \\\\ y_n \\end{pmatrix}$ = $\\begin{pmatrix} 1 & x_{11} & x_{12} & x_{13} & ... & x_{1k}\\\\ 1 & x_{21} & x_{22} & x_{23} & ... & x_{2k} \\\\ 1 & x_{31} & x_{32} & x_{33} & ... & x_{3k}\\\\ 1 & x_{41} & x_{42} & x_{43} & ... & x_{4k} \\\\ ... & ... & ... & ... & ... & ... \\\\ 1 & x_{n1} & x_{n2} & x_{n3} & ... & x_{nk}\\end{pmatrix}$ X $\\begin{pmatrix} \\alpha \\\\ \\beta_1 \\\\ \\beta_2 \\\\ \\beta_3 \\\\ ... \\\\ \\beta_k \\end{pmatrix}$ + $\\begin{pmatrix} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\epsilon_3 \\\\ \\epsilon_4 \\\\ ... \\\\ \\epsilon_n \\end{pmatrix}$\n\nOur estimates are then $\\mathbf{ \\hat y} = X\\hat \\beta$. What are the dimensions of this quantity?\n\nGelman and Hill Section 3.4, pg. 38 provides a nice visual of how this representation maps onto what a typical dataset may look like, where we will try to estimate a set of coefficients that map the relationship between the columns of $X$ and $\\mathbf y$:\n\n![](images/gh34.png){width=\"70%\"}\\\\\n\nThis is a good place to review Gelman and Hill 3.4 on different notations for representing the regression model.\n\n### Estimating the Coefficients\n\nModels generally start with some goal. In OLS, our goal is to minimize the sum of squared \"residuals.\" Here is a video I created to explain why we can represent this as $\\mathbf{e'}\\mathbf{e}$.\n\n{{< video src=\"https://www.youtube.com/watch?v=IFX_n16YTm0\" >}}\n\n*Note: at the end of the video it should read* $X\\hat\\beta$, not $\\hat X \\beta$\n\nWhat is a residual? It's the difference between y and our estimate of y: $y - \\hat y$. It represents the error in our prediction-- how far off our estimate is of the outcome.\n\nWe can write this in matrix notation in the following way where $\\mathbf e$ is an $n \\times 1$ vector of residuals-- a residual for each observation in the data:\n\n```{=tex}\n\\begin{align*}\n\\mathbf{e'}\\mathbf{e} &= (Y' - \\hat{\\beta}'X')(Y - X\\hat{\\beta})\\\\\n&=Y'Y - \\hat{\\beta}'X'Y - Y'X\\hat{\\beta} + \\hat{\\beta}'X'X\\hat{\\beta} \\\\\n&= Y'Y - 2\\hat{\\beta}'X'Y + \\hat{\\beta}'X'X\\hat{\\beta}\n\\end{align*}\n```\nRecall we want a line that minimizes this quantity. We minimize the sum of squared residuals by taking the derivative with respect to $\\beta$. (We want to identify the coefficients that help us achieve the goal of minimizing the squared error.) Because we are now deriving an estimate, we will use the hat over $\\beta$:\n\n-   $\\frac{\\delta }{\\delta \\hat \\beta} = -2X'Y + 2X'X\\hat{\\beta}$\n-   So what is our estimate for $\\hat{\\beta}$? We take first order conditions\n\n```{=tex}\n\\begin{align*}\n  0 &=-2X'Y + 2X'X\\hat{\\beta}\\\\\n   \\hat{\\beta} &= (X'X)^{-1}X'Y\n   \\end{align*}\n```\nYou may wonder how we got to these answers. Don't worry, you will get your chance to solve this! The important thing to note for now, is that we have an analytic solution to our coefficient estimates.\n\n## OLS Regression in R\n\nTo run a linear regression in R, we use the `lm()` function.\n\nThe syntax is `lm(y ~ x1, data = mydata)` for a regression with `y` as the name of your dependent variable and there is one explanatory variable `x1` where `mydata` is the name of your data frame.\n\n`lm(y ~ x1 + x2 , data = mydata)` is the syntax for a regression with two explanatory variables `x1` and `x2`, where you would add additional variables for larger multivariate regressions. By default, R will include an intercept term in the regression.\n\n### Example: Predicting Current Election Votes from Past Election Votes\n\nIn the American presidential election in 2000, there was an actual controversy in how ballots were cast in the state of Florida. Social scientists used data comparing the election results from 1996 in the state with 2000 as one way to help detect irregularities in the 2000 vote count. For more information on the background of this example, you can watch this [video](https://www.youtube.com/watch?v=D-nR_hmS6V0).\n\nWe will use the data `florida.csv` available [here](https://github.com/ktmccabe/teachingdata/blob/main/florida.csv):\n\n```{r}\n## Load Data\nflorida <- read.csv(\"https://raw.githubusercontent.com/ktmccabe/teachingdata/main/florida.csv\")\n```\n\nThis data set includes several variables described below, where each row represents the voting information for a particular county in Florida.\n\n| Name         |       Description        |\n|:-------------|:------------------------:|\n| `county`     |       county name        |\n| `Clinton96`  | Clinton's votes in 1996  |\n| `Dole96`     |   Dole's votes in 1996   |\n| `Perot96`    |  Perot's votes in 1996   |\n| `Bush00`     |   Bush's votes in 2000   |\n| `Gore00`     |   Gore's votes in 2000   |\n| `Buchanan00` | Buchanan's votes in 2000 |\n\nIn 2000, Buchanan was a third party candidate, similar to Perot in 1996. One might think that counties where Perot received a lot of votes in 1996 should also receive a lot in 2000. That is: with a one-vote increase in Perot's vote, we might expect an average increase in Buchanan's 2000 vote.\n\nWe can translate that language into a regression equation:\n\n-   $Buchanan2000 = \\alpha + Perot1996 * \\beta + \\epsilon$\n\nIn R, we run this regression the following way. We will save it as an object `fit.1`. You can name your regression objects anything you want.\n\n```{r}\nfit.1 <- lm(Buchanan00 ~ Perot96, data = florida)\n```\n\n-   `summary(model)` provides the summary statistics of the model. In particular, the following statistics are important\n\n    -   `Estimate`: point estimate of each coefficient\n    -   `Std. Error`: standard error of each estimate\n    -   `t value`: indicates the $t$-statistic of each coefficient under the null hypothesis that it equals zero\n    -   `Pr(>|t|)`: indicates the two-sided $p$-value corresponding to this $t$-statistic where asterisks indicate the level of statistical significance.\n    -   `Multiple R-squared`: The coefficient of determination\n    -   `Adjusted R-squared`: The coefficient of determination adjusting for the degrees of freedom\n\nWe will say more to define these quantities in future sections.\n\n```{r}\nsummary(fit.1)\n```\n\nR also allows several shortcuts for accessing particular elements of your regression results. Examples:\n\n```{r}\n## Vector of the coefficient estimates only\ncoef(fit.1)\n\n## Compute confidence intervals for these coefficients\nconfint(fit.1)\n\n## Table of coefficient results only\nsummary(fit.1)$coefficients\n\n## Extract standard errors only\nsummary(fit.1)$coefficients[,2]\n\n## Variance-Covariance matrix\nvcov(fit.1)\n\n## Note that the square root of the diagonal of this matrix provides the standard errors\nsqrt(diag(vcov(fit.1)))\n\n## Degrees of freedom\nfit.1$df.residual\n```\n\n### Plotting Regression Results\n\nWe often don't want to hide our data under a bushel basket or in complicated regression models. Instead, we might also want to visualize data in R. The function `plot()` and the function `ggplot()` from the package `ggplot2` are two terrific and flexible functions for visualizing data. We will use the `plot()` function to visualize the relationship between Perot and Buchanan votes. The example below provides a few arguments you can use within each of these functions, but they are capable of much more.\n\nAt the core, plotting functions generally work as coordinate systems. You tell R specifically at which x and y coordinates you want your points to be located (e.g., by providing R with a vector of x values and a vector of y values). Then, each function has its own way of allowing you to add bells and whistles to your figure, such as labels (e.g., `main, xlab, ylab`), point styles (`pch`), additional lines and points and text (e.g., `abline(), lines(), points(), text()`), or x and y scales for the dimensions of your axes (e.g., `xlim, ylim`). You can create a plot without these additional features, but most of the time, you will add them to make your plots look good! and be informative! We will do a lot of plotting this semester.\n\nNote: feel free to use `plot()` or `ggplot()` or both. `ggplot` has similar capabilities as `plot` but relies on a different \"grammar\" of graphics. For example, see the subtle differences in the two plots below.\n\n```{r, message=FALSE, fig.width=7}\n\n## Plot\nplot(x = florida$Perot96, # x-values\n     y = florida$Buchanan00, # y-values\n     main = \"Perot and Buchanan Votes\", # label for main title\n     ylab = \"Buchanan Votes\", # y-axis label\n     xlab = \"Perot Votes\", # x-axis label\n     pch = 20) # point type\nabline(fit.1, col = \"red\") # adds a red regression line\n```\n\n```{r, warning=FALSE}\n## ggplot version\nlibrary(ggplot2)\nggplot(data = florida, # which data frame\n       mapping = aes(x = Perot96, y = Buchanan00)) + # x and y coordinates\n  geom_point() +  # tells R we want a scatterplot\n  geom_smooth(method = \"lm\",\n              se = FALSE, colour = \"red\",\n              data = florida, aes(x=Perot96, y=Buchanan00)) + # adds lm regression line\n  ggtitle(\"Perot and Buchanan Votes\") + # main title\n  labs(x = \"Perot Votes\", y = \"Buchanan Votes\") + # x and y labels\n  theme_bw() # changes theme (e.g., color of background)\n\n## Note: data = florida, aes(x=Perot96, y=Buchanan00) in the geom_smooth line is not necessary if it is the same mapping at the first line. Required if data are different\n```\n\nTip: you might want to save your plots as .pdf or .png after you create it. You can do this straight from your R code. How you do it varies by function. The files will save to your working directory unless you specify a different file path. The code below is the same as above except it has additional lines for saving the plots:\n\n```{r, eval=FALSE}\n## Plot\npdf(file = \"myfirstmleplot.pdf\", width = 7, height = 5) # play around with the dimensions\nplot(x = florida$Perot96, # x-values\n     y = florida$Buchanan00, # y-values\n     main = \"Perot and Buchanan Votes\", # label for main title\n     ylab = \"Buchanan Votes\", # y-axis label\n     xlab = \"Perot Votes\", # x-axis label\n     pch = 20) # point type\nabline(fit.1, col = \"red\") # adds a red regression line\ndev.off() # this closes your pdf file\n\n## ggplot version\nggplot(data = florida, # which data frame\n       mapping = aes(x = Perot96, y = Buchanan00)) + # x and y coordinates\n  geom_point() +  # tells R we want a scatterplot\n  geom_smooth(method = \"lm\",  \n              se = FALSE, colour = \"red\",\n              data = florida, aes(x=Perot96, y=Buchanan00)) + # adds lm regression line\n  ggtitle(\"Perot and Buchanan Votes\") + # main title\n  labs(x = \"Perot Votes\", y = \"Buchanan Votes\") + # x and y labels\n  theme(plot.title = element_text(hjust = 0.5)) +# centers the title\n  theme_bw() # changes theme (e.g., color of background)\nggsave(\"myfirstmleggplot.png\", device=\"png\", width = 7, height = 5) # saves the last ggplot\n```\n\n### Finding Coefficients without `lm`\n\nLet's put our matrix algebra and R knowledge together. In the previous section, we found that $\\hat \\beta = (X'X)^{-1}X'Y$. If we do that math directly in R, there is no need to use `lm()` to find those coefficients.\n\nTo do so, we need $X$ and $Y$.\n\nRecall $Y$ is an $n \\times 1$ vector representing the outcome of our model. In this case, $Y$ is `Buchanan00`.\n\n```{r}\nY <- florida$Buchanan00\n```\n\nRecall, $X$ is a $n \\times k$ matrix representing our independent variables and a column of 1's for the intercept. Let's build this matrix using `cbind` which was introduced in section 2.\n\n```{r}\nX <- cbind(1, florida$Perot96)\ndim(X)\n```\n\nGreat, now we have $X$ and $Y$, so it's just about a little math. Because $Y$ is a vector, let's make sure R knows to treat it like an $n \\times 1$ matrix.\n\n```{r}\nY <- cbind(Y)\ndim(Y)\n```\n\nRecall the `solve()` and `t()` functions take the inverse and transpose of matrices.\n\n```{r}\nbetahat <- solve(t(X) %*% X) %*% t(X) %*% Y\n```\n\nFinally, let's compare the results from our model using `lm()` with these results.\n\n```{r, results='hold'}\nbetahat\ncoef(fit.1)\n```\n\nWe did it! In the problem set, you will get more experience using the analytic solutions to solve for quantities of interest instead of the built-in functions.\n\n### OLS Practice Problems\n\nHere are a couple of (ungraded) problems to modify the code above and gain additional practice with data wrangling and visualization in R. As you might have noticed in the example, there is a big outlier in the data. We will see how this observation affects the results.\n\n1.  Using a linear regression examine the relationship between Perot and Buchanan votes, controlling for Bill Clinton's 1996 votes.\n\n-   Provide a one sentence summary of the relationship between Perot and Buchanan's votes.\n-   Is the relationship significant at the $p < 0.05$ level? What about the relationship between Clinton and Buchanan votes?\n-   What are the confidence intervals for the Perot coefficient results?\n-   What is the residual for the estimate for Palm Beach County-- `PalmBeach` in the `county` variable?\n\n2.  Let's go back to the bivariate case.\n\n-   Subset the data to remove the county `PalmBeach`.\n-   Create a scatterplot of the relationship between Perot votes and Buchanan votes within this subset. This time make the points blue.\n-   Add a regression line based on this subset of data.\n-   Add a second regression line in a different color based on the initial bivariate regression we ran in the example, where all data were included.\n-   Describe the differences in the regression lines.\n\n### Code for solutions\n\n```{r, eval=TRUE, include=TRUE}\nfit.multiple <- lm(Buchanan00 ~ Perot96 + Clinton96, data = florida)\nsummary(fit.multiple)\nconfint(fit.multiple)[2,]\n\nflorida$res <- residuals(fit.multiple)\nflorida$res[florida$county == \"PalmBeach\"]\n\nflorida.pb <- subset(florida, subset = (county != \"PalmBeach\"))\nfit2 <- lm(Buchanan00 ~ Perot96, data = florida.pb)\n\nggplot(data = florida.pb, # which data frame\n       mapping = aes(x = Perot96, y = Buchanan00)) + # x and y coordinates\n  geom_point(color=\"blue\") +  # tells R we want a scatterplot\n  geom_smooth(method = \"lm\", \n              se = FALSE, colour = \"green\",\n              data = florida.pb, aes(x=Perot96, y=Buchanan00)) + # adds lm regression line\n    geom_smooth(method = \"lm\", \n              se = FALSE, colour = \"red\",\n              data = florida, aes(x=Perot96, y=Buchanan00)) + # adds lm regression line\n  ggtitle(\"Perot and Buchanan Votes\") + # main title\n  labs(x = \"Perot Votes\", y = \"Buchanan Votes\") + # x and y labels\n  theme(plot.title = element_text(hjust = 0.5)) +# centers the title\n  theme_bw() # changes theme (e.g., color of background)\n\n```\n\n## Extra practice building and breaking regression\n\nBelow is additional practice with regression and R, showing how to work with different variables and diagnosing errors.\n\n***What is the association between race and income?***\n\nLet's say we want to explore the relationship between race and income, where the people in our sample take on the values white, Black, Asian, and Hispanic for race. We can write this as:\n\n$Income_i = \\alpha + \\beta*race_i + \\epsilon$\n\nHowever, race is not a numeric variable. This complicates our regression equation because what exactly is a 1-unit change in race? Sure, we could assign numeric values to each racial category in our data (e.g., white = 1, Black = 2, Hispanic = 3, Asian = 4), but we would have no reason to assume that the change in income would be linear as you change in race by units. Why should the difference in income between white and Black individuals be estimated as the same difference between Black and Hispanic individuals?\n\nIn a linear regression, when you have categorical independent variables, what should you typically do?\n\n### Categorical variables\n\nLet's build some data\n\n*Build a matrix with dummy variables for each race*\n\nRun the code below and see what is in X.\n\n```{r }\n## Dummy variable example\nresprace <- c(\"white\", \"white\", \"asian\", \"black\", \"hispanic\",\n              \"hispanic\", \"hispanic\", \"asian\", \"white\", \"black\", \n              \"black\", \"black\", \"asian\", \"asian\", \"white\", \"white\")\n\n## \"Dummy variables\"\nwhite <- rep(0, length(resprace))\nwhite[resprace == \"white\"] <- 1\nasian <- rep(0, length(resprace))\nasian[resprace == \"asian\"] <- 1\nblack <- rep(0, length(resprace))\nblack[resprace == \"black\"] <- 1\nhispanic <- rep(0, length(resprace))\nhispanic[resprace == \"hispanic\"] <- 1\n\n## Matrix\nX <- cbind(white, asian, black, hispanic)\nX\n```\n\n*Let's build toward a regression model*\n\nLet's create a `Y` variable representing our outcome for income. Let's also add an intercept to our `X` matrix. Take a look into our new X.\n\n```{r}\n## Dependent variable\nY <- cbind(c(10, 11, 9, 8, 9, 7, 7, 13, 12, 11, 8, 7, 4, 13, 8, 7))\n\nX <- cbind(1, X)\n```\n\n*Let's now apply the formula* $(X'X)^{-1}X'Y$ to estimate our coefficients.\n\nAssign the output to an object called `betahat`\n\n```{r, eval=FALSE}\nbetahat <- solve(t(X) %*% X) %*% t(X) %*% Y\nbetahat\n```\n\n### Formalizing Linear Dependence\n\nWhy were our dummy variables linear dependent?\n\nIf we inspect `X` we can see that taking each column $\\mathbf{x_1} - \\mathbf{x_2} - \\mathbf{x_3}-\\mathbf{x_4}=0$. There is a linear relationship between the variables.\n\nFormally, a set of vectors (e.g.,$\\mathbf{x_1}, \\mathbf{x_2}, ...\\mathbf{x_k}$) is linearly independent if the equation $\\mathbf{x_1}*a_1 + \\mathbf{x_2}*a_2 +... + \\mathbf{x_k}*a_k= 0$ only in the trivial case where $a_1$ and $a_2$ through $a_k$ are 0. A set of vectors has a linearly dependent relation if there is a solution $\\mathbf{x_1}*a_1 + \\mathbf{x_2}*a_2 +... + \\mathbf{x_k}*a_k = 0$ where not all $a_1, a_2$ through $a_k$ are 0.\n\nFor OLS, we must assume no perfect collinearity.\n\n-   No independent variable is constant\n-   No exactly linear relationships among the independent variables\n-   The rank of X is $k$ where the rank of a matrix is the maximum number of linearly independent columns.\n\nAs discussed in the course notes, a square matrix is only invertible if its columns are linearly independent. In OLS, in order to estimate unique solutions for $\\hat \\beta$, we need to invert $(X'X)^{-1}$. When we have perfect collinearity, we cannot do this.\n\n*Note the linear dependence in X*\n\n```{r}\n## Matrix\nX[, 1] - X[, 2] - X[, 3] - X[, 4] - X[,5]\n```\n\n*Try to take* $(X'X)^{-1}$\n\n```{r, eval=F}\nsolve(t(X) %*% X)\n```\n\nHow do we correct this?\n\nTo address this, we are going to drop one of the categorical variables when we run the regression. Consequently, our coefficients will now be interpreted as differences between this reference category (the category left out, e.g., white) and the particular group (e.g., white vs. Asian or white vs. Black or white vs. Hispanic).\n\n*Redefine `X` removing the white column, and calculate* $(X'X)^{-1}X'Y$\n\n```{r}\nX <- cbind(1, asian, black, hispanic)\nbetahat <- solve(t(X) %*% X) %*% t(X) %*% Y\nbetahat \n```\n\n*Check this with the `lm()` function*\n\n```{r}\nsummary(lm(Y ~ asian + black + hispanic))\n```\n\nIn R and many other statistical softwares, the regression function will forcibly drop one of your variables if it encounters this type of linear dependence. See below when we include all four race dummies in the model.\n\n*Check what happens with the `lm()` function*\n\n```{r }\nsummary(lm(Y ~ white + asian + black + hispanic))\n```\n\nAn alternative way to enter categorical variables in a regression is to let the function create the dummy variables for you using `factor(var, levels = )` to make sure R knows it is a factor variables.\n\n```{r}\nresprace <- factor(resprace, levels = c(\"white\", \"asian\", \"black\", \"hispanic\"))\nsummary(lm(Y ~ resprace))\n```\n\n### Other examples of breaking the no perfect collinearity rule\n\nRecall, for OLS, we must assume no perfect collinearity.\n\n-   No independent variable is constant\n-   No exactly linear relationships among the independent variables\n-   The rank of X is $k$ where the rank of a matrix is the maximum number of linearly independent columns.\n\nLet's say we wanted to control for age, but all of our sample was 18 years old. Let's try to add this to the `X` matrix.\n\n*Redefine `X` adding age column, and calculate* $(X'X)^{-1}X'Y$\n\n```{r, eval=FALSE}\nage <- rep(18, length(resprace))\nX <- cbind(1, asian, black, hispanic, age)\nbetahat <- solve(t(X) %*% X) %*% t(X) %*% Y\nbetahat \n\n```\n\nLet's visit the Florida example. Let's say we had two `Perot96` variables-- one the raw votes and one where votes were multiplied by 1000 to adjust the order of magnitude.\n\n*Regress Buchanan's votes on Perot and Perot adjusted values*\n\n```{r, eval=FALSE}\nflorida <- read.csv(\"https://raw.githubusercontent.com/ktmccabe/teachingdata/main/florida.csv\")\n\nflorida$perotadjusted <- florida$Perot96 * 1000\n\nY <- florida$Buchanan00\nX <- cbind(1, florida$Perot96, florida$perotadjusted)\nbetahat <- solve(t(X) %*% X) %*% t(X) %*% Y\nbetahat \n```\n\n## Uncertainty and Regression\n\nWe have now gone through the process of minimizing the sum of squared errors ($\\mathbf{e'e}$) and deriving estimates for the OLS coefficients $\\hat \\beta = (X'X)^{-1}X'Y$. In this section, we will discuss how to generate estimates of the uncertainty around these estimates.\n\nWhere we are going:\n\n-   In the last section, we visited an example related to the 2000 election in Florida. We regressed county returns for Buchanan in 2000 (Y) on county returns for Perot in 1996 (X).\n\n```{r}\n## Load Data\nflorida <- read.csv(\"https://raw.githubusercontent.com/ktmccabe/teachingdata/main/florida.csv\")\nfit.1 <- lm(Buchanan00 ~ Perot96, data = florida)\nsummary(fit.1)\n```\n\nThe summary output of the model shows many different quantities in addition to the coefficient estimates. In particular, in the second column of the summary, we see the standard errors of the coefficients. Like many statistical software programs, the `lm()` function neatly places these right next to the coefficients. We will now discuss how we get to these values.\n\n### Variance of the Coefficients\n\nThe standard error is the square root of the variance, representing the typical deviation we would expect to see between our estimates $\\hat \\beta$ of the parameter $\\beta$ across repeated samples. So to get to the standard error, we just need to get to an estimate of the variance.\n\nLet's take the journey. First the math. As should start becoming familiar, we have our initial regression equation, which describes the relationship between the independent variables and dependent variables.\n\n-   Start with the model: $Y = X\\beta + \\epsilon$\n    -   We want to generate uncertainty for our estimate of $\\hat \\beta =(X'X)^{-1}X'Y$\n-   Note: Conditional on fixed values of $X$ (I say fixed values because this is our data. We know $X$ from our dataset.), the only random component is $\\epsilon$.\n    -   What does that mean? Essentially, the random error term in our regression equation is what is giving us the uncertainty. If $Y$ was a deterministic result of $X$, we would have no need for it, but it's not. The relationship is not exact, varies sample to sample, subject to random perturbations, represented by $\\epsilon$.\n\nBelow we go through how to arrive at the mathematical quantity representing the variance of $\\hat \\beta$ which we will notate as $\\mathbf{V}(\\hat\\beta)$. The first part of the math below is just substituting terms:\n\n```{=tex}\n\\begin{align*}\n\\mathbf{V}(\\widehat{\\beta}) &= \n\\mathbf{V}( (X^T X) ^{-1} X^T Y))  \\\\\n&= \\underbrace{\\mathbf{V}( (X^T X) ^{-1} X^T (X\\beta + \\epsilon))}_\\text{Sub in the expression for Y from above}  \\\\\n&= \\underbrace{\\mathbf{V}((X^T X) ^{-1} X^T X \\beta + (X^T X) ^{-1} X^T \\epsilon)}_\\text{Distribute the term to the items in the parentheses}  \\\\\n&= \\underbrace{\\mathbf{V}(\\beta + (X^T X) ^{-1} X^T \\epsilon)}_\\text{Using the rules of inverses, the two terms next to $\\beta$ canceled each other out}  \n\\end{align*}\n```\nThe next part of the math requires us to use knowledge of the definition of variance and the rules associated. We draw on two in particular:\n\n-   The variance of a constant is zero.\n-   When you have a constant multipled by a random variable, e.g., $\\mathbf{V}(4d)$, it can come out of the variance operator, but must be squared: $16\\mathbf{V}(d)$\n-   Putting these together: $\\mathbf{V}(2 + 4d)= 16\\mathbf{V}(d)$\n\nKnowing these rules, we can proceed: \\begin{align*}\n\\mathbf{V}(\\widehat{\\beta}) &= \\mathbf{V}(\\beta + (X^T X) ^{-1} X^T \\epsilon) \\\\\n&=\\underbrace{ \\mathbf{V}((X^T X) ^{-1} X^T \\epsilon)}_\\text{$\\beta$ drops out because in a regression it is an unkown \"parameter\"-- it's constant, which means its variance is zero.}\\\\\n&= \\underbrace{(X^T X)^{-1}X^T \\mathbf{V}( \\epsilon) ((X^T X)^{-1}X^T)^T}_\\text{We can move $(X^T X)^{-1}X^T$ out front because our data are fixed quantities, but in doing so, we have to \"square\" the matrix.}\\\\\n&= (X^T X)^{-1}X^T \\mathbf{V}( \\epsilon) X (X^T X)^{-1}\n\\end{align*}\n\nThe resulting quantity is our expression for the $\\mathbf{V}(\\hat \\beta)$. However, in OLS, we make an additional assumption that allows us to further simplify the expression. We assume homoscedasticity aka \"constant\" or \"equal error variance\" which says that the variance of the errors are the same across observations: $\\mathbf{V}(\\epsilon) = \\sigma^2 I_n$.\n\n-   If we assume homoscedastic errors, then Var$(\\epsilon) = \\sigma^2 I_n$\n\n```{=tex}\n\\begin{align*}\n\\mathbf{V}(\\widehat{\\beta})  &= (X^T X)^{-1}X^T \\mathbf{V}( \\epsilon) X (X^T X)^{-1}\\\\\n&= \\underbrace{(X^T X)^{-1}X^T \\sigma^2I_n X (X^T X)^{-1}}_\\text{Assume homoskedasticity}\\\\\n&= \\underbrace{\\sigma^2(X^T X)^{-1} X^T X (X^T X)^{-1}}_\\text{Because it is a constant, we can move it out in front of the matrix multiplication, and then simplify the terms.}  \\\\\n&= \\sigma^2(X^T X)^{-1} \n\\end{align*}\n```\nAll done! This expression: $\\sigma^2(X^T X)^{-1}$ represents the variance of our coefficient estimates. Note its dimensions: $k \\times k$. It has the same number of rows and columns as the number of our independent variables (plus the intercept).\n\nThere is one catch, though. How do we know what $\\sigma^2$ is? Well, we don't. Just like the unknown parameter $\\beta$, we have to estimate it in our regression model.\n\nJust like with the coefficients, we notate our estimate as $\\widehat{\\sigma}^2$. Our estimate is based on the observed residual errors in the model and is as follows:\n\n-   $\\widehat{\\sigma}^2 = \\frac{1}{N-K}\\sum_{i=1}^N \\widehat{\\epsilon_i^2} = \\frac{1}{N-K} \\mathbf{e'e}$\n\nThat means our **estimate** of the variance of the coefficients is found within: $\\hat \\sigma^2(X^T X)^{-1}$\n\nAgain, this is a $k \\times k$ matrix and is often called the variance covariance matrix. We can extract this quantity from our linear models in R using `vcov()`.\n\n```{r}\nvcov(fit.1)\n```\n\nThis is the same that we would get if manually we took the residuals and multiplied it by our $X$ matrix according to the formula above:\n\n```{r}\nX <- cbind(1, florida$Perot96)\ne <- cbind(residuals(fit.1))\nsigmahat <- ((t(e) %*% e) / (nrow(florida) -2)) \n## tell r to stop treating sigmahat as a matrix\nsigmahat <-as.numeric(sigmahat)\nXtX <- solve(t(X) %*%X)\nsigmahat * XtX\n```\n\nThe terms on the diagonal represent the variance of a particular coefficient in the model.The standard error of a particular coefficient $k$ is: s.e.($\\hat{\\beta_k}) = \\sqrt{\\widehat{\\sigma}^2 (X'X)^{-1}}_{kk}$. The off-diagonal components represent the covariances between the coefficients.\n\nRecall that the standard error is just the square root of the variance. So, to get the nice standard errors we saw in the summary output, we can take the square root of the quantities on the diagonal of this matrix.\n\n```{r}\nsqrt(diag(vcov(fit.1)))\nsummary(fit.1)$coefficients[,2]\n```\n\nWhy should I care?\n\n1.  Well R actually doesn't make it that easy to extract standard errors from the summary output. You can see above that the code for extracting the standard errors using what we know about them being the square root of the variance is about as efficient as extracting the second column of the coefficient component of the summary of the model.\n2.  Sometimes, we may think that the assumption of equal error variance is not feasible and that we have unequal error variance or \"heteroscedasticity.\" Researchers have developed alternative expressions to model unequal error variance. Generally, what this means is they can no longer make that simplifying assumption, have to stop at the step with the uglier expression $(X^T X)^{-1}X^T \\mathbf{V}( \\epsilon) X (X^T X)^{-1}$ and then assume something different about the structure of the errors in order to estimate the coefficients. These alternative variance estimators are generally what are referred to as \"robust standard errors.\" There are many different robust estimators, and you will likely come across them in your research.\n\nSome of you may have learned the formal, general definition for variance as defined in terms of expected value: $\\mathbb{E}[(\\widehat{m} - \\mathbb{E}(\\widehat{m}))^2 ]$. We could also start the derivation there. This is not required for the course, but it is below if you find it useful. In particular, it can help show why we wend up needing to square a term when we move it outside the variance operator:\n\n```{=tex}\n\\begin{align*}\n\\mathbf{V}(\\widehat{\\beta}) &= \\mathbb{E}[(\\widehat{\\beta} - \\mathbb{E}(\\hat \\beta))^2)] \\\\\n&= \\mathbb{E}[(\\widehat{\\beta} - \\beta)^2]\\\\\n&= \\mathbb{E}[(\\widehat{\\beta} - \\beta)(\\widehat{\\beta} - \\beta)^T ] \\\\ &= \n\\mathbb{E}[(X^T X) ^{-1} X^TY - \\beta)(X^T X) ^{-1} X^TY - \\beta)^T]  \\\\\n&= \\mathbb{E}[(X^T X) ^{-1} X^T(X\\beta + \\epsilon) - \\beta)(X^T X) ^{-1} X^T(X\\beta + \\epsilon) - \\beta)^T]\\\\\n&=  \\mathbb{E}[(X^T X) ^{-1} X^TX\\beta + (X^T X) ^{-1} X^T\\epsilon - \\beta)(X^T X) ^{-1} X^TX\\beta + (X^T X) ^{-1} X^T\\epsilon - \\beta)^T]\\\\\n&=  \\mathbb{E}[(\\beta + (X^T X) ^{-1} X^T\\epsilon - \\beta)(\\beta + (X^T X) ^{-1} X^T\\epsilon - \\beta)^T]\\\\\n&=  \\mathbb{E}[(X^T X) ^{-1} X^T\\epsilon)(X^T X) ^{-1} X^T\\epsilon)^T]\\\\\n&= (X^T X) ^{-1} X^T\\mathbb{E}(\\epsilon\\epsilon^T)X(X^T X) ^{-1}\\\\\n&= \\underbrace{(X^T X) ^{-1} X^T\\sigma^2I_nX(X^T X) ^{-1}}_\\text{Assume homoskedasticity}\\\\\n&= \\sigma^2(X^T X) ^{-1} X^TX(X^T X) ^{-1}\\\\\n&= \\sigma^2(X^T X) ^{-1}\n\\end{align*}\n```\nNote: Along the way, in writing $\\mathbb{E}(\\hat \\beta) = \\beta$, we have implicitly assumed that $\\hat \\beta$ is an \"unbiased\" estimator of $\\beta$. This is not free. It depends on an assumption that the error term in the regression $\\epsilon$ is independent of our independent variables. This can be violated in some situations, such as when we have omitted variable bias, which is discussed at the end of our OLS section.\n\n### Hypothesis Testing\n\nMost of the time in social science, we run a regression because we have some hypothesis about how a change in our independent variable affects the change in our outcome variable.\n\nIn OLS, we can perform a hypothesis test for each independent variable in our data. The structure of the hypothesis test is:\n\n-   Null hypothesis: $\\beta_k = 0$\n    -   This essentially means that we don't expect a particular $x_k$ independent variable to have a relationship with our outcome variable.\n-   Alternative hypothesis: $\\beta_k \\neq 0$\n    -   We do expect a positive or negative relationship between a particular $x_k$ and the dependent variable.\n\nWe can use our estimates for $\\hat \\beta$ coefficients and their standard errors to come to a conclusion about rejecting or failing to reject the null hypothesis of no relationship by using a t-test.\n\nIn a t-test, we take our coefficient estimates and divide them by the standard error in order to \"standardize\" them on a scale that we can use to determine how likely it is we would have observed a value for $\\hat \\beta$ as extreme or more extreme as the one we observed in a world where the true $\\beta = 0$. This is just like a t-test you might have encountered before for a difference in means between groups, except this time our estimate is $\\hat \\beta$.\n\n```{=tex}\n\\begin{align*}\nt_{\\hat \\beta_k} &= \\frac{\\hat \\beta_k}{s.e.(\\hat \\beta_k)}\n\\end{align*}\n```\nGenerally speaking, when $t$ is about +/-2 or greater in magnitude, the coefficient will be \"significant\" at conventional levels (i.e., $p <0.05$), meaning that we are saying that it is really unlikely we would have observed a value as big as $\\hat \\beta_k$ if the null hypothesis were true. Therefore, we can reject the null hypothesis.\n\nHowever, to get a specific quantity, we need to calculate the p-value, which depends on the t-statistic and the degrees of freedom in the model. The degrees of freedom in a regression model are $N-k$, the number of observations in the model minus the number of independent variables plus the intercept.\n\nIn R, we can calculate p-values using the `pt()` function. By default, most people use two-sided hypothesis tests for regression. So to do that, we are going to find the area on each side of the t values, or alternatively, multiply the area to the right of our positive t-value by 2.\n\n```{r}\n## Let's say t was 2.05 and \n## And there were 32 observations and 3 variables in the regression plus an intercept\nt <- 2.05\ndf.t <- 32 -4\np.value <- 2 * (pt(abs(t), df=df.t, lower.tail = FALSE))\np.value\n```\n\nLet's do this for the florida example. First, we can find t by dividing our coefficients by the standard errors.\n\n```{r}\nt <- coef(fit.1) / (sqrt(diag(vcov(fit.1))))\nt \n\n## Compare with output\nsummary(fit.1)$coefficients[, 3]\n```\n\nWe can then find the p-values.\n\n```{r}\nt <- coef(fit.1) / (sqrt(diag(vcov(fit.1))))\ndf.t <- fit.1$df.residual\np.value <- 2 * (pt(abs(t), df=df.t, lower.tail = FALSE))\np.value\nsummary(fit.1)$coefficients[, 4]\n```\n\nWe see that the coefficient for `Perot96` is significant. The p-value is tiny. In R, for small numbers, R automatically shifts to scientific notation. The 9.47e-12 means the p-value is essentially zero, with the stars in the summary output indicating the p-value is $p < 0.001$. R will also output a test of the significance of the intercept using the same formula as all other coefficients. This generally does not have much interpretive value, so you are usually safe to ignore it.\n\n*Confidence Intervals*\n\nInstead of representing the significance using p-values, sometimes it is helpful to report confidence intervals around the coefficients. This can be particularly useful when visualizing the coefficients. The 95% confidence interval represents roughly 2 standard errors above and below the coefficient. The key thing to look for is whether it overlaps with zero (not significant) or does not (in which case the coefficient is significant).\n\nThe precise formula is\n\n$\\widehat{\\beta}_k$ Confidence intervals: $\\widehat{\\beta}_k - t_{crit.value} \\times s.e._{\\widehat{\\beta}_k}, \\widehat{\\beta}_k + t_{crit.value} \\times s.e_{\\widehat{\\beta}_k}$\n\nIn R, we can use `qt()` to get the specific critical value associated with a 95% confidence interval. This will be around 2, but fluctuates depending on the degrees of freedom in your model (which are function of your sample size and how many variables you have in the model.) R also has a shortcut `confint()` function to extract the coefficients from the model. Below we do this for the `Perot96` coefficient.\n\n```{r}\n## Critical values from t distribution at .95 level\nqt(.975, df = fit.1$df.residual) # n- k degrees of freedom\n\n## Shortcut\nconfint(fit.1)[2,]\n\n## By hand\ncoef(fit.1)[2] - qt(.975, df = fit.1$df.residual)*sqrt(diag(vcov(fit.1)))[2]\ncoef(fit.1)[2] + qt(.975, df = fit.1$df.residual)*sqrt(diag(vcov(fit.1)))[2]\n```\n\n### Goodness of Fit\n\nA last noteworthy component to the standard regression output is the goodness of fit statistics. For this class, we can put less attention on these, though there will be some analogues when we get into likelihood.\n\nThese are measures of how much of the total variation in our outcome measure can be explained by our model, as well as how far off are our estimates from the truth.\n\nFor the first two measures R-squared and Adjusted R-squared, we draw on three quantities:\n\n-   Total Sum of Squares--how much variance in $Y_i$ is there to explain?\n\n    -   $TSS: \\sum_{i=1}^N (Y_i -\\overline Y_i)^2$\n\n-   Estimated Sum of Squares--how much of this variance do we explain?\n\n    -   $ESS: \\sum_{i=1}^N (\\widehat Y_i -\\overline Y_i)^2$\n\n-   Residual Sum of Squares--how much variance is unexplained?\n\n    -   $RSS: \\sum_{i=1}^N ( Y_i -\\widehat Y_i)^2$\n\n-   $TSS = ESS + RSS$\n\n-   Multiple R-squared: $\\frac{ESS}{TSS}$\n\n    -   This is a value from 0 to 1, representing the proportion of the variance in the outcome that can be explained by the model. Higher values are generally considered better, but there are many factors that can affect R-squared values. In most social science tasks where the goal is to engage in hypothesis testing of coefficients, this measure is of less value.\n\n-   Adjusted R-squared: $1 - \\frac{\\frac{RSS}{n - k}}{\\frac{TSS}{n - 1}}$\n\n    -   This is essentially a penalized version of R-squared. When you add additional predictors to a model, the R-squared value can never decrease, even if the predictors are useless. The Adjusted R-squared adds a consideration for the degrees of freedom into the equation, creating a penalty for adding more and more predictors.\n\n-   Residual standard error aka root mean squared error aka square root of the mean squared residual: $r.s.e = \\sqrt{\\frac{RSS}{n-k}}$\n\n    -   This represents the typical deviation of an estimate of the outcome from the actual outcome. This quantity is often used to assess the quality of prediction exercises. It is used less often in social science tasks where the goal is hypothesis testing of the relationship between one or more independent variables and the outcome.\n\n*F-Statistic*\n\nSo far we have conducted hypothesis tests for each individual coefficient. We can also conduct a global hypothesis test, where the null hypothesis is that all coefficients are zero, with the alternative being that at least one coefficient is nonzero. This is the test represented by the F-statistic in the regression output.\n\nThe F-statistic helps us test the null hypothesis that **all** of the regression slopes are 0: $H_0 = \\beta_1 = \\beta_2 = \\dots = \\beta_k = 0$\n\n-   $F_0 = \\frac{ESS/(k - 1)}{RSS/(n - k)}$\n-   The F-Statistic has two separate degrees of freedom.\n    -   The model sum of squares degrees of freedom (ESS) are $k - 1$.\n    -   The residual error degrees of freedom (RSS) are $n - k$.\n    -   In a regression output, the model degrees of freedom are generally the first presented: \"F-statistic: 3.595 on $(k - 1) = 1$ and $(n - k) = 48$ DF.\"\n\nNote: This test is different from our separate hypothesis tests that a $k$ regression slope is 0. For that, we use the t-tests discussed above.\n\n## Generating predictions from regression models\n\nThe regression coefficients tell us how much $Y$ is expected to change for a one-unit change in $x_k$. It does not immediately tell us the values we estimate our outcome ($\\hat Y$) to take conditional on particular values of $x_k$. While often knowing our independent variables have a significant effect on the outcome and the size of the coefficient is sufficient for testing our hypotheses, it can be helpful for interpretation's sake, to see the estimated values for the outcome. This is going to be particularly important once we get into models like logistic regression, where the coefficients won't be immediately interpretable.\n\nRecall that our equation for estimating values of our outcomes is:\n\n-   $\\hat Y = X\\hat \\beta$ This can also be written out in long form for any particular observation $i$:\n\n-   $\\hat y_i = \\hat \\alpha + \\hat \\beta_1*x_1i + \\hat \\beta_2*x_2i + ... \\hat\\beta_k*x_ki$\n\nThe estimated values of our regression $\\hat Y$ are often called the \"fitted values.\" In R, you can identify the estimated values for each observation using the `fitted()` command.\n\n```{r}\n## Y hat for the first observation in the data\nfitted(fit.1)[1]\n```\n\nAgain, this is just the multiplication of the matrix $X$ and $\\hat \\beta$. If we have already run a regression model in R, one shortcut for getting the $X$ matrix, is to use the `model.matrix` command. We can get $\\hat \\beta$ using the `coef()` command.\n\n```{r}\nX <- model.matrix(fit.1)\nhead(X) # head() shows about the first six values of an object\nbetahat <- coef(fit.1)\n```\n\nOur fitted values are then just\n\n```{r}\nyhat <- X %*% betahat\nhead(yhat)\n```\n\nIf I want to generate an estimate for any particular observation, I could also just extract its specific value for `Perot96`.\n\n```{r}\nflorida$Perot96[1]\n```\n\nLet's estimate the Buchanan 2000 votes for the first county in the data with Perot 96 votes of 8072. We can write it out as $\\hat Buchanan00_1 =\\hat \\alpha + \\hat \\beta*Perot96_1$\n\n```{r}\nbuch00hat <- coef(fit.1)[1] + coef(fit.1)[2]*florida$Perot96[1]\nbuch00hat\n```\n\nWhat is useful about this is that now we have the coefficient estimates, we can apply them to any values of $X$ we wish in order to generate estimates/predictions of the values $Y$ will take given particular values of our independent variables.\n\nOne function that is useful for this (as a shortcut) is the `predict(fit, newdata=newdataframe)` function in R. It allows you to enter in \"newdata\"-- meaning values of the $X$ variables for which you want to generate estimates of $Y$ based on the coefficient estimates of your regression model.\n\nFor example, let's repeat the calculation from above for `Perot96 = 8072`.\n\n```{r}\npredict(fit.1, newdata = data.frame(Perot96 = 8072))\n```\n\nWe can also generate confidence intervals around these estimates by adding `interval = \"confidence\"` in the command.\n\n```{r}\npredict(fit.1, newdata = data.frame(Perot96 = 8072), interval=\"confidence\")\n```\n\nWe can also simultaneously generate multiple predictions by supplying a vector of values in the `predict()` command. For example, let's see the estimated Buchanan votes for when the Perot 1996 votes took values of 1000 to 10,000 by intervals of 1,000.\n\n```{r}\npredict(fit.1, newdata = data.frame(Perot96 = c(1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000, 9000, 10000)))\n```\n\nThe important thing to note about the `predict()` command is that if you have multiple independent variables, you have to specify the values you want each of them to take when generating the estimated values of y.\n\n```{r}\nfit.2 <- lm(Buchanan00 ~ Perot96 + Clinton96, data = florida)\n```\n\nFor example, let's build a second model with `Clinton96` as an additional predictor. In order to generate the same prediction for different values of Perot 1996 votes, we need to tell R at what values we should \"hold constant\" `Clinton96.` I.e., we want to see how hypothetical changes in `Perot96` votes influence changes in Buchanan 2000 votes while also leaving the Clinton votes identical. This is that lightswitch metaphor-- flipping one switch, while keeping the rest untouched.\n\nThere are two common approaches to doing this. 1) We can hold constant `Clinton96` votes at its mean value in the data 2) We can keep `Clinton96` at its observed values in the data. In linear regression, it's not going to matter which approach you take. In other models we will talk about later, this distinction may matter more substantially because of how our quantities of interest change across different values of $X\\hat \\beta$.\n\nThe first approach is easily implemented in predict.\n\n```{r}\npredict(fit.2, newdata = data.frame(Perot96 = 8072, Clinton96 = mean(florida$Clinton96)))\n```\n\nFor the second approach, what we will do is generate an estimate for Buchanan's votes in 2000 when Perot96 takes 8072 votes, and we keep Clinton96's votes at whatever value it currently is in the data. That is, we will generate $n$ estimates for Buchanan's votes when Perot takes 8072. Then, we will take the mean of this as our \"average estimate\" of Buchanan's votes in 2000 based on Perot's votes at a level of 8072. We can do this in one of two ways:\n\n```{r}\n## Manipulating the X matrix\nX <- model.matrix(fit.2)\n## Replace Perot96 column with all 8072 values\nX[, \"Perot96\"] <- 8072\nhead(X) #take a peek\n\n## Generate yhat\nyhats <-X %*% coef(fit.2)\n\n## take the mean\nmean(yhats)\n```\n\n```{r}\n## Use predict\nyhats <- predict(fit.2, newdata = data.frame(Perot96=8072, Clinton96=florida$Clinton96))\nmean(yhats)\n```\n\nNow often, after we generate these predicted values, we want to display them for the whole world to see. You will get a chance to visualize values like this using the plotting functions in the problem sets. We have already seen one example of this in the simple bivariate case, when R plotted the bivariate regression line in section 4.3.2. However, the predict function extends are capabilities to plot very specific values of $X$ and $\\hat Y$ for bivariate or multiple regressions.\n\n-   The `predict()` function is also very relevant when we move to logistic, probit, etc. regressions. This is just the start of a [beautiful friendship](https://www.youtube.com/watch?v=DDybg9CNXcM&ab_channel=mylifeinmovies) between you and `predict()` and associated functions.\n\n## Wrapping up OLS\n\nLinear regression is a great way to explain the relationship between one or more independent variables and an outcome variables. However, there is no free lunch. We have already mentioned a couple of assumptions along the way. Below we will summarize these and other assumptions. These are things you should be mindful of when you use linear regression in your own work. Some conditions that generate violations of these assumptions can also motivate why we will seek out alternative methods, such as those that rely on maximum likelihood estimation.\n\nThis is a good place to review Gelman section 3.6.\n\n-   Exogeneity. This one we haven't discussed yet, but is an important assumption for letting us interpret our coefficients $\\hat \\beta$ as \"unbiased\" estimates of the true parameters $\\beta$. We assume that the random error in the regression model $\\epsilon$ is indeed random, and uncorrelated with and independent of our independent variables $X$. Formally:\n    -   $\\mathbb{E}(\\epsilon| X) = \\mathbb{E}(\\epsilon) = 0$.\n    -   This can be violated, for example, when we suffer from Omitted Variable Bias due to having an \"endogenous explanatory variable\" that is correlated with some unobserved or unaccounted for factor. This bias comes from a situation where there is some variable that we have left out of the model ($Z$), and is therefore a part of the unobserved error term. Moreover this variable which is correlated with--and a pre-cursor of-- our independent variables and is a cause of our dependent variable. A failure to account for omitted variables can create bias in our coefficient estimates. Concerns about omitted variable bias often prompt people to raise their hands in seminars and ask questions like, \"Well have you accounted for this? Have you accounted for that? How do you know it is $X$ driving your results and not $Z$?\" If we omit important covariates, we may wrongly attribute an effect to $X$ when it was really the result of our omitted factor $Z$. Messing discusses this [here](https://medium.com/pew-research-center-decoded/how-to-break-regression-f48230f0ca68).\n    -   This is a really tough assumption. The only real way to guarantee the independence of your error term and the independent variables is if you have randomly assigned values to the independent variables (such as what you do when you randomly assign people to different treatment conditions in an experiment). Beyond random assignment, you have to rely on theory to understand what variables you need to account for in the regression model to be able to plausibly claim your estimate of the relationship between a given independent variable and the dependent variable is unbiased. Failing to control for important factors can lead to misleading results, such as what happens in Simpson's paradox, referenced in the Messing [piece](https://medium.com/pew-research-center-decoded/how-to-break-regression-f48230f0ca68).\n    -   Danger Note 1: The danger here, though, is that the motivation for avoiding omitted variable bias might be to keep adding control after control after control into the regression model. However, model building in this way can sometimes be atheoretical and result in arbitrary fluctuations in the size of your coefficients and their significance. At its worse, it can lead to \"p-hacking\" where researchers keep changing their models until they find the results they like. The Lenz and Sahn article on Canvas talks more about the dangers of arbitrarily adding controls to the model.\n    -   Danger Note 2: We also want to avoid adding \"bad controls\" to the model. Messing talks about this in the medium [article](https://medium.com/pew-research-center-decoded/how-to-break-regression-f48230f0ca68) as it relates to collider bias. We want to avoid adding controls to our model, say $W$ that are actually causes of $Y$ and causes of $X$ instead of the other way around.\n    -   Model building is a delicate enterprise that depends a lot on having a solid theory that guides the choice of variables.\n-   Homoscedasticity. We saw this when defining the variance estimator for the OLS coefficients. We assume constant error variance. This can be violated when we think observations at certain values of our independent variables may have different magnitudes of error than observations at other values of our independent variables.\n    -   No correlation in the errors. The error terms are not correlated with each other. This can be violated in time series models (where we might think past, present, and future errors are correlated) or in cases where our observations are nested in some hierarchical structures (e.g., perhaps students in a school) and the errors are correlated.\n-   No perfect collinearity. The $X$ matrix must be full rank: We cannot have linear dependence between columns in our X matrix. We saw this in the tutorial when we tried to add the dummy variables for all of our racial groups into a regression at once. When there is perfect collinearity between variables, our regression will fail.\n    -   We should also avoid situations where we have severe multicollinearity. This can happen when we include two or more variables in a regression model that are highly correlated (just not perfectly correlated). While the regression will still run in this case, it can inflate the standard errors of the coefficients, making it harder to detect significant effects. This is particularly problematic in smaller samples.\n-   Linearity. The relationship between the independent and dependent variables needs to be linear in the parameters. It should be modeled as the addition of constants or parameters multiplied by the independent variables. If instead the model requires the multiplication of parameters, this is no longer linear (e.g., $\\beta^2$). Linearity also often refers to the shape of the model. Our coefficients tell us how much change we expect in the outcome for each one-unit change in an independent variable. We might think some relationships are nonlinear-- meaning this rate of change varies across values of the independent variables. If that is the case, we need to shift the way our model is specified to account for this or change modeling approaches.\n    -   For example, perhaps as people get older (one-unit changes in age), they become more politically engaged, but at some age level, their political engagement starts to decline. This would mean the slope (that expected change in political engagement for each one-unit change in age) is not constant across all levels of age. There, we might be violating linearity in the curvature of the relationship between the independent and dependent variables. This is sometimes why you might see $age^2$ or other nonlinear terms in regression equations to better model this curvature.\n    -   Likewise, perhaps each additional level of education doesn't result in the same average increase in $y$. If not, you could consider including categorical dummy variables for different levels of education instead of treating education as a numeric variable.\n-   Normality. We assume that the errors are normally distributed. As Gelman 3.6 notes, this is a less important assumption and is generally not required.\n\n*OLS Properties*\n\nWhy we like OLS. When we meet our assumptions, OLS produces the best linear unbiased estimates (BLUE). A discussion of this [here](https://www.youtube.com/watch?v=NjTpHS5xLP8&ab_channel=BenLambert). We have linearity in our parameters (e.g., $\\beta$ and not $\\beta^2$ for example). The unbiasedness means that the expected value (aka the average over repeated samples) of our estimates $\\mathbb{E}(\\hat \\beta)= \\beta$ is the true value. Our estimates are also efficient, which has to do with the variance, not only are our estimates true in expectation, but we also have lower variance than an alternative linear unbiased estimator could get us. If our assumptions fail, then we might no longer have BLUE. OLS estimates are also consistent, meaning that as the sample gets larger and larger, the estimates start converging to the truth.\n\nNow, a final hidden assumption in all of this is that the sample of our data is representative of the population we are trying to make inferences about. If that is not the case, then we may no longer be making unbiased observations to that population level. Further adjustments may be required (e.g., analyses of survey data sometimes use weights to adjust estimates to be more representative).\n\nWhen we violate these assumptions, OLS may no longer be best, and we may opt for other approaches. More soon!\n\n### Practice Problems\n\n1.  Let's use the `florida` data. Run a regression according to the following formula:\n    -   $Buchanan00_i = \\alpha + \\beta_1*Perot96_i + \\beta_2*Dole96 + \\beta_3*Gore00 + \\epsilon$\n2.  Report the coefficient for `Perot96`. What do you conclude about the null hypothesis that there is no relationship between 1996 Perot votes and 2000 Buchanan votes?\n3.  What is the confidence interval for the `Perot96` coefficient estimate?\n4.  When Perot 1996 vote is 5500, what is the expected 2000 Buchanan vote?\n\n### Practice Problem Code for Solutions\n\n```{r}\nfit.practice <- lm(Buchanan00 ~ Perot96 + Dole96 + Gore00, data = florida)\n\ncoef(fit.practice)[\"Perot96\"]\n\nconfint(fit.practice)[\"Perot96\", ]\n\nexpbuch <- model.matrix(fit.practice)\nexpbuch[,\"Perot96\"] <- 5500\nmean(expbuch %*% as.matrix(coef(fit.practice)))\n```\n\n## Week 2 Example\n\nThis example is based on Dancygier, Rafaela; Egami, Naoki; Jamal, Amaney; Rischke, Ramona, 2020, \"Hate Crimes and Gender Imbalances: Fears over Mate Competition and Violence against Refugees\", published in the *American Journal of Political Science*. Replication data is available [here](https://doi.org/10.7910/DVN/QXJDJ5). We will draw on the survey portion of the article and replicate Table 1 in the paper. The pre-print is available [here](https://naokiegami.com/paper/hatecrime.pdf).\n\nThe abstract is: *As the number of refugees rises across the world, anti-refugee violence has become a pressing concern. What explains the incidence and support of such hate crime? We argue that fears among native men that refugees pose a threat in the competition for female partners is a critical but understudied factor driving hate crime. Employing a comprehensive dataset on the incidence of hate crime across Germany, we first demonstrate that hate crime rises where men face disadvantages in local mating markets. Next, we complement this ecological evidence with original survey measures and confirm that individual-level support for hate crime increases when men fear that the inflow of refugees makes it more difficult to find female partners. Mate competition concerns remain a robust predictor even when controlling for antirefugee views, perceived job competition, general frustration, and aggressiveness. We conclude that a more complete understanding of hate crime and immigrant conflict must incorporate marriage markets and mate competition.*\n\nThe authors summarize their hypotheses as, \"the notion that male refugees are engaged in romantic relationships with German women has received considerable media attention from a variety of sources, with coverage ranging from the curious to the outright hostile. We argue that the prospect of refugee-native mate competition can trigger or compound resentment against refugees, including support for hate crime\" [pg. 14](https://naokiegami.com/paper/hatecrime.pdf)\n\n```{r}\nlibrary(foreign)\ndat_use <- read.dta(\"https://github.com/ktmccabe/teachingdata/blob/main/dat_use.dta?raw=true\")\n```\n\nThe data include wave 4 of an online survey fielded in Germany through Respondi from September 2016 to December 2017). Each wave was designed to be nationally representative on age (starting at 18), gender, and state (Bundesland) with a sample of about 3,000 respondents in each wave.\n\nKey variables include\n\n-   `hate_violence_means` representing respondents' agreement or disagreement to the Only Means question: \"When it comes to the refugee problem, violence is sometimes the only means that citizens have to get the attention of German politicians.\" from (1) disagree strongly to (4) agree strongly.\n-   `MateComp_cont`, Mate Competition. \"The inflow of refugees makes it more difficult for native men to find female partners.\" from (1) disagree strongly to (4) agree strongly.\n-   The data include several other variables related to the demographics of the respondents and measures representing potential alternative explanations, such as `JobComp_cont` (agreement with \"the inflow of young male refugees makes it more difficult for young native men to find apprenticeships and jobs\") and `LifeSatis_cont` (0-10 scale, ranging from extremely dissatisfied to extremely satisfied).\n\nLet's pause here to ask a few questions about research design.\n\n-   What is the outcome? What is the independent variable of interest?\n    -   How would we write out the bivariate regression model?\n-   Why OLS? (e.g., why not experiment?)\n-   What types of alternative explanations might exist?\n\n**Ok let's move to replication of the first two regression models in the table:**\n\n![](images/dancyetaltable.png){width=\"70%\"}\n\n<details>\n\n<summary>Try to code these on your own, then click for the solution</summary>\n\n```{r}\nlm1 <- lm(hate_violence_means ~ MateComp_cont, data=dat_use)\n\nlm2 <- lm(hate_violence_means ~ MateComp_cont + JobComp_cont + LifeSatis_cont, data=dat_use)\n```\n\n</details>\n\n**Now, let's compare the summary output of each output.**\n\n<details>\n\n<summary>Try on your own, then click for the solution</summary>\n\n```{r}\nsummary(lm1)\nsummary(lm2)\n```\n\n</details>\n\n**Questions about the output**\n\n-   How should we interpret the coefficients?\n    -   Do they support the researchers' hypotheses?\n-   How would we extract confidence intervals from the coefficients?\n-   How should we interpret the goodness of fit statistics at the bottom of the output?\n\n**Additional Models** We can also run regressions with even more covariates, as the authors do in models 3-6 in the paper.\n\n<details>\n\n<summary>Click to reveal regression code below.</summary>\n\n```{r}\nlm3 <- lm(hate_violence_means ~ MateComp_cont + JobComp_cont + LifeSatis_cont + \n            factor(age_group) +     # age group\n            factor(gender) +     # gender \n            factor(state) +     # state  \n            factor(citizenship) +    # german citizen\n            factor(marital) +    # marital status\n            factor(religion) +    # religious affiliation\n            eduyrs +    # education\n            factor(occupation) +    # main activity\n            factor(income) +   # income\n            factor(household_size) +   # household size\n            factor(self_econ),    # subjective social status\n          data=dat_use)\n\nlm4 <- lm(hate_violence_means ~ MateComp_cont + JobComp_cont + LifeSatis_cont + \n            factor(age_group) +   # age group\n            factor(gender) +   # gender \n            factor(state) +   # state  \n            factor(citizenship) +  # german citizen\n            factor(marital) +  # marital status\n            factor(religion) +  # religious affiliation\n            eduyrs +  # education\n            factor(occupation) +  # main activity\n            factor(income) + # income\n            factor(household_size) + # household size\n            factor(self_econ) + # subjective social status\n            factor(ref_integrating) + # Refugee Index (National-level; Q73) 8 in total\n            factor(ref_citizenship) + factor(ref_reduce) + factor(ref_moredone) + factor(ref_cultgiveup) + \n            factor(ref_economy) + factor(ref_crime) + factor(ref_terror),\n          data=dat_use)\n\nlm5 <- lm(hate_violence_means ~ MateComp_cont + JobComp_cont + LifeSatis_cont + \n            factor(age_group) +      # age group\n            factor(gender) +      # gender \n            factor(state) +      # state  \n            factor(citizenship) +     # german citizen\n            factor(marital) +     # marital status\n            factor(religion) +     # religious affiliation\n            eduyrs + # education\n            factor(occupation) +     # main activity\n            factor(income) +    # income\n            factor(household_size) +    # household size\n            factor(self_econ) +    # subjective social status\n            factor(ref_integrating) + # Refugee Index (National-level; Q73) 8 in total\n            factor(ref_citizenship) + factor(ref_reduce) + factor(ref_moredone) + factor(ref_cultgiveup) + \n            factor(ref_economy) + factor(ref_crime) + factor(ref_terror) + \n            factor(ref_loc_services) +    # Refugee Index (Local, Q75)\n            factor(ref_loc_economy) + factor(ref_loc_crime) + factor(ref_loc_culture) + factor(ref_loc_islam) + \n            factor(ref_loc_schools) + factor(ref_loc_housing) + factor(ref_loc_wayoflife), ## end\n          data=dat_use)\n\nformula.5 <- \n  as.character(\"hate_violence_means ~ MateComp_cont + JobComp_cont + \n               LifeSatis_cont +  factor(age_group) + factor(gender) + \n               factor(state) + factor(citizenship) + factor(marital) + \n               factor(religion) + eduyrs + factor(occupation) + \n               factor(income) + factor(household_size) + factor(self_econ) + \n               factor(ref_integrating) + factor(ref_citizenship) + factor(ref_reduce) + \n               factor(ref_moredone) + factor(ref_cultgiveup) + \n               factor(ref_economy) + factor(ref_crime) + factor(ref_terror)  + \n               factor(ref_loc_services) +  factor(ref_loc_economy) + factor(ref_loc_crime) + \n               factor(ref_loc_culture) + factor(ref_loc_islam) + \n               factor(ref_loc_schools) + factor(ref_loc_housing) + factor(ref_loc_wayoflife)\")\n\nformula.6 <- paste(formula.5, \"factor(distance_ref) + factor(settle_ref)\", \n                   \"lrscale + afd + muslim_ind + afd_ind + contact_ind\", \n                   sep=\"+\", collapse=\"+\") \n\nlm6 <- lm(as.formula(formula.6), data=dat_use)\n```\n\n```{r, message=F, results=\"asis\", echo=F}\nlibrary(texreg)\nlm.list.table1 <- list(lm1, lm2, lm3, lm4, lm5, lm6)\n\nhtmlreg(lm.list.table1,\n        custom.model.names = c(\"\", \"\", \"\", \"\", \"\", \"\"),\n        stars = c(0.01, 0.05, 0.10),\n        digit=4,\n        caption = \"Table 1: Mate Competition Predicts Support for Hate Crime.\")\n```\n\n</details>\n\n**Final Questions**\n\nEven with all these covariates accounted for, the authors still engage in a discussion about possible violations of the OLS assumptions that could bias their results, as well as potential alternative modelling strategies.\n\n-   Is their survey representative? They replicate using another polling firm.\n-   Are there even more alternative explanations?\n-   Is OLS the right choice?\n-   Validity (discussed in Gelman and Hill). Does the outcome accurately measure the concept? They consider alternative outcomes and visualize the coefficient results in Figure 4.\n    -   Message: Attacks against refugee homes are sometimes necessary to make it clear to politicians that we have a refugee problem.\n    -   Justified : Hostility against refugees is sometimes justified, even when it ends up in violence.\n    -   Prevent : Xenophobic acts of violence are defensible if they result in fewer refugees settling in town.\n    -   Condemn: Politicians should condemn attacks against refugees more forcefully.\n\n![](images/dancyetal.png){width=\"70%\"}\n\n**Additional Practice Questions.**\n\n1.  Find the average expected level of \"Only Means\" agreement at each level of mate competition. Plot the results. Base these results on `lm2`.\n2.  Fit `lm2` using the generalized linear model `glm` approach (with a normal distribution) instead of the `lm`\n3.  What are some of the conceptual differences between ordinary least squares and maximum likelihood estimation?\n","srcMarkdownNoYaml":"\n<script src=\"https://hypothes.is/embed.js\" async></script>\n\n```{=tex}\n\\newcommand\\bE{\\mathbf{E}}\n\\newcommand\\E{\\mathbb{E}}\n\\newcommand\\V{\\mathbb{V}}\n```\nThis section will provide a review of OLS.\n\nOLS is the workhorse of empirical political science. We will learn a lot beyond OLS, but OLS is often \"good enough\" and sometimes preferable for explaining the relationship between variables. That is to say, MLE will expand your toolkit, but OLS should remain a big part of your toolkit.\n\nI recommend that you review the following readings to familiarize yourself with regression. I will make note within this section where particular readings are most relevant. These readings are available on Canvas in the modules- Week 1 section.\n\n-   Wheelan, Charles. 2012. *Naked Statistics*. W.W. Norton. Chapter 11. This provides an accessible overview of regression and the interpretation of regression results.\n\n-   Gelman, Andrew, and Jennifer Hill. 2006. Data analysis using regression and multilevel/hierarchical models. Cambridge University Press. Chapter 3. This is a slightly more technical overview and includes some R code for running regressions.\n\n-   Building models and breaking models.\n\n    -   (Optional) Fox, John. 2015. Applied Regression Analysis and Generalized Linear Models, 2nd Edition. Sage. Chapter 11. This reading describes diagnostic tests to probe whether the model is a good fit of the data. We won't go into detail about this in this class, but is material classes focused on linear regression will generally cover.\n    -   Messing, Solomon. [\"How to break regression.\"](https://medium.com/pew-research-center-decoded/how-to-break-regression-f48230f0ca68)\n    -   Lenz, G., & Sahn, A. (2020). \"Achieving Statistical Significance with Control Variables and Without Transparency.\" *Political Analysis*, 1-14. doi:10.1017/pan.2020.31. This paper talks about how to build a regression model, and in particular, why adding more and more controls isn't always a good thing.\n\n## Introducing OLS Regression\n\nThe regression method describes how one variable depends on one or more other variables. Ordinary Least Squares regression is a linear model with the matrix representation:\n\n$Y = \\alpha + X\\beta + \\epsilon$\n\nGiven values of variables in $X$, the model predicts the average of an outcome variable $Y$. For example, if $Y$ is a measure of how wealthy a country is, $X$ may contain measures related to the country's natural resources and/or features of its institutions (things that we think might contribute to how wealthy a country is.) In this equation:\n\n-   $Y$ is the outcome variable ($n \\times 1$).[^04-reviewofols-1]\n-   $\\alpha$ is a parameter representing the intercept\n-   $\\beta$ is a parameter representing the slope/marginal effect ($k \\times 1$), and\n-   $\\epsilon$ is the error term ($n \\times 1$).\n\n[^04-reviewofols-1]: Recall this notation means rows by columns, $Y$ is a vector of length $n$ (the number of observations), and since there is only 1 outcome measure, it is 1 column.\n\nIn OLS, we estimate a line of best fit to predict $\\hat{Y}$ values for different values of X:\n\n-   $\\hat{Y} = \\hat{\\alpha} + X\\hat{\\beta}$.\n-   When you see a \"$\\hat{hat}$\" on top of a letter, that means it is an estimate of a parameter.\n-   As we will see in the next section, in multiple regression, sometimes this equation is represented as just $\\hat{Y} = X\\hat{\\beta}$, where this generally means that $X$ is a matrix that includes several variables and $\\hat \\beta$ is a vector that includes several coefficients, including a coefficient representing the intercept $\\hat \\alpha$\n\nWe interpret linear regression coefficients as describing how a dependent variable is expected to change when a particular independent variable changes by a certain amount. Specifically:\n\n-   \"Associated with each one unit increase in a variable $x_1$, there is a $\\hat{\\beta_1}$ estimated expected average increase in $y$.\"\n-   If we have more than one explanatory variable (i.e., a multiple regression), we add the phrase \"controlling on/ holding constant other observed factors included in the model.\"\n\nWe can think of the interpretation of a coefficient in multiple regression using an analogy to a set of light switches:\n\n![](images/lightswitch.jpeg){width=\"40%\"}\n\nWe ask: How much does the light in the room change when we flip one switch, while holding constant the position of all the other switches?\n\nThis would be a good place to review the Wheelan chapter and Gelman and Hill 3.1 and 3.2 to reinforce what a regression is and how to interpret regression results.\n\n## Diving Deeper into OLS Matrix Representation\n\nIn this section, we will review the matrix representation of the OLS regression in more detail and discuss how to derive the estimators for the regression coefficients.[^04-reviewofols-2]\n\n[^04-reviewofols-2]: This [video](https://www.youtube.com/watch?v=GMVh02WGhoc&ab_channel=BenLambert) from Ben Lambert provides additional intuition for understanding OLS in a matrix form and how it can be useful.\n\nOLS in Matrix Form: Let $X$ be an $n \\times k$ matrix where we have observations on k independent variables for n observations. Since our model will usually contain a constant term, one of the columns in the X matrix will contain only ones. This column should be treated exactly the same as any other column in the X matrix.\n\n-   Let $Y$ be an $n \\times 1$ vector of observations on the dependent variable. Note: because $Y$ is a vector (a matrix with just one column), sometimes it is written in lowercase notation as $\\mathbf y$.\n-   Let $\\epsilon$ be an $n \\times 1$ vector of disturbances or errors.\n-   Let $\\beta$ be an $k \\times 1$ vector of unknown population parameters that we want to estimate.\n\n$\\begin{pmatrix} y_1 \\\\ y_2 \\\\ y_3 \\\\ y_4 \\\\ ... \\\\ y_n \\end{pmatrix}$ = $\\begin{pmatrix} 1 & x_{11} & x_{12} & x_{13} & ... & x_{1k}\\\\ 1 & x_{21} & x_{22} & x_{23} & ... & x_{2k} \\\\ 1 & x_{31} & x_{32} & x_{33} & ... & x_{3k}\\\\ 1 & x_{41} & x_{42} & x_{43} & ... & x_{4k} \\\\ ... & ... & ... & ... & ... & ... \\\\ 1 & x_{n1} & x_{n2} & x_{n3} & ... & x_{nk}\\end{pmatrix}$ X $\\begin{pmatrix} \\alpha \\\\ \\beta_1 \\\\ \\beta_2 \\\\ \\beta_3 \\\\ ... \\\\ \\beta_k \\end{pmatrix}$ + $\\begin{pmatrix} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\epsilon_3 \\\\ \\epsilon_4 \\\\ ... \\\\ \\epsilon_n \\end{pmatrix}$\n\nOur estimates are then $\\mathbf{ \\hat y} = X\\hat \\beta$. What are the dimensions of this quantity?\n\nGelman and Hill Section 3.4, pg. 38 provides a nice visual of how this representation maps onto what a typical dataset may look like, where we will try to estimate a set of coefficients that map the relationship between the columns of $X$ and $\\mathbf y$:\n\n![](images/gh34.png){width=\"70%\"}\\\\\n\nThis is a good place to review Gelman and Hill 3.4 on different notations for representing the regression model.\n\n### Estimating the Coefficients\n\nModels generally start with some goal. In OLS, our goal is to minimize the sum of squared \"residuals.\" Here is a video I created to explain why we can represent this as $\\mathbf{e'}\\mathbf{e}$.\n\n{{< video src=\"https://www.youtube.com/watch?v=IFX_n16YTm0\" >}}\n\n*Note: at the end of the video it should read* $X\\hat\\beta$, not $\\hat X \\beta$\n\nWhat is a residual? It's the difference between y and our estimate of y: $y - \\hat y$. It represents the error in our prediction-- how far off our estimate is of the outcome.\n\nWe can write this in matrix notation in the following way where $\\mathbf e$ is an $n \\times 1$ vector of residuals-- a residual for each observation in the data:\n\n```{=tex}\n\\begin{align*}\n\\mathbf{e'}\\mathbf{e} &= (Y' - \\hat{\\beta}'X')(Y - X\\hat{\\beta})\\\\\n&=Y'Y - \\hat{\\beta}'X'Y - Y'X\\hat{\\beta} + \\hat{\\beta}'X'X\\hat{\\beta} \\\\\n&= Y'Y - 2\\hat{\\beta}'X'Y + \\hat{\\beta}'X'X\\hat{\\beta}\n\\end{align*}\n```\nRecall we want a line that minimizes this quantity. We minimize the sum of squared residuals by taking the derivative with respect to $\\beta$. (We want to identify the coefficients that help us achieve the goal of minimizing the squared error.) Because we are now deriving an estimate, we will use the hat over $\\beta$:\n\n-   $\\frac{\\delta }{\\delta \\hat \\beta} = -2X'Y + 2X'X\\hat{\\beta}$\n-   So what is our estimate for $\\hat{\\beta}$? We take first order conditions\n\n```{=tex}\n\\begin{align*}\n  0 &=-2X'Y + 2X'X\\hat{\\beta}\\\\\n   \\hat{\\beta} &= (X'X)^{-1}X'Y\n   \\end{align*}\n```\nYou may wonder how we got to these answers. Don't worry, you will get your chance to solve this! The important thing to note for now, is that we have an analytic solution to our coefficient estimates.\n\n## OLS Regression in R\n\nTo run a linear regression in R, we use the `lm()` function.\n\nThe syntax is `lm(y ~ x1, data = mydata)` for a regression with `y` as the name of your dependent variable and there is one explanatory variable `x1` where `mydata` is the name of your data frame.\n\n`lm(y ~ x1 + x2 , data = mydata)` is the syntax for a regression with two explanatory variables `x1` and `x2`, where you would add additional variables for larger multivariate regressions. By default, R will include an intercept term in the regression.\n\n### Example: Predicting Current Election Votes from Past Election Votes\n\nIn the American presidential election in 2000, there was an actual controversy in how ballots were cast in the state of Florida. Social scientists used data comparing the election results from 1996 in the state with 2000 as one way to help detect irregularities in the 2000 vote count. For more information on the background of this example, you can watch this [video](https://www.youtube.com/watch?v=D-nR_hmS6V0).\n\nWe will use the data `florida.csv` available [here](https://github.com/ktmccabe/teachingdata/blob/main/florida.csv):\n\n```{r}\n## Load Data\nflorida <- read.csv(\"https://raw.githubusercontent.com/ktmccabe/teachingdata/main/florida.csv\")\n```\n\nThis data set includes several variables described below, where each row represents the voting information for a particular county in Florida.\n\n| Name         |       Description        |\n|:-------------|:------------------------:|\n| `county`     |       county name        |\n| `Clinton96`  | Clinton's votes in 1996  |\n| `Dole96`     |   Dole's votes in 1996   |\n| `Perot96`    |  Perot's votes in 1996   |\n| `Bush00`     |   Bush's votes in 2000   |\n| `Gore00`     |   Gore's votes in 2000   |\n| `Buchanan00` | Buchanan's votes in 2000 |\n\nIn 2000, Buchanan was a third party candidate, similar to Perot in 1996. One might think that counties where Perot received a lot of votes in 1996 should also receive a lot in 2000. That is: with a one-vote increase in Perot's vote, we might expect an average increase in Buchanan's 2000 vote.\n\nWe can translate that language into a regression equation:\n\n-   $Buchanan2000 = \\alpha + Perot1996 * \\beta + \\epsilon$\n\nIn R, we run this regression the following way. We will save it as an object `fit.1`. You can name your regression objects anything you want.\n\n```{r}\nfit.1 <- lm(Buchanan00 ~ Perot96, data = florida)\n```\n\n-   `summary(model)` provides the summary statistics of the model. In particular, the following statistics are important\n\n    -   `Estimate`: point estimate of each coefficient\n    -   `Std. Error`: standard error of each estimate\n    -   `t value`: indicates the $t$-statistic of each coefficient under the null hypothesis that it equals zero\n    -   `Pr(>|t|)`: indicates the two-sided $p$-value corresponding to this $t$-statistic where asterisks indicate the level of statistical significance.\n    -   `Multiple R-squared`: The coefficient of determination\n    -   `Adjusted R-squared`: The coefficient of determination adjusting for the degrees of freedom\n\nWe will say more to define these quantities in future sections.\n\n```{r}\nsummary(fit.1)\n```\n\nR also allows several shortcuts for accessing particular elements of your regression results. Examples:\n\n```{r}\n## Vector of the coefficient estimates only\ncoef(fit.1)\n\n## Compute confidence intervals for these coefficients\nconfint(fit.1)\n\n## Table of coefficient results only\nsummary(fit.1)$coefficients\n\n## Extract standard errors only\nsummary(fit.1)$coefficients[,2]\n\n## Variance-Covariance matrix\nvcov(fit.1)\n\n## Note that the square root of the diagonal of this matrix provides the standard errors\nsqrt(diag(vcov(fit.1)))\n\n## Degrees of freedom\nfit.1$df.residual\n```\n\n### Plotting Regression Results\n\nWe often don't want to hide our data under a bushel basket or in complicated regression models. Instead, we might also want to visualize data in R. The function `plot()` and the function `ggplot()` from the package `ggplot2` are two terrific and flexible functions for visualizing data. We will use the `plot()` function to visualize the relationship between Perot and Buchanan votes. The example below provides a few arguments you can use within each of these functions, but they are capable of much more.\n\nAt the core, plotting functions generally work as coordinate systems. You tell R specifically at which x and y coordinates you want your points to be located (e.g., by providing R with a vector of x values and a vector of y values). Then, each function has its own way of allowing you to add bells and whistles to your figure, such as labels (e.g., `main, xlab, ylab`), point styles (`pch`), additional lines and points and text (e.g., `abline(), lines(), points(), text()`), or x and y scales for the dimensions of your axes (e.g., `xlim, ylim`). You can create a plot without these additional features, but most of the time, you will add them to make your plots look good! and be informative! We will do a lot of plotting this semester.\n\nNote: feel free to use `plot()` or `ggplot()` or both. `ggplot` has similar capabilities as `plot` but relies on a different \"grammar\" of graphics. For example, see the subtle differences in the two plots below.\n\n```{r, message=FALSE, fig.width=7}\n\n## Plot\nplot(x = florida$Perot96, # x-values\n     y = florida$Buchanan00, # y-values\n     main = \"Perot and Buchanan Votes\", # label for main title\n     ylab = \"Buchanan Votes\", # y-axis label\n     xlab = \"Perot Votes\", # x-axis label\n     pch = 20) # point type\nabline(fit.1, col = \"red\") # adds a red regression line\n```\n\n```{r, warning=FALSE}\n## ggplot version\nlibrary(ggplot2)\nggplot(data = florida, # which data frame\n       mapping = aes(x = Perot96, y = Buchanan00)) + # x and y coordinates\n  geom_point() +  # tells R we want a scatterplot\n  geom_smooth(method = \"lm\",\n              se = FALSE, colour = \"red\",\n              data = florida, aes(x=Perot96, y=Buchanan00)) + # adds lm regression line\n  ggtitle(\"Perot and Buchanan Votes\") + # main title\n  labs(x = \"Perot Votes\", y = \"Buchanan Votes\") + # x and y labels\n  theme_bw() # changes theme (e.g., color of background)\n\n## Note: data = florida, aes(x=Perot96, y=Buchanan00) in the geom_smooth line is not necessary if it is the same mapping at the first line. Required if data are different\n```\n\nTip: you might want to save your plots as .pdf or .png after you create it. You can do this straight from your R code. How you do it varies by function. The files will save to your working directory unless you specify a different file path. The code below is the same as above except it has additional lines for saving the plots:\n\n```{r, eval=FALSE}\n## Plot\npdf(file = \"myfirstmleplot.pdf\", width = 7, height = 5) # play around with the dimensions\nplot(x = florida$Perot96, # x-values\n     y = florida$Buchanan00, # y-values\n     main = \"Perot and Buchanan Votes\", # label for main title\n     ylab = \"Buchanan Votes\", # y-axis label\n     xlab = \"Perot Votes\", # x-axis label\n     pch = 20) # point type\nabline(fit.1, col = \"red\") # adds a red regression line\ndev.off() # this closes your pdf file\n\n## ggplot version\nggplot(data = florida, # which data frame\n       mapping = aes(x = Perot96, y = Buchanan00)) + # x and y coordinates\n  geom_point() +  # tells R we want a scatterplot\n  geom_smooth(method = \"lm\",  \n              se = FALSE, colour = \"red\",\n              data = florida, aes(x=Perot96, y=Buchanan00)) + # adds lm regression line\n  ggtitle(\"Perot and Buchanan Votes\") + # main title\n  labs(x = \"Perot Votes\", y = \"Buchanan Votes\") + # x and y labels\n  theme(plot.title = element_text(hjust = 0.5)) +# centers the title\n  theme_bw() # changes theme (e.g., color of background)\nggsave(\"myfirstmleggplot.png\", device=\"png\", width = 7, height = 5) # saves the last ggplot\n```\n\n### Finding Coefficients without `lm`\n\nLet's put our matrix algebra and R knowledge together. In the previous section, we found that $\\hat \\beta = (X'X)^{-1}X'Y$. If we do that math directly in R, there is no need to use `lm()` to find those coefficients.\n\nTo do so, we need $X$ and $Y$.\n\nRecall $Y$ is an $n \\times 1$ vector representing the outcome of our model. In this case, $Y$ is `Buchanan00`.\n\n```{r}\nY <- florida$Buchanan00\n```\n\nRecall, $X$ is a $n \\times k$ matrix representing our independent variables and a column of 1's for the intercept. Let's build this matrix using `cbind` which was introduced in section 2.\n\n```{r}\nX <- cbind(1, florida$Perot96)\ndim(X)\n```\n\nGreat, now we have $X$ and $Y$, so it's just about a little math. Because $Y$ is a vector, let's make sure R knows to treat it like an $n \\times 1$ matrix.\n\n```{r}\nY <- cbind(Y)\ndim(Y)\n```\n\nRecall the `solve()` and `t()` functions take the inverse and transpose of matrices.\n\n```{r}\nbetahat <- solve(t(X) %*% X) %*% t(X) %*% Y\n```\n\nFinally, let's compare the results from our model using `lm()` with these results.\n\n```{r, results='hold'}\nbetahat\ncoef(fit.1)\n```\n\nWe did it! In the problem set, you will get more experience using the analytic solutions to solve for quantities of interest instead of the built-in functions.\n\n### OLS Practice Problems\n\nHere are a couple of (ungraded) problems to modify the code above and gain additional practice with data wrangling and visualization in R. As you might have noticed in the example, there is a big outlier in the data. We will see how this observation affects the results.\n\n1.  Using a linear regression examine the relationship between Perot and Buchanan votes, controlling for Bill Clinton's 1996 votes.\n\n-   Provide a one sentence summary of the relationship between Perot and Buchanan's votes.\n-   Is the relationship significant at the $p < 0.05$ level? What about the relationship between Clinton and Buchanan votes?\n-   What are the confidence intervals for the Perot coefficient results?\n-   What is the residual for the estimate for Palm Beach County-- `PalmBeach` in the `county` variable?\n\n2.  Let's go back to the bivariate case.\n\n-   Subset the data to remove the county `PalmBeach`.\n-   Create a scatterplot of the relationship between Perot votes and Buchanan votes within this subset. This time make the points blue.\n-   Add a regression line based on this subset of data.\n-   Add a second regression line in a different color based on the initial bivariate regression we ran in the example, where all data were included.\n-   Describe the differences in the regression lines.\n\n### Code for solutions\n\n```{r, eval=TRUE, include=TRUE}\nfit.multiple <- lm(Buchanan00 ~ Perot96 + Clinton96, data = florida)\nsummary(fit.multiple)\nconfint(fit.multiple)[2,]\n\nflorida$res <- residuals(fit.multiple)\nflorida$res[florida$county == \"PalmBeach\"]\n\nflorida.pb <- subset(florida, subset = (county != \"PalmBeach\"))\nfit2 <- lm(Buchanan00 ~ Perot96, data = florida.pb)\n\nggplot(data = florida.pb, # which data frame\n       mapping = aes(x = Perot96, y = Buchanan00)) + # x and y coordinates\n  geom_point(color=\"blue\") +  # tells R we want a scatterplot\n  geom_smooth(method = \"lm\", \n              se = FALSE, colour = \"green\",\n              data = florida.pb, aes(x=Perot96, y=Buchanan00)) + # adds lm regression line\n    geom_smooth(method = \"lm\", \n              se = FALSE, colour = \"red\",\n              data = florida, aes(x=Perot96, y=Buchanan00)) + # adds lm regression line\n  ggtitle(\"Perot and Buchanan Votes\") + # main title\n  labs(x = \"Perot Votes\", y = \"Buchanan Votes\") + # x and y labels\n  theme(plot.title = element_text(hjust = 0.5)) +# centers the title\n  theme_bw() # changes theme (e.g., color of background)\n\n```\n\n## Extra practice building and breaking regression\n\nBelow is additional practice with regression and R, showing how to work with different variables and diagnosing errors.\n\n***What is the association between race and income?***\n\nLet's say we want to explore the relationship between race and income, where the people in our sample take on the values white, Black, Asian, and Hispanic for race. We can write this as:\n\n$Income_i = \\alpha + \\beta*race_i + \\epsilon$\n\nHowever, race is not a numeric variable. This complicates our regression equation because what exactly is a 1-unit change in race? Sure, we could assign numeric values to each racial category in our data (e.g., white = 1, Black = 2, Hispanic = 3, Asian = 4), but we would have no reason to assume that the change in income would be linear as you change in race by units. Why should the difference in income between white and Black individuals be estimated as the same difference between Black and Hispanic individuals?\n\nIn a linear regression, when you have categorical independent variables, what should you typically do?\n\n### Categorical variables\n\nLet's build some data\n\n*Build a matrix with dummy variables for each race*\n\nRun the code below and see what is in X.\n\n```{r }\n## Dummy variable example\nresprace <- c(\"white\", \"white\", \"asian\", \"black\", \"hispanic\",\n              \"hispanic\", \"hispanic\", \"asian\", \"white\", \"black\", \n              \"black\", \"black\", \"asian\", \"asian\", \"white\", \"white\")\n\n## \"Dummy variables\"\nwhite <- rep(0, length(resprace))\nwhite[resprace == \"white\"] <- 1\nasian <- rep(0, length(resprace))\nasian[resprace == \"asian\"] <- 1\nblack <- rep(0, length(resprace))\nblack[resprace == \"black\"] <- 1\nhispanic <- rep(0, length(resprace))\nhispanic[resprace == \"hispanic\"] <- 1\n\n## Matrix\nX <- cbind(white, asian, black, hispanic)\nX\n```\n\n*Let's build toward a regression model*\n\nLet's create a `Y` variable representing our outcome for income. Let's also add an intercept to our `X` matrix. Take a look into our new X.\n\n```{r}\n## Dependent variable\nY <- cbind(c(10, 11, 9, 8, 9, 7, 7, 13, 12, 11, 8, 7, 4, 13, 8, 7))\n\nX <- cbind(1, X)\n```\n\n*Let's now apply the formula* $(X'X)^{-1}X'Y$ to estimate our coefficients.\n\nAssign the output to an object called `betahat`\n\n```{r, eval=FALSE}\nbetahat <- solve(t(X) %*% X) %*% t(X) %*% Y\nbetahat\n```\n\n### Formalizing Linear Dependence\n\nWhy were our dummy variables linear dependent?\n\nIf we inspect `X` we can see that taking each column $\\mathbf{x_1} - \\mathbf{x_2} - \\mathbf{x_3}-\\mathbf{x_4}=0$. There is a linear relationship between the variables.\n\nFormally, a set of vectors (e.g.,$\\mathbf{x_1}, \\mathbf{x_2}, ...\\mathbf{x_k}$) is linearly independent if the equation $\\mathbf{x_1}*a_1 + \\mathbf{x_2}*a_2 +... + \\mathbf{x_k}*a_k= 0$ only in the trivial case where $a_1$ and $a_2$ through $a_k$ are 0. A set of vectors has a linearly dependent relation if there is a solution $\\mathbf{x_1}*a_1 + \\mathbf{x_2}*a_2 +... + \\mathbf{x_k}*a_k = 0$ where not all $a_1, a_2$ through $a_k$ are 0.\n\nFor OLS, we must assume no perfect collinearity.\n\n-   No independent variable is constant\n-   No exactly linear relationships among the independent variables\n-   The rank of X is $k$ where the rank of a matrix is the maximum number of linearly independent columns.\n\nAs discussed in the course notes, a square matrix is only invertible if its columns are linearly independent. In OLS, in order to estimate unique solutions for $\\hat \\beta$, we need to invert $(X'X)^{-1}$. When we have perfect collinearity, we cannot do this.\n\n*Note the linear dependence in X*\n\n```{r}\n## Matrix\nX[, 1] - X[, 2] - X[, 3] - X[, 4] - X[,5]\n```\n\n*Try to take* $(X'X)^{-1}$\n\n```{r, eval=F}\nsolve(t(X) %*% X)\n```\n\nHow do we correct this?\n\nTo address this, we are going to drop one of the categorical variables when we run the regression. Consequently, our coefficients will now be interpreted as differences between this reference category (the category left out, e.g., white) and the particular group (e.g., white vs. Asian or white vs. Black or white vs. Hispanic).\n\n*Redefine `X` removing the white column, and calculate* $(X'X)^{-1}X'Y$\n\n```{r}\nX <- cbind(1, asian, black, hispanic)\nbetahat <- solve(t(X) %*% X) %*% t(X) %*% Y\nbetahat \n```\n\n*Check this with the `lm()` function*\n\n```{r}\nsummary(lm(Y ~ asian + black + hispanic))\n```\n\nIn R and many other statistical softwares, the regression function will forcibly drop one of your variables if it encounters this type of linear dependence. See below when we include all four race dummies in the model.\n\n*Check what happens with the `lm()` function*\n\n```{r }\nsummary(lm(Y ~ white + asian + black + hispanic))\n```\n\nAn alternative way to enter categorical variables in a regression is to let the function create the dummy variables for you using `factor(var, levels = )` to make sure R knows it is a factor variables.\n\n```{r}\nresprace <- factor(resprace, levels = c(\"white\", \"asian\", \"black\", \"hispanic\"))\nsummary(lm(Y ~ resprace))\n```\n\n### Other examples of breaking the no perfect collinearity rule\n\nRecall, for OLS, we must assume no perfect collinearity.\n\n-   No independent variable is constant\n-   No exactly linear relationships among the independent variables\n-   The rank of X is $k$ where the rank of a matrix is the maximum number of linearly independent columns.\n\nLet's say we wanted to control for age, but all of our sample was 18 years old. Let's try to add this to the `X` matrix.\n\n*Redefine `X` adding age column, and calculate* $(X'X)^{-1}X'Y$\n\n```{r, eval=FALSE}\nage <- rep(18, length(resprace))\nX <- cbind(1, asian, black, hispanic, age)\nbetahat <- solve(t(X) %*% X) %*% t(X) %*% Y\nbetahat \n\n```\n\nLet's visit the Florida example. Let's say we had two `Perot96` variables-- one the raw votes and one where votes were multiplied by 1000 to adjust the order of magnitude.\n\n*Regress Buchanan's votes on Perot and Perot adjusted values*\n\n```{r, eval=FALSE}\nflorida <- read.csv(\"https://raw.githubusercontent.com/ktmccabe/teachingdata/main/florida.csv\")\n\nflorida$perotadjusted <- florida$Perot96 * 1000\n\nY <- florida$Buchanan00\nX <- cbind(1, florida$Perot96, florida$perotadjusted)\nbetahat <- solve(t(X) %*% X) %*% t(X) %*% Y\nbetahat \n```\n\n## Uncertainty and Regression\n\nWe have now gone through the process of minimizing the sum of squared errors ($\\mathbf{e'e}$) and deriving estimates for the OLS coefficients $\\hat \\beta = (X'X)^{-1}X'Y$. In this section, we will discuss how to generate estimates of the uncertainty around these estimates.\n\nWhere we are going:\n\n-   In the last section, we visited an example related to the 2000 election in Florida. We regressed county returns for Buchanan in 2000 (Y) on county returns for Perot in 1996 (X).\n\n```{r}\n## Load Data\nflorida <- read.csv(\"https://raw.githubusercontent.com/ktmccabe/teachingdata/main/florida.csv\")\nfit.1 <- lm(Buchanan00 ~ Perot96, data = florida)\nsummary(fit.1)\n```\n\nThe summary output of the model shows many different quantities in addition to the coefficient estimates. In particular, in the second column of the summary, we see the standard errors of the coefficients. Like many statistical software programs, the `lm()` function neatly places these right next to the coefficients. We will now discuss how we get to these values.\n\n### Variance of the Coefficients\n\nThe standard error is the square root of the variance, representing the typical deviation we would expect to see between our estimates $\\hat \\beta$ of the parameter $\\beta$ across repeated samples. So to get to the standard error, we just need to get to an estimate of the variance.\n\nLet's take the journey. First the math. As should start becoming familiar, we have our initial regression equation, which describes the relationship between the independent variables and dependent variables.\n\n-   Start with the model: $Y = X\\beta + \\epsilon$\n    -   We want to generate uncertainty for our estimate of $\\hat \\beta =(X'X)^{-1}X'Y$\n-   Note: Conditional on fixed values of $X$ (I say fixed values because this is our data. We know $X$ from our dataset.), the only random component is $\\epsilon$.\n    -   What does that mean? Essentially, the random error term in our regression equation is what is giving us the uncertainty. If $Y$ was a deterministic result of $X$, we would have no need for it, but it's not. The relationship is not exact, varies sample to sample, subject to random perturbations, represented by $\\epsilon$.\n\nBelow we go through how to arrive at the mathematical quantity representing the variance of $\\hat \\beta$ which we will notate as $\\mathbf{V}(\\hat\\beta)$. The first part of the math below is just substituting terms:\n\n```{=tex}\n\\begin{align*}\n\\mathbf{V}(\\widehat{\\beta}) &= \n\\mathbf{V}( (X^T X) ^{-1} X^T Y))  \\\\\n&= \\underbrace{\\mathbf{V}( (X^T X) ^{-1} X^T (X\\beta + \\epsilon))}_\\text{Sub in the expression for Y from above}  \\\\\n&= \\underbrace{\\mathbf{V}((X^T X) ^{-1} X^T X \\beta + (X^T X) ^{-1} X^T \\epsilon)}_\\text{Distribute the term to the items in the parentheses}  \\\\\n&= \\underbrace{\\mathbf{V}(\\beta + (X^T X) ^{-1} X^T \\epsilon)}_\\text{Using the rules of inverses, the two terms next to $\\beta$ canceled each other out}  \n\\end{align*}\n```\nThe next part of the math requires us to use knowledge of the definition of variance and the rules associated. We draw on two in particular:\n\n-   The variance of a constant is zero.\n-   When you have a constant multipled by a random variable, e.g., $\\mathbf{V}(4d)$, it can come out of the variance operator, but must be squared: $16\\mathbf{V}(d)$\n-   Putting these together: $\\mathbf{V}(2 + 4d)= 16\\mathbf{V}(d)$\n\nKnowing these rules, we can proceed: \\begin{align*}\n\\mathbf{V}(\\widehat{\\beta}) &= \\mathbf{V}(\\beta + (X^T X) ^{-1} X^T \\epsilon) \\\\\n&=\\underbrace{ \\mathbf{V}((X^T X) ^{-1} X^T \\epsilon)}_\\text{$\\beta$ drops out because in a regression it is an unkown \"parameter\"-- it's constant, which means its variance is zero.}\\\\\n&= \\underbrace{(X^T X)^{-1}X^T \\mathbf{V}( \\epsilon) ((X^T X)^{-1}X^T)^T}_\\text{We can move $(X^T X)^{-1}X^T$ out front because our data are fixed quantities, but in doing so, we have to \"square\" the matrix.}\\\\\n&= (X^T X)^{-1}X^T \\mathbf{V}( \\epsilon) X (X^T X)^{-1}\n\\end{align*}\n\nThe resulting quantity is our expression for the $\\mathbf{V}(\\hat \\beta)$. However, in OLS, we make an additional assumption that allows us to further simplify the expression. We assume homoscedasticity aka \"constant\" or \"equal error variance\" which says that the variance of the errors are the same across observations: $\\mathbf{V}(\\epsilon) = \\sigma^2 I_n$.\n\n-   If we assume homoscedastic errors, then Var$(\\epsilon) = \\sigma^2 I_n$\n\n```{=tex}\n\\begin{align*}\n\\mathbf{V}(\\widehat{\\beta})  &= (X^T X)^{-1}X^T \\mathbf{V}( \\epsilon) X (X^T X)^{-1}\\\\\n&= \\underbrace{(X^T X)^{-1}X^T \\sigma^2I_n X (X^T X)^{-1}}_\\text{Assume homoskedasticity}\\\\\n&= \\underbrace{\\sigma^2(X^T X)^{-1} X^T X (X^T X)^{-1}}_\\text{Because it is a constant, we can move it out in front of the matrix multiplication, and then simplify the terms.}  \\\\\n&= \\sigma^2(X^T X)^{-1} \n\\end{align*}\n```\nAll done! This expression: $\\sigma^2(X^T X)^{-1}$ represents the variance of our coefficient estimates. Note its dimensions: $k \\times k$. It has the same number of rows and columns as the number of our independent variables (plus the intercept).\n\nThere is one catch, though. How do we know what $\\sigma^2$ is? Well, we don't. Just like the unknown parameter $\\beta$, we have to estimate it in our regression model.\n\nJust like with the coefficients, we notate our estimate as $\\widehat{\\sigma}^2$. Our estimate is based on the observed residual errors in the model and is as follows:\n\n-   $\\widehat{\\sigma}^2 = \\frac{1}{N-K}\\sum_{i=1}^N \\widehat{\\epsilon_i^2} = \\frac{1}{N-K} \\mathbf{e'e}$\n\nThat means our **estimate** of the variance of the coefficients is found within: $\\hat \\sigma^2(X^T X)^{-1}$\n\nAgain, this is a $k \\times k$ matrix and is often called the variance covariance matrix. We can extract this quantity from our linear models in R using `vcov()`.\n\n```{r}\nvcov(fit.1)\n```\n\nThis is the same that we would get if manually we took the residuals and multiplied it by our $X$ matrix according to the formula above:\n\n```{r}\nX <- cbind(1, florida$Perot96)\ne <- cbind(residuals(fit.1))\nsigmahat <- ((t(e) %*% e) / (nrow(florida) -2)) \n## tell r to stop treating sigmahat as a matrix\nsigmahat <-as.numeric(sigmahat)\nXtX <- solve(t(X) %*%X)\nsigmahat * XtX\n```\n\nThe terms on the diagonal represent the variance of a particular coefficient in the model.The standard error of a particular coefficient $k$ is: s.e.($\\hat{\\beta_k}) = \\sqrt{\\widehat{\\sigma}^2 (X'X)^{-1}}_{kk}$. The off-diagonal components represent the covariances between the coefficients.\n\nRecall that the standard error is just the square root of the variance. So, to get the nice standard errors we saw in the summary output, we can take the square root of the quantities on the diagonal of this matrix.\n\n```{r}\nsqrt(diag(vcov(fit.1)))\nsummary(fit.1)$coefficients[,2]\n```\n\nWhy should I care?\n\n1.  Well R actually doesn't make it that easy to extract standard errors from the summary output. You can see above that the code for extracting the standard errors using what we know about them being the square root of the variance is about as efficient as extracting the second column of the coefficient component of the summary of the model.\n2.  Sometimes, we may think that the assumption of equal error variance is not feasible and that we have unequal error variance or \"heteroscedasticity.\" Researchers have developed alternative expressions to model unequal error variance. Generally, what this means is they can no longer make that simplifying assumption, have to stop at the step with the uglier expression $(X^T X)^{-1}X^T \\mathbf{V}( \\epsilon) X (X^T X)^{-1}$ and then assume something different about the structure of the errors in order to estimate the coefficients. These alternative variance estimators are generally what are referred to as \"robust standard errors.\" There are many different robust estimators, and you will likely come across them in your research.\n\nSome of you may have learned the formal, general definition for variance as defined in terms of expected value: $\\mathbb{E}[(\\widehat{m} - \\mathbb{E}(\\widehat{m}))^2 ]$. We could also start the derivation there. This is not required for the course, but it is below if you find it useful. In particular, it can help show why we wend up needing to square a term when we move it outside the variance operator:\n\n```{=tex}\n\\begin{align*}\n\\mathbf{V}(\\widehat{\\beta}) &= \\mathbb{E}[(\\widehat{\\beta} - \\mathbb{E}(\\hat \\beta))^2)] \\\\\n&= \\mathbb{E}[(\\widehat{\\beta} - \\beta)^2]\\\\\n&= \\mathbb{E}[(\\widehat{\\beta} - \\beta)(\\widehat{\\beta} - \\beta)^T ] \\\\ &= \n\\mathbb{E}[(X^T X) ^{-1} X^TY - \\beta)(X^T X) ^{-1} X^TY - \\beta)^T]  \\\\\n&= \\mathbb{E}[(X^T X) ^{-1} X^T(X\\beta + \\epsilon) - \\beta)(X^T X) ^{-1} X^T(X\\beta + \\epsilon) - \\beta)^T]\\\\\n&=  \\mathbb{E}[(X^T X) ^{-1} X^TX\\beta + (X^T X) ^{-1} X^T\\epsilon - \\beta)(X^T X) ^{-1} X^TX\\beta + (X^T X) ^{-1} X^T\\epsilon - \\beta)^T]\\\\\n&=  \\mathbb{E}[(\\beta + (X^T X) ^{-1} X^T\\epsilon - \\beta)(\\beta + (X^T X) ^{-1} X^T\\epsilon - \\beta)^T]\\\\\n&=  \\mathbb{E}[(X^T X) ^{-1} X^T\\epsilon)(X^T X) ^{-1} X^T\\epsilon)^T]\\\\\n&= (X^T X) ^{-1} X^T\\mathbb{E}(\\epsilon\\epsilon^T)X(X^T X) ^{-1}\\\\\n&= \\underbrace{(X^T X) ^{-1} X^T\\sigma^2I_nX(X^T X) ^{-1}}_\\text{Assume homoskedasticity}\\\\\n&= \\sigma^2(X^T X) ^{-1} X^TX(X^T X) ^{-1}\\\\\n&= \\sigma^2(X^T X) ^{-1}\n\\end{align*}\n```\nNote: Along the way, in writing $\\mathbb{E}(\\hat \\beta) = \\beta$, we have implicitly assumed that $\\hat \\beta$ is an \"unbiased\" estimator of $\\beta$. This is not free. It depends on an assumption that the error term in the regression $\\epsilon$ is independent of our independent variables. This can be violated in some situations, such as when we have omitted variable bias, which is discussed at the end of our OLS section.\n\n### Hypothesis Testing\n\nMost of the time in social science, we run a regression because we have some hypothesis about how a change in our independent variable affects the change in our outcome variable.\n\nIn OLS, we can perform a hypothesis test for each independent variable in our data. The structure of the hypothesis test is:\n\n-   Null hypothesis: $\\beta_k = 0$\n    -   This essentially means that we don't expect a particular $x_k$ independent variable to have a relationship with our outcome variable.\n-   Alternative hypothesis: $\\beta_k \\neq 0$\n    -   We do expect a positive or negative relationship between a particular $x_k$ and the dependent variable.\n\nWe can use our estimates for $\\hat \\beta$ coefficients and their standard errors to come to a conclusion about rejecting or failing to reject the null hypothesis of no relationship by using a t-test.\n\nIn a t-test, we take our coefficient estimates and divide them by the standard error in order to \"standardize\" them on a scale that we can use to determine how likely it is we would have observed a value for $\\hat \\beta$ as extreme or more extreme as the one we observed in a world where the true $\\beta = 0$. This is just like a t-test you might have encountered before for a difference in means between groups, except this time our estimate is $\\hat \\beta$.\n\n```{=tex}\n\\begin{align*}\nt_{\\hat \\beta_k} &= \\frac{\\hat \\beta_k}{s.e.(\\hat \\beta_k)}\n\\end{align*}\n```\nGenerally speaking, when $t$ is about +/-2 or greater in magnitude, the coefficient will be \"significant\" at conventional levels (i.e., $p <0.05$), meaning that we are saying that it is really unlikely we would have observed a value as big as $\\hat \\beta_k$ if the null hypothesis were true. Therefore, we can reject the null hypothesis.\n\nHowever, to get a specific quantity, we need to calculate the p-value, which depends on the t-statistic and the degrees of freedom in the model. The degrees of freedom in a regression model are $N-k$, the number of observations in the model minus the number of independent variables plus the intercept.\n\nIn R, we can calculate p-values using the `pt()` function. By default, most people use two-sided hypothesis tests for regression. So to do that, we are going to find the area on each side of the t values, or alternatively, multiply the area to the right of our positive t-value by 2.\n\n```{r}\n## Let's say t was 2.05 and \n## And there were 32 observations and 3 variables in the regression plus an intercept\nt <- 2.05\ndf.t <- 32 -4\np.value <- 2 * (pt(abs(t), df=df.t, lower.tail = FALSE))\np.value\n```\n\nLet's do this for the florida example. First, we can find t by dividing our coefficients by the standard errors.\n\n```{r}\nt <- coef(fit.1) / (sqrt(diag(vcov(fit.1))))\nt \n\n## Compare with output\nsummary(fit.1)$coefficients[, 3]\n```\n\nWe can then find the p-values.\n\n```{r}\nt <- coef(fit.1) / (sqrt(diag(vcov(fit.1))))\ndf.t <- fit.1$df.residual\np.value <- 2 * (pt(abs(t), df=df.t, lower.tail = FALSE))\np.value\nsummary(fit.1)$coefficients[, 4]\n```\n\nWe see that the coefficient for `Perot96` is significant. The p-value is tiny. In R, for small numbers, R automatically shifts to scientific notation. The 9.47e-12 means the p-value is essentially zero, with the stars in the summary output indicating the p-value is $p < 0.001$. R will also output a test of the significance of the intercept using the same formula as all other coefficients. This generally does not have much interpretive value, so you are usually safe to ignore it.\n\n*Confidence Intervals*\n\nInstead of representing the significance using p-values, sometimes it is helpful to report confidence intervals around the coefficients. This can be particularly useful when visualizing the coefficients. The 95% confidence interval represents roughly 2 standard errors above and below the coefficient. The key thing to look for is whether it overlaps with zero (not significant) or does not (in which case the coefficient is significant).\n\nThe precise formula is\n\n$\\widehat{\\beta}_k$ Confidence intervals: $\\widehat{\\beta}_k - t_{crit.value} \\times s.e._{\\widehat{\\beta}_k}, \\widehat{\\beta}_k + t_{crit.value} \\times s.e_{\\widehat{\\beta}_k}$\n\nIn R, we can use `qt()` to get the specific critical value associated with a 95% confidence interval. This will be around 2, but fluctuates depending on the degrees of freedom in your model (which are function of your sample size and how many variables you have in the model.) R also has a shortcut `confint()` function to extract the coefficients from the model. Below we do this for the `Perot96` coefficient.\n\n```{r}\n## Critical values from t distribution at .95 level\nqt(.975, df = fit.1$df.residual) # n- k degrees of freedom\n\n## Shortcut\nconfint(fit.1)[2,]\n\n## By hand\ncoef(fit.1)[2] - qt(.975, df = fit.1$df.residual)*sqrt(diag(vcov(fit.1)))[2]\ncoef(fit.1)[2] + qt(.975, df = fit.1$df.residual)*sqrt(diag(vcov(fit.1)))[2]\n```\n\n### Goodness of Fit\n\nA last noteworthy component to the standard regression output is the goodness of fit statistics. For this class, we can put less attention on these, though there will be some analogues when we get into likelihood.\n\nThese are measures of how much of the total variation in our outcome measure can be explained by our model, as well as how far off are our estimates from the truth.\n\nFor the first two measures R-squared and Adjusted R-squared, we draw on three quantities:\n\n-   Total Sum of Squares--how much variance in $Y_i$ is there to explain?\n\n    -   $TSS: \\sum_{i=1}^N (Y_i -\\overline Y_i)^2$\n\n-   Estimated Sum of Squares--how much of this variance do we explain?\n\n    -   $ESS: \\sum_{i=1}^N (\\widehat Y_i -\\overline Y_i)^2$\n\n-   Residual Sum of Squares--how much variance is unexplained?\n\n    -   $RSS: \\sum_{i=1}^N ( Y_i -\\widehat Y_i)^2$\n\n-   $TSS = ESS + RSS$\n\n-   Multiple R-squared: $\\frac{ESS}{TSS}$\n\n    -   This is a value from 0 to 1, representing the proportion of the variance in the outcome that can be explained by the model. Higher values are generally considered better, but there are many factors that can affect R-squared values. In most social science tasks where the goal is to engage in hypothesis testing of coefficients, this measure is of less value.\n\n-   Adjusted R-squared: $1 - \\frac{\\frac{RSS}{n - k}}{\\frac{TSS}{n - 1}}$\n\n    -   This is essentially a penalized version of R-squared. When you add additional predictors to a model, the R-squared value can never decrease, even if the predictors are useless. The Adjusted R-squared adds a consideration for the degrees of freedom into the equation, creating a penalty for adding more and more predictors.\n\n-   Residual standard error aka root mean squared error aka square root of the mean squared residual: $r.s.e = \\sqrt{\\frac{RSS}{n-k}}$\n\n    -   This represents the typical deviation of an estimate of the outcome from the actual outcome. This quantity is often used to assess the quality of prediction exercises. It is used less often in social science tasks where the goal is hypothesis testing of the relationship between one or more independent variables and the outcome.\n\n*F-Statistic*\n\nSo far we have conducted hypothesis tests for each individual coefficient. We can also conduct a global hypothesis test, where the null hypothesis is that all coefficients are zero, with the alternative being that at least one coefficient is nonzero. This is the test represented by the F-statistic in the regression output.\n\nThe F-statistic helps us test the null hypothesis that **all** of the regression slopes are 0: $H_0 = \\beta_1 = \\beta_2 = \\dots = \\beta_k = 0$\n\n-   $F_0 = \\frac{ESS/(k - 1)}{RSS/(n - k)}$\n-   The F-Statistic has two separate degrees of freedom.\n    -   The model sum of squares degrees of freedom (ESS) are $k - 1$.\n    -   The residual error degrees of freedom (RSS) are $n - k$.\n    -   In a regression output, the model degrees of freedom are generally the first presented: \"F-statistic: 3.595 on $(k - 1) = 1$ and $(n - k) = 48$ DF.\"\n\nNote: This test is different from our separate hypothesis tests that a $k$ regression slope is 0. For that, we use the t-tests discussed above.\n\n## Generating predictions from regression models\n\nThe regression coefficients tell us how much $Y$ is expected to change for a one-unit change in $x_k$. It does not immediately tell us the values we estimate our outcome ($\\hat Y$) to take conditional on particular values of $x_k$. While often knowing our independent variables have a significant effect on the outcome and the size of the coefficient is sufficient for testing our hypotheses, it can be helpful for interpretation's sake, to see the estimated values for the outcome. This is going to be particularly important once we get into models like logistic regression, where the coefficients won't be immediately interpretable.\n\nRecall that our equation for estimating values of our outcomes is:\n\n-   $\\hat Y = X\\hat \\beta$ This can also be written out in long form for any particular observation $i$:\n\n-   $\\hat y_i = \\hat \\alpha + \\hat \\beta_1*x_1i + \\hat \\beta_2*x_2i + ... \\hat\\beta_k*x_ki$\n\nThe estimated values of our regression $\\hat Y$ are often called the \"fitted values.\" In R, you can identify the estimated values for each observation using the `fitted()` command.\n\n```{r}\n## Y hat for the first observation in the data\nfitted(fit.1)[1]\n```\n\nAgain, this is just the multiplication of the matrix $X$ and $\\hat \\beta$. If we have already run a regression model in R, one shortcut for getting the $X$ matrix, is to use the `model.matrix` command. We can get $\\hat \\beta$ using the `coef()` command.\n\n```{r}\nX <- model.matrix(fit.1)\nhead(X) # head() shows about the first six values of an object\nbetahat <- coef(fit.1)\n```\n\nOur fitted values are then just\n\n```{r}\nyhat <- X %*% betahat\nhead(yhat)\n```\n\nIf I want to generate an estimate for any particular observation, I could also just extract its specific value for `Perot96`.\n\n```{r}\nflorida$Perot96[1]\n```\n\nLet's estimate the Buchanan 2000 votes for the first county in the data with Perot 96 votes of 8072. We can write it out as $\\hat Buchanan00_1 =\\hat \\alpha + \\hat \\beta*Perot96_1$\n\n```{r}\nbuch00hat <- coef(fit.1)[1] + coef(fit.1)[2]*florida$Perot96[1]\nbuch00hat\n```\n\nWhat is useful about this is that now we have the coefficient estimates, we can apply them to any values of $X$ we wish in order to generate estimates/predictions of the values $Y$ will take given particular values of our independent variables.\n\nOne function that is useful for this (as a shortcut) is the `predict(fit, newdata=newdataframe)` function in R. It allows you to enter in \"newdata\"-- meaning values of the $X$ variables for which you want to generate estimates of $Y$ based on the coefficient estimates of your regression model.\n\nFor example, let's repeat the calculation from above for `Perot96 = 8072`.\n\n```{r}\npredict(fit.1, newdata = data.frame(Perot96 = 8072))\n```\n\nWe can also generate confidence intervals around these estimates by adding `interval = \"confidence\"` in the command.\n\n```{r}\npredict(fit.1, newdata = data.frame(Perot96 = 8072), interval=\"confidence\")\n```\n\nWe can also simultaneously generate multiple predictions by supplying a vector of values in the `predict()` command. For example, let's see the estimated Buchanan votes for when the Perot 1996 votes took values of 1000 to 10,000 by intervals of 1,000.\n\n```{r}\npredict(fit.1, newdata = data.frame(Perot96 = c(1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000, 9000, 10000)))\n```\n\nThe important thing to note about the `predict()` command is that if you have multiple independent variables, you have to specify the values you want each of them to take when generating the estimated values of y.\n\n```{r}\nfit.2 <- lm(Buchanan00 ~ Perot96 + Clinton96, data = florida)\n```\n\nFor example, let's build a second model with `Clinton96` as an additional predictor. In order to generate the same prediction for different values of Perot 1996 votes, we need to tell R at what values we should \"hold constant\" `Clinton96.` I.e., we want to see how hypothetical changes in `Perot96` votes influence changes in Buchanan 2000 votes while also leaving the Clinton votes identical. This is that lightswitch metaphor-- flipping one switch, while keeping the rest untouched.\n\nThere are two common approaches to doing this. 1) We can hold constant `Clinton96` votes at its mean value in the data 2) We can keep `Clinton96` at its observed values in the data. In linear regression, it's not going to matter which approach you take. In other models we will talk about later, this distinction may matter more substantially because of how our quantities of interest change across different values of $X\\hat \\beta$.\n\nThe first approach is easily implemented in predict.\n\n```{r}\npredict(fit.2, newdata = data.frame(Perot96 = 8072, Clinton96 = mean(florida$Clinton96)))\n```\n\nFor the second approach, what we will do is generate an estimate for Buchanan's votes in 2000 when Perot96 takes 8072 votes, and we keep Clinton96's votes at whatever value it currently is in the data. That is, we will generate $n$ estimates for Buchanan's votes when Perot takes 8072. Then, we will take the mean of this as our \"average estimate\" of Buchanan's votes in 2000 based on Perot's votes at a level of 8072. We can do this in one of two ways:\n\n```{r}\n## Manipulating the X matrix\nX <- model.matrix(fit.2)\n## Replace Perot96 column with all 8072 values\nX[, \"Perot96\"] <- 8072\nhead(X) #take a peek\n\n## Generate yhat\nyhats <-X %*% coef(fit.2)\n\n## take the mean\nmean(yhats)\n```\n\n```{r}\n## Use predict\nyhats <- predict(fit.2, newdata = data.frame(Perot96=8072, Clinton96=florida$Clinton96))\nmean(yhats)\n```\n\nNow often, after we generate these predicted values, we want to display them for the whole world to see. You will get a chance to visualize values like this using the plotting functions in the problem sets. We have already seen one example of this in the simple bivariate case, when R plotted the bivariate regression line in section 4.3.2. However, the predict function extends are capabilities to plot very specific values of $X$ and $\\hat Y$ for bivariate or multiple regressions.\n\n-   The `predict()` function is also very relevant when we move to logistic, probit, etc. regressions. This is just the start of a [beautiful friendship](https://www.youtube.com/watch?v=DDybg9CNXcM&ab_channel=mylifeinmovies) between you and `predict()` and associated functions.\n\n## Wrapping up OLS\n\nLinear regression is a great way to explain the relationship between one or more independent variables and an outcome variables. However, there is no free lunch. We have already mentioned a couple of assumptions along the way. Below we will summarize these and other assumptions. These are things you should be mindful of when you use linear regression in your own work. Some conditions that generate violations of these assumptions can also motivate why we will seek out alternative methods, such as those that rely on maximum likelihood estimation.\n\nThis is a good place to review Gelman section 3.6.\n\n-   Exogeneity. This one we haven't discussed yet, but is an important assumption for letting us interpret our coefficients $\\hat \\beta$ as \"unbiased\" estimates of the true parameters $\\beta$. We assume that the random error in the regression model $\\epsilon$ is indeed random, and uncorrelated with and independent of our independent variables $X$. Formally:\n    -   $\\mathbb{E}(\\epsilon| X) = \\mathbb{E}(\\epsilon) = 0$.\n    -   This can be violated, for example, when we suffer from Omitted Variable Bias due to having an \"endogenous explanatory variable\" that is correlated with some unobserved or unaccounted for factor. This bias comes from a situation where there is some variable that we have left out of the model ($Z$), and is therefore a part of the unobserved error term. Moreover this variable which is correlated with--and a pre-cursor of-- our independent variables and is a cause of our dependent variable. A failure to account for omitted variables can create bias in our coefficient estimates. Concerns about omitted variable bias often prompt people to raise their hands in seminars and ask questions like, \"Well have you accounted for this? Have you accounted for that? How do you know it is $X$ driving your results and not $Z$?\" If we omit important covariates, we may wrongly attribute an effect to $X$ when it was really the result of our omitted factor $Z$. Messing discusses this [here](https://medium.com/pew-research-center-decoded/how-to-break-regression-f48230f0ca68).\n    -   This is a really tough assumption. The only real way to guarantee the independence of your error term and the independent variables is if you have randomly assigned values to the independent variables (such as what you do when you randomly assign people to different treatment conditions in an experiment). Beyond random assignment, you have to rely on theory to understand what variables you need to account for in the regression model to be able to plausibly claim your estimate of the relationship between a given independent variable and the dependent variable is unbiased. Failing to control for important factors can lead to misleading results, such as what happens in Simpson's paradox, referenced in the Messing [piece](https://medium.com/pew-research-center-decoded/how-to-break-regression-f48230f0ca68).\n    -   Danger Note 1: The danger here, though, is that the motivation for avoiding omitted variable bias might be to keep adding control after control after control into the regression model. However, model building in this way can sometimes be atheoretical and result in arbitrary fluctuations in the size of your coefficients and their significance. At its worse, it can lead to \"p-hacking\" where researchers keep changing their models until they find the results they like. The Lenz and Sahn article on Canvas talks more about the dangers of arbitrarily adding controls to the model.\n    -   Danger Note 2: We also want to avoid adding \"bad controls\" to the model. Messing talks about this in the medium [article](https://medium.com/pew-research-center-decoded/how-to-break-regression-f48230f0ca68) as it relates to collider bias. We want to avoid adding controls to our model, say $W$ that are actually causes of $Y$ and causes of $X$ instead of the other way around.\n    -   Model building is a delicate enterprise that depends a lot on having a solid theory that guides the choice of variables.\n-   Homoscedasticity. We saw this when defining the variance estimator for the OLS coefficients. We assume constant error variance. This can be violated when we think observations at certain values of our independent variables may have different magnitudes of error than observations at other values of our independent variables.\n    -   No correlation in the errors. The error terms are not correlated with each other. This can be violated in time series models (where we might think past, present, and future errors are correlated) or in cases where our observations are nested in some hierarchical structures (e.g., perhaps students in a school) and the errors are correlated.\n-   No perfect collinearity. The $X$ matrix must be full rank: We cannot have linear dependence between columns in our X matrix. We saw this in the tutorial when we tried to add the dummy variables for all of our racial groups into a regression at once. When there is perfect collinearity between variables, our regression will fail.\n    -   We should also avoid situations where we have severe multicollinearity. This can happen when we include two or more variables in a regression model that are highly correlated (just not perfectly correlated). While the regression will still run in this case, it can inflate the standard errors of the coefficients, making it harder to detect significant effects. This is particularly problematic in smaller samples.\n-   Linearity. The relationship between the independent and dependent variables needs to be linear in the parameters. It should be modeled as the addition of constants or parameters multiplied by the independent variables. If instead the model requires the multiplication of parameters, this is no longer linear (e.g., $\\beta^2$). Linearity also often refers to the shape of the model. Our coefficients tell us how much change we expect in the outcome for each one-unit change in an independent variable. We might think some relationships are nonlinear-- meaning this rate of change varies across values of the independent variables. If that is the case, we need to shift the way our model is specified to account for this or change modeling approaches.\n    -   For example, perhaps as people get older (one-unit changes in age), they become more politically engaged, but at some age level, their political engagement starts to decline. This would mean the slope (that expected change in political engagement for each one-unit change in age) is not constant across all levels of age. There, we might be violating linearity in the curvature of the relationship between the independent and dependent variables. This is sometimes why you might see $age^2$ or other nonlinear terms in regression equations to better model this curvature.\n    -   Likewise, perhaps each additional level of education doesn't result in the same average increase in $y$. If not, you could consider including categorical dummy variables for different levels of education instead of treating education as a numeric variable.\n-   Normality. We assume that the errors are normally distributed. As Gelman 3.6 notes, this is a less important assumption and is generally not required.\n\n*OLS Properties*\n\nWhy we like OLS. When we meet our assumptions, OLS produces the best linear unbiased estimates (BLUE). A discussion of this [here](https://www.youtube.com/watch?v=NjTpHS5xLP8&ab_channel=BenLambert). We have linearity in our parameters (e.g., $\\beta$ and not $\\beta^2$ for example). The unbiasedness means that the expected value (aka the average over repeated samples) of our estimates $\\mathbb{E}(\\hat \\beta)= \\beta$ is the true value. Our estimates are also efficient, which has to do with the variance, not only are our estimates true in expectation, but we also have lower variance than an alternative linear unbiased estimator could get us. If our assumptions fail, then we might no longer have BLUE. OLS estimates are also consistent, meaning that as the sample gets larger and larger, the estimates start converging to the truth.\n\nNow, a final hidden assumption in all of this is that the sample of our data is representative of the population we are trying to make inferences about. If that is not the case, then we may no longer be making unbiased observations to that population level. Further adjustments may be required (e.g., analyses of survey data sometimes use weights to adjust estimates to be more representative).\n\nWhen we violate these assumptions, OLS may no longer be best, and we may opt for other approaches. More soon!\n\n### Practice Problems\n\n1.  Let's use the `florida` data. Run a regression according to the following formula:\n    -   $Buchanan00_i = \\alpha + \\beta_1*Perot96_i + \\beta_2*Dole96 + \\beta_3*Gore00 + \\epsilon$\n2.  Report the coefficient for `Perot96`. What do you conclude about the null hypothesis that there is no relationship between 1996 Perot votes and 2000 Buchanan votes?\n3.  What is the confidence interval for the `Perot96` coefficient estimate?\n4.  When Perot 1996 vote is 5500, what is the expected 2000 Buchanan vote?\n\n### Practice Problem Code for Solutions\n\n```{r}\nfit.practice <- lm(Buchanan00 ~ Perot96 + Dole96 + Gore00, data = florida)\n\ncoef(fit.practice)[\"Perot96\"]\n\nconfint(fit.practice)[\"Perot96\", ]\n\nexpbuch <- model.matrix(fit.practice)\nexpbuch[,\"Perot96\"] <- 5500\nmean(expbuch %*% as.matrix(coef(fit.practice)))\n```\n\n## Week 2 Example\n\nThis example is based on Dancygier, Rafaela; Egami, Naoki; Jamal, Amaney; Rischke, Ramona, 2020, \"Hate Crimes and Gender Imbalances: Fears over Mate Competition and Violence against Refugees\", published in the *American Journal of Political Science*. Replication data is available [here](https://doi.org/10.7910/DVN/QXJDJ5). We will draw on the survey portion of the article and replicate Table 1 in the paper. The pre-print is available [here](https://naokiegami.com/paper/hatecrime.pdf).\n\nThe abstract is: *As the number of refugees rises across the world, anti-refugee violence has become a pressing concern. What explains the incidence and support of such hate crime? We argue that fears among native men that refugees pose a threat in the competition for female partners is a critical but understudied factor driving hate crime. Employing a comprehensive dataset on the incidence of hate crime across Germany, we first demonstrate that hate crime rises where men face disadvantages in local mating markets. Next, we complement this ecological evidence with original survey measures and confirm that individual-level support for hate crime increases when men fear that the inflow of refugees makes it more difficult to find female partners. Mate competition concerns remain a robust predictor even when controlling for antirefugee views, perceived job competition, general frustration, and aggressiveness. We conclude that a more complete understanding of hate crime and immigrant conflict must incorporate marriage markets and mate competition.*\n\nThe authors summarize their hypotheses as, \"the notion that male refugees are engaged in romantic relationships with German women has received considerable media attention from a variety of sources, with coverage ranging from the curious to the outright hostile. We argue that the prospect of refugee-native mate competition can trigger or compound resentment against refugees, including support for hate crime\" [pg. 14](https://naokiegami.com/paper/hatecrime.pdf)\n\n```{r}\nlibrary(foreign)\ndat_use <- read.dta(\"https://github.com/ktmccabe/teachingdata/blob/main/dat_use.dta?raw=true\")\n```\n\nThe data include wave 4 of an online survey fielded in Germany through Respondi from September 2016 to December 2017). Each wave was designed to be nationally representative on age (starting at 18), gender, and state (Bundesland) with a sample of about 3,000 respondents in each wave.\n\nKey variables include\n\n-   `hate_violence_means` representing respondents' agreement or disagreement to the Only Means question: \"When it comes to the refugee problem, violence is sometimes the only means that citizens have to get the attention of German politicians.\" from (1) disagree strongly to (4) agree strongly.\n-   `MateComp_cont`, Mate Competition. \"The inflow of refugees makes it more difficult for native men to find female partners.\" from (1) disagree strongly to (4) agree strongly.\n-   The data include several other variables related to the demographics of the respondents and measures representing potential alternative explanations, such as `JobComp_cont` (agreement with \"the inflow of young male refugees makes it more difficult for young native men to find apprenticeships and jobs\") and `LifeSatis_cont` (0-10 scale, ranging from extremely dissatisfied to extremely satisfied).\n\nLet's pause here to ask a few questions about research design.\n\n-   What is the outcome? What is the independent variable of interest?\n    -   How would we write out the bivariate regression model?\n-   Why OLS? (e.g., why not experiment?)\n-   What types of alternative explanations might exist?\n\n**Ok let's move to replication of the first two regression models in the table:**\n\n![](images/dancyetaltable.png){width=\"70%\"}\n\n<details>\n\n<summary>Try to code these on your own, then click for the solution</summary>\n\n```{r}\nlm1 <- lm(hate_violence_means ~ MateComp_cont, data=dat_use)\n\nlm2 <- lm(hate_violence_means ~ MateComp_cont + JobComp_cont + LifeSatis_cont, data=dat_use)\n```\n\n</details>\n\n**Now, let's compare the summary output of each output.**\n\n<details>\n\n<summary>Try on your own, then click for the solution</summary>\n\n```{r}\nsummary(lm1)\nsummary(lm2)\n```\n\n</details>\n\n**Questions about the output**\n\n-   How should we interpret the coefficients?\n    -   Do they support the researchers' hypotheses?\n-   How would we extract confidence intervals from the coefficients?\n-   How should we interpret the goodness of fit statistics at the bottom of the output?\n\n**Additional Models** We can also run regressions with even more covariates, as the authors do in models 3-6 in the paper.\n\n<details>\n\n<summary>Click to reveal regression code below.</summary>\n\n```{r}\nlm3 <- lm(hate_violence_means ~ MateComp_cont + JobComp_cont + LifeSatis_cont + \n            factor(age_group) +     # age group\n            factor(gender) +     # gender \n            factor(state) +     # state  \n            factor(citizenship) +    # german citizen\n            factor(marital) +    # marital status\n            factor(religion) +    # religious affiliation\n            eduyrs +    # education\n            factor(occupation) +    # main activity\n            factor(income) +   # income\n            factor(household_size) +   # household size\n            factor(self_econ),    # subjective social status\n          data=dat_use)\n\nlm4 <- lm(hate_violence_means ~ MateComp_cont + JobComp_cont + LifeSatis_cont + \n            factor(age_group) +   # age group\n            factor(gender) +   # gender \n            factor(state) +   # state  \n            factor(citizenship) +  # german citizen\n            factor(marital) +  # marital status\n            factor(religion) +  # religious affiliation\n            eduyrs +  # education\n            factor(occupation) +  # main activity\n            factor(income) + # income\n            factor(household_size) + # household size\n            factor(self_econ) + # subjective social status\n            factor(ref_integrating) + # Refugee Index (National-level; Q73) 8 in total\n            factor(ref_citizenship) + factor(ref_reduce) + factor(ref_moredone) + factor(ref_cultgiveup) + \n            factor(ref_economy) + factor(ref_crime) + factor(ref_terror),\n          data=dat_use)\n\nlm5 <- lm(hate_violence_means ~ MateComp_cont + JobComp_cont + LifeSatis_cont + \n            factor(age_group) +      # age group\n            factor(gender) +      # gender \n            factor(state) +      # state  \n            factor(citizenship) +     # german citizen\n            factor(marital) +     # marital status\n            factor(religion) +     # religious affiliation\n            eduyrs + # education\n            factor(occupation) +     # main activity\n            factor(income) +    # income\n            factor(household_size) +    # household size\n            factor(self_econ) +    # subjective social status\n            factor(ref_integrating) + # Refugee Index (National-level; Q73) 8 in total\n            factor(ref_citizenship) + factor(ref_reduce) + factor(ref_moredone) + factor(ref_cultgiveup) + \n            factor(ref_economy) + factor(ref_crime) + factor(ref_terror) + \n            factor(ref_loc_services) +    # Refugee Index (Local, Q75)\n            factor(ref_loc_economy) + factor(ref_loc_crime) + factor(ref_loc_culture) + factor(ref_loc_islam) + \n            factor(ref_loc_schools) + factor(ref_loc_housing) + factor(ref_loc_wayoflife), ## end\n          data=dat_use)\n\nformula.5 <- \n  as.character(\"hate_violence_means ~ MateComp_cont + JobComp_cont + \n               LifeSatis_cont +  factor(age_group) + factor(gender) + \n               factor(state) + factor(citizenship) + factor(marital) + \n               factor(religion) + eduyrs + factor(occupation) + \n               factor(income) + factor(household_size) + factor(self_econ) + \n               factor(ref_integrating) + factor(ref_citizenship) + factor(ref_reduce) + \n               factor(ref_moredone) + factor(ref_cultgiveup) + \n               factor(ref_economy) + factor(ref_crime) + factor(ref_terror)  + \n               factor(ref_loc_services) +  factor(ref_loc_economy) + factor(ref_loc_crime) + \n               factor(ref_loc_culture) + factor(ref_loc_islam) + \n               factor(ref_loc_schools) + factor(ref_loc_housing) + factor(ref_loc_wayoflife)\")\n\nformula.6 <- paste(formula.5, \"factor(distance_ref) + factor(settle_ref)\", \n                   \"lrscale + afd + muslim_ind + afd_ind + contact_ind\", \n                   sep=\"+\", collapse=\"+\") \n\nlm6 <- lm(as.formula(formula.6), data=dat_use)\n```\n\n```{r, message=F, results=\"asis\", echo=F}\nlibrary(texreg)\nlm.list.table1 <- list(lm1, lm2, lm3, lm4, lm5, lm6)\n\nhtmlreg(lm.list.table1,\n        custom.model.names = c(\"\", \"\", \"\", \"\", \"\", \"\"),\n        stars = c(0.01, 0.05, 0.10),\n        digit=4,\n        caption = \"Table 1: Mate Competition Predicts Support for Hate Crime.\")\n```\n\n</details>\n\n**Final Questions**\n\nEven with all these covariates accounted for, the authors still engage in a discussion about possible violations of the OLS assumptions that could bias their results, as well as potential alternative modelling strategies.\n\n-   Is their survey representative? They replicate using another polling firm.\n-   Are there even more alternative explanations?\n-   Is OLS the right choice?\n-   Validity (discussed in Gelman and Hill). Does the outcome accurately measure the concept? They consider alternative outcomes and visualize the coefficient results in Figure 4.\n    -   Message: Attacks against refugee homes are sometimes necessary to make it clear to politicians that we have a refugee problem.\n    -   Justified : Hostility against refugees is sometimes justified, even when it ends up in violence.\n    -   Prevent : Xenophobic acts of violence are defensible if they result in fewer refugees settling in town.\n    -   Condemn: Politicians should condemn attacks against refugees more forcefully.\n\n![](images/dancyetal.png){width=\"70%\"}\n\n**Additional Practice Questions.**\n\n1.  Find the average expected level of \"Only Means\" agreement at each level of mate competition. Plot the results. Base these results on `lm2`.\n2.  Fit `lm2` using the generalized linear model `glm` approach (with a normal distribution) instead of the `lm`\n3.  What are some of the conceptual differences between ordinary least squares and maximum likelihood estimation?\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"toc-depth":4,"output-file":"04-ReviewofOLS.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.353","editor":"visual","theme":"yeti","toc-title":"Section Contents","toc-location":"left","linkcolor":"red","hypothesis":true,"title":"Review of OLS"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}